<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 20 Linear Regression | R for Audit Analytics</title>
<meta name="author" content="Anil Goyal">
<meta name="description" content="Regression models are a class of statistical models that let us explore the relationship between a response variable and one or more explanatory variables. If such a relationship exists and is...">
<meta name="generator" content="bookdown 0.40 with bs4_book()">
<meta property="og:title" content="Chapter 20 Linear Regression | R for Audit Analytics">
<meta property="og:type" content="book">
<meta property="og:description" content="Regression models are a class of statistical models that let us explore the relationship between a response variable and one or more explanatory variables. If such a relationship exists and is...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 20 Linear Regression | R for Audit Analytics">
<meta name="twitter:description" content="Regression models are a class of statistical models that let us explore the relationship between a response variable and one or more explanatory variables. If such a relationship exists and is...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.7.0/transition.js"></script><script src="libs/bs3compat-0.7.0/tabs.js"></script><script src="libs/bs3compat-0.7.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
<link rel="stylesheet" href="style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">R for Audit Analytics</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome to R for Audit Analytics</a></li>
<li><a class="" href="preface.html">Preface</a></li>
<li><a class="" href="about-author.html">About author</a></li>
<li><a class="" href="gearing-up.html">Gearing up</a></li>
<li><a class="" href="part-i-basic-r-programming-concepts.html">Part-I: Basic R Programming Concepts</a></li>
<li><a class="" href="r-programming-language.html"><span class="header-section-number">1</span> R Programming Language</a></li>
<li><a class="" href="subset.html"><span class="header-section-number">2</span> Subsetting R objects or accesing specific elements</a></li>
<li><a class="" href="func.html"><span class="header-section-number">3</span> Functions and operations in R</a></li>
<li><a class="" href="existing-and-useful-functions-in-base-r.html"><span class="header-section-number">4</span> Existing and useful functions in base R</a></li>
<li><a class="" href="pipes-in-r.html"><span class="header-section-number">5</span> Pipes in R</a></li>
<li><a class="" href="control-statements.html"><span class="header-section-number">6</span> Control statements</a></li>
<li><a class="" href="functional-programming.html"><span class="header-section-number">7</span> Functional Programming</a></li>
<li><a class="" href="file.html"><span class="header-section-number">8</span> File handling operations in R</a></li>
<li><a class="" href="read.html"><span class="header-section-number">9</span> Getting data in and out of R</a></li>
<li><a class="" href="data-cleaning-in-r.html"><span class="header-section-number">10</span> Data Cleaning in R</a></li>
<li><a class="" href="merging-large-number-of-similar-datasets-into-one.html"><span class="header-section-number">11</span> Merging large number of similar datasets into one</a></li>
<li><a class="" href="part-ii-exploratory-data-analysis.html">Part-II: Exploratory Data Analysis</a></li>
<li><a class="" href="visualisations-in-base-r.html"><span class="header-section-number">12</span> Visualisations in Base R</a></li>
<li><a class="" href="visualising-data-with-ggplot2.html"><span class="header-section-number">13</span> Visualising data with ggplot2</a></li>
<li><a class="" href="data-transformation-in-dplyr.html"><span class="header-section-number">14</span> Data Transformation in dplyr</a></li>
<li><a class="" href="combining-tablestabular-data.html"><span class="header-section-number">15</span> Combining Tables/tabular data</a></li>
<li><a class="" href="data-wrangling-in-tidyr.html"><span class="header-section-number">16</span> Data Wrangling in tidyr</a></li>
<li><a class="" href="generating-descriptive-statistics.html"><span class="header-section-number">17</span> Generating Descriptive statistics</a></li>
<li><a class="" href="part-iii-probability-and-sampling-in-r.html">Part-III: Probability and Sampling in R</a></li>
<li><a class="" href="probability-in-r.html"><span class="header-section-number">18</span> Probability in R</a></li>
<li><a class="" href="random-sampling-in-r.html"><span class="header-section-number">19</span> Random sampling in R</a></li>
<li><a class="" href="part-iv-machine-learning-in-r.html">Part-IV: Machine Learning in R</a></li>
<li><a class="active" href="linear-regression.html"><span class="header-section-number">20</span> Linear Regression</a></li>
<li><a class="" href="principal-component-analysis-in-r.html"><span class="header-section-number">21</span> Principal Component Analysis in R</a></li>
<li><a class="" href="clustering-in-r-using-kmeans-algorithm.html"><span class="header-section-number">22</span> Clustering in R (Using Kmeans algorithm)</a></li>
<li><a class="" href="association-rule-mining-in-r-apriori.html"><span class="header-section-number">23</span> Association Rule Mining in R (Apriori)</a></li>
<li><a class="" href="part-v-time-series-analysis.html">Part-V: Time Series Analysis</a></li>
<li><a class="" href="manipulating-datedate-time-objects-using-lubridate.html"><span class="header-section-number">24</span> Manipulating Date/Date-time objects using lubridate</a></li>
<li><a class="" href="time-series-analysis.html"><span class="header-section-number">25</span> Time Series Analysis</a></li>
<li><a class="" href="part-vi-network-analytics.html">Part VI: Network Analytics</a></li>
<li><a class="" href="network-analyticsgraph-theory-in-r.html"><span class="header-section-number">26</span> Network Analytics/Graph theory in R</a></li>
<li><a class="" href="applying-network-analysis-in-auditfraud-detection.html"><span class="header-section-number">27</span> Applying network analysis in audit/fraud detection</a></li>
<li><a class="" href="part-vii-text-analytics-in-r.html">Part-VII: Text Analytics in R</a></li>
<li><a class="" href="string-manipulation-in-stringr.html"><span class="header-section-number">28</span> String manipulation in stringr</a></li>
<li><a class="" href="regex---a-quick-introduction.html"><span class="header-section-number">29</span> Regex - A quick introduction</a></li>
<li><a class="" href="regex-in-human-readble-format-using-rebus.html"><span class="header-section-number">30</span> Regex in human readble format using rebus</a></li>
<li><a class="" href="text-analytics-in-r.html"><span class="header-section-number">31</span> Text Analytics in R</a></li>
<li><a class="" href="sentiment-analysis.html"><span class="header-section-number">32</span> Sentiment Analysis</a></li>
<li><a class="" href="visualising-text-analytics-through-wordcloud-etc..html"><span class="header-section-number">33</span> Visualising Text analytics through Wordcloud, etc.</a></li>
<li><a class="" href="finding-string-similarity.html"><span class="header-section-number">34</span> Finding string similarity</a></li>
<li><a class="" href="part-viii-geo-computation-in-r.html">Part-VIII: Geo computation in R</a></li>
<li><a class="" href="maps-in-r.html"><span class="header-section-number">35</span> Maps in R</a></li>
<li><a class="" href="geo-coding-in-r.html"><span class="header-section-number">36</span> Geo-Coding in R</a></li>
<li><a class="" href="reverse-geo-coding.html"><span class="header-section-number">37</span> Reverse geo-coding</a></li>
<li><a class="" href="part-ix-identifying-anamolous-observations-for-audit.html">Part-IX: Identifying anamolous observations for audit</a></li>
<li><a class="" href="benford-testsanalysis.html"><span class="header-section-number">38</span> Benford Tests/Analysis</a></li>
<li><a class="" href="anomaly-detection.html"><span class="header-section-number">39</span> Anomaly Detection</a></li>
<li><a class="" href="finding-anamolous-outliers.html"><span class="header-section-number">40</span> Finding Anamolous Outliers</a></li>
<li><a class="" href="dup.html"><span class="header-section-number">41</span> Duplicate Detection</a></li>
<li><a class="" href="detecting-gaps-in-sequences.html"><span class="header-section-number">42</span> Detecting gaps in sequences</a></li>
<li><a class="" href="data-envelopment-analysis.html"><span class="header-section-number">43</span> Data Envelopment Analysis</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="colors-in-r.html"><span class="header-section-number">A</span> Colors in R</a></li>
<li><a class="" href="various-datasets-in-base-r-used-in-this-book.html"><span class="header-section-number">B</span> Various Datasets, in base R, used in this book</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="linear-regression" class="section level1" number="20">
<h1>
<span class="header-section-number">20</span> Linear Regression<a class="anchor" aria-label="anchor" href="#linear-regression"><i class="fas fa-link"></i></a>
</h1>
<p>Regression models are a class of statistical models that let us explore the relationship between a response variable and one or more explanatory variables. If such a relationship exists and is detected, we can make predictions about the value of the response variable given some explanatory variables. As an example, if a relationship between number of employees in an office and its monthly expenses on salary is established, we can predict the monthly expenses incurred by that office on salary if we know the number of employees. That lets us do thought of experiments like asking how much the a new company had to pay say 10 employees on salary expenses monthly.</p>
<p>Explanatory variables are sometimes also referred to as <em>regressors</em> or <em>independent</em> or <em>predictor</em> variables whereas response variable is also called as <em>dependent</em> or <em>outcome</em> variable. When the response variable is numeric and the relationship is linear, the regression is called as linear regression. Of course, there are certain other assumptions, which we will discuss later on in the chapter.</p>
<div id="basic-concepts-1" class="section level2" number="20.1">
<h2>
<span class="header-section-number">20.1</span> Basic concepts<a class="anchor" aria-label="anchor" href="#basic-concepts-1"><i class="fas fa-link"></i></a>
</h2>
<p>Before we start creating regression models, it’s always a good practice to visualize the data. It will help us ascertaining the nature of relationship between the predictors and response variable, if any. To visualize the relationship between two numeric variables, we can use a scatter plot. See the two examples in figure <a href="linear-regression.html#fig:concept">20.1</a>. In the first example (plot) we can see a near perfect linear relationship between GNP and Population of the countries, whereas a moderate but negative relationship between two variables is seen in second example.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:concept"></span>
<img src="DauR_files/figure-html/concept-1.png" alt="Linear Regression - Intuition" width="47%" height="45%"><img src="DauR_files/figure-html/concept-2.png" alt="Linear Regression - Intuition" width="47%" height="45%"><p class="caption">
Figure 20.1: Linear Regression - Intuition
</p>
</div>
<p>Linear regression is a perfect model to predict outcome or response variable when it has linear relationship with explanatory variables. In that case our predicted values will lie on the assumed line (in linear relationship) ideally. Maths behind estimating or predicting outcomes is thus, finding the following algebraic equation <a href="linear-regression.html#eq:lr1">(20.1)</a>.</p>
<p><span class="math display" id="eq:lr1">\[\begin{equation}
y = mx + c
\tag{20.1}
\end{equation}\]</span></p>
<p>Where -</p>
<ul>
<li>
<code>m</code> is the slope of the line (in linear relationship)</li>
<li>
<code>c</code> is the intercept of Y-axis. (value of <span class="math inline">\(y\)</span> when <span class="math inline">\(x\)</span> is <code>0</code> )</li>
</ul>
<p>Interpreting this equation in real world is like estimating the coefficients i.e. both <span class="math inline">\(m\)</span> and <span class="math inline">\(c\)</span> in above equation. The goal of our exercise will thus be to estimate the best values of <span class="math inline">\(m\)</span> and <span class="math inline">\(c\)</span>. These two parameters, <span class="math inline">\(m\)</span> and <span class="math inline">\(c\)</span> are sometimes also referred to as <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_0\)</span> respectively. Those familiar with mathematics behind fitting the equation of a line (in two dimensional space), may know that we require only two variables (data points i.e. <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> value pairs) values to find these parameters. So it means, that if we have a fair amount of data points available (which will in rarest of circumstances be collinear i.e. lying on one same line), we can actually get many such regression lines. Our goal is thus, to find best of these lines. But, how?</p>
<p>To answer this question, let us also understand that the values of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> pairs in actual practice, don’t lie on the regression line because these points will rarely be collinear (See plots in Figure <a href="linear-regression.html#fig:concept">20.1</a> and note that none of the data point actually lie on the line). So for each data point of response variable, there is an actual value and one fitted value (the one falling on the regression line). The difference between these values is called <em>error term</em> or <em>residual</em>. Technically these are not errors but random noise that the model is not able to explain. Mathematically, if <span class="math inline">\(\hat{y}\)</span> is fitted value, <span class="math inline">\(y\)</span> is actual value, the difference also called error (of prediction) term say <span class="math inline">\(\epsilon\)</span> can be denoted as -</p>
<p><span class="math display" id="eq:lr2">\[\begin{equation}
\epsilon = y - \hat{y}
\tag{20.2}
\end{equation}\]</span></p>
<p>OR, we can say that</p>
<p><span class="math display" id="eq:lr3">\[\begin{equation}
y = \beta_0 + \beta_1x + \epsilon
\tag{20.3}
\end{equation}\]</span></p>
<p>Now, one method to find best fit regression line is to minimize the error terms. Theoretically, it means to capture pattern/relationship between data points as much as possible so that what’s left behind is true random noise. One way could be to minimise the mean of these error terms. But these error terms can be both positive or negative. See figure <a href="linear-regression.html#fig:errors">20.2</a>. So to ensure that these are not cancelled out while taking mean, we can minimise either the mean of their absolute values or their squares. Most commonly accepted method is to take mean of squares and minimise it. One of the benefit of adopting it over another, is that while squaring errors or residuals, large residuals get higher weight than lower residuals. That’s why this linear regression technique is also sometimes referred to as <strong>Ordinary Least Squares</strong> or <strong>OLS</strong> regression.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:errors"></span>
<img src="DauR_files/figure-html/errors-1.png" alt="Residuals - Intuition and Concept" width="672" height="35%"><p class="caption">
Figure 20.2: Residuals - Intuition and Concept
</p>
</div>
</div>
<div id="simple-linear-regression-in-r" class="section level2" number="20.2">
<h2>
<span class="header-section-number">20.2</span> Simple Linear Regression in R<a class="anchor" aria-label="anchor" href="#simple-linear-regression-in-r"><i class="fas fa-link"></i></a>
</h2>
<p>Don’t worry, in R we do not have to do this minimisation job ourselves. In fact, base R has a fantastic function <code>lm</code> which can fit a best regression line, for a given set of variables, for us. The syntax is simple.</p>
<pre><code>lm(formula, data, ...)</code></pre>
<p>where -</p>
<ul>
<li>
<code>formula</code> an object of class “formula” (or one that can be coerced to that class): a symbolic description of the model to be fitted. For our example above, we can write <code>y ~ x</code>
</li>
<li>
<code>data</code> an optional data frame.</li>
</ul>
<p>It actually returns a special object, which can be printed directly like other R objects. However, it is best printed with the <code>summary</code> function. Firstly we will see an example of simple linear regression which is a linear regression with only one independent variable.</p>
<p>Example-1: <strong>Problem Statement:</strong> <code>iris</code> which is a famous (Fisher’s or Anderson’s) data set, loaded by default in R, gives the measurements in centimeters of the variables sepal length (<code>Sepal.Length</code>) and width (<code>Sepal.Width</code>) and petal length (<code>Petal.Length</code>) and width (<code>Petal.Width</code>), respectively, for 50 flowers from each of 3 species (<code>Species</code>) of iris. The species are Iris <code>setosa</code>, <code>versicolor</code>, and <code>virginica</code>. Let us try to establish a relationship between <code>Sepal.Length</code> and <code>Sepal.Width</code> variables of <code>setosa</code> Species i.e. iris` data-set, (first 50 records only).</p>
<p>As already stated above, it is always a good practice to visualize the data, if possible. So let’s make a scatter-plot, as seen in Figure <a href="linear-regression.html#fig:ex1plot">20.3</a>.</p>
<div class="sourceCode" id="cb1077"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">iris</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="co"># Extract First 50 records</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="fl">50</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">Sepal.Length</span>, <span class="va">Sepal.Width</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="co"># Scatter plot</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="co"># adding a trend line without error bands</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_smooth.html">geom_smooth</a></span><span class="op">(</span>method <span class="op">=</span> <span class="st">'lm'</span>, se<span class="op">=</span><span class="cn">FALSE</span>, formula <span class="op">=</span> <span class="st">"y~x"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="co"># Theme Black and White</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_bw</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:ex1plot"></span>
<img src="DauR_files/figure-html/ex1plot-1.png" alt="Relationship between sepal widths and lengths in setosa species" width="672" height="30%"><p class="caption">
Figure 20.3: Relationship between sepal widths and lengths in setosa species
</p>
</div>
<p>The relationship seem fairly linear (Figure <a href="linear-regression.html#fig:ex1plot">20.3</a>), so let’s build the model.</p>
<div class="sourceCode" id="cb1078"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">lin_reg1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span>formula <span class="op">=</span> <span class="va">Sepal.Width</span> <span class="op">~</span> <span class="va">Sepal.Length</span>, data <span class="op">=</span> <span class="va">iris</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">50</span>,<span class="op">]</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Let us print the object directly</span></span>
<span><span class="va">lin_reg1</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Sepal.Width ~ Sepal.Length, data = iris[1:50, ])
## 
## Coefficients:
##  (Intercept)  Sepal.Length  
##      -0.5694        0.7985</code></pre>
<div class="sourceCode" id="cb1080"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Try printing it with summary()</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">lin_reg1</span><span class="op">)</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Sepal.Width ~ Sepal.Length, data = iris[1:50, ])
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.72394 -0.18273 -0.00306  0.15738  0.51709 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   -0.5694     0.5217  -1.091    0.281    
## Sepal.Length   0.7985     0.1040   7.681 6.71e-10 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.2565 on 48 degrees of freedom
## Multiple R-squared:  0.5514, Adjusted R-squared:  0.542 
## F-statistic: 58.99 on 1 and 48 DF,  p-value: 6.71e-10</code></pre>
<p>Observing the outputs above, we can notice that simply printing the object returns coefficients whereas printing with <code>summary</code> gives us a lot of other information. But how to interpret this information? Before proceeding to interpret the output, let us understand a few more concepts which are essential here. These concepts are basically some assumptions, which we have made while finding the best fit line or in other words estimating the parameters statistically.</p>
</div>
<div id="assumptions-of-linear-regression" class="section level2" number="20.3">
<h2>
<span class="header-section-number">20.3</span> Assumptions of Linear Regression<a class="anchor" aria-label="anchor" href="#assumptions-of-linear-regression"><i class="fas fa-link"></i></a>
</h2>
<p>Linear regression makes several assumptions about the data, such as :</p>
<ul>
<li>
<em>Linearity of the data</em>. The relationship between the predictor (<span class="math inline">\(x\)</span>) and the outcome (<span class="math inline">\(y\)</span>) is assumed to be linear. Obviously, the relationship should be linear. If we would try to fit non-linear relationship through linear regression our results wouldn’t be correct. Also when we use multiple predictors, as we will see shortly, we make another assumption that the model is additive in nature besides being linear. Refer figure <a href="linear-regression.html#fig:linearity">20.4</a>. It is clear that if we try to establish a linear relationship (red line) when it is actually cubic (green dashed line) our model will give erroneous results.</li>
</ul>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:linearity"></span>
<img src="DauR_files/figure-html/linearity-1.png" alt="Is the relationship linear?" width="50%"><p class="caption">
Figure 20.4: Is the relationship linear?
</p>
</div>
<ul>
<li><p><em>Normality of residuals</em>. The residuals are assumed to be normally distributed. Actually, this assumption is followed by the assumption that our dependent variable is normally distributed and not concentrated anywhere. Thus, if dependent variable is normally distributed, and if we have been able to capture the relationship available, then what has been left must be true noise and it should be normally distributed with a mean of <span class="math inline">\(0\)</span>.</p></li>
<li><p><em>Homogeneity of residuals variance</em>. The residuals are assumed to have a constant variance, statistically known as homoscedasticity. It shows that residuals that are left out of regression model are true noise and not related to fitted values, which in that case would have meant that the model was insufficient to capture the actual relationship. Heteroscedasticity (the violation of homoskedasticity) is present when the size of the error term differs across values of an independent variable. This can be best understood by plots in Figures <a href="linear-regression.html#fig:homod">20.5</a> where residuals in left plot indicate equal variance and thus homoskedasticity whereas in the right plot heteroskedasticity is indicated clearly.</p></li>
</ul>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:homod"></span>
<img src="DauR_files/figure-html/homod-1.png" alt="Homoskedasticity (left) Vs. Heteroskedasticity (right)
Sample data created by author for demonstration only" width="47%"><img src="DauR_files/figure-html/homod-2.png" alt="Homoskedasticity (left) Vs. Heteroskedasticity (right)
Sample data created by author for demonstration only" width="47%"><p class="caption">
Figure 20.5: Homoskedasticity (left) Vs. Heteroskedasticity (right)
Sample data created by author for demonstration only
</p>
</div>
<ul>
<li>
<em>Independence</em> of residuals error terms. This assumption is also followed by original assumption that our dependent variable is independent in itself and any <span class="math inline">\(y\)</span> value is not dependent upon another set of <span class="math inline">\(y\)</span> values. In our example, we can understand it like that Sepal width of one sample is not affecting width of another.</li>
</ul>
</div>
<div id="interpreting-the-output-of-lm" class="section level2" number="20.4">
<h2>
<span class="header-section-number">20.4</span> Interpreting the output of <code>lm</code><a class="anchor" aria-label="anchor" href="#interpreting-the-output-of-lm"><i class="fas fa-link"></i></a>
</h2>
<p>Let us discuss each of the component or section of <code>lm</code> output, hereinafter referred to as model.</p>
<div id="call" class="section level3" number="20.4.1">
<h3>
<span class="header-section-number">20.4.1</span> <code>call</code><a class="anchor" aria-label="anchor" href="#call"><i class="fas fa-link"></i></a>
</h3>
<p>The <code>call</code> section shows us the formula that we have used to fit the regression model. So <code>Sepal.Width</code> is our dependent or response variable, <code>Sepal.Length</code> is predictor or independent variable. These variables refer to our data-set which is, first 50 rows of <code>iris</code> or <code>iris[1:50,]</code>.</p>
</div>
<div id="residuals" class="section level3" number="20.4.2">
<h3>
<span class="header-section-number">20.4.2</span> <code>Residuals</code><a class="anchor" aria-label="anchor" href="#residuals"><i class="fas fa-link"></i></a>
</h3>
<p>This depicts the quantile or five point summary of error terms or the residuals, which also as discussed, are the difference between the actual values and the predicted values. We can generate these same values by taking the actual values of <span class="math inline">\(y\)</span> variable and subtracting these from its predicted values of the model.</p>
<div class="sourceCode" id="cb1082"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">iris</span><span class="op">$</span><span class="va">Sepal.Width</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">50</span><span class="op">]</span> <span class="op">-</span> <span class="va">lin_reg1</span><span class="op">$</span><span class="va">fitted.values</span><span class="op">)</span></span></code></pre></div>
<pre><code>##      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. 
## -0.723945 -0.182730 -0.003062  0.000000  0.157380  0.517085</code></pre>
<p>Ideally, the median of error values/residuals should be centered around <code>0</code> thus telling us that these are somewhat symmetrical and our model is predicting fairly at both positive/higher and negative/lower side. Any skewness will thus show that errors are not normally distributed and our model may be biased towards that side.</p>
In our example, we can observe a slight left-skewed distribution of error terms which indicates that our model is not doing that well for higher sepal lengths as it is doing for lower ones. This can also be seen in Figure <a href="linear-regression.html#fig:ex1hist">20.6</a> i.e. histogram of residuals.
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:ex1hist"></span>
<img src="DauR_files/figure-html/ex1hist-1.png" alt="Histogram of Resuduals" width="672"><p class="caption">
Figure 20.6: Histogram of Resuduals
</p>
</div>
</div>
<div id="coefficients" class="section level3" number="20.4.3">
<h3>
<span class="header-section-number">20.4.3</span> <code>Coefficients</code><a class="anchor" aria-label="anchor" href="#coefficients"><i class="fas fa-link"></i></a>
</h3>
<p>Remember that these were our goals of the exercise. Here coefficients will be written as coefficient for that predictor and an intercept term, i.e. our <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_0\)</span> respectively. We can easily interpret that positive coefficients means positive relation and negative coefficient means value of outcome variable will decrease as corresponding independent variable increase. So, in our example, we can deduce that -</p>
<ul>
<li>for <code>0</code> sepal length, the sepal width will be <code>-0.5694</code> (Though mathematically only as physically having <code>0</code> and negative width and lengths are not possible).</li>
<li>for every unit i.e. <code>1</code> i.e. unit increase in sepal length, width increases by <code>0.7985</code>
</li>
</ul>
<p>Thus our regression line equation is -</p>
<p><span class="math display" id="eq:lr4">\[\begin{equation}
Sepal.Width = -0.5694 + 0.7985 * Sepal.Length
\tag{20.4}
\end{equation}\]</span></p>
<p><strong>Now one thing to note here that, since we are adopting OLS approach to find out the (estimated) equation of best line, the coefficients we have arrived at, are only the estimated values of mean of these coefficients. Actually, we started (behind the scenes) with a null hypothesis that there is no linear relationship or, in other words, that the coefficients are zero. Alternate hypothesis, in this case, as you may have guessed by now, was that these coefficients are not zero. The coefficients may follow a statistical/ probabilistic distribution and thus, we may infer only its estimated (mean) value.</strong></p>
<p><strong>We can see the confidence intervals of each of the coefficient using function <code>confint</code>. By default, the 95% confidence intervals may be generated. See the following.</strong></p>
<div class="sourceCode" id="cb1084"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/confint.html">confint</a></span><span class="op">(</span><span class="va">lin_reg1</span><span class="op">)</span></span></code></pre></div>
<pre><code>##                   2.5 %    97.5 %
## (Intercept)  -1.6184048 0.4795395
## Sepal.Length  0.5894925 1.0075641</code></pre>
<p>Now, these estimated distribution must also have some standard error, a probability statistic and a p-value.</p>
<ul>
<li>The <em>standard error</em> of the coefficient is an estimate of the standard deviation of the coefficient. It tells us how much uncertainty there is with our coefficient. We can build confidence intervals of coefficients using this statistic, as shown above.</li>
<li>The t-statistic is simply the estimated coefficient divided by the standard error. By now you may have understood that we are applying student’s t-distribution while estimating the parameters.</li>
<li>Finally <code>p value</code> i.e. <strong>Pr(&gt;|t|)</strong> gives us the probability value and tells us how significant is our coefficient value.</li>
</ul>
</div>
<div id="signif.-codes" class="section level3" number="20.4.4">
<h3>
<span class="header-section-number">20.4.4</span> <code>Signif. codes</code><a class="anchor" aria-label="anchor" href="#signif.-codes"><i class="fas fa-link"></i></a>
</h3>
<p>These are nothing but code legends which are simply telling us how significant out p-value may be for each case. Notice three asterisks in from of coefficient estimate of <code>Sepal.Length</code> which indicate that coefficient is extremely significant and we can reject null hypothesis that <span class="math inline">\(\beta_1\)</span> is <span class="math inline">\(0\)</span>.</p>
<p>These codes give us a quick way to visually see which coefficients are significant to the model.</p>
</div>
<div id="residual-standard-error" class="section level3" number="20.4.5">
<h3>
<span class="header-section-number">20.4.5</span> <code>Residual standard error</code><a class="anchor" aria-label="anchor" href="#residual-standard-error"><i class="fas fa-link"></i></a>
</h3>
<p>The residual standard error is a measure, and one of the metrics, telling us how well the model fits the data. This is actually the standard deviation of all error terms with the difference that instead of taking <code>n</code> terms we are taking <code>degrees of freedom</code>.</p>
<p><span class="math display" id="eq:lr5">\[\begin{equation}
RSE = \sqrt{\frac{1}{(n-2)} \sum_{i=1}^n (y_i - \hat{y_i})^2}
\tag{20.5}
\end{equation}\]</span></p>
<p>Obviously <span class="math inline">\(df\)</span> is <span class="math inline">\(n-2\)</span>, as there is one regressor and one intercept. We can verify equation <a href="linear-regression.html#eq:lr5">(20.5)</a> by calculation.</p>
<div class="sourceCode" id="cb1086"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="op">(</span><span class="va">iris</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">50</span>, <span class="st">"Sepal.Width"</span><span class="op">]</span> <span class="op">-</span> <span class="va">lin_reg1</span><span class="op">$</span><span class="va">fitted.values</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">/</span><span class="fl">48</span><span class="op">)</span></span></code></pre></div>
<pre><code>## [1] 0.2565263</code></pre>
</div>
<div id="r-squared-both-multiple-and-adjusted" class="section level3" number="20.4.6">
<h3>
<span class="header-section-number">20.4.6</span> <code>R-Squared</code> both Multiple and Adjusted<a class="anchor" aria-label="anchor" href="#r-squared-both-multiple-and-adjusted"><i class="fas fa-link"></i></a>
</h3>
<p><em>Multiple R-Squared</em> is also called the coefficient of determination. Often this is the most cited measurement of how well the model is fitting to the data. It tells us what percentage of the variation within our dependent variable that the independent variable is explaining through the model. By looking at output we can say that about 55% of variation is explained through the model. We will discuss it in detail, in the next section.</p>
<p><em>Adjusted R squared</em> on the other hand, shows us what percentage of the variation within our dependent variable that all predictors are explaining. Thus, it is helpful when there are multiple regressors i.e. in Multiple Linear Regression. The difference between these two metrics might be subtle where we adjust for the variance attributed by adding multiple variables.</p>
</div>
<div id="f-statistic-and-p-value" class="section level3" number="20.4.7">
<h3>
<span class="header-section-number">20.4.7</span> <code>F-Statistic</code> and <code>p-value</code><a class="anchor" aria-label="anchor" href="#f-statistic-and-p-value"><i class="fas fa-link"></i></a>
</h3>
<p>So why a p-value again? Is there a hypothesis again? Yes, When running a regression model, a hypothesis test is being run on the global model, that there is no relationship between the dependent variable and the independent variable(s). The alternative hypothesis is that there is a relationship. In other words, alternate hypothesis means at least one coefficient of regression is non-zero. This hypothesis is tested on F-statistic and hence the two values. <code>p-value</code> in our example is very small which lead us to reject the null hypothesis and conclude that there is strong evidence that a relationship does exist between <code>Sepal.Length</code> and <code>Sepal.Width</code>.</p>
<p>The reason for this test is based on the fact that if we run multiple hypothesis tests on our coefficients, it is likely that a variable is included which isn’t actually significant.</p>
</div>
</div>
<div id="model-evaluation-metrics" class="section level2" number="20.5">
<h2>
<span class="header-section-number">20.5</span> Model Evaluation Metrics<a class="anchor" aria-label="anchor" href="#model-evaluation-metrics"><i class="fas fa-link"></i></a>
</h2>
<p>To evaluate the model’s performance and accuracy, evaluation metrics are needed. There are several types of metrics which are used to evaluate the performance of model we have built. We will discuss a few of them here -</p>
<div id="mae---mean-absolute-error" class="section level3" number="20.5.1">
<h3>
<span class="header-section-number">20.5.1</span> MAE - Mean Absolute Error<a class="anchor" aria-label="anchor" href="#mae---mean-absolute-error"><i class="fas fa-link"></i></a>
</h3>
<p>As the name suggests it is mean of absolute values of errors or residuals. The formula thus, can be written as equation <a href="linear-regression.html#eq:lr6">(20.6)</a>.</p>
<p><span class="math display" id="eq:lr6">\[\begin{equation}
{MAE} = \frac{1}{N}\sum_{i = 1}^{N}{\lvert}{y_i - \hat{y_i}}{\rvert}
\tag{20.6}
\end{equation}\]</span></p>
<p>Clearly, it is average value of residuals and a larger value denotes lesser accurate model. In isolation, the MAE is not very useful, however, to compare performance of several models while fitting a best regression model, obviously we can use this metric to choose a better model.</p>
<p>Moreover, once we extract <code>$residuals</code> out of the model, calculating the metric is easy. In our example-</p>
<div class="sourceCode" id="cb1088"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Mean Absolute Error</span></span>
<span><span class="va">lin_reg1</span><span class="op">$</span><span class="va">residuals</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">abs</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<pre><code>## [1] 0.199243</code></pre>
</div>
<div id="mse---mean-square-error-and-rmse---root-mean-square-error" class="section level3" number="20.5.2">
<h3>
<span class="header-section-number">20.5.2</span> MSE - Mean Square Error and RMSE - Root Mean Square Error<a class="anchor" aria-label="anchor" href="#mse---mean-square-error-and-rmse---root-mean-square-error"><i class="fas fa-link"></i></a>
</h3>
<p>Again as the name suggests, the mean of square of all residuals is mean square error or MSE. The formula may be written as in equation <a href="linear-regression.html#eq:lr7">(20.7)</a>.</p>
<p><span class="math display" id="eq:lr7">\[\begin{equation}
{MSE} = \frac{1}{N}\sum_{i = 1}^{N}({y_i - \hat{y_i}})^2
\tag{20.7}
\end{equation}\]</span></p>
<p>MSE penalizes the higher residuals by squaring them. It may be thus thought as weighted average where more and more weight is allocated as the residual value rises. Similar, to MAE, we can use this metric to choose a better model out of the several validating models.</p>
<p>Interestingly, by definition it is also cost function in regression, as while finding parameters, we are actually minimising MSE only. Similar to MAE, calculating this require no special skills.</p>
<div class="sourceCode" id="cb1090"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Mean Square Error</span></span>
<span><span class="va">lin_reg1</span><span class="op">$</span><span class="va">residuals</span><span class="op">^</span><span class="fl">2</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<pre><code>## [1] 0.0631735</code></pre>
<p>RMSE or root mean square error is square root of MSE.</p>
<p><span class="math display" id="eq:lr8">\[\begin{equation}
{RMSE} = \sqrt{\frac{1}{N}\sum_{i = 1}^{N}({y_i - \hat{y_i}})^2}
\tag{20.8}
\end{equation}\]</span></p>
<p>In our Example-</p>
<div class="sourceCode" id="cb1092"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Root Mean Square Error</span></span>
<span><span class="va">lin_reg1</span><span class="op">$</span><span class="va">residuals</span><span class="op">^</span><span class="fl">2</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<pre><code>## [1] 0.2513434</code></pre>
</div>
<div id="mape---mean-absolute-percentage-error" class="section level3" number="20.5.3">
<h3>
<span class="header-section-number">20.5.3</span> MAPE - Mean Absolute Percentage Error<a class="anchor" aria-label="anchor" href="#mape---mean-absolute-percentage-error"><i class="fas fa-link"></i></a>
</h3>
<p>This metric instead of taking residual value in isolation, takes residual value as percentage of actual values. The formula is thus,</p>
<p><span class="math display" id="eq:lr9">\[\begin{equation}
{MAPE} = \frac{1}{N}\sum_{i = 1}^{N}{\lvert}\frac{({y_i - \hat{y_i}})}{y_i}\cdot{100}\%{\rvert}
\tag{20.9}
\end{equation}\]</span></p>
<p>Clearly, MAPE is independent of the scale of the variables since its error estimates are in terms of percentage. In our example, we can calculate MAPE-</p>
<div class="sourceCode" id="cb1094"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Mean Absolute Percentage Error</span></span>
<span><span class="op">{</span><span class="va">lin_reg1</span><span class="op">$</span><span class="va">residuals</span><span class="op">/</span><span class="va">iris</span><span class="op">$</span><span class="va">Sepal.Width</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">50</span><span class="op">]</span><span class="op">}</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="op">{</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">abs</a></span><span class="op">(</span><span class="va">.</span><span class="op">)</span><span class="op">*</span><span class="fl">100</span><span class="op">}</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">.</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/sprintf.html">sprintf</a></span><span class="op">(</span><span class="st">"MAPE is %1.2f%%"</span>, <span class="va">.</span><span class="op">)</span></span></code></pre></div>
<pre><code>## [1] "MAPE is 5.95%"</code></pre>
</div>
<div id="r---squared-and-adjusted-r-squared" class="section level3" number="20.5.4">
<h3>
<span class="header-section-number">20.5.4</span> R - Squared and adjusted R-squared<a class="anchor" aria-label="anchor" href="#r---squared-and-adjusted-r-squared"><i class="fas fa-link"></i></a>
</h3>
<p>As already discussed, it is coefficient of determination or goodness of fit of regression. The can be written as equation <a href="linear-regression.html#eq:lr10">(20.10)</a>.</p>
<p><span class="math display" id="eq:lr10">\[\begin{equation}
R^2 = 1 - \frac{\sum_{i=1}^{N}(y_i - \hat{y_i})^2}{\sum_{i=1}^{N}(y_i - \bar{y_i})^2}
\tag{20.10}
\end{equation}\]</span></p>
<p>The numerator in the fraction above, is also called as <span class="math inline">\(SSE\)</span> or Sum of Squares of Errors; and denominator is also called as <span class="math inline">\(TSS\)</span> or Total Sum of Squares. Actually the ratio (or fraction in above formula) i.e. <span class="math inline">\(\frac{SSE}{TSS}\)</span> denotes ratio of variance in errors to the variance (about mean) in the actual values. Thus <span class="math inline">\(R^2\)</span> actually denotes how much variance is explained by the model; and clearly as the variance errors or <span class="math inline">\(SSE\)</span> minimises and approaches <span class="math inline">\(0\)</span>, <span class="math inline">\(R^2\)</span> increases and approaches <span class="math inline">\(1\)</span> i.e. a perfect model.</p>
<p>We can easily verify the formula from the results obtained.</p>
<div class="sourceCode" id="cb1096"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">## Sum of Squares of Errors</span></span>
<span><span class="va">y_bar</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">iris</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">50</span>,<span class="st">"Sepal.Width"</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="op">(</span><span class="va">SSE</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">lin_reg1</span><span class="op">$</span><span class="va">residuals</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code>## [1] 3.158675</code></pre>
<div class="sourceCode" id="cb1098"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="op">(</span><span class="va">TSS</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="op">(</span><span class="va">iris</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">50</span>,<span class="st">"Sepal.Width"</span><span class="op">]</span> <span class="op">-</span> <span class="va">y_bar</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code>## [1] 7.0408</code></pre>
<div class="sourceCode" id="cb1100"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="op">(</span><span class="va">r_sq</span> <span class="op">&lt;-</span> <span class="fl">1</span> <span class="op">-</span> <span class="va">SSE</span><span class="op">/</span><span class="va">TSS</span><span class="op">)</span></span></code></pre></div>
<pre><code>## [1] 0.5513756</code></pre>
<p>Now, we can think of this <span class="math inline">\(R^2\)</span> in one more way, as it is simply the square of the correlation between the actual and predicted values. We can verify once again</p>
<div class="sourceCode" id="cb1102"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cor</a></span><span class="op">(</span><span class="va">iris</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">50</span>, <span class="st">'Sepal.Width'</span><span class="op">]</span>, <span class="va">lin_reg1</span><span class="op">$</span><span class="va">fitted.values</span><span class="op">)</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span></span></code></pre></div>
<pre><code>## [1] 0.5513756</code></pre>
<p>Usually, when we keep on adding independent variables to our regression model <span class="math inline">\(R^2\)</span> increases and it can easily incorporate over-fitting in itself. For this reason, sometimes adjusted r squared is used, which penalizes R squared for the number of predictors used in the model.</p>
<p><span class="math display" id="eq:lr11">\[\begin{equation}
{Adjusted}\;{ R^2} = 1 - \frac{(1-R^2)(N - 1)}{(N - p - 1)}
\tag{20.11}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(p\)</span> is the number of predictors included in model. The <code>summary</code> function returns both these metrics. We can verify the calculation-</p>
<div class="sourceCode" id="cb1104"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fl">1</span> <span class="op">-</span> <span class="op">(</span><span class="op">(</span><span class="fl">1</span><span class="op">-</span><span class="va">r_sq</span><span class="op">)</span><span class="op">*</span><span class="op">(</span><span class="fl">50</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span><span class="op">/</span><span class="op">(</span><span class="fl">50</span><span class="op">-</span><span class="fl">1</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code>## [1] 0.5420292</code></pre>
</div>
</div>
<div id="plotting-the-results-and-their-interpretion" class="section level2" number="20.6">
<h2>
<span class="header-section-number">20.6</span> Plotting the results and their interpretion<a class="anchor" aria-label="anchor" href="#plotting-the-results-and-their-interpretion"><i class="fas fa-link"></i></a>
</h2>
<p>The output of <code>lm</code> can be plotted with <code>plot</code> command to see six diagnostics plots, one by one, which can be chosen using <code>which</code> argument. These six plots are -</p>
<ul>
<li>Residuals Vs. Fitted Values</li>
<li>Normal Q-Q</li>
<li>Scale-Location</li>
<li>Cook’s distance</li>
<li>Residuals vs. leverage</li>
<li>Cook’s distance vs. leverage</li>
</ul>
<p>Let us see these, for the example above.</p>
<div class="sourceCode" id="cb1106"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">2</span><span class="op">)</span>, oma <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">0</span>, <span class="fl">2</span>, <span class="fl">0</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">lin_reg1</span>, which <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fl">2</span>, sub.caption <span class="op">=</span> <span class="st">""</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:plots1"></span>
<img src="DauR_files/figure-html/plots1-1.png" alt="First two diagnostic plots" width="672" height="40%"><p class="caption">
Figure 20.7: First two diagnostic plots
</p>
</div>
<ol style="list-style-type: decimal">
<li>
<strong>Residuals Vs. Fitted Values</strong>: This plot is used to determine if the residuals exhibit non-linear patterns. If the red line across the center of the plot is roughly horizontal then we can assume that the residuals follow a linear pattern. In our example we can see that the red line deviates from a perfect horizontal line but not severely. We would likely declare that the residuals follow a roughly linear pattern and that a linear regression model is appropriate for this data-set. This plot is useful to check first assumption of linear regression i.e. linearity of the data.</li>
<li>
<strong>Normal Q-Q</strong>: This plot is used to determine if the residuals of the regression model are normally distributed, which was our another assumption. If the points in this plot fall roughly along a straight diagonal line, then we can assume the residuals are normally distributed.</li>
</ol>
<p>Moreover, notice that the extreme outlier values impacting our modelling will be labeled. We can see that values from rows, 23, 33 and 42 are labeled.</p>
<div class="sourceCode" id="cb1107"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">2</span><span class="op">)</span>, oma <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">0</span>, <span class="fl">2</span>, <span class="fl">0</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">lin_reg1</span>, which <span class="op">=</span> <span class="fl">3</span><span class="op">:</span><span class="fl">4</span>, sub.caption <span class="op">=</span> <span class="st">""</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:plots2"></span>
<img src="DauR_files/figure-html/plots2-1.png" alt="Next two diagnostic plots" width="672" height="40%"><p class="caption">
Figure 20.8: Next two diagnostic plots
</p>
</div>
<ol start="3" style="list-style-type: decimal">
<li>
<strong>Scale-Location</strong>: This plot is used to check the assumption of equal variance, i.e. “homoskedasticity” among the residuals in our regression model. If the red line is roughly horizontal across the plot, then the assumption of equal variance is likely met.</li>
<li>
<strong>Cook’s Distance</strong>: An influential value is a value, which inclusion or exclusion can alter the results of the regression analysis. Such a value is associated with a large residual. Not all outliers (or extreme data points) are influential in linear regression analysis. A metric called Cook’s distance, is used to determine the influence of a value. This metric defines influence as a combination of leverage and residual size.</li>
</ol>
<div class="sourceCode" id="cb1108"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">2</span><span class="op">)</span>, oma <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">0</span>, <span class="fl">2</span>, <span class="fl">0</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">lin_reg1</span>, which <span class="op">=</span> <span class="fl">5</span><span class="op">:</span><span class="fl">6</span>, sub.caption <span class="op">=</span> <span class="st">""</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:plots3"></span>
<img src="DauR_files/figure-html/plots3-1.png" alt="Last two diagnostic plots" width="672" height="40%"><p class="caption">
Figure 20.9: Last two diagnostic plots
</p>
</div>
<ol start="5" style="list-style-type: decimal">
<li>
<strong>Residuals vs. leverage</strong>: This plot is used to identify influential observations. If any points in this plot fall outside of Cook’s distance (the dashed lines) then it is an influential observation. Actually, a data point has high leverage, if it has extreme predictor x values.</li>
<li>
<strong>Sixth plot i.e. Cooks distance vs. leverage</strong> is also used to identify extreme values that may be impacting our model.</li>
</ol>
<p>Though the base R’s command <code>plot</code> can generate all the plots, we may make use of library <code>performance</code> to generate all the relevant diagnostic plots beautifully and with small interpretation. See</p>
<div class="sourceCode" id="cb1109"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://easystats.github.io/performance/">performance</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://easystats.github.io/performance/reference/check_model.html">check_model</a></span><span class="op">(</span><span class="va">lin_reg1</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:perf"></span>
<img src="DauR_files/figure-html/perf-1.png" alt="Output from package-performance" width="672"><p class="caption">
Figure 20.10: Output from package-performance
</p>
</div>
<p>The warning is obvious, we cannot have <code>multi-collinearity</code> problem as there is only one regressor. Actually multi-collinearity is about another assumption, we would have made in case there were more than one independent variables. This assumption would have been that the independent variables are not mutually collinear. We will see detailed explanation in case of multiple linear regression in the subsequent section.</p>
</div>
<div id="using-lm-for-predictions" class="section level2" number="20.7">
<h2>
<span class="header-section-number">20.7</span> Using <code>lm</code> for predictions<a class="anchor" aria-label="anchor" href="#using-lm-for-predictions"><i class="fas fa-link"></i></a>
</h2>
<p>The output of <code>lm</code> is actually a list which contains much more information than we saw above. See which info is contained here -</p>
<div class="sourceCode" id="cb1110"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="va">lin_reg1</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html">as.data.frame</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/setNames.html">setNames</a></span><span class="op">(</span><span class="st">'Objects'</span><span class="op">)</span></span></code></pre></div>
<pre><code>##          Objects
## 1   coefficients
## 2      residuals
## 3        effects
## 4           rank
## 5  fitted.values
## 6         assign
## 7             qr
## 8    df.residual
## 9        xlevels
## 10          call
## 11         terms
## 12         model</code></pre>
<p>We may extract any of the as per requirement. E.g.</p>
<div class="sourceCode" id="cb1112"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">lin_reg1</span><span class="op">$</span><span class="va">coefficients</span></span></code></pre></div>
<pre><code>##  (Intercept) Sepal.Length 
##   -0.5694327    0.7985283</code></pre>
<p>There is a package <code>broom</code> which uses <code>tidy</code> fundamentals to returns all the useful information by a single function <code>augment</code>.</p>
<div class="sourceCode" id="cb1114"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://broom.tidymodels.org/">broom</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://generics.r-lib.org/reference/augment.html">augment</a></span><span class="op">(</span><span class="va">lin_reg1</span><span class="op">)</span></span></code></pre></div>
<pre><code>## # A tibble: 50 × 8
##    Sepal.Width Sepal.Length .fitted   .resid   .hat .sigma    .cooksd .std.resid
##          &lt;dbl&gt;        &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;
##  1         3.5          5.1    3.50 -0.00306 0.0215  0.259 0.00000160    -0.0121
##  2         3            4.9    3.34 -0.343   0.0218  0.254 0.0205        -1.35  
##  3         3.2          4.7    3.18  0.0163  0.0354  0.259 0.0000772      0.0649
##  4         3.1          4.6    3.10 -0.00380 0.0471  0.259 0.00000568    -0.0152
##  5         3.6          5      3.42  0.177   0.0200  0.258 0.00495        0.696 
##  6         3.9          5.4    3.74  0.157   0.0455  0.258 0.00940        0.628 
##  7         3.4          4.6    3.10  0.296   0.0471  0.255 0.0346         1.18  
##  8         3.4          5      3.42 -0.0232  0.0200  0.259 0.0000853     -0.0914
##  9         2.9          4.4    2.94 -0.0441  0.0803  0.259 0.00140       -0.179 
## 10         3.1          4.9    3.34 -0.243   0.0218  0.257 0.0103        -0.959 
## # ℹ 40 more rows</code></pre>
<p>Let’s also visualise the predicted values vis-a-vis actual values/residuals in Figure <a href="linear-regression.html#fig:predvsact">20.11</a>.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:predvsact"></span>
<img src="DauR_files/figure-html/predvsact-1.png" alt="Predicted Vs. Actual Values (Left) and Residuals (Right)" width="48%"><img src="DauR_files/figure-html/predvsact-2.png" alt="Predicted Vs. Actual Values (Left) and Residuals (Right)" width="48%"><p class="caption">
Figure 20.11: Predicted Vs. Actual Values (Left) and Residuals (Right)
</p>
</div>
<p>So if we have predict output from a new data, just ensure that data is in exactly same format as of regressor and we can use <code>predict</code> from base R directly. See this example.</p>
<div class="sourceCode" id="cb1116"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">new_vals</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">10</span>, <span class="fl">5</span>, <span class="fl">1</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html">as.data.frame</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/setNames.html">setNames</a></span><span class="op">(</span><span class="st">'Sepal.Length'</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">lin_reg1</span>, <span class="va">new_vals</span><span class="op">)</span></span></code></pre></div>
<pre><code>##        1        2        3        4        5        6        7        8 
## 3.416249 3.552228 2.691731 5.185508 2.801470 3.523949 4.037588 2.183525 
##        9       10 
## 4.190880 3.667629</code></pre>
</div>
<div id="multiple-linear-regression" class="section level2" number="20.8">
<h2>
<span class="header-section-number">20.8</span> Multiple Linear Regression<a class="anchor" aria-label="anchor" href="#multiple-linear-regression"><i class="fas fa-link"></i></a>
</h2>
<p>As the name suggests, multiple linear regression is the model where multiple independent variables may have linear relationship with dependent variable. In this case, the regression equation will be -</p>
<p><span class="math display" id="eq:lr12">\[\begin{equation}
y = \beta_0 + \beta_1x_1 + \beta_2x_2 + \beta_3x_3 + ... + \epsilon
\tag{20.12}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span>, <span class="math inline">\(x_3\)</span> …, <span class="math inline">\(x_n\)</span> will be <span class="math inline">\(n\)</span> independent variables and <span class="math inline">\(y\)</span> will be dependent variable as usual.</p>
<p>It may be clear that this equation represents an equation of a plane if there are two regressors. If there are <span class="math inline">\(n\)</span> regressors, the equation will represent equation of a hyperplane of <span class="math inline">\(n-1\)</span> dimensions. However, visualizing the variables and relation between them can be a bit tricky if there are multiple variables. We may decide about the type of the visualization will suit the requirement in that case.</p>
<p>Now, as already stated, there is an additional assumption, <strong>that all the independent variables are mutually independent too i.e. do not have multi-collinearity between them.</strong> Let’s build an example model again. We have to just add the predictors (independent variables) using the <code>+</code> operator in the formula <code>call</code>.</p>
<p><strong>Problem Statement:</strong> Example-2. Let’s try to establish the relationship between cars’ mileage (<code>mpg</code> variable in <code>mtcars</code> data-set) with engine displacement <code>disp</code>, horse power <code>hp</code> and weight <code>wt</code>. First of all let’s visualise the individual relationships between the variables through three plots as in Figure <a href="linear-regression.html#fig:ex2vis">20.12</a>.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:ex2vis"></span>
<img src="DauR_files/figure-html/ex2vis-1.png" alt="Relationship between regressors and outcome variables" width="31%"><img src="DauR_files/figure-html/ex2vis-2.png" alt="Relationship between regressors and outcome variables" width="31%"><img src="DauR_files/figure-html/ex2vis-3.png" alt="Relationship between regressors and outcome variables" width="31%"><p class="caption">
Figure 20.12: Relationship between regressors and outcome variables
</p>
</div>
<p>Let’s also visualise the correlation between all these variables. See Figure <a href="linear-regression.html#fig:ex2vis2">20.13</a>.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:ex2vis2"></span>
<img src="DauR_files/figure-html/ex2vis2-1.png" alt="Correlation between the variables in Example-2" width="672"><p class="caption">
Figure 20.13: Correlation between the variables in Example-2
</p>
</div>
<p>Now let’s build the model.</p>
<div class="sourceCode" id="cb1118"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">data</span> <span class="op">&lt;-</span> <span class="va">mtcars</span><span class="op">[</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"mpg"</span>, <span class="st">"disp"</span>, <span class="st">"hp"</span>, <span class="st">"wt"</span><span class="op">)</span><span class="op">]</span></span>
<span><span class="va">lin_reg2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">mpg</span> <span class="op">~</span> <span class="va">.</span>, </span>
<span>               data <span class="op">=</span> <span class="va">data</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">lin_reg2</span><span class="op">)</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = mpg ~ ., data = data)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -3.891 -1.640 -0.172  1.061  5.861 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 37.105505   2.110815  17.579  &lt; 2e-16 ***
## disp        -0.000937   0.010350  -0.091  0.92851    
## hp          -0.031157   0.011436  -2.724  0.01097 *  
## wt          -3.800891   1.066191  -3.565  0.00133 ** 
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 2.639 on 28 degrees of freedom
## Multiple R-squared:  0.8268, Adjusted R-squared:  0.8083 
## F-statistic: 44.57 on 3 and 28 DF,  p-value: 8.65e-11</code></pre>
<p>In formula call, please note that I have used <code>.</code> instead of naming all the variables. In fact <code>.</code> is a shorthand style of mentioning that all the variables except that mentioned as y variable will be our independent variables/predictors. In results, there is one coefficient for slope for each predictors and one intercept term in the output. The output equation can be written as equation <a href="linear-regression.html#eq:lr13">(20.13)</a>.</p>
<p><span class="math display" id="eq:lr13">\[\begin{equation}
{mpg} = -0.000937\cdot{disp} - 0.031157\cdot{hp} - 3.800891\cdot{wt} + 37.105505
\tag{20.13}
\end{equation}\]</span></p>
<p><strong>Interpretation:</strong> Notice that all slopes are in negative meaning that mileage drops by increase in each of weight, displacement and horsepower. Interpreting the equation would be on similar lines. We can now say that <em>keeping other variables constant</em>, the effect of change (increase) of 1 unit in <code>disp</code> will result in decrease of mileage by 0.000937 miles per gallon. Keeping other variables constant is important here. Of course, by above equation we may deduce that if other factors change, the effect on response variable would be different.</p>
<p><strong>Results:</strong> Analysing results, we may notice that our model is explains 83% of variance in data. Of course, adjusted r-squared is lower which means that adding extra variables may have increased the r-squared. Global p-value is highly significant which means that at least of the coefficients is non-zero. Of course, the least significant coefficient is that of <code>disp</code> which we can remove and re-run the model to check the parameters again.</p>
</div>
<div id="including-categorical-or-factor-variables-in-lm" class="section level2" number="20.9">
<h2>
<span class="header-section-number">20.9</span> Including categorical or factor variables in <code>lm</code><a class="anchor" aria-label="anchor" href="#including-categorical-or-factor-variables-in-lm"><i class="fas fa-link"></i></a>
</h2>
<p>By now we have seen that linear regression is useful for predicting numerical output and through the examples we have seen that our regressors were numerical too. But what if there’s an input variable which is categorical or nominal?</p>
<p>In such case, we will have to ensure that categorical variable is of type <code>factor</code> before proceeding to build a model.</p>
<p><strong>Problem Statement:</strong> Example-3. Let’s try to predict <span class="math inline">\({Sepal.Width}\)</span> from <span class="math inline">\({Species}\)</span> in the iris data-set. This time we will take complete data-set. Since, we know that <code>Species</code> is already of factor type we need not convert it into one. Before moving on let’s visualize the relation between the two variables using ggplot2. Box-plots are best suited here.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:fact1"></span>
<img src="DauR_files/figure-html/fact1-1.png" alt="Species Vs. Sepal Width" width="672" height="30%"><p class="caption">
Figure 20.14: Species Vs. Sepal Width
</p>
</div>
<p>Now let’s build the model.</p>
<div class="sourceCode" id="cb1120"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">lm_fact</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Sepal.Width</span> <span class="op">~</span> <span class="va">Species</span>, data <span class="op">=</span> <span class="va">iris</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">lm_fact</span><span class="op">)</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Sepal.Width ~ Species, data = iris)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -1.128 -0.228  0.026  0.226  0.972 
## 
## Coefficients:
##                   Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)        3.42800    0.04804  71.359  &lt; 2e-16 ***
## Speciesversicolor -0.65800    0.06794  -9.685  &lt; 2e-16 ***
## Speciesvirginica  -0.45400    0.06794  -6.683 4.54e-10 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.3397 on 147 degrees of freedom
## Multiple R-squared:  0.4008, Adjusted R-squared:  0.3926 
## F-statistic: 49.16 on 2 and 147 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>In results we now got one intercept and two slopes for single regressor <code>Species</code>. So what happened now?</p>
<p><strong>Interpretation:</strong> Actually, factor data type requirement for categorical regressor was due to the fact that this factor variable is encoded as dummy variable for each of the category available in it. Dummy variable is numeric and we can now run linear regression as earlier. The first category available in it will be baseline level. Since there were three levels included in <code>Species</code> it has been encoded into two dummy variables. To see what happened behind the scenes, we may use <code>contrasts</code> function.</p>
<div class="sourceCode" id="cb1122"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/contrasts.html">contrasts</a></span><span class="op">(</span><span class="va">iris</span><span class="op">$</span><span class="va">Species</span><span class="op">)</span></span></code></pre></div>
<pre><code>##            versicolor virginica
## setosa              0         0
## versicolor          1         0
## virginica           0         1</code></pre>
<p><code>model.matrix</code></p>
<p>It is clear that when <code>versicolor</code> is <code>1</code> the other variable is <code>0</code> and vice versa. Obviously when both are <code>0</code> it means that <code>Species</code> is <code>setosa</code> and that’s why no separate slope for that is present in output. Now we can write our regression line equation as <a href="linear-regression.html#eq:lr14">(20.14)</a></p>
<p><span class="math display" id="eq:lr14">\[\begin{equation}
{Sepal.Width} = 3.428 + (-0.658)\cdot{Speciesversicolor} + (-0.454)\cdot{Speciesvirginica}
\tag{20.14}
\end{equation}\]</span></p>
<p>Interpreting above equation is now easy. For each <code>versicolor</code> the <code>Sepal.Width</code> may be <code>3.428 - 0.658</code> or 2.77. Obviously when two dummy variables are zero, the Sepal.Width would be equal to intercept; and thus, we can conclude that intercept is nothing but prediction for base-line level. In fact we can subtract a <code>1</code> to obtain these interpreted results.</p>
<div class="sourceCode" id="cb1124"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">lm_fact</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Sepal.Width</span> <span class="op">~</span> <span class="va">Species</span> <span class="op">-</span><span class="fl">1</span>, data <span class="op">=</span> <span class="va">iris</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">lm_fact</span><span class="op">)</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Sepal.Width ~ Species - 1, data = iris)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -1.128 -0.228  0.026  0.226  0.972 
## 
## Coefficients:
##                   Estimate Std. Error t value Pr(&gt;|t|)    
## Speciessetosa      3.42800    0.04804   71.36   &lt;2e-16 ***
## Speciesversicolor  2.77000    0.04804   57.66   &lt;2e-16 ***
## Speciesvirginica   2.97400    0.04804   61.91   &lt;2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.3397 on 147 degrees of freedom
## Multiple R-squared:  0.9881, Adjusted R-squared:  0.9879 
## F-statistic:  4083 on 3 and 147 DF,  p-value: &lt; 2.2e-16</code></pre>
<p><strong>Notice that other two coefficients have now been adjusted automatically.</strong></p>
<p><strong>Results:</strong> Observing the figure <a href="linear-regression.html#fig:fact1">20.14</a>, we may notice that the coefficients are nothing but mean Sepal Widths for each Species, which is obvious and logical too. Those, who are interested in seeing equation (without intercept) can also make one, as in equation <a href="linear-regression.html#eq:lr15">(20.15)</a>.</p>
<p><span class="math display" id="eq:lr15">\[\begin{equation}
{Sepal.Width} = 3.428\cdot{Speciessetosa} + (2.77)\cdot{Speciesversicolor} + (2.974)\cdot{Speciesvirginica}
\tag{20.15}
\end{equation}\]</span></p>
<p>Since, these coefficients are not slopes in true sense, we will refer these as intercepts, in next examples/sections.</p>
<div id="parallel-slopes-regression" class="section level3" number="20.9.1">
<h3>
<span class="header-section-number">20.9.1</span> Parallel slopes regression<a class="anchor" aria-label="anchor" href="#parallel-slopes-regression"><i class="fas fa-link"></i></a>
</h3>
<p>To understand how categorical response variable acts, when there are other numerical variables in regression model, let us build a model step by step.</p>
<p><strong>Problem statement:</strong> Example-4. We are taking <code>mpg</code> data-set included by default with <code>ggplot2</code> package. This data-set shows <em>Fuel economy data from 1999 to 2008 for 38 popular models of cars</em>. Let us predict highway mileage <code>hwy</code> from engine displacement <code>displ</code> and year of the model <code>year</code>. Let us visualize the variables. From Figure <a href="linear-regression.html#fig:ex4vis">20.15</a> it is clear that highway mileage is linearly associated with displacement.</p>
<div class="sourceCode" id="cb1126"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">mpg</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">hwy</span>, <span class="va">displ</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_smooth.html">geom_smooth</a></span><span class="op">(</span>method <span class="op">=</span> <span class="st">"lm"</span>, se <span class="op">=</span> <span class="cn">FALSE</span>, formula <span class="op">=</span> <span class="st">"y ~ x"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/facet_wrap.html">facet_wrap</a></span><span class="op">(</span><span class="va">.</span><span class="op">~</span> <span class="va">year</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_bw</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:ex4vis"></span>
<img src="DauR_files/figure-html/ex4vis-1.png" alt="Highway Mileage Vs. Displacement over cars manufactured in 1998 Vs. 2008" width="672" height="35%"><p class="caption">
Figure 20.15: Highway Mileage Vs. Displacement over cars manufactured in 1998 Vs. 2008
</p>
</div>
<p><strong>Step-1:</strong> Let us first include a single numerical variable <code>displ</code> to predict highway mileage <code>hwy</code>; and examine the coefficients.</p>
<div class="sourceCode" id="cb1127"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">par_slop1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">hwy</span> <span class="op">~</span> <span class="va">displ</span>, data <span class="op">=</span> <span class="va">mpg</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">par_slop1</span><span class="op">)</span></span></code></pre></div>
<pre><code>## (Intercept)       displ 
##   35.697651   -3.530589</code></pre>
<p>We got one intercept and one slope coefficient. We can even see related visualisation in figure <a href="linear-regression.html#fig:parrslop">20.16</a> (left).</p>
<p><strong>Step-2:</strong> Now let us try to predict mileage on the basis of <code>year</code> of manufacture only. So as already stated, we have to convert it into factor. We can do that directly in the formula. <em>Also note that we are subtracting <span class="math inline">\(1\)</span> from the response variables, which actually replaces intercept with the baseline level explicitly.</em> Now see the coefficients-</p>
<div class="sourceCode" id="cb1129"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://ggplot2.tidyverse.org">ggplot2</a></span><span class="op">)</span></span>
<span><span class="va">par_slop2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">hwy</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">factor</a></span><span class="op">(</span><span class="va">year</span><span class="op">)</span> <span class="op">-</span> <span class="fl">1</span>, data <span class="op">=</span> <span class="va">mpg</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">par_slop2</span><span class="op">)</span></span></code></pre></div>
<pre><code>## factor(year)1999 factor(year)2008 
##         23.42735         23.45299</code></pre>
<p>Notice that we got intercept for each of the category available in factor variable. By seeing plot in Figure <a href="linear-regression.html#fig:parrslop">20.16</a>-(Right) that these intercepts are nothing but means for each category.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:parrslop"></span>
<img src="DauR_files/figure-html/parrslop-1.png" alt="Mileage vs. Displacement (Left) and Year (right)" width="47%"><img src="DauR_files/figure-html/parrslop-2.png" alt="Mileage vs. Displacement (Left) and Year (right)" width="47%"><p class="caption">
Figure 20.16: Mileage vs. Displacement (Left) and Year (right)
</p>
</div>
<p><strong>Step-3:</strong> Now we will build the complete model by including both variables together; and examine the coefficients.</p>
<div class="sourceCode" id="cb1131"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">par_slop</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">hwy</span> <span class="op">~</span> <span class="va">displ</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">factor</a></span><span class="op">(</span><span class="va">year</span><span class="op">)</span> <span class="op">-</span> <span class="fl">1</span>, data <span class="op">=</span> <span class="va">mpg</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">par_slop</span><span class="op">)</span></span></code></pre></div>
<pre><code>##            displ factor(year)1999 factor(year)2008 
##        -3.610986        35.275706        36.677842</code></pre>
<p>We can see one slope coefficient for numerical variable and two different intercepts for each of the Years. Try visualising these. In fact there are two different parallel lines (same slope). Refer figure <a href="linear-regression.html#fig:parrslop2">20.17</a>.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:parrslop2"></span>
<img src="DauR_files/figure-html/parrslop2-1.png" alt="Parallel Slopes" width="95%" height="35%"><p class="caption">
Figure 20.17: Parallel Slopes
</p>
</div>
<p>This is also evident, if we write out the equation <a href="linear-regression.html#eq:lr16">(20.16)</a>.</p>
<p><span class="math display" id="eq:lr16">\[\begin{equation}
{hwy} = -3.610986\cdot{displ} + 35.275706\cdot{year1999} + 36.677842\cdot{year2008}
\tag{20.16}
\end{equation}\]</span></p>
<p>Either one of the dummy variables will be 0 and another 1, so that variable with <code>1</code> will act as intercept term, but the slope term will remain same. In other words, whatever be the year, the mileage will vary with displacement at the same rate. This is in actual circumstances, rare. Rate of change of response variable will change as per factor variables (regressor) change their values. So how to incorporate these changes in our model? The answer is <code>interaction</code> which has been discussed in next section.</p>
</div>
<div id="extending-multiple-linear-regression-by-including-interactions" class="section level3" number="20.9.2">
<h3>
<span class="header-section-number">20.9.2</span> Extending multiple linear regression by including <code>interactions</code><a class="anchor" aria-label="anchor" href="#extending-multiple-linear-regression-by-including-interactions"><i class="fas fa-link"></i></a>
</h3>
<p>The parallel slopes model, we saw in previous section enforced a common slope for each category. That’s not always the best option.</p>
<p><strong>Problem Statement:</strong> In same example-4 (earlier section) we can introduce <code>interaction</code> between two predictors using special operator OR shorthand notation <code>:</code> in formula call. See the following example-</p>
<div class="sourceCode" id="cb1133"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">new_model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">hwy</span> <span class="op">~</span> <span class="va">displ</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">factor</a></span><span class="op">(</span><span class="va">year</span><span class="op">)</span> <span class="op">+</span> <span class="va">displ</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/factor.html">factor</a></span><span class="op">(</span><span class="va">year</span><span class="op">)</span> <span class="op">-</span><span class="fl">1</span> , data <span class="op">=</span> <span class="va">mpg</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">new_model</span><span class="op">)</span></span></code></pre></div>
<pre><code>##                  displ       factor(year)1999       factor(year)2008 
##              -3.768406              35.792231              36.136730 
## displ:factor(year)2008 
##               0.305168</code></pre>
<p>Now notice an extra coefficient, though small which is change in slope when moving from baseline category to category of year- 2008. This, in fact represents that the model has a different slope for each category; refer Figure <a href="linear-regression.html#fig:parrslop3">20.18</a>.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:parrslop3"></span>
<img src="DauR_files/figure-html/parrslop3-1.png" alt="Changing Slopes with interaction" width="95%" height="35%"><p class="caption">
Figure 20.18: Changing Slopes with interaction
</p>
</div>
<p>Interpreting these models are now, not that difficult as it seem earlier. Let’s generate summary first.</p>
<div class="sourceCode" id="cb1135"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">new_model</span><span class="op">)</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = hwy ~ displ + factor(year) + displ:factor(year) - 
##     1, data = mpg)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -7.8595 -2.4360 -0.2103  1.6037 15.3677 
## 
## Coefficients:
##                        Estimate Std. Error t value Pr(&gt;|t|)    
## displ                   -3.7684     0.2788 -13.517   &lt;2e-16 ***
## factor(year)1999        35.7922     0.9794  36.546   &lt;2e-16 ***
## factor(year)2008        36.1367     1.0492  34.442   &lt;2e-16 ***
## displ:factor(year)2008   0.3052     0.3882   0.786    0.433    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 3.784 on 230 degrees of freedom
## Multiple R-squared:  0.9759, Adjusted R-squared:  0.9755 
## F-statistic:  2332 on 4 and 230 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>We may write our equation now (equation <a href="linear-regression.html#eq:lr17">(20.17)</a>.</p>
<p><span class="math display" id="eq:lr17">\[\begin{equation}
{hwy} = -3.7684\cdot{displ} + 35.7922\cdot{year1999} + 36.1367\cdot{year2008} + 0.3052\cdot{displ}\cdot{year2008}
\tag{20.17}
\end{equation}\]</span></p>
<p>Clearly, when year is 2008, the slope for <code>displ</code> changes by <code>0.3052</code>.</p>
<p>We can add as many interactions as we would like to, using the shorthand <code>:</code>; however, when there are many interactions we may make use of another shorthand operator <code>*</code>. So <code>x*z</code> would mean <code>x + z + x:z</code> and <code>x*z*w</code> would mean <code>x + z + w + x:z + x:w + z:w</code>. This obviously wouldn’t make any difference but would save us a lot of typing.</p>
</div>
</div>
<div id="multi-collinearity-and-variance-inflation-factor-vif" class="section level2" number="20.10">
<h2>
<span class="header-section-number">20.10</span> Multi-collinearity and Variance Inflation Factor (VIF)<a class="anchor" aria-label="anchor" href="#multi-collinearity-and-variance-inflation-factor-vif"><i class="fas fa-link"></i></a>
</h2>
<p>Multi-collinearity indicates a strong linear relationship among the predictor variables. This can create challenges in the regression analysis because it becomes difficult to determine the individual effects of each independent variable on the dependent variable accurately. Multi-collinearity can lead to unstable and unreliable coefficient estimates, making it harder to interpret the results and draw meaningful conclusions from the model. It is essential to detect and address multi-collinearity to ensure the validity and robustness of regression models.</p>
<p>But why it poses a problem in regression analysis. Actually, multi-collinearity means that one independent variable can be predicted from another and it in turn means that independent variables are no longer independent.</p>
<p>Multi-collinearity can be detected using many different methods. One of the method can be to use correlation plots, which is explained in next section. Another method is to use Variance Inflation Factor or VIF. VIF determines the strength of the correlation between the independent variables. It is predicted by taking a variable and regressing it against every other variable. In other words, VIF score of an independent variable represents how well the variable is explained by other independent variables.</p>
<p><span class="math display" id="eq:lr18">\[\begin{equation}
{VIF} = \frac{1}{1-R^2}
\tag{20.18}
\end{equation}\]</span></p>
<p>So the closer <span class="math inline">\(R^2\)</span> value to <span class="math inline">\(1\)</span> the higher the <span class="math inline">\({VIF}\)</span>.</p>
<ul>
<li>
<span class="math inline">\(VIF\)</span> starts at <span class="math inline">\(1\)</span> and has no upper limit</li>
<li>
<span class="math inline">\({VIF} = 1\)</span> means there is no correlation between the independent variable and the other variables</li>
<li>
<span class="math inline">\({VIF}\)</span> exceeding <span class="math inline">\(5\)</span> indicates high multi-collinearity between that independent variable and others.</li>
</ul>
<p>Again in R, we do not have to manually calculate <span class="math inline">\({VIF}\)</span> for each variable. Using <code><a href="https://rdrr.io/pkg/car/man/vif.html">vif()</a></code> function, of library <code>car</code> we can calculate this. Let’s use it on another model built on <code>mtcars</code> data where <code>mpg</code> variable is predicted using all other variables. For all other variables, instead of using names, we will use another shorthand operator <code>.</code>.</p>
<div class="sourceCode" id="cb1137"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">mod</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">mpg</span> <span class="op">~</span> <span class="va">.</span> , data <span class="op">=</span> <span class="va">mtcars</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://r-forge.r-project.org/projects/car/">car</a></span><span class="op">)</span></span></code></pre></div>
<pre><code>## Loading required package: carData</code></pre>
<pre><code>## 
## Attaching package: 'car'</code></pre>
<pre><code>## The following object is masked from 'package:psych':
## 
##     logit</code></pre>
<pre><code>## The following object is masked from 'package:dplyr':
## 
##     recode</code></pre>
<pre><code>## The following object is masked from 'package:purrr':
## 
##     some</code></pre>
<div class="sourceCode" id="cb1143"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">car</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/car/man/vif.html">vif</a></span><span class="op">(</span><span class="va">mod</span><span class="op">)</span></span></code></pre></div>
<pre><code>##       cyl      disp        hp      drat        wt      qsec        vs        am 
## 15.373833 21.620241  9.832037  3.374620 15.164887  7.527958  4.965873  4.648487 
##      gear      carb 
##  5.357452  7.908747</code></pre>
<p>We may notice high collinearity among some of these predictors as also indicated in above output.</p>
</div>
<div id="one-complete-example" class="section level2" number="20.11">
<h2>
<span class="header-section-number">20.11</span> One Complete Example<a class="anchor" aria-label="anchor" href="#one-complete-example"><i class="fas fa-link"></i></a>
</h2>
<p><strong>Problem Statement:</strong> Example-5. The data pertains to inter-country Life-Cycle Savings 1960-1970. Under the life-cycle savings hypothesis as developed by <em>Franco Modigliani</em>, the savings ratio (aggregate personal saving divided by disposable income) <code>sr</code> is explained by per-capita disposable income <code>dpi</code>, the percentage rate of change in per-capita disposable income <code>ddpi</code>, and two demographic variables: the percentage of population less than 15 years old <code>pop15</code> and the percentage of the population over 75 years old <code>pop75</code>. The data are averaged over the decade 1960–1970 to remove the business cycle or other short-term fluctuations.</p>
<p>Let’s try linear regression on <code>LifeCycleSavings</code> data.</p>
<div class="sourceCode" id="cb1145"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Visualise first 6 rows</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">LifeCycleSavings</span><span class="op">)</span></span></code></pre></div>
<pre><code>##              sr pop15 pop75     dpi ddpi
## Australia 11.43 29.35  2.87 2329.68 2.87
## Austria   12.07 23.32  4.41 1507.99 3.93
## Belgium   13.17 23.80  4.43 2108.47 3.82
## Bolivia    5.75 41.89  1.67  189.13 0.22
## Brazil    12.88 42.19  0.83  728.47 4.56
## Canada     8.79 31.72  2.85 2982.88 2.43</code></pre>
<div class="sourceCode" id="cb1147"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Build a model</span></span>
<span><span class="va">ex1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">sr</span> <span class="op">~</span> <span class="va">.</span> , data <span class="op">=</span> <span class="va">LifeCycleSavings</span><span class="op">)</span></span></code></pre></div>
<p>In <code>call</code> formula above, notice the shorthand <code>.</code> operator which here means all variable other than y variable are treated as input variables. See its output-</p>
<div class="sourceCode" id="cb1148"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">ex1</span><span class="op">)</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = sr ~ ., data = LifeCycleSavings)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -8.2422 -2.6857 -0.2488  2.4280  9.7509 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 28.5660865  7.3545161   3.884 0.000334 ***
## pop15       -0.4611931  0.1446422  -3.189 0.002603 ** 
## pop75       -1.6914977  1.0835989  -1.561 0.125530    
## dpi         -0.0003369  0.0009311  -0.362 0.719173    
## ddpi         0.4096949  0.1961971   2.088 0.042471 *  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 3.803 on 45 degrees of freedom
## Multiple R-squared:  0.3385, Adjusted R-squared:  0.2797 
## F-statistic: 5.756 on 4 and 45 DF,  p-value: 0.0007904</code></pre>
<p>Multiple R squared is about 34% which means nearly 34% variability is explained by linear model. Let us see some diagnostics plots (figure <a href="linear-regression.html#fig:perf2">20.19</a>)</p>
<div class="sourceCode" id="cb1150"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">performance</span><span class="fu">::</span><span class="fu"><a href="https://easystats.github.io/performance/reference/check_model.html">check_model</a></span><span class="op">(</span><span class="va">ex1</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:perf2"></span>
<img src="DauR_files/figure-html/perf2-1.png" alt="Diagnostic Plots" width="672"><p class="caption">
Figure 20.19: Diagnostic Plots
</p>
</div>
<p>We may notice some multi-collinearity, between two population variables. See figures and <a href="linear-regression.html#fig:perf2">20.19</a> and <a href="linear-regression.html#fig:multi2">20.20</a>.</p>
<div class="sourceCode" id="cb1151"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">LifeCycleSavings</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">pop15</span>, <span class="va">pop75</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_smooth.html">geom_smooth</a></span><span class="op">(</span>method <span class="op">=</span> <span class="st">'lm'</span>, formula <span class="op">=</span> <span class="st">'y~x'</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_bw</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:multi2"></span>
<img src="DauR_files/figure-html/multi2-1.png" alt="Are x and y correlated?" width="672" height="30%"><p class="caption">
Figure 20.20: Are x and y correlated?
</p>
</div>
<p>This can also be verified by corrplots in figure <a href="linear-regression.html#fig:corrp">20.21</a>.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:corrp"></span>
<img src="DauR_files/figure-html/corrp-1.png" alt="Correlation Plots" width="672" height="45%"><p class="caption">
Figure 20.21: Correlation Plots
</p>
</div>
<p>Let us see the VIF among the predictors.</p>
<div class="sourceCode" id="cb1152"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">car</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/car/man/vif.html">vif</a></span><span class="op">(</span><span class="va">ex1</span><span class="op">)</span></span></code></pre></div>
<pre><code>##    pop15    pop75      dpi     ddpi 
## 5.937661 6.629105 2.884369 1.074309</code></pre>
<p>Thus, the model should be tuned better by removing this multi-collinear variable. Let us try to remove <code>pop75</code> and re-run the model.</p>
<div class="sourceCode" id="cb1154"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">ex2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">sr</span> <span class="op">~</span> <span class="va">pop15</span> <span class="op">+</span> <span class="va">dpi</span> <span class="op">+</span> <span class="va">ddpi</span>, data <span class="op">=</span> <span class="va">LifeCycleSavings</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">ex2</span><span class="op">)</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = sr ~ pop15 + dpi + ddpi, data = LifeCycleSavings)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -7.6889 -2.8813  0.0296  1.7989 10.4330 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 19.2771687  4.3888974   4.392 6.53e-05 ***
## pop15       -0.2883861  0.0945354  -3.051  0.00378 ** 
## dpi         -0.0008704  0.0008795  -0.990  0.32755    
## ddpi         0.3929355  0.1989390   1.975  0.05427 .  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 3.862 on 46 degrees of freedom
## Multiple R-squared:  0.3026, Adjusted R-squared:  0.2572 
## F-statistic: 6.654 on 3 and 46 DF,  p-value: 0.0007941</code></pre>
<p>Notice that multiple R-squared has now reduced to 30%. Let us try to remove the variable <code>dpi</code> the coefficient of which is not that significant.</p>
<div class="sourceCode" id="cb1156"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">ex3</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">sr</span> <span class="op">~</span> <span class="va">pop15</span> <span class="op">+</span> <span class="va">ddpi</span>, data <span class="op">=</span> <span class="va">LifeCycleSavings</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">ex3</span><span class="op">)</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = sr ~ pop15 + ddpi, data = LifeCycleSavings)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -7.5831 -2.8632  0.0453  2.2273 10.4753 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 15.59958    2.33439   6.682 2.48e-08 ***
## pop15       -0.21638    0.06033  -3.586 0.000796 ***
## ddpi         0.44283    0.19240   2.302 0.025837 *  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 3.861 on 47 degrees of freedom
## Multiple R-squared:  0.2878, Adjusted R-squared:  0.2575 
## F-statistic: 9.496 on 2 and 47 DF,  p-value: 0.0003438</code></pre>
<p>Multiple R squared increased slightly i.e. now reached around 29%. Let us try to visualise the relationship through a scatter plot.</p>
<div class="sourceCode" id="cb1158"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">LifeCycleSavings</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">pop15</span>, <span class="va">sr</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_smooth.html">geom_smooth</a></span><span class="op">(</span>method <span class="op">=</span> <span class="st">'lm'</span>, formula <span class="op">=</span> <span class="st">'y ~ x'</span>, se <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_bw</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:unnamed-chunk-380"></span>
<img src="DauR_files/figure-html/unnamed-chunk-380-1.png" alt="Ascertaining linear relationship" width="672" height="30%"><p class="caption">
Figure 20.22: Ascertaining linear relationship
</p>
</div>
<p>Clearly the relationship between the predictor and response variable is not that strong and hence the results.</p>
</div>
<div id="simpsons-paradox" class="section level2" number="20.12">
<h2>
<span class="header-section-number">20.12</span> Simpson’s paradox<a class="anchor" aria-label="anchor" href="#simpsons-paradox"><i class="fas fa-link"></i></a>
</h2>
<p>Edward Hugh Simpson, a statistician and former cryptanalyst at Bletchley Park, described this statistical phenomenon in a paper in 1951. It is classic example how regressions, without including necessary terms, can be misleading. At its core, the paradox arises when a trend that appears in different subgroups of data is either <em>reversed</em> or <em>disappears</em> when the subgroups are combined. This seemingly counter-intuitive occurrence can lead to misleading conclusions and thus underscores the importance of careful analysis and interpretation of data.</p>
<p><strong>Problem Statement:</strong> Example-6. Here is data of <code>palmerpenguins</code> where let’s try to establish relationship between penguins bills’ depth and lengths i.e. <code>bill_depth_mm</code> and <code>bill_length_mm</code>. See the plot in fig <a href="linear-regression.html#fig:ex3">20.23</a>.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:ex3"></span>
<img src="DauR_files/figure-html/ex3-1.png" alt="Regression having variable hidden (left) and exposed (right)" width="47%"><img src="DauR_files/figure-html/ex3-2.png" alt="Regression having variable hidden (left) and exposed (right)" width="47%"><p class="caption">
Figure 20.23: Regression having variable hidden (left) and exposed (right)
</p>
</div>
<p>In plot (left) a negative relationship is seen, but when the lurking variable is exposed (right), we can see a positive relationship for each of the species . Thus, species is a significant confounding variable to assess linear relationship here. Thus before finalizing the model, we have to be sure that we are not missing any important variable. To avoid incorrect results due to underlying Simpson’s Paradox, we must ensure to:</p>
<p><strong>Identify Confounding Variables:</strong> Be vigilant in identifying potential confounding variables that could affect the relationship between the variables under study.</p>
<p><strong>Consider All Levels:</strong> Analyze data at different levels, including subgroup and aggregate levels, to gain a comprehensive understanding of the relationship.</p>
<p><strong>Utilise Statistical Techniques:</strong> such as regression analysis or propensity score matching, to control for confounding variables and obtain more accurate insights.</p>
<p><strong>Transparent Reporting:</strong> Clearly report the methodology, assumptions, and limitations of the analysis to ensure that others can critically evaluate the findings.</p>
<p>Simpson’s Paradox, thus, serves as a powerful reminder that data analysis is an intricate process that requires careful consideration of underlying factors.</p>
</div>
<div id="conclusion-and-final-thoughts" class="section level2" number="20.13">
<h2>
<span class="header-section-number">20.13</span> Conclusion and Final thoughts<a class="anchor" aria-label="anchor" href="#conclusion-and-final-thoughts"><i class="fas fa-link"></i></a>
</h2>
<p>In above sections, we learned techniques of regression analysis, which is a powerful and useful tool in data analytics while auditing. We may use regression analysis, inter alia, for -</p>
<p><strong>Detection of Anomalies and Outliers:</strong> Regression analysis can help auditors in identifying anomalies, outliers, or unexpected patterns in financial data. Unusual relationships between variables can signal potential errors, fraud, or irregularities that require further investigation.</p>
<p><strong>Risk Assessment:</strong> By analyzing the relationships between various financial or operational variables, regression analysis can assist auditors in assessing the level of risk associated with different aspects of an organization’s operations. This helps auditors prioritize their efforts and allocate resources effectively.</p>
<p><strong>Control Testing:</strong> Regression analysis can aid us in testing the effectiveness of internal controls within an organization. By examining the relationship between control variables and outcomes, auditors can assess whether controls are functioning as intended. We can also use regression analysis to compare an organization’s financial performance against industry benchmarks or similar companies. Deviations from expected relationships can highlight areas that warrant closer examination.</p>
<hr>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="part-iv-machine-learning-in-r.html">Part-IV: Machine Learning in R</a></div>
<div class="next"><a href="principal-component-analysis-in-r.html"><span class="header-section-number">21</span> Principal Component Analysis in R</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#linear-regression"><span class="header-section-number">20</span> Linear Regression</a></li>
<li><a class="nav-link" href="#basic-concepts-1"><span class="header-section-number">20.1</span> Basic concepts</a></li>
<li><a class="nav-link" href="#simple-linear-regression-in-r"><span class="header-section-number">20.2</span> Simple Linear Regression in R</a></li>
<li><a class="nav-link" href="#assumptions-of-linear-regression"><span class="header-section-number">20.3</span> Assumptions of Linear Regression</a></li>
<li>
<a class="nav-link" href="#interpreting-the-output-of-lm"><span class="header-section-number">20.4</span> Interpreting the output of lm</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#call"><span class="header-section-number">20.4.1</span> call</a></li>
<li><a class="nav-link" href="#residuals"><span class="header-section-number">20.4.2</span> Residuals</a></li>
<li><a class="nav-link" href="#coefficients"><span class="header-section-number">20.4.3</span> Coefficients</a></li>
<li><a class="nav-link" href="#signif.-codes"><span class="header-section-number">20.4.4</span> Signif. codes</a></li>
<li><a class="nav-link" href="#residual-standard-error"><span class="header-section-number">20.4.5</span> Residual standard error</a></li>
<li><a class="nav-link" href="#r-squared-both-multiple-and-adjusted"><span class="header-section-number">20.4.6</span> R-Squared both Multiple and Adjusted</a></li>
<li><a class="nav-link" href="#f-statistic-and-p-value"><span class="header-section-number">20.4.7</span> F-Statistic and p-value</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#model-evaluation-metrics"><span class="header-section-number">20.5</span> Model Evaluation Metrics</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#mae---mean-absolute-error"><span class="header-section-number">20.5.1</span> MAE - Mean Absolute Error</a></li>
<li><a class="nav-link" href="#mse---mean-square-error-and-rmse---root-mean-square-error"><span class="header-section-number">20.5.2</span> MSE - Mean Square Error and RMSE - Root Mean Square Error</a></li>
<li><a class="nav-link" href="#mape---mean-absolute-percentage-error"><span class="header-section-number">20.5.3</span> MAPE - Mean Absolute Percentage Error</a></li>
<li><a class="nav-link" href="#r---squared-and-adjusted-r-squared"><span class="header-section-number">20.5.4</span> R - Squared and adjusted R-squared</a></li>
</ul>
</li>
<li><a class="nav-link" href="#plotting-the-results-and-their-interpretion"><span class="header-section-number">20.6</span> Plotting the results and their interpretion</a></li>
<li><a class="nav-link" href="#using-lm-for-predictions"><span class="header-section-number">20.7</span> Using lm for predictions</a></li>
<li><a class="nav-link" href="#multiple-linear-regression"><span class="header-section-number">20.8</span> Multiple Linear Regression</a></li>
<li>
<a class="nav-link" href="#including-categorical-or-factor-variables-in-lm"><span class="header-section-number">20.9</span> Including categorical or factor variables in lm</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#parallel-slopes-regression"><span class="header-section-number">20.9.1</span> Parallel slopes regression</a></li>
<li><a class="nav-link" href="#extending-multiple-linear-regression-by-including-interactions"><span class="header-section-number">20.9.2</span> Extending multiple linear regression by including interactions</a></li>
</ul>
</li>
<li><a class="nav-link" href="#multi-collinearity-and-variance-inflation-factor-vif"><span class="header-section-number">20.10</span> Multi-collinearity and Variance Inflation Factor (VIF)</a></li>
<li><a class="nav-link" href="#one-complete-example"><span class="header-section-number">20.11</span> One Complete Example</a></li>
<li><a class="nav-link" href="#simpsons-paradox"><span class="header-section-number">20.12</span> Simpson’s paradox</a></li>
<li><a class="nav-link" href="#conclusion-and-final-thoughts"><span class="header-section-number">20.13</span> Conclusion and Final thoughts</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>R for Audit Analytics</strong>" was written by Anil Goyal. It was last built on 2024-07-12.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
