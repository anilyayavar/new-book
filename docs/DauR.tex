% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{book}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\usepackage[a4paper, total={6in, 9in}]{geometry}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{apalike}
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Data Analytics in Audit, using R},
  pdfauthor={Anil Goyal},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Data Analytics in Audit, using R}
\author{Anil Goyal}
\date{2024-07-09}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{preface}{%
\chapter*{Preface}\label{preface}}
\addcontentsline{toc}{chapter}{Preface}

\begin{quote}
``R is the Swiss Army knife of data science. It's a versatile and powerful tool that can handle nearly any data-related task, from data cleaning and preparation to statistical modeling and machine learning. And because it's open-source, there's a vast ecosystem of packages and tools available to extend its capabilities even further.'' - Norman Matloff, professor of computer science at the University of California, Davis.
\end{quote}

Welcome to my book on R language, designed specifically for auditors who are interested in learning data analytics using open source resources. In today's digital age, data is abundant and ubiquitous, and its importance cannot be overstated. As a result, auditors must be equipped with the necessary skills to leverage this data and draw insights from it. This book is a result of my love for R and my passion for data analytics. In this book, I have tried to present R programming concepts and case studies that are useful in audit analysis as well as forensic audit and fraud investigation.

This book is written based on my notes on R while I was learning the language myself. I have included sufficient figures and examples to make the concepts easy to understand for readers. Most of the figures have been created by me except a few fore which due credit has been given.

R is an open-source programming language that has been gaining popularity in recent years due to its versatility and flexibility in data analysis. My journey with R began during Covid-19 lockdown, and I was introduced to R for data analysis by one of my colleagues. Since then, R has become an essential tool in my toolbox for data analysis.

In this book, I have tried to make the concepts of R programming and data analytics as accessible as possible for auditors who may have little or zero knowledge of programming. The first part of the book covers R programming concepts which are absolutely necessary to work in R. Second part onwards covers data wrangling/transformation techniques as well as case studies in forensic audit and fraud investigation using R.

I hope that this book will be a useful resource for auditors who want to learn data analytics using open source resources like R. I invite readers to share their suggestions and comments on the book to help me improve it further.

Happy reading and happy learning!

\hypertarget{acknowledgments}{%
\section*{Acknowledgments}\label{acknowledgments}}
\addcontentsline{toc}{section}{Acknowledgments}

Writing this book has been a journey filled with learning, challenges, and invaluable support from those around me. First and foremost, I extend my deepest gratitude to my wife, \textbf{Reena}, whose unwavering encouragement and belief in my abilities persuaded me to embark on this endeavor. Her steadfast support has been the cornerstone of my motivation throughout this process.

I am also profoundly grateful to my esteemed colleagues, \textbf{Chandersheel} and \textbf{Niti}, whose expertise, insights, and feedback have greatly enriched the content of this book. Their willingness to share their knowledge, provide suggestions, and meticulously point out errors and mistakes have been instrumental in refining the quality and accuracy of the material presented herein.

Furthermore, I extend my thanks to all those who have contributed to this project in various capacities, whether through discussions, reviews, or moral support. Your contributions have been invaluable and have undoubtedly played a significant role in shaping this book.

Lastly, I would like to express my appreciation to the readers who will engage with this book. It is my sincere hope that the knowledge imparted within these pages proves valuable and contributes to the advancement of auditing practices utilizing the R language.

Thank you, from the depths of my heart, for being a part of this journey.

\hypertarget{r-not-just-a-letter}{%
\section*{R, not just a letter}\label{r-not-just-a-letter}}
\addcontentsline{toc}{section}{R, not just a letter}

R programming language is the extended version of the S programming language. John Chambers, the creator of the S programming language in 1976 at Bell laboratories. In 1988, the official version of the S language came into existence with the name S-PLUS. The R language is almost the unchanged version of S-PLUS.

In 1991, R was created by \textbf{Ross Ihaka} \index{Ihaka, Ross} and \textbf{Robert Gentleman} \index{Gentleman, Robert} in the Department of Statistics at the University of Auckland. Ross's and Robert's experience developing R is documented in a 1996 paper in the Journal of Computational and Graphical Statistics \citep{10.2307/1390807}. In 1997 the R Core Group was formed, containing some people associated with S and S-PLUS. Currently, the core group controls the source code for R and is solely able to check in changes to the main R source tree. Finally, in 2000 R version 1.0.0 was released to the public. \index{History of R}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/history} 

}

\caption{A Brief History of R}\label{fig:history}
\end{figure}

\hypertarget{why-r}{%
\section*{Why R?}\label{why-r}}
\addcontentsline{toc}{section}{Why R?}

R programming language is an open-source programming language for statistical computation. It supports n number of statistical analysis techniques, machine learning models, and graphical visualization for data analysis. It serves the purpose of the free software environment for statistical computation and graphics. R is easy to understand and implement. The packages are available to create an effective R program, data models, and graphical charts. For research and analytics purposes, it is a popular language among statisticians and data scientists.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/whyR} 

}

\caption{Why R}\label{fig:whyr}
\end{figure}

I always admire \textbf{Hadley Wickham}'s contributions to the development of R programming concepts, that has made data analysis more accessible and efficient. The tidyverse, a collection of R packages developed by Wickham, has transformed the way data analysts and data scientists work with data. The tidyverse promotes a consistent and coherent way of working with data, making it easier to write code that is easier to read, understand, and maintain. Wickham's contributions to R have become the foundation of many data analysis tools in other programming languages, including Python and Julia. I hope that this book will inspire readers to explore the vast potential of R and the contributions made by Wickham in data analytics.

One of the advantages of using free/open source tools like R is that they can be easily customized and extended to suit the specific needs of the user. Additionally, free/open source tools are often updated more frequently than licensed tools, ensuring that users have access to the latest features and bug fixes. Using licensed data analytics tools like Caseware IDEA, Tableau, Power BI and others can be expensive, and their licensing fees can be a significant burden on smaller organizations or individuals. By using open source tools like R, users can significantly reduce their costs while still having access to powerful data analytics capabilities.

Another strength of R is its extensive library of packages, which includes many tools for statistical analysis and data visualization.

\hypertarget{about-author}{%
\chapter*{About author}\label{about-author}}
\addcontentsline{toc}{chapter}{About author}

Anil Goyal is a data analytics enthusiast who has been working in the Indian Audit and Accounts Department since 1998. Anil has a passion for learning and applying programming languages/other tools such as R and Tableau to solve data-related challenges.

Anil is a self-taught expert in R, and has been involved in a variety of data analytics projects and audits.

Anil holds a post-graduate degree in Mathematics from the University of Rajasthan, Jaipur, which he received in 1998. This book is his first book. He continues to expand his skill set and knowledge in this field. Anil also loves to solve problems raised by various users on StackOverflow.com mainly related to R language.

When not working with data, Anil enjoys pursuing his personal interests, which include photographying birds, nature; reading and watching movies, etc..

\mainmatter

\hypertarget{gearing-up}{%
\chapter*{Gearing up}\label{gearing-up}}
\addcontentsline{toc}{chapter}{Gearing up}

\hypertarget{download-and-installation}{%
\section{Download and installation}\label{download-and-installation}}

The R programming language for your local computer can be downloaded from web portal of \textbf{The Comprehensive R Archive Network},\index{CRAN} in short mostly referred to as \textbf{CRAN},\index{CRAN} which is a network of ftp and web servers around the world that store identical, up-to-date, versions of code and documentation for R. The portal address is \url{https://cran.r-project.org/} -

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{images/CRAN} 

}

\caption{CRAN Portal}\label{fig:cranportal}
\end{figure}

Download the specific (as per the operating system) file from the port and install it following the instructions. The R programming interface looks like-

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/workspace} 

}

\caption{R Workspace}\label{fig:workspace}
\end{figure}

\hypertarget{writing-your-first-code}{%
\section{Writing your first code}\label{writing-your-first-code}}

Writing code in R is pretty easy. Just type the command in front of \texttt{\textgreater{}}, as shown in figure \ref{fig:workspace} prompt and press \texttt{Enter(Return)} key. R will display the results in next line.

\begin{figure}

{\centering \includegraphics[width=0.49\linewidth,height=0.49\textheight]{images/first_code} \includegraphics[width=0.49\linewidth,height=0.49\textheight]{images/indent} 

}

\caption{Left - Writing first Code in R; Right - Indenting code not necessary but recommended}\label{fig:first}
\end{figure}

\hypertarget{remember}{%
\section{Remember}\label{remember}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  R is case sensitive. This will have to be remebered while writing/storing/calling functions or other objects. So \texttt{Anil}, \texttt{ANIL}, \texttt{anil} all are different objects in R.
\item
  White spaces between different pieces of codes don't matter. See figure-\ref{fig:first} above. Both \texttt{3+4} and \texttt{3\ +\ 4} will evaluate same. However, for better readability it is always better to use spaces.
\item
  Parenthesis \texttt{()} are generally used to change the natural order of precedence. Moreover, these are also used in passing arguments to functions, which will be discussed in detail in chapter-\ref{func} and onwards.
\item
  Multi-line code(s) aren't required to be indented in R. In R, indents have no meaning. However, following best practices to write a code that is understandable by readers, proper indentation is suggested. See figure-\ref{fig:first} (right) above.
\item
  If an incomplete code is written in the first line of the code (useful when a single line is not sufficient to write complete code), R will automatically prompt as displaying \texttt{+} at the beginning of line, instead of a \texttt{\textgreater{}}. See figure-\ref{fig:first} (right) above.
\item
  Indices in R always start from 1 (and not from 0). This has been discussed in detail in chapter-\ref{subset}.
\item
  Code that start with hash symbol \texttt{\#} do not execute. Even in a line if \texttt{\#} appears in between the line, the code from that place does not get executed. See the following example. Comments may be used in codes for either of the purposes -

  \begin{itemize}
  \tightlist
  \item
    Code Readability
  \item
    Explanation of code
  \item
    Inclusion of metadata, other references, etc.
  \item
    Prevent execution of certain line of code
  \end{itemize}
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# 1 + 3 (this won\textquotesingle{}t be executed)}
\DecValTok{1} \SpecialCharTok{+} \DecValTok{3} \CommentTok{\# +5}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 4
\end{verbatim}

\begin{quote}
Tip: to clear the workspace, just click \texttt{ctrl} + \texttt{l}.
\end{quote}

Normally R code files have an extension \texttt{.R} but other R files may have other extensions, such as project files \texttt{.Rproj}, markdown files \texttt{.Rmd}, and many more.

All of the programming/code writing may be done in R. But you may have noticed that code once executed cannot be edited. The code has to written again (Tip: To get previous executed command just use scroll up key on keyboard). Thus, in order to use many other smart features, we will write our code as R scripts i.e.~in \texttt{.R} files using most popular IDE for R which is \texttt{R\ Studio}.

\begin{quote}
Using R studio IDE is so much popular that many persons using R, do not distinguish between R and its IDE. Even Stack Overflow which is a popular forum to seek online help explicitly asks users not to tag `R studio' in general R code problems\footnote{\url{https://stackoverflow.com/tags/rstudio/info}}.
\end{quote}

\hypertarget{r-studio-ide}{%
\section{R studio IDE}\label{r-studio-ide}}

RStudio\index{R studio} is free and open source IDE (Integrated Development Environment) for R, which is available for Windows, Mac OS and LINUX. It can be downloaded from its portal \href{https://www.rstudio.com/products/rstudio/download/}{https://www.rstudio.com}. For our most of the data analytics needs, we require Rstudio desktop version, which is available for free to download and installation.

It includes a console, syntax-highlighting editor that supports direct code execution, and a variety of robust tools for plotting, viewing history, debugging and managing your work-space. After downloading and installing it the local machine, a work-space/UI similar to that shown in following figure, is opened.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/rstudio} 

}

\caption{R Studio interface}\label{fig:rstud}
\end{figure}

There are four panels

\begin{itemize}
\tightlist
\item
  Top-left:

  \begin{itemize}
  \tightlist
  \item
    \textbf{scripts and files:} The script files which we will be working on, will be opened and displayed here. To open a new script, you just need to click the new script button \includegraphics{images/new_script.png} which is just below the \emph{file menu.}; or using keyboard shortcut \texttt{ctrl\ +\ Shift\ +\ n}
  \end{itemize}
\item
  Bottom-left:

  \begin{itemize}
  \tightlist
  \item
    \textbf{R console:} is where the R commands can be written and see the output. Even the commands run on script will show the output in this panel.
  \item
    \textbf{Terminal:} Her we can access our system shell.
  \end{itemize}
\item
  Top-right:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Environment:} To see the objects saved in current environment. This panel is also used to import data in current environment.
  \item
    \textbf{history} To view the history of commands run, in the current session
  \item
    \textbf{Connections:} Used to connect/import with external database/data
  \end{itemize}
\item
  Bottom-right:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Files} having tree of folders, to see the file structure of current working directory
  \item
    \textbf{Plots} graph window, if the output of r command is a plot/graph, it will be generated here
  \item
    \textbf{packages}, to download and load the external packages using mouse click
  \item
    \textbf{Help}, window to get help on desired functions. Even the help sought through r command will be displayed in this window.
  \item
    \textbf{viewer:} can be used to view local web content.
  \end{itemize}
\end{itemize}

\hypertarget{packages-and-libraries}{%
\section{Packages and libraries}\label{packages-and-libraries}}

As already stated, one of the strength of R is that numerous user-written packages (or \emph{libraries})\index{packages, external} \index{library} are available on \textbf{Comprehensive R Archive Network} i.e.~\href{https://cran.r-project.org/}{CRAN}\index{CRAN}. Package installation is perhaps easiest of the jobs in R.

The command \index{install.packages() function} is fairly simple -

\begin{verbatim}
install.packages("library_name")
\end{verbatim}

which downloads the given package name (to be given in quotes and is case-sensitive), compiles it and then load it into the specified/default directory. This will however, not load into the memory. The library once downloaded need not be downloaded every time but need to be loaded every time using the command- \index{library()}

\begin{verbatim}
library(library_name)
\end{verbatim}

Quotes here are optional but package name is still case sensitive. So to install and load \texttt{tidyverse} we need to run first command once (which will download the package into your local computer) but second command (to load it in the current R session) at every new session.

\begin{verbatim}
install.packages('tidyverse')
library(tidyverse)
\end{verbatim}

\hypertarget{double-colon-operator}{%
\subsection{\texorpdfstring{Double Colon operator \texttt{::}}{Double Colon operator ::}}\label{double-colon-operator}}

In R, we can use double colon operator \index{double colon operator, ::} i.e.~\texttt{::} to access functions that are defined as part of the internal functions that a package uses. These may be used in at least two cases-

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  To call a function say \texttt{filter} from package \texttt{dplyr} we may use \texttt{dplyr::filter()} without actually loading it.\\
\item
  In cases of conflicts (e.g.~when two or more packages have same function names) if we want to use the function specifically from a package. E.g. While loading \texttt{dplyr} the function \texttt{filter} masks the function with same available in \texttt{stats} package (a part of base R). So, if the requirement is to use function masked \texttt{filter} from \texttt{stats} we can use \texttt{stats::filter()}.
\end{enumerate}

\hypertarget{help}{%
\section{Getting Help within R}\label{help}}

Once R is installed, there is a comprehensive built-in help system. We can use any of the following commands-

\begin{verbatim}
help.start()   # general help
help(foo)      # help about function `foo`
?foo           # same as above
apropos("foo") # show all functions containing word `foo`
example(foo)   # show an example of function `foo`
\end{verbatim}

Alternatively, features under the Help menu or help pane, can also be used.

\hypertarget{tidyverse}{%
\section{tidyverse}\label{tidyverse}}

The \href{https://www.tidyverse.org/}{tidyverse} is a \emph{package of packages} that work in harmony because they share common data representations and `API' design. This package is designed to make all these easy to install and load multiple `tidyverse' packages in a single step.

Though \texttt{tidyverse} is a collection 20+ packages (in fact 80+ packages will be installed including depended packages) which are all installed by \texttt{install.packages("tidyverse")} command, yet \texttt{library(tidyverse)} load \href{https://www.tidyverse.org/packages/}{nine} of them. Others (like \texttt{readxl}) will have to loaded explicitly.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \href{https://ggplot2.tidyverse.org/}{\textbf{ggplot2}} is a system for declaratively creating graphics, based on \href{https://link.springer.com/book/10.1007/0-387-28695-0}{\emph{The Grammar of Graphics}}.
\item
  \href{https://dplyr.tidyverse.org/}{\textbf{dplyr}} provides a grammar of data manipulation, providing a consistent set of verbs that solve the most common data manipulation challenges.
\item
  \href{https://tidyr.tidyverse.org/}{\textbf{tidyr}} provides a set of functions useful for data transformation.
\item
  \href{https://readr.tidyverse.org/}{\textbf{readr}} is used to read and write rectangular/tabular data formats.
\item
  \href{https://purrr.tidyverse.org/}{\textbf{purrr}} is functional programming (FP) toolkit for working with functions and vectors.
\item
  \href{https://tibble.tidyverse.org/}{\textbf{tibble}} provides functionalities related to displaying data frames.
\item
  \href{https://stringr.tidyverse.org/}{\textbf{stringr}} provides set of functions designed to work with strings. It is built on top of another package \href{https://cran.r-project.org/package=stringi}{stringi}.
\item
  \href{https://forcats.tidyverse.org/}{\textbf{forcats}} provides a suite of useful tools that solve common problems with factors.
\item
  \href{https://lubridate.tidyverse.org/}{\textbf{lubridate}} makes it easier to do the things R does with date-times.
\end{enumerate}

With latest version of Tidyverse, while loading it \href{https://lubridate.tidyverse.org/}{\textbf{lubridate}} also loads with default.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/tidyverse} 

}

\caption{tidyverse}\label{fig:tidyverse}
\end{figure}

There are several other \texttt{tidyverse} packages which we will be working with-

\begin{itemize}
\tightlist
\item
  \texttt{hms}
\item
  \texttt{readxl}
\end{itemize}

\hypertarget{part-i-basic-r-programming-concepts}{%
\chapter*{Part-I: Basic R Programming Concepts}\label{part-i-basic-r-programming-concepts}}
\addcontentsline{toc}{chapter}{Part-I: Basic R Programming Concepts}

\hypertarget{r-programming-language}{%
\chapter{R Programming Language}\label{r-programming-language}}

\hypertarget{calculator}{%
\section{Use R as a calculator}\label{calculator}}

To start learning R, just start entering equations directly at the command prompt \texttt{\textgreater{}} and press enter. So, \texttt{3+4} will give you result \texttt{7}. Common mathematical operators are listed in table \ref{tab:table3}.\index{common mathematical operators}

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1667}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.5000}}@{}}
\caption{\label{tab:table3} Common Mathematical Operators in R}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
Operator/ function
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Meaning
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
Operator/ function
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Meaning
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{+} & Addition & \texttt{4\ +\ 5} is \texttt{9} \\
\texttt{-} & Substraction & \texttt{4\ -\ 5} is \texttt{-1} \\
\texttt{*} & Multiplication & \texttt{4\ *\ 5} is \texttt{20} \\
\texttt{/} & Division & \texttt{4/5} is \texttt{0.8} \\
\texttt{\^{}} & Exponent & \texttt{2\^{}4} is \texttt{16} \\
\texttt{\%\%} & Modulus (Remainder from division) & \texttt{15\ \%\%\ 12} is \texttt{3} \\
\texttt{\%/\%} & Integer Division & \texttt{15\ \%/\%\ 12} is \texttt{1} \\
\end{longtable}

Strings or Characters have to be enclosed in single \texttt{\textquotesingle{}} or double\texttt{"} quotes (more on strings in section \ref{string}). So a few examples of calculations that can be performed in R could be-

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{4} \SpecialCharTok{+} \DecValTok{3} \SpecialCharTok{\^{}} \DecValTok{2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 13
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{8} \SpecialCharTok{*}\NormalTok{ (}\DecValTok{9} \SpecialCharTok{+} \DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 104
\end{verbatim}

\begin{quote}
Note that R follows common mathematical order of precedence while evalauting expressions. That may be changed using simple parenthesis i.e.~\texttt{()}. Also note that other brackets/braces i.e.~curly braces \texttt{\{\}} and \texttt{{[}{]}} have been assigned different meaning, so to change nested order of operations only \texttt{()} may be used.
\end{quote}

\hypertarget{basic-concepts}{%
\section{Basic Concepts}\label{basic-concepts}}

\hypertarget{object-assignment}{%
\subsection{Object Assignment}\label{object-assignment}}

R is an object-oriented language.\citep{R-base} This means that \emph{objects} \index{objects in R}are created and stored in R environment so that they can be used later.

So what is an object? An object can be something as simple as a number (value) that can be assigned to a variable. Think of it like this; Suppose we have greet each user by his/her name prefixing \emph{hello} to his/her name. Now user's name may be saved in our work environment for later use. Thus, once the user name is saved in a variable then can be retrieved later on, by calling the variable name instead of asking the user name again and again. An object can be also be a data-set or complex model output or some function. Thus, an object created in R can hold multiple values.

The other important thing about objects is that objects are created in R, using the assignment operator\index{assignment operator} \texttt{\textless{}-}. Use of equals sign \texttt{=} to set something as an object is not recommended thought it will work properly in some cases. For now we will stick with the assignment operator, and interpret it as the left side is the object name that is storing the object information specified on the right side. \emph{If \texttt{-\textgreater{}} right hand side assignment is used, needless to say things mentioned above will interchange.}\index{right assignment operator}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# user name}
\NormalTok{user\_name }\OtherTok{\textless{}{-}} \StringTok{\textquotesingle{}Anil Goyal\textquotesingle{}}

\CommentTok{\# when the above variable is called}
\NormalTok{user\_name}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Anil Goyal"
\end{verbatim}

\begin{quote}
\textbf{Case sensitive nature:}\index{case sensitive nature} Names of variables even all objects in R are case sensitive, and thus \texttt{user}, \texttt{USER} and \texttt{useR}; all are different variables.
\end{quote}

\hypertarget{atomic-data-types-in-r}{%
\section{Atomic data types in R}\label{atomic-data-types-in-r}}

\index{atomic data types}We have seen that objects in R can be created to store some values/data. Even these objects can contain other objects as well. So a question arises, what is the most atomic/basic data type in R. By atomic we mean that the object cannot be split any further. Thus, the atomic objects created in R can be thought of variables holding one single value. E.g. user's name, user's age, etc. Now atomic objects created in R can be of six types-

\begin{itemize}
\tightlist
\item
  logical (or Boolean i.e.~TRUE FALSE etc.)
\item
  integer (having non-decimal numeric values like 0, 1, etc.)
\item
  double ( or floating decimal type i.e.~having numeric values in decimal i.e.~1.0 or 5.25, etc.)
\item
  character (or string data type having some alphanumeric value)
\item
  complex (numbers having both real and imaginary parts e.g.~1+1i)
\item
  raw (not discussed here)
\end{itemize}

\begin{figure}

{\centering \includegraphics[width=11.92in,height=0.6\textheight]{images/datatypes} 

}

\caption{Data types in R}\label{fig:datatypes}
\end{figure}

Let us discuss all of these.

\begin{quote}
Note: We will use a pre-built function \texttt{typeof()} to check the type of given value/variable. However, functions as such will be discussed later-on.
\end{quote}

\hypertarget{logical}{%
\subsection{Logical}\label{logical}}

In R logical \index{logical data type}values are stored as either \texttt{TRUE} or \texttt{FALSE} (all in caps)

\begin{Shaded}
\begin{Highlighting}[]
\ConstantTok{TRUE}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{typeof}\NormalTok{(}\ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "logical"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_val }\OtherTok{\textless{}{-}} \ConstantTok{TRUE}
\FunctionTok{typeof}\NormalTok{(my\_val)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "logical"
\end{verbatim}

\textbf{\texttt{NA}}: There is one special type of logical value i.e.~\texttt{NA}\index{missing values in R} (short for \emph{Not Available})\index{NA in R}. This is used for missing data.

\begin{quote}
Remember missing data is not an empty string. The difference between the two is explained in section \ref{string}.
\end{quote}

\hypertarget{integer}{%
\subsection{Integer}\label{integer}}

Numeric \index{Numeric data types}values can either be integer\index{integer data type} (i.e.~without a floating point decimal) or with a floating decimal value (called \texttt{double} in r)\index{double data type in R}. Now integers in R are differentiated by a suffix \texttt{L}\index{L suffix}. E.g.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_val1 }\OtherTok{\textless{}{-}}\NormalTok{ 2L}
\FunctionTok{typeof}\NormalTok{(my\_val1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "integer"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{typeof}\NormalTok{(}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "double"
\end{verbatim}

\hypertarget{double}{%
\subsection{Double}\label{double}}

Numeric values with decimals are stored in objects of type \texttt{double}. It should be kept in mind that if storing an integer value directly to a variable, suffix \texttt{L} must be used otherwise the object will be stored as \texttt{double} type as shown in above example.

In double type, exponential formats or hexadecimal formats to store these numerals may also be used.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_val2 }\OtherTok{\textless{}{-}} \FloatTok{2.5}
\NormalTok{my\_val3 }\OtherTok{\textless{}{-}} \FloatTok{1.23e4}
\NormalTok{my\_val4 }\OtherTok{\textless{}{-}} \DecValTok{0xcafe} \CommentTok{\# hexadecimal format (prefixed by 0x)}

\FunctionTok{typeof}\NormalTok{(my\_val2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "double"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{typeof}\NormalTok{(my\_val3)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "double"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{typeof}\NormalTok{(my\_val4)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "double"
\end{verbatim}

\begin{quote}
Note: Suffix \texttt{L} may also be used with numerals in hexadecimal (e.g.~\texttt{0xcafeL}) or exponential formats (e.g.~\texttt{1.23e4L}), which will coerce these numerals in \texttt{integer} format.
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{typeof}\NormalTok{(0xcafeL)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "integer"
\end{verbatim}

Thus, both \texttt{integer} and \texttt{double} data types may be understood in R as having sub-types of \texttt{numeric} data. There are three other types of special numerals (specifically doubles) \texttt{Inf}\index{Inf data type}, \texttt{-Inf} and \texttt{NaN}\index{NaN data type}. The first two are infinity (positive and negative) and the last one denotes an indefinite number (\texttt{NaN} short for \emph{Not a Number}).

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{1}\SpecialCharTok{/}\DecValTok{0}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] Inf
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\SpecialCharTok{{-}}\DecValTok{45}\SpecialCharTok{/}\DecValTok{0}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -Inf
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{0}\SpecialCharTok{/}\DecValTok{0}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] NaN
\end{verbatim}

\hypertarget{string}{%
\subsection{Character}\label{string}}

Strings \index{characters in R}are\index{strings} stored in R as a character type. Strings should either be surrounded by single quotes \texttt{\textquotesingle{}\textquotesingle{}} or double quotes \texttt{""}\index{quotes}\footnote{Single and double quotes can be used interchangeably and won't have any difference in the objects created using any of these. Only thing that should be kept in mind is that the quote used to start the string must be used to close the string, otherwise an error may be thrown. However, this may be used to store objects with either type of string in the data itself.}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_val5 }\OtherTok{\textless{}{-}} \StringTok{\textquotesingle{}Anil Goyal\textquotesingle{}}
\NormalTok{my\_val6 }\OtherTok{\textless{}{-}} \StringTok{"Anil Goyal"}
\NormalTok{my\_val7 }\OtherTok{\textless{}{-}} \StringTok{""} \CommentTok{\# empty string}
\NormalTok{my\_missing\_val }\OtherTok{\textless{}{-}} \ConstantTok{NA} \CommentTok{\# missing value}

\FunctionTok{typeof}\NormalTok{(my\_val5)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "character"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{typeof}\NormalTok{(my\_val6)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "character"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{typeof}\NormalTok{(my\_val7)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "character"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{typeof}\NormalTok{(my\_missing\_val)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "logical"
\end{verbatim}

\begin{quote}
\href{\%5BNotes:\%5D(Notes:)\%7B.uri\%7D}{{[}Notes:\textbackslash\textbackslash{]}(Notes:)\{.uri\}} 1. Though \texttt{NA} is basically of type logical yet it will be used to store missing values in any other data type also as shown in subsequent chapter(s). 2. Special characters are escaped with \texttt{\textbackslash{}}; Type \texttt{?Quotes} in console and check documentation for full details. 3. A simple use of \texttt{\textbackslash{}} escape character may be to use \texttt{"} or \texttt{\textquotesingle{}} within these quotes. Check Example-3 below.
\end{quote}

Example-1: Usage of double and single quote interchangeably.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_val8 }\OtherTok{\textless{}{-}} \StringTok{"R\textquotesingle{}s book"}
\NormalTok{my\_val8}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "R's book"
\end{verbatim}

Example-2: Usage of escape character.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{"This is first line.}\SpecialCharTok{\textbackslash{}n}\StringTok{This is new line"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## This is first line.
## This is new line
\end{verbatim}

Example-3: Usage of escape character to store single/double quotes as string themselves.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{"}\SpecialCharTok{\textbackslash{}\textquotesingle{}}\StringTok{ is single quote and }\SpecialCharTok{\textbackslash{}"}\StringTok{ is double quote"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## ' is single quote and " is double quote
\end{verbatim}

\textbf{Note:} If absence of indices has been noticed in above code output, learn more about \texttt{cat} function \protect\hyperlink{cat}{here}.

\hypertarget{null}{%
\subsection{NULL}\label{null}}

\texttt{NULL} (note: all caps) is a specific data type used to create an empty vector\index{NULL in R}\index{empty vector in R}. Even this \texttt{NULL} can be used as a vector in itself.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{typeof}\NormalTok{(}\ConstantTok{NULL}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "NULL"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{vec }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{:}\DecValTok{5}
\NormalTok{vec}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1 2 3 4 5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{vec }\OtherTok{\textless{}{-}} \ConstantTok{NULL}
\NormalTok{vec}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## NULL
\end{verbatim}

\hypertarget{complex}{%
\subsection{Complex}\label{complex}}

Complex numbers \index{complex numbers - data type}are made up of real and imaginary parts. As these will not be used in the data analysis tasks, it is not discussed in detail here.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_complex\_no }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{+}\NormalTok{1i}
\FunctionTok{typeof}\NormalTok{(my\_complex\_no)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "complex"
\end{verbatim}

\hypertarget{data-structuresobject-types-in-r}{%
\section{Data structures/Object Types in R}\label{data-structuresobject-types-in-r}}

Objects\index{data structures in R} in R can be either homogeneous or heterogeneous.

\begin{figure}

{\centering \includegraphics[width=0.49\linewidth,height=0.49\textheight]{images/homgeneous} \includegraphics[width=0.49\linewidth,height=0.49\textheight]{images/hetero} 

}

\caption{Objects/Data structures in R, can either be homogeneous (left) or heterogeneous (right)}\label{fig:datastr}
\end{figure}

\hypertarget{homogeneous-objects}{%
\subsection*{Homogeneous objects}\label{homogeneous-objects}}
\addcontentsline{toc}{subsection}{Homogeneous objects}

\hypertarget{vectors}{%
\subsection{Vectors}\label{vectors}}

What is a vector? A vector is simply a collection of values/data of same type.\index{vectors in R}

\begin{figure}

{\centering \includegraphics[width=0.99\linewidth]{images/vec_buck} 

}

\caption{Vectors are homegeneous data structures in R}\label{fig:vecs}
\end{figure}

\hypertarget{simple-vectors-unnamed-vectors}{%
\subsubsection{Simple vectors (Unnamed vectors)}\label{simple-vectors-unnamed-vectors}}

Though, \texttt{Vector} is the most atomic data type used in R, yet it can hold multiple values (of same type) simultaneously. In fact vector is a collection of multiple values of same type. So why vector is atomic when it can hold multiple values? You may have noticed a \texttt{{[}1{]}} printed at the start of line of output whenever a variable was called/printed. This \texttt{{[}1{]}} actually is the index of that element. Thus, in R instead of having \emph{scalar(s)} as most atomic type, we have \emph{vector(s)} containing only one element. Whenever a vector is called all the values stored in it are displayed with its index at the start of each new line only.

Even processing of multiple values simultaneously, stored in a vector, to produce a desired output, is one of the most powerful strengths of R. The three variables shown in the figure below, all are vectors.

\begin{figure}

{\centering \includegraphics[width=0.99\linewidth]{images/example_vecs} 

}

\caption{Examples of Vectors}\label{fig:exvecs}
\end{figure}

How to create a vector? Vectors in R are created using either -

\begin{itemize}
\tightlist
\item
  \texttt{c()} function\index{c() function} which is shortest and most commonly used function in r. The elements are concatenated (and hence the shortcut \texttt{c} for this function) using a comma \texttt{,} ; \emph{OR}
\item
  \texttt{vector()}\index{vector() function} produces vector of given \texttt{length} and \texttt{mode}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_vector }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\NormalTok{my\_vector}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1 2 3
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_vector2 }\OtherTok{\textless{}{-}} \FunctionTok{vector}\NormalTok{(}\AttributeTok{mode =} \StringTok{\textquotesingle{}integer\textquotesingle{}}\NormalTok{, }\AttributeTok{length =} \DecValTok{15}\NormalTok{)}
\NormalTok{my\_vector2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
\end{verbatim}

Function \texttt{c()} can also be used to \textbf{join two or more vectors}\index{vector concatenation}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{vec1 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{)}
\NormalTok{vec2 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{11}\NormalTok{, }\DecValTok{12}\NormalTok{)}
\NormalTok{vec3 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(vec1, vec2)}
\NormalTok{vec3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  1  2 11 12
\end{verbatim}

\begin{figure}

{\centering \includegraphics[width=6.86in,height=0.2\textheight]{images/vector_concatenation} 

}

\caption{Vector Concatenation}\label{fig:vecconcat}
\end{figure}

\hypertarget{useful-functions-to-create-new-vectors}{%
\subsubsection*{Useful Functions to create new vectors}\label{useful-functions-to-create-new-vectors}}
\addcontentsline{toc}{subsubsection}{Useful Functions to create new vectors}

There are some more useful functions to create new vectors in R, which we should discuss here as we will be using these vectors in subsequent chapters.

\hypertarget{generate-integer-sequences-with-colon-operator}{%
\subsubsection*{\texorpdfstring{Generate integer sequences with Colon Operator \texttt{:}}{Generate integer sequences with Colon Operator :}}\label{generate-integer-sequences-with-colon-operator}}
\addcontentsline{toc}{subsubsection}{Generate integer sequences with Colon Operator \texttt{:}}

This function generates a sequence from the number preceding \texttt{:} \index{colon operator}\index{: operator}to next specified number, in arithmetical difference of \texttt{1} or \texttt{-1} as the case may be. Notice that output vector type is of \texttt{integer}.

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{1}\SpecialCharTok{:}\DecValTok{25}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{25}\SpecialCharTok{:}\DecValTok{30}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 25 26 27 28 29 30
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{10}\SpecialCharTok{:}\DecValTok{1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 10  9  8  7  6  5  4  3  2  1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{typeof}\NormalTok{(}\DecValTok{2}\SpecialCharTok{:}\DecValTok{250}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "integer"
\end{verbatim}

\begin{quote}
Note: One of the common mistakes with colon operator is assuming its \textbf{operator precedence}. In R, colon operator has calculation precedence over any mathematical operator. Think of outputs you may get with these-
\end{quote}

\begin{verbatim}
n <- 5
1:n+1
1:n*2
\end{verbatim}

\hypertarget{generate-specific-sequences-with-function-seq}{%
\subsubsection*{\texorpdfstring{Generate specific sequences with function \texttt{seq}}{Generate specific sequences with function seq}}\label{generate-specific-sequences-with-function-seq}}
\addcontentsline{toc}{subsubsection}{Generate specific sequences with function \texttt{seq}}

This function\index{seq() function} generates a sequence from a given number to another number, similar to \texttt{:}, but it gives us more control over the output desired. We can provide the difference specifically (\texttt{double} type also) in the \texttt{by} argument. Otherwise if \texttt{length.out} argument is provided it calculates the difference automatically.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{5}\NormalTok{, }\AttributeTok{by =} \FloatTok{0.3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 1.0 1.3 1.6 1.9 2.2 2.5 2.8 3.1 3.4 3.7 4.0 4.3 4.6 4.9
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\AttributeTok{length.out =} \DecValTok{11}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 1.0 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2.0
\end{verbatim}

\hypertarget{repeat-a-patternvector-with-function-rep}{%
\subsubsection*{\texorpdfstring{Repeat a pattern/vector with function \texttt{rep}}{Repeat a pattern/vector with function rep}}\label{repeat-a-patternvector-with-function-rep}}
\addcontentsline{toc}{subsubsection}{Repeat a pattern/vector with function \texttt{rep}}

As the name suggests \texttt{rep}\index{rep() function} is short for \emph{repeat} and thus it repeat a given element, a given number of times.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rep}\NormalTok{(}\StringTok{\textquotesingle{}repeat this\textquotesingle{}}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "repeat this" "repeat this" "repeat this" "repeat this" "repeat this"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# We can even repeat already created vectors}
\NormalTok{vec }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\FunctionTok{rep}\NormalTok{(vec, }\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1]  1 10  1 10  1 10  1 10  1 10
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rep}\NormalTok{(vec, }\AttributeTok{each =} \DecValTok{5}\NormalTok{) }\CommentTok{\# notice the difference in results}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1]  1  1  1  1  1 10 10 10 10 10
\end{verbatim}

\hypertarget{generate-english-alphabet-with-letters-letters}{%
\subsubsection*{\texorpdfstring{Generate english alphabet with \texttt{LETTERS} / \texttt{letters}}{Generate english alphabet with LETTERS / letters}}\label{generate-english-alphabet-with-letters-letters}}
\addcontentsline{toc}{subsubsection}{Generate english alphabet with \texttt{LETTERS} / \texttt{letters}}

These are two inbuilt vectors in R having all 26 alphabets in upper and lower cases respectively.\index{LETTERS}\index{letters}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{LETTERS}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "A" "B" "C" "D" "E" "F" "G" "H" "I" "J" "K" "L" "M" "N" "O" "P" "Q" "R" "S"
## [20] "T" "U" "V" "W" "X" "Y" "Z"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{letters}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "a" "b" "c" "d" "e" "f" "g" "h" "i" "j" "k" "l" "m" "n" "o" "p" "q" "r" "s"
## [20] "t" "u" "v" "w" "x" "y" "z"
\end{verbatim}

\hypertarget{generate-gregorian-calendar-month-names-with-month.name-month.abb}{%
\subsubsection*{\texorpdfstring{Generate gregorian calendar month names with \texttt{month.name} / \texttt{month.abb}}{Generate gregorian calendar month names with month.name / month.abb}}\label{generate-gregorian-calendar-month-names-with-month.name-month.abb}}
\addcontentsline{toc}{subsubsection}{Generate gregorian calendar month names with \texttt{month.name} / \texttt{month.abb}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{month.name}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "January"   "February"  "March"     "April"     "May"       "June"     
##  [7] "July"      "August"    "September" "October"   "November"  "December"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{month.abb}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "Jan" "Feb" "Mar" "Apr" "May" "Jun" "Jul" "Aug" "Sep" "Oct" "Nov" "Dec"
\end{verbatim}

\hypertarget{named-vectors}{%
\subsubsection{Named Vectors}\label{named-vectors}}

Vectors in R, can be named also, i.e.~where each of the element has a name.\index{named vectors} E.g.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ages }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\AttributeTok{A =} \DecValTok{10}\NormalTok{, }\AttributeTok{B =} \DecValTok{20}\NormalTok{, }\AttributeTok{C =} \DecValTok{15}\NormalTok{)}
\NormalTok{ages}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  A  B  C 
## 10 20 15
\end{verbatim}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{images/named_vector} 

}

\caption{Vector elements can have names}\label{fig:namedvec}
\end{figure}

\textbf{Note} here that while assigning names to each element, the names are not enclosed in quotes similar to variable assignment. Also notice that this time R has not printed the numeric indices/index of first element (on each new line). There are other ways to assign names to an existing vector. We can use \texttt{names()} function\index{names() function}, which displays the names of all elements in that vector ( \emph{and this time in quotes as these are displayed in a vector}).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{names}\NormalTok{(ages)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "A" "B" "C"
\end{verbatim}

Using this function we can assign names to existing vector. See

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{vec1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1 2
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{names}\NormalTok{(vec1) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}first\_element\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}second\_element\textquotesingle{}}\NormalTok{)}
\NormalTok{vec1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  first_element second_element 
##              1              2
\end{verbatim}

Names may also be assigned using \texttt{setNames()}\index{setNames() function} while creating the vector simultaneously.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{new\_vec }\OtherTok{\textless{}{-}} \FunctionTok{setNames}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{26}\NormalTok{, LETTERS)}
\NormalTok{new\_vec}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  A  B  C  D  E  F  G  H  I  J  K  L  M  N  O  P  Q  R  S  T  U  V  W  X  Y  Z 
##  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26
\end{verbatim}

Function \texttt{unname()}\index{unname() function} may be used to remove all names. Even all the names can be removed by assigning \texttt{NULL} to \texttt{names} of that vector. Also remember that \texttt{unname} does not modify vector in place. To have this change we will have to assigned unnamed vector to that vector again. Check this,

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{unname}\NormalTok{(new\_vec)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
## [26] 26
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{new\_vec}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  A  B  C  D  E  F  G  H  I  J  K  L  M  N  O  P  Q  R  S  T  U  V  W  X  Y  Z 
##  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{new\_vec }\OtherTok{\textless{}{-}} \FunctionTok{unname}\NormalTok{(new\_vec)}
\NormalTok{new\_vec}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
## [26] 26
\end{verbatim}

\hypertarget{type-coercion}{%
\subsubsection*{Type coercion}\label{type-coercion}}
\addcontentsline{toc}{subsubsection}{Type coercion}

There are occasions\index{type coercion} when different classes of R objects get mixed together. Sometimes this happens by accident but it can also happen on purpose. Let us deal with each of these.

But prior to this let us learn how to check the type of a vector. Of course we can check the type of any vector using function \texttt{typeof()} but what if we want to check whether any vector is of a specific type. So there are \texttt{is.*()}\index{is.*() functions} functions to check this, and all these functions return either \texttt{TRUE} or \texttt{FALSE}.

\begin{itemize}
\tightlist
\item
  \texttt{is.logical()}\index{is.logical() function}
\item
  \texttt{is.integer()}\index{is.integer() function}
\item
  \texttt{is.double()}\index{is.double() function}
\item
  \texttt{is.character()}\index{is.character() function}
\item
  \texttt{is.complex()}\index{is.complex() function}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{is.integer}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{is.logical}\NormalTok{(LETTERS)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] FALSE
\end{verbatim}

\hypertarget{implicit-coercion}{%
\subsubsection*{Implicit Coercion}\label{implicit-coercion}}
\addcontentsline{toc}{subsubsection}{Implicit Coercion}

As already stated\index{implicit coercion}, vector is the most atomic data object in R. Even all the elements of a vector (having multiple elements) are vectors in themselves. We have also discussed that vectors are homogeneous in types. So what happens when we try to mix elements of different types in a vector.

In fact when we try to mix elements of different types in a vector, the resultant vector is coerced to the type which is most feasible. Since a numeral say \texttt{56} can easily be converted into a complex number (\texttt{56+0i}) or character (\texttt{"56"}), but alphabet say \texttt{A}, cannot be converted into a numeral, the atomic data types normally follow the order of precedence, tabulated in table \ref{tab:rank}.

\begin{longtable}[]{@{}cc@{}}
\caption{\label{tab:rank} Order of Precedence for Atomic Data Types}\tabularnewline
\toprule\noalign{}
Rank & Type \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
Rank & Type \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & Character \\
2 & Complex \\
3 & Double \\
4 & Integer \\
5 & Logical \\
\end{longtable}

For e.g.~in the following diagram, notice all individual elements in first vector. Out of the types of all elements therein, character type is having highest rank and thus resultant vector will be silently coerced to a character vector. Similarly, second and third vectors are coerced to \texttt{double} (second element) and \texttt{integer} (first element) respectively.

\begin{figure}

{\centering \includegraphics[width=0.99\linewidth]{images/implicit_coercion} 

}

\caption{Implicit Coercion of Vectors}\label{fig:impcoer}
\end{figure}

It is also important to note here that this implicit coercion is without any warning and is silently performed. This implicit coercion is also carried out when two (or more) vectors having different data types are concatenated together.

Example- \texttt{vec} is an existing vector of type \texttt{integer}. When we try to add an extra element say of \texttt{character} type, \texttt{vec} type is coerced to \texttt{character}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{vec }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{:}\DecValTok{5}
\FunctionTok{typeof}\NormalTok{(vec)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "integer"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{vec }\OtherTok{\textless{}{-}} \FunctionTok{append}\NormalTok{(vec, }\StringTok{\textquotesingle{}ABCD\textquotesingle{}}\NormalTok{)}
\FunctionTok{typeof}\NormalTok{(vec)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "character"
\end{verbatim}

R also implicitly coerces vectors to appropriate type when we try to perform calculations on vectors of other types. Example

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(}\ConstantTok{TRUE} \SpecialCharTok{==} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+} \DecValTok{1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{typeof}\NormalTok{(}\ConstantTok{TRUE} \SpecialCharTok{+} \DecValTok{1}\SpecialCharTok{:}\DecValTok{100}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "integer"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{typeof}\NormalTok{(}\ConstantTok{FALSE} \SpecialCharTok{+} \DecValTok{56}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "double"
\end{verbatim}

\hypertarget{explicit-coercion}{%
\subsubsection*{Explicit Coercion}\label{explicit-coercion}}
\addcontentsline{toc}{subsubsection}{Explicit Coercion}

We can explicitly coerce\index{explicit coercion} by using an \texttt{as.*()} function, like \texttt{as.logical()}, \texttt{as.integer()}, \texttt{as.double()}, or \texttt{as.character()}. \textbf{Failed coercion of strings generates a warning and a missing value:}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{as.double}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\ConstantTok{TRUE}\NormalTok{, }\ConstantTok{FALSE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1 0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{as.integer}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\StringTok{\textquotesingle{}one\textquotesingle{}}\NormalTok{, 1L))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: NAs introduced by coercion
\end{verbatim}

\begin{verbatim}
## [1]  1 NA  1
\end{verbatim}

\hypertarget{coercion-precedence}{%
\subsubsection{Coercion precedence}\label{coercion-precedence}}

Sometimes, inside R both coercion happen at same time. So which one to precede other? Actually, implicit coercion will precede explicit coercion always. Consider this example. However, without seeing the result try to guess the output.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{as.logical}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}TRUE\textquotesingle{}}\NormalTok{, }\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE   NA
\end{verbatim}

Explanation: the vector \texttt{c(\textquotesingle{}TRUE\textquotesingle{},\ 1)} coerces to \texttt{c(\textquotesingle{}TRUE\textquotesingle{},\ \textquotesingle{}1\textquotesingle{})} due to implicit coercion first and thereafter explicit coercion forces second element \texttt{as.logical(\textquotesingle{}1\textquotesingle{})} to \texttt{NA}. Though \texttt{as.logical(1)} would have resulted into \texttt{TRUE} but \texttt{as.logical("1")} would result into \texttt{NA}.

\hypertarget{checking-dimensions}{%
\subsubsection*{Checking dimensions}\label{checking-dimensions}}
\addcontentsline{toc}{subsubsection}{Checking dimensions}

Now a vector can have \texttt{n} number of vectors (recall that each element is a vector in itself) and at times we may need to check how many elements a given vector contains. Using function \texttt{length()}, we can check the number of elements.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{length}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{100}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 100
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{length}\NormalTok{(LETTERS)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 26
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{length}\NormalTok{(}\StringTok{\textquotesingle{}LENGTH\textquotesingle{}}\NormalTok{) }\CommentTok{\# If you thought its output should have been 6, check again.}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1
\end{verbatim}

\hypertarget{matrix-matrices}{%
\subsection{Matrix (Matrices)}\label{matrix-matrices}}

Matrix (or plural matrices) is a two dimensional arrangement (similar to a matrix in linear algebra and hence its name) of elements of again same type as in vectors. E.g.

\[\begin{array}{ccc}
x_{11} & x_{12} & x_{13}\\
x_{21} & x_{22} & x_{23}
\end{array}\]

Thus, matrices are vectors with an attribute named \emph{dimension}.

\begin{quote}
The dimension attribute is itself an integer vector of length 2 (number of rows, number of columns).
\end{quote}

\hypertarget{create-a-new-matrix}{%
\subsubsection*{Create a new matrix}\label{create-a-new-matrix}}
\addcontentsline{toc}{subsubsection}{Create a new matrix}

A new matrix can be created using function \texttt{matrix()} where a vector is given which is to be converted into a matrix and either number of rows \texttt{nrow} or number of columns \texttt{ncol} may be given.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{matrix}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{12}\NormalTok{, }\AttributeTok{nrow =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2] [,3] [,4]
## [1,]    1    4    7   10
## [2,]    2    5    8   11
## [3,]    3    6    9   12
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{matrix}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{12}\NormalTok{, }\AttributeTok{ncol=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2] [,3]
## [1,]    1    5    9
## [2,]    2    6   10
## [3,]    3    7   11
## [4,]    4    8   12
\end{verbatim}

Another useful argument is \texttt{byrow} \index{byrow argument in matrix function}which by default is \texttt{FALSE}. So if it is explicitly changed, we get

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{matrix}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{12}\NormalTok{, }\AttributeTok{ncol=}\DecValTok{3}\NormalTok{, }\AttributeTok{byrow =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2] [,3]
## [1,]    1    2    3
## [2,]    4    5    6
## [3,]    7    8    9
## [4,]   10   11   12
\end{verbatim}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{images/byrow} 

}

\caption{Arrangement of Matrix, if byrow argument is used}\label{fig:byrow}
\end{figure}

Matrix can be of any type. But rules of explicit and implicit coercion (as explained in vectors) also apply here.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{matrix}\NormalTok{(LETTERS, }\AttributeTok{nrow =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13]
## [1,] "A"  "C"  "E"  "G"  "I"  "K"  "M"  "O"  "Q"  "S"   "U"   "W"   "Y"  
## [2,] "B"  "D"  "F"  "H"  "J"  "L"  "N"  "P"  "R"  "T"   "V"   "X"   "Z"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(LETTERS, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{), }\AttributeTok{nrow=}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2] [,3] [,4] [,5] [,6]
## [1,] "A"  "F"  "K"  "P"  "U"  "Z" 
## [2,] "B"  "G"  "L"  "Q"  "V"  "1" 
## [3,] "C"  "H"  "M"  "R"  "W"  "2" 
## [4,] "D"  "I"  "N"  "S"  "X"  "3" 
## [5,] "E"  "J"  "O"  "T"  "Y"  "4"
\end{verbatim}

\hypertarget{names-in-matrices}{%
\subsubsection*{Names in matrices}\label{names-in-matrices}}
\addcontentsline{toc}{subsubsection}{Names in matrices}

Similar to vectors, rows or columns or both in matrices may have names\index{named matrix}. Check \texttt{?matrix()} for complete documentation.

\hypertarget{dimension}{%
\subsubsection*{Dimension}\label{dimension}}
\addcontentsline{toc}{subsubsection}{Dimension}

To check dimension of a matrix\index{dimensions of matrix} we can use \texttt{dim()}\index{dim function} (short for dimension) (similar to \texttt{length} in case of vectors) which will return a vector with two numbers (rows first, followed by columns).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_mat }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(LETTERS, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{), }\AttributeTok{nrow=}\DecValTok{5}\NormalTok{)}
\FunctionTok{dim}\NormalTok{(my\_mat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 5 6
\end{verbatim}

This gives us another method to create matrix from a vector. See

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_mat2 }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{:}\DecValTok{10}
\FunctionTok{dim}\NormalTok{(my\_mat2) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{5}\NormalTok{)}
\NormalTok{my\_mat2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2] [,3] [,4] [,5]
## [1,]    1    3    5    7    9
## [2,]    2    4    6    8   10
\end{verbatim}

\hypertarget{have-a-check-on-replication}{%
\subsubsection*{Have a check on replication}\label{have-a-check-on-replication}}
\addcontentsline{toc}{subsubsection}{Have a check on replication}

What happens when product of given dimensions is less than or greater than given vector to be converted. It replicates but it is advised to check these properly as resultant vector may not be as desired. Check these cases, and notice when R gives result silently and when with a warning.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{matrix}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{, }\AttributeTok{nrow=}\DecValTok{5}\NormalTok{, }\AttributeTok{ncol=}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in matrix(1:10, nrow = 5, ncol = 5): data length differs from size of
## matrix: [10 != 5 x 5]
\end{verbatim}

\begin{verbatim}
##      [,1] [,2] [,3] [,4] [,5]
## [1,]    1    6    1    6    1
## [2,]    2    7    2    7    2
## [3,]    3    8    3    8    3
## [4,]    4    9    4    9    4
## [5,]    5   10    5   10    5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{matrix}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{1000}\NormalTok{, }\AttributeTok{nrow=}\DecValTok{2}\NormalTok{, }\AttributeTok{ncol=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in matrix(1:1000, nrow = 2, ncol = 3): data length [1000] is not a
## sub-multiple or multiple of the number of columns [3]
\end{verbatim}

\begin{verbatim}
##      [,1] [,2] [,3]
## [1,]    1    3    5
## [2,]    2    4    6
\end{verbatim}

\hypertarget{combining-matrices}{%
\subsubsection*{Combining matrices}\label{combining-matrices}}
\addcontentsline{toc}{subsubsection}{Combining matrices}

Using \texttt{cbind()} or \texttt{rbind()} we can combine two matrices column-wise or row-wise respectively.

\begin{figure}

{\centering \includegraphics[width=10.39in,height=0.6\textheight]{images/cbind_vs_rbind} 

}

\caption{Binding of Two or more matrices together}\label{fig:bind}
\end{figure}

See these two examples.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mat1 }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{, }\AttributeTok{nrow =} \DecValTok{2}\NormalTok{)}
\NormalTok{mat2 }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\DecValTok{5}\SpecialCharTok{:}\DecValTok{8}\NormalTok{, }\AttributeTok{nrow =} \DecValTok{2}\NormalTok{)}
\FunctionTok{cbind}\NormalTok{(mat1, mat2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2] [,3] [,4]
## [1,]    1    3    5    7
## [2,]    2    4    6    8
\end{verbatim}

Example-2

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rbind}\NormalTok{(mat1, mat2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2]
## [1,]    1    3
## [2,]    2    4
## [3,]    5    7
## [4,]    6    8
\end{verbatim}

\hypertarget{arrays}{%
\subsection{Arrays}\label{arrays}}

Till now we have seen that elements in one dimension are represented as vectors and in two dimension as matrices. So a question arises here, how many dimensions we can have. Actually we can have n number of dimensions in r, in object type \texttt{array}, but they'll become increasingly difficult to comprehend and are not thus discussed here. Check these however for your understanding,

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{array}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{24}\NormalTok{, }\AttributeTok{dim =} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{4}\NormalTok{)) }\CommentTok{\# a three dimensional array}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## , , 1
## 
##      [,1] [,2]
## [1,]    1    4
## [2,]    2    5
## [3,]    3    6
## 
## , , 2
## 
##      [,1] [,2]
## [1,]    7   10
## [2,]    8   11
## [3,]    9   12
## 
## , , 3
## 
##      [,1] [,2]
## [1,]   13   16
## [2,]   14   17
## [3,]   15   18
## 
## , , 4
## 
##      [,1] [,2]
## [1,]   19   22
## [2,]   20   23
## [3,]   21   24
\end{verbatim}

Try creating 4 or 5 dimensional arrays in your console and see the results.

Further properties of vectors, matrices will be discussed in next chapter on sub-setting and indexing where we will learn how to retrieve specific elements of vector/matrices/etc. But till now we have created objects which have elements of same type. What if we want to have different types of elements/data retaining their types, together in a single variable? Answer is in next section, where we will discuss hetergeneous objects.

\hypertarget{heterogeneous-objects}{%
\subsection*{Heterogeneous objects}\label{heterogeneous-objects}}
\addcontentsline{toc}{subsection}{Heterogeneous objects}

\hypertarget{lists}{%
\subsection{Lists}\label{lists}}

So lists are used when we want to combine elements of different types together. Function used to create a list is \texttt{list()}. Check this

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{list}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\StringTok{\textquotesingle{}My string\textquotesingle{}}\NormalTok{, }\ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [[1]]
## [1] 1
## 
## [[2]]
## [1] 2
## 
## [[3]]
## [1] 3
## 
## [[4]]
## [1] "My string"
## 
## [[5]]
## [1] TRUE
\end{verbatim}

Pictorially this list can be depicted as

\begin{figure}

{\centering \includegraphics[width=0.99\linewidth]{images/list_ex} 

}

\caption{A list in R is a heterogeneous object}\label{fig:exlist}
\end{figure}

Interestingly list can contain vectors, matrices, arrays as individual elements. See

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{list}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{, LETTERS, }\ConstantTok{TRUE}\NormalTok{, my\_mat2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [[1]]
## [1] 1 2 3
## 
## [[2]]
##  [1] "A" "B" "C" "D" "E" "F" "G" "H" "I" "J" "K" "L" "M" "N" "O" "P" "Q" "R" "S"
## [20] "T" "U" "V" "W" "X" "Y" "Z"
## 
## [[3]]
## [1] TRUE
## 
## [[4]]
##      [,1] [,2] [,3] [,4] [,5]
## [1,]    1    3    5    7    9
## [2,]    2    4    6    8   10
\end{verbatim}

\begin{figure}

{\centering \includegraphics[width=0.99\linewidth]{images/list_ex2} 

}

\caption{A list in R, can contain vector, matrices, array or even lists}\label{fig:exlist2}
\end{figure}

Similar to vectors these elements can be named also.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{list}\NormalTok{(}\AttributeTok{first\_item =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{, }\AttributeTok{second\_item =}\NormalTok{ my\_mat2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $first_item
## [1] 1 2 3 4 5
## 
## $second_item
##      [,1] [,2] [,3] [,4] [,5]
## [1,]    1    3    5    7    9
## [2,]    2    4    6    8   10
\end{verbatim}

OR

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_list }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{first=}\FunctionTok{c}\NormalTok{(}\AttributeTok{A=}\DecValTok{1}\NormalTok{, }\AttributeTok{B=}\DecValTok{2}\NormalTok{, }\AttributeTok{C=}\DecValTok{3}\NormalTok{),}\AttributeTok{second=}\NormalTok{my\_mat2)}
\NormalTok{my\_list}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $first
## A B C 
## 1 2 3 
## 
## $second
##      [,1] [,2] [,3] [,4] [,5]
## [1,]    1    3    5    7    9
## [2,]    2    4    6    8   10
\end{verbatim}

\begin{figure}

{\centering \includegraphics[width=0.99\linewidth]{images/named_list} 

}

\caption{Similar to vector elements, the elements in list can be named also}\label{fig:namedlist}
\end{figure}

OR

More interestingly, lists can even contain another lists.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_list2 }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(my\_list, }\AttributeTok{new\_item =}\NormalTok{ LETTERS)}
\NormalTok{my\_list2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [[1]]
## [[1]]$first
## A B C 
## 1 2 3 
## 
## [[1]]$second
##      [,1] [,2] [,3] [,4] [,5]
## [1,]    1    3    5    7    9
## [2,]    2    4    6    8   10
## 
## 
## $new_item
##  [1] "A" "B" "C" "D" "E" "F" "G" "H" "I" "J" "K" "L" "M" "N" "O" "P" "Q" "R" "S"
## [20] "T" "U" "V" "W" "X" "Y" "Z"
\end{verbatim}

Number of items at first level can be checked using \texttt{length} as in vectors. Checking number of items in second level onward will be covered in subsequent chapter(s).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{length}\NormalTok{(my\_list)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{length}\NormalTok{(my\_list2) }\CommentTok{\# If you thought its output should have been 3, think again.}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2
\end{verbatim}

\hypertarget{data-frame}{%
\subsection{Data Frame}\label{data-frame}}

Data frames are used to store tabular data (or rectangular) in R. They are an important type of object in R.

\begin{figure}

{\centering \includegraphics[width=0.99\linewidth]{images/dataframe} 

}

\caption{An example data frame}\label{fig:dframe}
\end{figure}

Data frames are represented as a special type of list where every element of the list has to have the same length. Each element of the list can be thought of as a column and the length of each element of the list is the number of rows.

\begin{figure}

{\centering \includegraphics[width=0.99\linewidth]{images/list_vs_df} 

}

\caption{A data frame in R, is just a special kind of list}\label{fig:listvsdf}
\end{figure}

Unlike matrices, data frames can store different classes of objects in each column. (Remember that matrices must have every element be the same class).

To create a data frame from scratch we will use function \texttt{data.frame()}. See

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{emp\_name =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}Thomas\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Andrew\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Jonathan\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Bob\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Charles\textquotesingle{}}\NormalTok{),}
                    \AttributeTok{department =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}HR\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Accounts\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Accounts\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Execution\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Tech\textquotesingle{}}\NormalTok{),}
                    \AttributeTok{age =} \FunctionTok{c}\NormalTok{(}\DecValTok{40}\NormalTok{, }\DecValTok{43}\NormalTok{, }\DecValTok{39}\NormalTok{, }\DecValTok{42}\NormalTok{, }\DecValTok{25}\NormalTok{),}
                    \AttributeTok{salary =} \FunctionTok{c}\NormalTok{(}\DecValTok{20000}\NormalTok{, }\DecValTok{22000}\NormalTok{, }\DecValTok{21000}\NormalTok{, }\DecValTok{25000}\NormalTok{, }\ConstantTok{NA}\NormalTok{),}
                    \AttributeTok{whether\_permanent =} \FunctionTok{c}\NormalTok{(}\ConstantTok{TRUE}\NormalTok{, }\ConstantTok{TRUE}\NormalTok{, }\ConstantTok{FALSE}\NormalTok{, }\ConstantTok{NA}\NormalTok{, }\ConstantTok{NA}\NormalTok{))}
\NormalTok{my\_df}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   emp_name department age salary whether_permanent
## 1   Thomas         HR  40  20000              TRUE
## 2   Andrew   Accounts  43  22000              TRUE
## 3 Jonathan   Accounts  39  21000             FALSE
## 4      Bob  Execution  42  25000                NA
## 5  Charles       Tech  25     NA                NA
\end{verbatim}

\textbf{Note} that R, on its own, has allocated row names that are numbers to each of the row on its own.

Of course at most of the times we will have data frames ready for us to analyse and thus we will learn to import/read external data in r, in subsequent chapters. To check dimensions of a data frame use \texttt{dim} as in matrix.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{dim}\NormalTok{(my\_df)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 5 5
\end{verbatim}

Thus, the object types in R, can be depicted as in adjoining figure.

\begin{figure}

{\centering \includegraphics[width=0.99\linewidth]{images/Objects} 

}

\caption{Most important Data structures, in R}\label{fig:impdstr}
\end{figure}

\hypertarget{other-data-types}{%
\section{Other Data types}\label{other-data-types}}

Of course, there are other data types in R of which three are particularly useful \texttt{factor}, \texttt{date} and \texttt{date-time}. These types are actually built over the base atomic types, \texttt{integer}, \texttt{double} and \texttt{double} respectively and that's why these are being discussed separately. These types are built as \texttt{S3\ objects} in R, and users may also define their own data types in \texttt{object\ oriented\ programming}. \textbf{OOP} being concept of core programming concepts and therefore are out of the scope here.

However, to understand the S3 objects better, we have to understand that atomic objects (for the sake of simplicity consider only vectors) can have attributes.

\textbf{Example} One of the attributes that each vector has is \texttt{names}, which for unnamed vector is empty (NULL). Attributes of any object can be viewed/called from function \texttt{attributes()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Let us create a vector}
\NormalTok{vec }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{:}\DecValTok{26}
\CommentTok{\# Convert this to a named vector using function setNames()}
\CommentTok{\# This function takes first argument as vector}
\CommentTok{\# Second argument should be a character vector of equal length.}
\NormalTok{vec }\OtherTok{\textless{}{-}} \FunctionTok{setNames}\NormalTok{(vec, LETTERS)}
\CommentTok{\# let\textquotesingle{}s check what are the attributes of \textasciigrave{}vec\textasciigrave{}}
\FunctionTok{attributes}\NormalTok{(vec)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $names
##  [1] "A" "B" "C" "D" "E" "F" "G" "H" "I" "J" "K" "L" "M" "N" "O" "P" "Q" "R" "S"
## [20] "T" "U" "V" "W" "X" "Y" "Z"
\end{verbatim}

Using \texttt{attr()} we may assign any new attribute to any R object/variable.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Let\textquotesingle{}s also assign a new attribute say \textasciigrave{}x\textasciigrave{} having value "New Attribute" to \textasciigrave{}vec\textasciigrave{}}
\FunctionTok{attr}\NormalTok{(vec, }\StringTok{"x"}\NormalTok{) }\OtherTok{\textless{}{-}} \StringTok{"New Attribute"}
\CommentTok{\# Now let\textquotesingle{}s check its attributes again}
\FunctionTok{attributes}\NormalTok{(vec)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $names
##  [1] "A" "B" "C" "D" "E" "F" "G" "H" "I" "J" "K" "L" "M" "N" "O" "P" "Q" "R" "S"
## [20] "T" "U" "V" "W" "X" "Y" "Z"
## 
## $x
## [1] "New Attribute"
\end{verbatim}

We can see, in above example, how a new attribute has been added to a vector. It should have been clear by now that apart from \texttt{names}, other \texttt{attributes} may also be assigned to a vector.

\hypertarget{factors}{%
\subsection{Factors}\label{factors}}

A factor is a vector that can contain only predefined values. It is used to store categorical data. Factors are built on top of an integer vector with two attributes: a \emph{class}, `factor', which makes it behave differently from regular integer vectors, and \emph{levels}, which defines the set of allowed values. To create factors we will use function \texttt{factor}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fac }\OtherTok{\textless{}{-}} \FunctionTok{factor}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}a\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}b\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}c\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}a\textquotesingle{}}\NormalTok{))}
\NormalTok{fac}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] a b c a
## Levels: a b c
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{typeof}\NormalTok{(fac) }\CommentTok{\# notice its output}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "integer"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{attributes}\NormalTok{(fac)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $levels
## [1] "a" "b" "c"
## 
## $class
## [1] "factor"
\end{verbatim}

So if \texttt{typeof} of a factor is returning integer, how will we check its type? We may use \texttt{class} or \texttt{is.factor} in this case.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{class}\NormalTok{(fac)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "factor"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{is.factor}\NormalTok{(fac)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

Now a factor can be ordered also. We may use its argument \texttt{ordered\ =\ TRUE} along with another argument \texttt{levels}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_degrees }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"PG"}\NormalTok{, }\StringTok{"PG"}\NormalTok{, }\StringTok{"Doctorate"}\NormalTok{, }\StringTok{"UG"}\NormalTok{, }\StringTok{"PG"}\NormalTok{)}
\NormalTok{my\_factor }\OtherTok{\textless{}{-}} \FunctionTok{factor}\NormalTok{(my\_degrees, }\AttributeTok{levels =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}UG\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}PG\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Doctorate\textquotesingle{}}\NormalTok{), }\AttributeTok{ordered =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{my\_factor }\CommentTok{\# notice output here}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] PG        PG        Doctorate UG        PG       
## Levels: UG < PG < Doctorate
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{is.ordered}\NormalTok{(my\_factor)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

Another argument \texttt{labels} can also be used to display the labels, which may be different from levels.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_factor }\OtherTok{\textless{}{-}} \FunctionTok{factor}\NormalTok{(my\_degrees, }\AttributeTok{levels =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}UG\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}PG\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Doctorate\textquotesingle{}}\NormalTok{), }
                    \AttributeTok{labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"Under{-}Graduate"}\NormalTok{, }\StringTok{"Post Graduate"}\NormalTok{, }\StringTok{"Ph.D"}\NormalTok{),}
                    \AttributeTok{ordered =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{my\_factor }\CommentTok{\# notice output here}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] Post Graduate  Post Graduate  Ph.D           Under-Graduate Post Graduate 
## Levels: Under-Graduate < Post Graduate < Ph.D
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{is.factor}\NormalTok{(}\FunctionTok{c}\NormalTok{(my\_factor, }\StringTok{"UG"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] FALSE
\end{verbatim}

Attribute \texttt{levels} can be used as a function to retrieve/modify these.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{levels}\NormalTok{(my\_factor)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Under-Graduate" "Post Graduate"  "Ph.D"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{levels}\NormalTok{(my\_factor) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Grad"}\NormalTok{, }\StringTok{"Masters"}\NormalTok{, }\StringTok{"Doctorate"}\NormalTok{)}
\NormalTok{my\_factor}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] Masters   Masters   Doctorate Grad      Masters  
## Levels: Grad < Masters < Doctorate
\end{verbatim}

Remember that while factors look like (and often behave like) character vectors, they are built on top of integers. Try to think of output of this \texttt{is.factor(c(my\_factor,\ "UG"))} before running it in your console.

\hypertarget{date}{%
\subsection{Date}\label{date}}

Date vectors are built on top of double vectors. They have class ``Date'' and no other attributes. A common way to create \texttt{date} vectors in R, is converting a character string to date using \texttt{as.Date()} (see case carefully),

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_date }\OtherTok{\textless{}{-}} \FunctionTok{as.Date}\NormalTok{(}\StringTok{"1970{-}01{-}31"}\NormalTok{)}
\NormalTok{my\_date}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "1970-01-31"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{attributes}\NormalTok{(my\_date)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $class
## [1] "Date"
\end{verbatim}

Do check other arguments of as.Date by running \texttt{?as.Date()} in your console. To check whether a given variable is of type Date in r, there is no function like \texttt{is.Date} in base r, so we may use \texttt{inherits()} in this case.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{inherits}\NormalTok{(my\_date, }\StringTok{\textquotesingle{}Date\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

\hypertarget{date-time-posixct}{%
\subsection{\texorpdfstring{Date-time (\texttt{POSIXct})}{Date-time (POSIXct)}}\label{date-time-posixct}}

Times are represented by the \texttt{POSIXct} or the \texttt{POSIXlt} class.

\begin{itemize}
\tightlist
\item
  POSIXct is just a very large integer under the hood. It use a useful class when you want to store times in something like a data frame.
\item
  POSIXlt is a list underneath and it stores a bunch of other useful information like the day of the week, day of the year, month, day of the month.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_time }\OtherTok{\textless{}{-}} \FunctionTok{Sys.time}\NormalTok{()}
\NormalTok{my\_time}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "2024-07-09 13:15:32 IST"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{class}\NormalTok{(my\_time)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "POSIXct" "POSIXt"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_time2 }\OtherTok{\textless{}{-}} \FunctionTok{as.POSIXlt}\NormalTok{(my\_time)}
\FunctionTok{class}\NormalTok{(my\_time2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "POSIXlt" "POSIXt"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{names}\NormalTok{(}\FunctionTok{unclass}\NormalTok{(my\_time2))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "sec"    "min"    "hour"   "mday"   "mon"    "year"   "wday"   "yday"  
##  [9] "isdst"  "zone"   "gmtoff"
\end{verbatim}

\hypertarget{duration-difftime}{%
\subsection{\texorpdfstring{Duration (\texttt{difftime})}{Duration (difftime)}}\label{duration-difftime}}

Duration, which represent the amount of time between pairs of dates or date-times, are stored in \texttt{difftimes}. \texttt{Difftimes} are built on top of doubles, and have a units attribute that determines how the integer should be interpreted.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{two\_days }\OtherTok{\textless{}{-}} \FunctionTok{as.difftime}\NormalTok{(}\DecValTok{2}\NormalTok{, }\AttributeTok{units =} \StringTok{\textquotesingle{}days\textquotesingle{}}\NormalTok{)}
\NormalTok{two\_days}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Time difference of 2 days
\end{verbatim}

These over the top, data types will be discussed in more detail in subsequent chapters.

\hypertarget{subset}{%
\chapter{Subsetting R objects or accesing specific elements}\label{subset}}

There are multiple methods for sub-setting R objects (vectors, matrices, data frames, lists, etc.) and each have its own uses and benefits. We will discuss each one of them. Three operators \texttt{{[}}, \texttt{{[}{[}} \& \texttt{\$} will be used.

\hypertarget{subsetting-vectors}{%
\section{Subsetting vectors}\label{subsetting-vectors}}

Let us first start sub-setting vectors, which is as we have learned, atomic object in R. To subset the vectors we will use \texttt{{[}}.
For this we will use following \texttt{x} vector, which has 6 elements (names) each starting with alphabets A to F.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}Andrew\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Bob\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Chris\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Danny\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Edmund\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Freddie\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{images/subset_vec.png}

\hypertarget{subsetting-through-a-vector-of-positive-integers}{%
\subsection{Subsetting through a vector of positive integers}\label{subsetting-through-a-vector-of-positive-integers}}

Sub-setting through positive integers will give us elements at those given position (indices). See this

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# fourth element}
\NormalTok{x[}\DecValTok{4}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Danny"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# third to fifth element}
\NormalTok{x[}\DecValTok{3}\SpecialCharTok{:}\DecValTok{5}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Chris"  "Danny"  "Edmund"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# first and fifth element}
\NormalTok{x[}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Andrew" "Chris"
\end{verbatim}

\emph{Note: Check what happens when the integer vector has repeated integers.}

\hypertarget{subsetting-through-a-vector-of-negative-integers}{%
\subsection{Subsetting through a vector of negative integers}\label{subsetting-through-a-vector-of-negative-integers}}

Sub-setting through negative integers will give us all elements \textbf{except} those at given indices. See

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# all elements except that at fourth}
\NormalTok{x[}\SpecialCharTok{{-}}\DecValTok{4}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Andrew"  "Bob"     "Chris"   "Edmund"  "Freddie"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# all elements except third to fifth}
\NormalTok{x[}\SpecialCharTok{{-}}\NormalTok{(}\DecValTok{3}\SpecialCharTok{:}\DecValTok{5}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Andrew"  "Bob"     "Freddie"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# all elements except first and fifth}
\NormalTok{x[}\SpecialCharTok{{-}}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{5}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Bob"     "Chris"   "Danny"   "Freddie"
\end{verbatim}

\emph{Note: Try mixing sub-setting with a vector having both positive and negative integers in your console and check what happens.}

\hypertarget{subsetting-through-a-logical-vector}{%
\subsection{Subsetting through a logical vector}\label{subsetting-through-a-logical-vector}}

We can also subset a given vector through another vector having \texttt{logical} values i.e.~\texttt{TRUE} and \texttt{FALSE}. As you can understand output/result will have elements at places having \texttt{TRUE} only.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# First, third and fifth element only}
\NormalTok{x[}\FunctionTok{c}\NormalTok{(}\ConstantTok{TRUE}\NormalTok{, }\ConstantTok{FALSE}\NormalTok{, }\ConstantTok{TRUE}\NormalTok{, }\ConstantTok{TRUE}\NormalTok{, }\ConstantTok{FALSE}\NormalTok{, }\ConstantTok{FALSE}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Andrew" "Chris"  "Danny"
\end{verbatim}

\textbf{Recycling} is an important concept while sub-setting though a logical vector. It recycles the given logical vector up to the length of vector to be subset. Thus, \texttt{x{[}TRUE{]}} will give us original \texttt{x} only.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x[}\ConstantTok{TRUE}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Andrew"  "Bob"     "Chris"   "Danny"   "Edmund"  "Freddie"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x[}\FunctionTok{c}\NormalTok{(}\ConstantTok{TRUE}\NormalTok{, }\ConstantTok{FALSE}\NormalTok{)] }\CommentTok{\# will give elements at odd indices}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Andrew" "Chris"  "Edmund"
\end{verbatim}

\emph{Note: Try to subset a vector through a logical vector having missing values i.e.~\texttt{NA} along with \texttt{TRUE} and/or \texttt{FALSE} in your console and check what happens.}

\textbf{Sub-setting through logical vector is most important and used sub-setting method as we will see it subsequent chapter/sections when we will filter a vector on the basis of some conditions.}

\hypertarget{subsetting-through-a-character-vector}{%
\subsection{Subsetting through a character vector}\label{subsetting-through-a-character-vector}}

This method is used when the given vector is named. We can pass desired names inside \texttt{{[}{]}} to get/filter those desired elements. See this example.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# let us create a named vector \textasciigrave{}y\textasciigrave{}}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{setNames}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{6}\NormalTok{, LETTERS[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{6}\NormalTok{])}
\CommentTok{\# display \textasciigrave{}y\textasciigrave{}}
\NormalTok{y}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## A B C D E F 
## 1 2 3 4 5 6
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# subset elements named \textasciigrave{}A\textasciigrave{} and \textasciigrave{}C\textasciigrave{}}
\NormalTok{y[}\FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}A\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}C\textquotesingle{}}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## A C 
## 1 3
\end{verbatim}

\emph{Note that we have used quotes in above method of sub-setting.} We can use this method when we have names saved in another variable. See this

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{var }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}A\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}C\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}E\textquotesingle{}}\NormalTok{)}
\CommentTok{\# subset those elements from \textasciigrave{}y\textasciigrave{} which are named as per \textasciigrave{}var\textasciigrave{}}
\NormalTok{y[var] }\CommentTok{\# notice that since \textasciigrave{}var\textasciigrave{} is a variable, we have not used quotes.}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## A C E 
## 1 3 5
\end{verbatim}

\emph{Note: Similar to positive integer indexing we will get repeated values if character vector has repeated names.}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y[}\FunctionTok{c}\NormalTok{(}\StringTok{"A"}\NormalTok{, }\StringTok{"A"}\NormalTok{, }\StringTok{"C"}\NormalTok{, }\StringTok{"A"}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## A A C A 
## 1 1 3 1
\end{verbatim}

\textbf{Other two methods of indexing will not be used frequently but are important to know for debugging the code as sometimes your subset vector may be \texttt{NULL} or \texttt{zero}}

\hypertarget{subsetting-through-nothing}{%
\subsection{Subsetting through nothing}\label{subsetting-through-nothing}}

Indexing through nothing i.e.~simply with \texttt{{[}{]}} will give us original vector.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x[]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Andrew"  "Bob"     "Chris"   "Danny"   "Edmund"  "Freddie"
\end{verbatim}

\hypertarget{subsetting-through-zero}{%
\subsection{Subsetting through Zero}\label{subsetting-through-zero}}

Sub-setting through \texttt{NULL} or \texttt{0} will give us a zero length vector.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x[}\ConstantTok{NULL}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## character(0)
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y[}\DecValTok{0}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## named integer(0)
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{is.null}\NormalTok{(x[}\ConstantTok{NULL}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] FALSE
\end{verbatim}

It is important and interesting to note here that subsetting through NULL is not NULL and is a zero length vector instead.

\hypertarget{subsetting-matrices-and-arrays}{%
\section{Subsetting Matrices and arrays}\label{subsetting-matrices-and-arrays}}

We can subset higher dimensional structures (Matrix - 2 dimensional and arrays - dimension greater than 2) using (i) multiple vectors, (ii) single vector and (iii) matrix.

Let us first create a 5x5 matrix say \texttt{mat} with elements named \(A_{mn}\) where \texttt{m} will denote row number and \texttt{n} will denote column number.

\begin{verbatim}
##      [,1]  [,2]  [,3]  [,4]  [,5] 
## [1,] "A11" "A12" "A13" "A14" "A15"
## [2,] "A21" "A22" "A23" "A24" "A25"
## [3,] "A31" "A32" "A33" "A34" "A35"
## [4,] "A41" "A42" "A43" "A44" "A45"
## [5,] "A51" "A52" "A53" "A54" "A55"
\end{verbatim}

\hypertarget{indexing-through-multiple-vectors}{%
\subsection{Indexing through Multiple vectors}\label{indexing-through-multiple-vectors}}

This is extension of all sub-setting methods explained for a vector. In objects with higher dimensionality we will have to provide one vector for each dimension. Blank values, as you may understood (ref - sub-setting through nothing explained above) will do nothing and return that dimension complete.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# first and second row with third and fifth column}
\NormalTok{mat[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{, }\FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{5}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1]  [,2] 
## [1,] "A13" "A15"
## [2,] "A23" "A25"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# third to fifth column, all rows}
\NormalTok{mat[,}\DecValTok{3}\SpecialCharTok{:}\DecValTok{5}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1]  [,2]  [,3] 
## [1,] "A13" "A14" "A15"
## [2,] "A23" "A24" "A25"
## [3,] "A33" "A34" "A35"
## [4,] "A43" "A44" "A45"
## [5,] "A53" "A54" "A55"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# all columns except third}
\NormalTok{mat[, }\SpecialCharTok{{-}}\DecValTok{3}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1]  [,2]  [,3]  [,4] 
## [1,] "A11" "A12" "A14" "A15"
## [2,] "A21" "A22" "A24" "A25"
## [3,] "A31" "A32" "A34" "A35"
## [4,] "A41" "A42" "A44" "A45"
## [5,] "A51" "A52" "A54" "A55"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Odd rows, all columns}
\NormalTok{mat[}\FunctionTok{c}\NormalTok{(}\ConstantTok{TRUE}\NormalTok{, }\ConstantTok{FALSE}\NormalTok{),]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1]  [,2]  [,3]  [,4]  [,5] 
## [1,] "A11" "A12" "A13" "A14" "A15"
## [2,] "A31" "A32" "A33" "A34" "A35"
## [3,] "A51" "A52" "A53" "A54" "A55"
\end{verbatim}

The idea can be extended to a named matrix also.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# First create a named matrix}
\FunctionTok{rownames}\NormalTok{(mat) }\OtherTok{\textless{}{-}} \FunctionTok{paste0}\NormalTok{(}\StringTok{"Row"}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{)}
\FunctionTok{colnames}\NormalTok{(mat) }\OtherTok{\textless{}{-}} \FunctionTok{paste0}\NormalTok{(}\StringTok{"Col"}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{)}
\NormalTok{mat}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      Col1  Col2  Col3  Col4  Col5 
## Row1 "A11" "A12" "A13" "A14" "A15"
## Row2 "A21" "A22" "A23" "A24" "A25"
## Row3 "A31" "A32" "A33" "A34" "A35"
## Row4 "A41" "A42" "A43" "A44" "A45"
## Row5 "A51" "A52" "A53" "A54" "A55"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# filter desired rows/columns}
\NormalTok{mat[}\FunctionTok{c}\NormalTok{(}\StringTok{"Row1"}\NormalTok{), }\FunctionTok{c}\NormalTok{(}\StringTok{"Col2"}\NormalTok{, }\StringTok{"Col3"}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  Col2  Col3 
## "A12" "A13"
\end{verbatim}

In the above example you must have noticed that indexing objects with higher dimensionality may return the objects with lower dimensionality. E.g. sub-setting a matrix may return a vector. \textbf{We can control the dimensionality reduction through the argument \texttt{drop=FALSE} which is by default TRUE and may thus introduce bugs in the code.}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mat[}\FunctionTok{c}\NormalTok{(}\StringTok{"Row1"}\NormalTok{), }\FunctionTok{c}\NormalTok{(}\StringTok{"Col2"}\NormalTok{, }\StringTok{"Col3"}\NormalTok{), drop}\OtherTok{=}\ConstantTok{FALSE}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      Col2  Col3 
## Row1 "A12" "A13"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#check this}
\FunctionTok{dim}\NormalTok{(mat[}\FunctionTok{c}\NormalTok{(}\StringTok{"Row1"}\NormalTok{), }\FunctionTok{c}\NormalTok{(}\StringTok{"Col2"}\NormalTok{, }\StringTok{"Col3"}\NormalTok{), }\AttributeTok{drop=}\ConstantTok{FALSE}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1 2
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#versus this}
\FunctionTok{dim}\NormalTok{(mat[}\FunctionTok{c}\NormalTok{(}\StringTok{"Row1"}\NormalTok{), }\FunctionTok{c}\NormalTok{(}\StringTok{"Col2"}\NormalTok{, }\StringTok{"Col3"}\NormalTok{)])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## NULL
\end{verbatim}

\hypertarget{subsetting-through-one-vector}{%
\subsection{Subsetting through one vector}\label{subsetting-through-one-vector}}

By now it should be clear that objects with higher dimensionality like matrices, array are actually vectors at the core of r, displayed and acting like objects having more than one dimension. So sub-setting with single vector on these objects coerce the behavior of these objects as vectors only and give output exactly as shown in previous section.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mat[}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{25}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "A11" "A52" "A53" "A55"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# OR}
\NormalTok{mat[}\FunctionTok{c}\NormalTok{(}\ConstantTok{TRUE}\NormalTok{, }\ConstantTok{FALSE}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "A11" "A31" "A51" "A22" "A42" "A13" "A33" "A53" "A24" "A44" "A15" "A35"
## [13] "A55"
\end{verbatim}

\hypertarget{subsetting-through-a-matrix}{%
\subsection{Subsetting through a matrix}\label{subsetting-through-a-matrix}}

We can also subset objects with higher dimensionality with integer matrix (having number of columns equal to dimensions). In other words, to subset a matrix (2D) with the help of other matrix we will need a 2 column matrix where first column will indicate row number and second column will indicate column number. See

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{selection\_matrix }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{, }\CommentTok{\# Element at Row 1 Col 1}
                             \DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{, }\CommentTok{\# Element at Row 2 Col 2}
                             \DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{), }\CommentTok{\# Element at Row 3 Col 3}
                           \AttributeTok{ncol =} \DecValTok{2}\NormalTok{, }
                           \AttributeTok{byrow =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{mat[selection\_matrix]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "A11" "A22" "A33"
\end{verbatim}

\hypertarget{subsetting-lists}{%
\section{Subsetting lists}\label{subsetting-lists}}

List sub-setting can be done using either \texttt{{[}{]}}, \texttt{{[}{[}{]}{]}} or \texttt{\$}. To understand the difference between these, let us consider these one by one. As done earlier let us consider a list of 4 elements - one vector, one matrix, one list and one data frame. For now let us consider that list is unnamed.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_list }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}
  \DecValTok{11}\SpecialCharTok{:}\DecValTok{20}\NormalTok{,                                                       }\CommentTok{\# first element}
  \FunctionTok{outer}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{, }\AttributeTok{FUN =} \ControlFlowTok{function}\NormalTok{(x, y) }\FunctionTok{paste0}\NormalTok{(}\StringTok{\textquotesingle{}B\textquotesingle{}}\NormalTok{, x, y)),     }\CommentTok{\# second element}
  \FunctionTok{list}\NormalTok{(LETTERS[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{8}\NormalTok{], }\ConstantTok{TRUE}\NormalTok{),                                    }\CommentTok{\# third element  }
  \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{col1 =}\NormalTok{ letters[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{], }\AttributeTok{col2 =} \DecValTok{5}\SpecialCharTok{:}\DecValTok{8}\NormalTok{)                  }\CommentTok{\# fourth element}
\NormalTok{)}
\CommentTok{\# display the list}
\NormalTok{my\_list}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [[1]]
##  [1] 11 12 13 14 15 16 17 18 19 20
## 
## [[2]]
##      [,1]  [,2]  [,3]  [,4] 
## [1,] "B11" "B12" "B13" "B14"
## [2,] "B21" "B22" "B23" "B24"
## [3,] "B31" "B32" "B33" "B34"
## [4,] "B41" "B42" "B43" "B44"
## 
## [[3]]
## [[3]][[1]]
## [1] "A" "B" "C" "D" "E" "F" "G" "H"
## 
## [[3]][[2]]
## [1] TRUE
## 
## 
## [[4]]
##   col1 col2
## 1    a    5
## 2    b    6
## 3    c    7
## 4    d    8
\end{verbatim}

\hypertarget{subsetting-lists-with}{%
\subsection{\texorpdfstring{Subsetting lists with \texttt{{[}{]}}}{Subsetting lists with {[}{]}}}\label{subsetting-lists-with}}

Sub-setting lists with \texttt{{[}{]}} will always result a list containing desired element(s).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_list[}\DecValTok{2}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [[1]]
##      [,1]  [,2]  [,3]  [,4] 
## [1,] "B11" "B12" "B13" "B14"
## [2,] "B21" "B22" "B23" "B24"
## [3,] "B31" "B32" "B33" "B34"
## [4,] "B41" "B42" "B43" "B44"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{class}\NormalTok{(my\_list[}\DecValTok{1}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "list"
\end{verbatim}

We can apply other ideas of vector sub-setting as explained earlier with this list sub-setting. The output will also be list containing one or more items.

\hypertarget{subsetting-lists-with-1}{%
\subsection{\texorpdfstring{Subsetting lists with \texttt{{[}{[}{]}{]}}}{Subsetting lists with {[}{[}{]}{]}}}\label{subsetting-lists-with-1}}

Sub-setting list with \texttt{{[}{[}{]}{]}} will return that specific item (as per index given) but the output will be of type of that specific item.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_list[[}\DecValTok{2}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1]  [,2]  [,3]  [,4] 
## [1,] "B11" "B12" "B13" "B14"
## [2,] "B21" "B22" "B23" "B24"
## [3,] "B31" "B32" "B33" "B34"
## [4,] "B41" "B42" "B43" "B44"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{class}\NormalTok{(my\_list[[}\DecValTok{4}\NormalTok{]])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "data.frame"
\end{verbatim}

\emph{Notice the difference in outputs created with \texttt{my\_list{[}2{]}} and \texttt{my\_list{[}{[}2{]}{]}} in above 2 code blocks.}

\emph{Needless to say, one cannot index/subset lists using multiple indices.} Check \texttt{my\_list{[}{[}1:2{]}{]}} in your console as the results may not be as what you think.

\hypertarget{chaining-or-multiple-subsetting}{%
\subsection{Chaining or multiple subsetting}\label{chaining-or-multiple-subsetting}}

We can further subset/index a vector/variable in R using \textbf{chaining} i.e.~by combining one or methods as we have discussed here.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# third element of second element}
\NormalTok{my\_list[[}\DecValTok{2}\NormalTok{]][}\DecValTok{3}\NormalTok{] }\CommentTok{\# recall that by default matrix is by column}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "B31"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# or}
\NormalTok{my\_list[[}\DecValTok{2}\NormalTok{]][}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{,}\DecValTok{2}\SpecialCharTok{:}\DecValTok{4}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1]  [,2]  [,3] 
## [1,] "B12" "B13" "B14"
## [2,] "B22" "B23" "B24"
## [3,] "B32" "B33" "B34"
\end{verbatim}

\hypertarget{subsetting-with}{%
\subsection{\texorpdfstring{Subsetting with \texttt{\$}}{Subsetting with \$}}\label{subsetting-with}}

\texttt{\$} is a shorthand operator: \texttt{x\$y} is roughly equivalent to \texttt{x{[}{[}"y"{]}{]}}. To check this let us assign our list some names.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{names}\NormalTok{(my\_list) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"first"}\NormalTok{, }\StringTok{"second"}\NormalTok{, }\StringTok{"third"}\NormalTok{, }\StringTok{"fourth"}\NormalTok{)}
\CommentTok{\# Now see}
\NormalTok{my\_list}\SpecialCharTok{$}\NormalTok{first}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 11 12 13 14 15 16 17 18 19 20
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_list}\SpecialCharTok{$}\NormalTok{fourth}\SpecialCharTok{$}\NormalTok{col2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 5 6 7 8
\end{verbatim}

\emph{Notice that rules for dimensionality reduction also applies with \texttt{\$}}.

Another difference between \texttt{{[}{[}} sub-setting versus \texttt{\$} sub-setting is partial matching (\emph{left to right only}), which is possible with \texttt{\$} only and not with \texttt{{[}{[}}. See

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_list}\SpecialCharTok{$}\NormalTok{fir}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 11 12 13 14 15 16 17 18 19 20
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_list[[}\StringTok{\textquotesingle{}fir\textquotesingle{}}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## NULL
\end{verbatim}

\hypertarget{data-frames}{%
\section{Data frames}\label{data-frames}}

As already explained data frames are basically lists with each element having equal length, rules for sub-setting lists all apply with data frames. One addition is that data frames can also be subset using rules for matrix sub-setting.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mtcars }\CommentTok{\# it is a default data frame in r}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                      mpg cyl  disp  hp drat    wt  qsec vs am gear carb
## Mazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4
## Mazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4
## Datsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1
## Hornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1
## Hornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2
## Valiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1
## Duster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4
## Merc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2
## Merc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2
## Merc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4
## Merc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4
## Merc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3
## Merc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3
## Merc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3
## Cadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4
## Lincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4
## Chrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4
## Fiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1
## Honda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2
## Toyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1
## Toyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1
## Dodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2
## AMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2
## Camaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4
## Pontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2
## Fiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1
## Porsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2
## Lotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2
## Ford Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4
## Ferrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6
## Maserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8
## Volvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# list type sub{-}setting}
\NormalTok{mtcars[[}\DecValTok{2}\NormalTok{]]  }\CommentTok{\# second column }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 6 6 4 6 8 6 8 4 4 6 6 8 8 8 8 8 8 4 4 4 4 8 8 8 8 4 4 4 8 6 8 4
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# matrix type}
\NormalTok{mtcars[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{, }\DecValTok{2}\SpecialCharTok{:}\DecValTok{3}\NormalTok{] }\CommentTok{\# first four rows with second \& third columns}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                cyl disp
## Mazda RX4        6  160
## Mazda RX4 Wag    6  160
## Datsun 710       4  108
## Hornet 4 Drive   6  258
\end{verbatim}

\textbf{Remember}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  If sub-setting data frames with single vector, data frame behave like lists, by default. If we have to get output as data.frame we will have use \texttt{drop=\ FALSE} argument.
\item
  If however, sub-setting data frame through two vectors, these behave like matrices.
\end{enumerate}

Examples

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Default}
\NormalTok{mtcars[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{, }\DecValTok{2}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 6 6 4 6 8
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# using drop argument}
\NormalTok{mtcars[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{, }\DecValTok{2}\NormalTok{, drop }\OtherTok{=} \ConstantTok{FALSE}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                   cyl
## Mazda RX4           6
## Mazda RX4 Wag       6
## Datsun 710          4
## Hornet 4 Drive      6
## Hornet Sportabout   8
\end{verbatim}

\hypertarget{subsetting-and-assignment}{%
\section{Subsetting and assignment}\label{subsetting-and-assignment}}

All the sub-setting that we have seen can be used for assignment as well.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_list}\SpecialCharTok{$}\NormalTok{first }\OtherTok{\textless{}{-}}\NormalTok{ mtcars[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{, }\DecValTok{2}\SpecialCharTok{:}\DecValTok{3}\NormalTok{]}
\NormalTok{my\_list}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $first
##                cyl disp
## Mazda RX4        6  160
## Mazda RX4 Wag    6  160
## Datsun 710       4  108
## Hornet 4 Drive   6  258
## 
## $second
##      [,1]  [,2]  [,3]  [,4] 
## [1,] "B11" "B12" "B13" "B14"
## [2,] "B21" "B22" "B23" "B24"
## [3,] "B31" "B32" "B33" "B34"
## [4,] "B41" "B42" "B43" "B44"
## 
## $third
## $third[[1]]
## [1] "A" "B" "C" "D" "E" "F" "G" "H"
## 
## $third[[2]]
## [1] TRUE
## 
## 
## $fourth
##   col1 col2
## 1    a    5
## 2    b    6
## 3    c    7
## 4    d    8
\end{verbatim}

\hypertarget{func}{%
\chapter{Functions and operations in R}\label{func}}

What is a function? Mathematically, a function \(f\) is a relationship which map an input \(x\) to an specific output, which is denoted as \(f(x)\). There are only two conditions i.e.~every input should have an output, and same input if passed into same function multiple times, it should produce same output each time. So if \(x=y\) we should have \(f(x)=f(y)\).

\begin{figure}

{\centering \includegraphics[width=0.99\linewidth]{images/function} 

}

\caption{Author's illustration of a function}\label{fig:unnamed-chunk-88}
\end{figure}

For example \texttt{squaring} if considered on numbers is a function. We denote this as \(f(x)=x^2\). Or, \texttt{square-root} on positive numbers is also a function.

Now there may be more than one input, let us assume three inputs \texttt{x}, \texttt{y} and \texttt{z} and our function's job is to add three times \texttt{x}, two times \texttt{z} and one time \texttt{y} together. We will write this function as \(f(x,y,z) = 3x+y+2z\). Each programming language has some pre-defined functions. Here inputs are usually termed as \texttt{arguments}. Normally values to \texttt{arguments} should be passed by users, but many times there's a default value for these arguments. So if the value of that argument is not supplier by the user/coder explicitly, that function uses that default value silently and produces a result.

R's engine then calculates the output as per definition of that function and gives us the output. If that output is assigned to some variable R does not displays/prints anything but if function is performed only the output is displayed usually, with the exception that many times function is carried out silently and nothing is returned.

In this chapter we will learn about some of the pre-defined functions which shall be used in our data analysis operations. We can also define our own custom functions which we will learn in chapter \ref{cust}.

As an example, \texttt{sum()} is a predefined function available in R, which produces sum of one or more vectors passed in the function as arguments.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sum}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{, }\DecValTok{15}\SpecialCharTok{:}\DecValTok{45}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 985
\end{verbatim}

To check the arguments available for any pre-defined function, we can use another function \texttt{args()} which take a \emph{function name} as an argument and returns all the available arguments to that function.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{args}\NormalTok{(sum)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## function (..., na.rm = FALSE) 
## NULL
\end{verbatim}

Here we see that there is an argument (which is anmed argument0 \texttt{na.rm} having a default value \texttt{FALSE}. Actually, this argument silently takes default value and produces results. But if \texttt{TRUE} is required as a value to this argument that need to be explicitly mentioned.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sum}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{, }\ConstantTok{NA}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] NA
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sum}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{, }\ConstantTok{NA}\NormalTok{, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 55
\end{verbatim}

To get the definition of any existing function, we may just type its name without parenthesis on console, and the definition will be returned as an output.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sum}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## function (..., na.rm = FALSE)  .Primitive("sum")
\end{verbatim}

To get further help about any existing function, refer section \ref{help}.

\hypertarget{cust}{%
\section{Custom Functions}\label{cust}}

One of R's greatest strengths is the user's ability to add functions\index{custom function}. In fact, many of the functions in R are functions of existing functions. The structure of a function looks like this:

\begin{verbatim}
myfunctionname <- function(arg1, arg2, ... ){
  statements
  return(object)
}
\end{verbatim}

\textbf{Note:} Objects in the function are local to the function. The object returned can be any data type, from scalar to list.

Let's take a look at an example. We will create a function which will take 3 numbers, will give an output by adding thrice of first, second and twice of third.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_fun1 }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(first,second,third)\{}
\NormalTok{  first}\SpecialCharTok{*}\DecValTok{3}\SpecialCharTok{+}\NormalTok{second}\SpecialCharTok{+}\NormalTok{third}\SpecialCharTok{*}\DecValTok{2}
\NormalTok{\}}
\CommentTok{\# let\textquotesingle{}s check whether it is working as desired}
\FunctionTok{my\_fun1}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 30
\end{verbatim}

\begin{itemize}
\tightlist
\item
  If the arguments provided are not named, it will take all arguments in the order these are defined.
\item
  However, we can provide named arguments in any order. See this
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{my\_fun1}\NormalTok{(}\AttributeTok{second=}\DecValTok{3}\NormalTok{, }\AttributeTok{first=}\DecValTok{1}\NormalTok{, }\AttributeTok{third=}\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 26
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Partial matching of names are also allowed. Example
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{my\_fun1}\NormalTok{(}\AttributeTok{sec=}\DecValTok{3}\NormalTok{,}\AttributeTok{fir=}\DecValTok{1}\NormalTok{,}\AttributeTok{thi=}\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 26
\end{verbatim}

\begin{itemize}
\tightlist
\item
  We can also provide default values to any argument. These default values are however, overridden when specific values are given. See this example.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# let\textquotesingle{}s create a new function which adds twice the second argument to first argument, which in turn by default is 10}
\NormalTok{my\_fun2 }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(}\AttributeTok{first=}\DecValTok{10}\NormalTok{, second)\{}
\NormalTok{  first}\SpecialCharTok{+}\NormalTok{second}\SpecialCharTok{*}\DecValTok{2}
\NormalTok{\}}
\FunctionTok{my\_fun2}\NormalTok{(}\AttributeTok{second =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 30
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{my\_fun2}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 21
\end{verbatim}

\begin{itemize}
\tightlist
\item
  There may be functions which do not require any argument. See this example
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_fun3 }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{()\{}
  \FunctionTok{print}\NormalTok{(}\StringTok{\textquotesingle{}Hi\textquotesingle{}}\NormalTok{)}
\NormalTok{\}}
\FunctionTok{my\_fun3}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Hi"
\end{verbatim}

\hypertarget{special-argument-ellipsis-...}{%
\subsection*{\texorpdfstring{Special argument ellipsis \texttt{...}}{Special argument ellipsis ...}}\label{special-argument-ellipsis-...}}
\addcontentsline{toc}{subsection}{Special argument ellipsis \texttt{...}}

While searching for help of a function in r, you may have came across something like this \texttt{sum(...,\ na.rm\ =\ FALSE)}. The three dots \texttt{...}\index{... ellipsis} here are referred to as ellipsis\index{ellipsis}. Basically it means that the function is designed to take any number of named or unnamed arguments.

Thus it means we can provide any number of arguments in place of \texttt{...}. Now the point to be noted here is that values to all agruments occurring after \texttt{...} must only be named. See this example-

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sum}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{100}\NormalTok{, }\ConstantTok{NA}\NormalTok{, }\ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] NA
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sum}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{100}\NormalTok{, }\ConstantTok{NA}\NormalTok{, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 5050
\end{verbatim}

Now we can even use these three dots in our own custom functions. Just unpack these before writing the actual statement for that function. See this simple example-

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_ellipsis\_func }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(...)\{}
\NormalTok{  l }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(...) }\CommentTok{\# unpack ellipsis}
  \FunctionTok{length}\NormalTok{(l) }\CommentTok{\# return length of l}
\NormalTok{\}}
\FunctionTok{my\_ellipsis\_func}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{, }\DecValTok{11}\SpecialCharTok{:}\DecValTok{20}\NormalTok{, }\StringTok{\textquotesingle{}a string\textquotesingle{}}\NormalTok{) }\CommentTok{\# we are passing three arguments}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3
\end{verbatim}

\hypertarget{environment-issues}{%
\subsection*{Environment issues}\label{environment-issues}}
\addcontentsline{toc}{subsection}{Environment issues}

\begin{itemize}
\tightlist
\item
  Any of the argument values are not saved/updated in global environment\index{global environment}. See this example
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OtherTok{\textless{}{-}} \DecValTok{10}
\NormalTok{my\_fun4 }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x)\{}
\NormalTok{  x}\SpecialCharTok{*}\DecValTok{2}
\NormalTok{\}}
\FunctionTok{my\_fun4}\NormalTok{(}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 4
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 10
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Even if we create another variable inside the function, that variable is not available outside that function's environment.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y }\OtherTok{\textless{}{-}} \DecValTok{5}
\NormalTok{my\_fun5 }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{()\{}
\NormalTok{  y }\OtherTok{\textless{}{-}} \DecValTok{1}
  \FunctionTok{return}\NormalTok{(y)}
\NormalTok{\}}
\FunctionTok{my\_fun5}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 5
\end{verbatim}

\begin{itemize}
\tightlist
\item
  If however, we want to create a variable (or update existing variable) inside the function intentionally, we may use \texttt{forced\ assignment} denoted as \texttt{\textless{}\textless{}-}. See this example
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y }\OtherTok{\textless{}{-}} \DecValTok{5}
\NormalTok{my\_fun5 }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{()\{}
\NormalTok{  y }\OtherTok{\textless{}\textless{}{-}} \DecValTok{1}
  \FunctionTok{return}\NormalTok{(y)}
\NormalTok{\}}
\FunctionTok{my\_fun5}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1
\end{verbatim}

\begin{itemize}
\tightlist
\item
  As already stated, we can create object of any type using a custom function.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_list\_fun }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x)\{}
  \FunctionTok{list}\NormalTok{(}\AttributeTok{sum=}\FunctionTok{sum}\NormalTok{(x),}
       \AttributeTok{mean =} \FunctionTok{mean}\NormalTok{(x),}
       \AttributeTok{sd =} \FunctionTok{sd}\NormalTok{(x))}
\NormalTok{\}}
\FunctionTok{my\_list\_fun}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $sum
## [1] 55
## 
## $mean
## [1] 5.5
## 
## $sd
## [1] 3.02765
\end{verbatim}

\hypertarget{existing-and-useful-functions-in-base-r}{%
\section{Existing and useful functions in base R}\label{existing-and-useful-functions-in-base-r}}

R has a lot of inbuilt/existing functions that are useful and therefore it is good to know about them. Let us discuss a few of these existing functions which are useful for data analytics and other allied jobs.

Firstly, let's learn logical operators that will be useful to check various conditions. For those who doesn't know what operators are, they may simply think of operators being special kind of functions having exactly two arguments.

\hypertarget{conditions-and-logical-operatorsoperands}{%
\subsection{Conditions and logical operators/operands}\label{conditions-and-logical-operatorsoperands}}

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1667}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.5000}}@{}}
\caption{\label{tab:table2} Conditions and logical operators/operands}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
Operator/ function
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Meaning
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
Operator/ function
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Meaning
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{==} & Is RHS equal to LHS? & \texttt{5\ ==\ 2} will return FALSE \\
& & \texttt{\textquotesingle{}Anil\textquotesingle{}\ ==\ \textquotesingle{}anil\textquotesingle{}} is FALSE \\
\texttt{!=} & Is RHS not equal to LHS? & \texttt{\textquotesingle{}ABCD\textquotesingle{}\ !=\ \textquotesingle{}abcd\textquotesingle{}} is TRUE \\
\texttt{\textgreater{}=} & Is LHS greater than or equal to RHS? & \texttt{5\ \textgreater{}=\ 2} will return TRUE \\
\texttt{\textless{}=} & Is LHS less than or equal to RHS? & \texttt{15\ \textless{}=\ 2} will return FALSE \\
\texttt{\textgreater{}} & Is LHS strictly greater than RHS? & \texttt{2\ \textgreater{}\ 2} will return FALSE \\
\texttt{\textless{}} & Is LHS strictly less than RHS? & \texttt{12\ \textless{}\ 12} will return FALSE \\
\texttt{is.na()} & Whether the argument passed is NA & \texttt{is.na(NA)} is TRUE \\
\texttt{is.null()} & Whether the argument passed is null & \texttt{is.null(NA)} is FALSE \\
\texttt{\textbar{}} & Logical OR & \texttt{TRUE\ \textbar{}\ FALSE} will return \texttt{TRUE} \\
\texttt{\&} & Logical AND & \texttt{TRUE\ \&\ FALSE} will return \texttt{FALSE} \\
\texttt{!} & Logical NOT & \texttt{!TRUE} will return \texttt{FALSE} \\
\texttt{\textbar{}\textbar{}} & Element wise Logical OR & Examines only the first element of the operands resulting into a single length logical vector \\
\texttt{\&\&} & Element wise Logical AND & Examines only the first element of the operands resulting into a single length logical vector \\
\texttt{\%in\%} & LHS \textbf{IN} RHS & Checks whether LHS elements are present in RHS vector \\
\end{longtable}

\hypertarget{vectorisation-of-operations-and-functions}{%
\subsection*{Vectorisation of operations and functions}\label{vectorisation-of-operations-and-functions}}
\addcontentsline{toc}{subsection}{Vectorisation of operations and functions}

All the above mentioned operators are \textbf{vectorised}. Except \texttt{\textbar{}\textbar{}} and \texttt{\&\&} will return vector of same length as we are comparing. Check

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{LETTERS[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{] }\SpecialCharTok{==}\NormalTok{ letters[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] FALSE FALSE FALSE FALSE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{10}\SpecialCharTok{:}\DecValTok{1} \SpecialCharTok{\textgreater{}=} \DecValTok{1}\SpecialCharTok{:}\DecValTok{10}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# TRUE will act as 1 and FALSE as 0}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\ConstantTok{TRUE}\NormalTok{, }\ConstantTok{FALSE}\NormalTok{, }\ConstantTok{FALSE}\NormalTok{, }\ConstantTok{TRUE}\NormalTok{)}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\NormalTok{x }\SpecialCharTok{==}\NormalTok{ y}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  TRUE  TRUE FALSE FALSE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Examples of element wise operations}
\NormalTok{x }\SpecialCharTok{\&}\NormalTok{ y}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  TRUE FALSE FALSE  TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\SpecialCharTok{|}\NormalTok{ y}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  TRUE FALSE  TRUE  TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# character strings may be checked for alphabetic order}
\StringTok{\textquotesingle{}ABCD\textquotesingle{}} \SpecialCharTok{\textgreater{}=} \StringTok{\textquotesingle{}AACD\textquotesingle{}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

\hypertarget{recycling}{%
\subsection*{Recycling}\label{recycling}}
\addcontentsline{toc}{subsection}{Recycling}

\textbf{Recycling} rules apply when two vectors are not of equal length. See these examples.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Notice that results are displayed silently}
\NormalTok{LETTERS[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{] }\SpecialCharTok{==} \StringTok{\textquotesingle{}A\textquotesingle{}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  TRUE FALSE FALSE FALSE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Notice that results are displayed with a warning}
\NormalTok{LETTERS[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{] }\SpecialCharTok{==}\NormalTok{ LETTERS[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in LETTERS[1:5] == LETTERS[1:3]: longer object length is not a multiple
## of shorter object length
\end{verbatim}

\begin{verbatim}
## [1]  TRUE  TRUE  TRUE FALSE FALSE
\end{verbatim}

The \textbf{operator \texttt{\%in\%}} behaves slightly different from above. Each searches each element of LHS in RHS and gives result in a logical vector equal to length of LHS vector. See these examples carefully.

\begin{Shaded}
\begin{Highlighting}[]
\StringTok{\textquotesingle{}A\textquotesingle{}} \SpecialCharTok{\%in\%}\NormalTok{ LETTERS}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{LETTERS }\SpecialCharTok{\%in\%}\NormalTok{ LETTERS[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1]  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
## [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
## [25] FALSE FALSE
\end{verbatim}

\hypertarget{handling-missing-values-na-in-these-operations}{%
\subsection*{\texorpdfstring{Handling Missing values \texttt{NA} in these operations}{Handling Missing values NA in these operations}}\label{handling-missing-values-na-in-these-operations}}
\addcontentsline{toc}{subsection}{Handling Missing values \texttt{NA} in these operations}

While checking for any condition to be \texttt{TRUE} or \texttt{FALSE} missing values \texttt{NA} and/or \texttt{NaN} should be handled carefully or a bug may be introduced. See these examples-

\begin{Shaded}
\begin{Highlighting}[]
\ConstantTok{FALSE} \SpecialCharTok{!=} \ConstantTok{NA}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] NA
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ConstantTok{TRUE} \SpecialCharTok{!=} \ConstantTok{NA}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] NA
\end{verbatim}

Thus, if any of the condition is evaluated on a vector, we can have \texttt{NA} in our output along with \texttt{TRUE} and \texttt{FALSE}. See this example

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{15}\NormalTok{, }\ConstantTok{NA}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\NormalTok{x }\SpecialCharTok{\textless{}=} \DecValTok{5}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  TRUE  TRUE FALSE    NA  TRUE  TRUE
\end{verbatim}

These missing values however behaves slightly different with logical operators \texttt{\&} \texttt{\textbar{}}. See these examples.

\begin{Shaded}
\begin{Highlighting}[]
\ConstantTok{TRUE} \SpecialCharTok{|} \ConstantTok{NA}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ConstantTok{FALSE} \SpecialCharTok{\&} \ConstantTok{NA}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] FALSE
\end{verbatim}

\hypertarget{use-of-above-logical-operators-for-subsetting}{%
\subsection*{Use of above logical operators for subsetting}\label{use-of-above-logical-operators-for-subsetting}}
\addcontentsline{toc}{subsection}{Use of above logical operators for subsetting}

Since the logical operations on vectors gives a \texttt{logical} vector as output, these can be used for sub-setting as well. See these examples.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_ages }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{40}\NormalTok{, }\DecValTok{45}\NormalTok{, }\DecValTok{31}\NormalTok{, }\DecValTok{51}\NormalTok{, }\DecValTok{25}\NormalTok{, }\DecValTok{27}\NormalTok{, }\DecValTok{59}\NormalTok{, }\DecValTok{45}\NormalTok{)}
\CommentTok{\# filter ages greater than or equal to 30}
\NormalTok{my\_ages[my\_ages }\SpecialCharTok{\textgreater{}=} \DecValTok{30}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 40 45 31 51 59 45
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_names }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Andrew"}\NormalTok{, }\StringTok{"Bob"}\NormalTok{, }\StringTok{"Carl"}\NormalTok{, }\StringTok{"Daven"}\NormalTok{, }\StringTok{"Earl"}\NormalTok{)}
\CommentTok{\# filter names which start with alphabet either A, B or C}
\NormalTok{my\_names[my\_names }\SpecialCharTok{\textless{}=} \StringTok{"D"}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Andrew" "Bob"    "Carl"
\end{verbatim}

\hypertarget{conditions-with-ifelse}{%
\subsection*{\texorpdfstring{Conditions with \texttt{ifelse}}{Conditions with ifelse}}\label{conditions-with-ifelse}}
\addcontentsline{toc}{subsection}{Conditions with \texttt{ifelse}}

Syntax \texttt{ifelse(test,\ yes,\ no)} will be used to return value (of same shape as \texttt{test}) which is filled with elements selected from either \texttt{yes} or \texttt{no} depending on whether the elements of \texttt{test} are \texttt{TRUE} or \texttt{FALSE}. See this example

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{, }\ConstantTok{NA}\NormalTok{, }\DecValTok{16}\SpecialCharTok{:}\DecValTok{20}\NormalTok{)}
\FunctionTok{ifelse}\NormalTok{(x}\SpecialCharTok{\textgreater{}}\DecValTok{5}\NormalTok{, }\StringTok{\textquotesingle{}Greater than 5\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Upto 5\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "Upto 5"         "Upto 5"         "Upto 5"         "Upto 5"        
##  [5] "Upto 5"         NA               "Greater than 5" "Greater than 5"
##  [9] "Greater than 5" "Greater than 5" "Greater than 5"
\end{verbatim}

\hypertarget{functions-all-and-any}{%
\subsection*{\texorpdfstring{Functions \texttt{all()} and \texttt{any()}}{Functions all() and any()}}\label{functions-all-and-any}}
\addcontentsline{toc}{subsection}{Functions \texttt{all()} and \texttt{any()}}

These are shortcut functions to tell us whether \texttt{all} or \texttt{any} of the elements of given object are \texttt{TRUE}. See This example

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OtherTok{\textless{}{-}} \DecValTok{11}\SpecialCharTok{:}\DecValTok{20}
\FunctionTok{all}\NormalTok{(x }\SpecialCharTok{\textgreater{}} \DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{any}\NormalTok{(x }\SpecialCharTok{\textgreater{}} \DecValTok{20}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] FALSE
\end{verbatim}

All of the above mentioned operators (along with those listed in section \ref{calculator}) are \textbf{vectorised}. Check these examples.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{:}\DecValTok{5}
\NormalTok{y }\OtherTok{\textless{}{-}} \DecValTok{6}\SpecialCharTok{:}\DecValTok{10}

\NormalTok{x }\SpecialCharTok{+}\NormalTok{ y}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  7  9 11 13 15
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\SpecialCharTok{{-}}\NormalTok{ y}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -5 -5 -5 -5 -5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\SpecialCharTok{*}\NormalTok{ y}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  6 14 24 36 50
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\SpecialCharTok{/}\NormalTok{ y}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.1666667 0.2857143 0.3750000 0.4444444 0.5000000
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\SpecialCharTok{\^{}}\NormalTok{ y}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]       1     128    6561  262144 9765625
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Caution: here RHS is not a vector}
\NormalTok{y }\SpecialCharTok{\%\%} \DecValTok{3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0 1 2 0 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y }\SpecialCharTok{\%/\%} \DecValTok{3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2 2 2 3 3
\end{verbatim}

\textbf{Recycling} also applies on mathematical operators. See these examples and notice when R gives results silently and when with a warning.

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{10}\SpecialCharTok{:}\DecValTok{15} \SpecialCharTok{+} \DecValTok{4}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 14 15 16 17 18 19
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{100}\SpecialCharTok{:}\DecValTok{110} \SpecialCharTok{{-}} \DecValTok{50}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 50 51 52 53 54 55 56 57 58 59 60
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# when length of one vector is multiple of length of smaller vector}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{9}\NormalTok{)}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{7}\NormalTok{, }\DecValTok{8}\NormalTok{)}
\NormalTok{x }\SpecialCharTok{+}\NormalTok{ y}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 12 10 14 17
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# when length of one vector is not multiple of length of smaller vector}
\NormalTok{x }\SpecialCharTok{+} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in x + c(1, 2, 3): longer object length is not a multiple of shorter
## object length
\end{verbatim}

\begin{verbatim}
## [1]  6  4 10 10
\end{verbatim}

All the above-mentioned operators/functions may also be used on matrices, arrays of larger dimension, since we have already seen that matrices/arrays are actually vectors at the core.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mat1 }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\hypertarget{common-arithmetical-functions}{%
\section{Common arithmetical Functions}\label{common-arithmetical-functions}}

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1596}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2447}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2553}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3404}}@{}}
\caption{\label{tab:table4} Common Arithmetical Functions}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
Function
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Meaning
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Input
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Output
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
Function
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Meaning
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Input
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Output
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{sum()} & Adds all elements & One or more Vector, matrix, array & Vector having 1 element only \\
\texttt{prod()} & Returns product of all elements & One or more Vector, matrix, array & Vector having 1 element only \\
\texttt{mean()} & Returns the arithmetic mean & One Vector, matrix, array & Vector having 1 element only \\
\texttt{max()} & Returns maximum value & One or more Vector, matrix, array & Vector having 1 element only \\
\texttt{min()} & Returns minimum value & One or more Vector, matrix, array & Vector having 1 element only \\
\texttt{ceiling()} & Returns integer(s) not less than given values & One Vector, matrix, array & Vector, matrix, array having same \texttt{dim} \\
\texttt{floor()} & Returns largest integers not greater than given values & One Vector, matrix, array & Vector, matrix, array having same \texttt{dim} \\
\texttt{trunc()} & returns integers formed by truncating the values towards 0 & One Vector, matrix, array & Vector, matrix, array having same \texttt{dim} \\
\texttt{round(x,\ digits\ =\ 0)} & Rounds the given value(s) to number of decimal places provided & One Vector, matrix, array & Vector, matrix, array having same \texttt{dim} \\
\texttt{signif(x,\ digits\ =\ 6)} & Round to \texttt{significant} digits & One Vector, matrix, array & Vector, matrix, array having same \texttt{dim} \\
\texttt{factorial()} & Returns factorial & One Vector, matrix, array of \texttt{integer} type & Vector having 1 element \\
\texttt{sqrt()} & Returns square root & One Vector, matrix, array & Vector, matrix, array having same \texttt{dim} \\
\texttt{log10()} or \texttt{log2()} & Logrithm with base 10 or 2 respectively & One Vector, matrix, array & Vector, matrix, array having same \texttt{dim} \\
\texttt{exp(x)} & returns exponential & One Vector, matrix, array & Vector, matrix, array having same \texttt{dim} \\
\end{longtable}

See these examples.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sum}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{100}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 5105
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Mat1 }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{, }\AttributeTok{nrow =} \DecValTok{2}\NormalTok{)}
\NormalTok{Mat2 }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{, }\AttributeTok{nrow =} \DecValTok{2}\NormalTok{)}

\FunctionTok{prod}\NormalTok{(Mat1, Mat2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 87091200
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sqrt}\NormalTok{(Mat2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          [,1]     [,2]
## [1,] 1.000000 1.732051
## [2,] 1.414214 2.000000
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{log10}\NormalTok{(Mat1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         [,1]      [,2]      [,3]     [,4]      [,5]
## [1,] 0.00000 0.4771213 0.6989700 0.845098 0.9542425
## [2,] 0.30103 0.6020600 0.7781513 0.903090 1.0000000
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{factorial}\NormalTok{(}\DecValTok{10}\SpecialCharTok{:}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 3628800  362880   40320    5040     720     120      24       6       2
## [10]       1
\end{verbatim}

\hypertarget{missing-values}{%
\subsection*{Missing values}\label{missing-values}}
\addcontentsline{toc}{subsection}{Missing values}

If the vector on which we are calculating \texttt{sum} etc., has missing values, we will have to use argument \texttt{na.rm\ =\ TRUE} in these functions (Check documentation of these functions individually once). See these examples -

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{50}\NormalTok{, }\ConstantTok{NA}\NormalTok{)}
\FunctionTok{sum}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] NA
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sum}\NormalTok{(x, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1275
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{(x, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 25.5
\end{verbatim}

\hypertarget{some-statistical-functions}{%
\section{Some Statistical functions}\label{some-statistical-functions}}

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1596}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2447}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2553}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3404}}@{}}
\caption{\label{tab:table5} Some commonly used Statistical Functions}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
Function
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Meaning
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Input
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Output
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
Function
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Meaning
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Input
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Output
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{sd()} & Returns standard deviation & One Vector, matrix, array & Vector having 1 element only \\
\texttt{var()} & Returns variance & One or more Vector, matrix, array & Vector having 1 element only \\
\texttt{median()} & Returns median value & One Vector, matrix, array & Vector having 1 element only \\
\texttt{range()} & Returns range & One Vector, matrix, array & Vector having 2 elements \\
\texttt{IQR()} & Computes interquartile range of the x values & One Vector, matrix, array & Vector having 1 element only \\
\texttt{quantile()} & Computes percentile of given values for the given probabilities in \texttt{probs} argument & One Vector, matrix, array & Named Vector having 5 elements by default, OR equal to the length of \texttt{probs} vector given \\
\end{longtable}

Examples-

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{median}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{100}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 50.5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{range}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{100}\NormalTok{, }\DecValTok{45}\NormalTok{, }\DecValTok{789}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]   1 789
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{quantile}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{100}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     0%    25%    50%    75%   100% 
##   1.00  25.75  50.50  75.25 100.00
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{quantile}\NormalTok{(}\DecValTok{0}\SpecialCharTok{:}\DecValTok{100}\NormalTok{, }\AttributeTok{probs =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{10} \SpecialCharTok{/} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  10%  20%  30%  40%  50%  60%  70%  80%  90% 100% 
##   10   20   30   40   50   60   70   80   90  100
\end{verbatim}

\hypertarget{prob}{%
\section{Functions related to sampling and probability distributions}\label{prob}}

\hypertarget{set-the-random-seed-with-set.seed}{%
\subsubsection*{\texorpdfstring{Set the random seed with \texttt{set.seed()}}{Set the random seed with set.seed()}}\label{set-the-random-seed-with-set.seed}}
\addcontentsline{toc}{subsubsection}{Set the random seed with \texttt{set.seed()}}

It is a way to specify the random seed which is an integer vector, containing the random number generator (RNG) state for random number generation in R. It does not given any output but makes your code reproducible for further use.

\hypertarget{generate-random-numbers-with-rnorm-runif-rpois-etc.}{%
\subsubsection*{\texorpdfstring{Generate random numbers with \texttt{rnorm()} / \texttt{runif()} / \texttt{rpois()} etc.}{Generate random numbers with rnorm() / runif() / rpois() etc.}}\label{generate-random-numbers-with-rnorm-runif-rpois-etc.}}
\addcontentsline{toc}{subsubsection}{Generate random numbers with \texttt{rnorm()} / \texttt{runif()} / \texttt{rpois()} etc.}

Used to generate random numbers from normal, uniform and poisson distributions respectively. Of course there are numerous other functions not only to calculate random numbers but to calculate probability, density of these and other probability distributions (such as binomial, t), but those are beyond the scope of this book. E.g.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n=}\DecValTok{10}\NormalTok{) }\CommentTok{\#default mean is 0 and SD is 1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1]  0.59055198 -0.96394090 -0.08338458 -1.35615133 -0.86143140 -0.48814507
##  [7]  0.29985816  0.86630968  0.27151584  0.27940490
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n=}\DecValTok{10}\NormalTok{) }\CommentTok{\# notice these will produce different results each time.}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1]  1.14718988 -1.06385908  0.06620761 -0.45037494  1.30109267  2.34139400
##  [7] -1.84607954 -0.23498638  1.64025014  0.13526982
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# If however seed is fixed as above, these will be reproducible.}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\FunctionTok{runif}\NormalTok{(}\DecValTok{10}\NormalTok{) }\CommentTok{\# default min and max are 0 and 1 respectively}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 0.2875775 0.7883051 0.4089769 0.8830174 0.9404673 0.0455565 0.5281055
##  [8] 0.8924190 0.5514350 0.4566147
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\FunctionTok{runif}\NormalTok{(}\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 0.2875775 0.7883051 0.4089769 0.8830174 0.9404673 0.0455565 0.5281055
##  [8] 0.8924190 0.5514350 0.4566147
\end{verbatim}

\hypertarget{random-sample-with-sample}{%
\subsubsection*{\texorpdfstring{Random Sample with \texttt{sample()}}{Random Sample with sample()}}\label{random-sample-with-sample}}
\addcontentsline{toc}{subsubsection}{Random Sample with \texttt{sample()}}

Used to take a sample of the specified \texttt{size} from the elements of x using either with or without replacement. E.g.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\FunctionTok{sample}\NormalTok{(LETTERS, }\DecValTok{5}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "O" "S" "N" "C" "J"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{111}\NormalTok{)}
\FunctionTok{sample}\NormalTok{(LETTERS, }\DecValTok{15}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "N" "T" "S" "O" "Y" "E" "C" "H" "Z" "Q" "M" "J" "D" "O" "H"
\end{verbatim}

If the sampling is proportionate to given \texttt{probabilities} the same can be provided in \texttt{prob} argument.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{12}\NormalTok{)}
\FunctionTok{sample}\NormalTok{(LETTERS, }\DecValTok{5}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{prob =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{26}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Z" "K" "F" "V" "X"
\end{verbatim}

\hypertarget{other-mathematical-functions}{%
\section{Other Mathematical functions}\label{other-mathematical-functions}}

\hypertarget{progressive-calculations-with-cumsum-cumprod}{%
\subsubsection*{\texorpdfstring{Progressive calculations with \texttt{cumsum()} /\texttt{cumprod()}}{Progressive calculations with cumsum() /cumprod()}}\label{progressive-calculations-with-cumsum-cumprod}}
\addcontentsline{toc}{subsubsection}{Progressive calculations with \texttt{cumsum()} /\texttt{cumprod()}}

Used to calculate running total or product. Output vector length will be equal to that of input vector.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cumsum}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1]  1  3  6 10 15 21 28 36 45 55
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cumprod}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{5}\SpecialCharTok{:}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1]   -5   20  -60  120 -120    0    0    0    0    0    0
\end{verbatim}

Other similar functions like \texttt{cummax()} (cumulative maximum) and \texttt{cummin()} may also be useful.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{100}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\FunctionTok{cummin}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 68 39  1  1  1  1  1  1  1  1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cummax}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 68 68 68 68 87 87 87 87 87 87
\end{verbatim}

\hypertarget{progressive-difference-diff}{%
\subsubsection*{\texorpdfstring{Progressive difference \texttt{diff()}}{Progressive difference diff()}}\label{progressive-difference-diff}}
\addcontentsline{toc}{subsubsection}{Progressive difference \texttt{diff()}}

Used to calculate running difference (difference between two consecutive elements) in the given numeric vector. Output will be shorter by one element. E.g.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{10}\NormalTok{)}
\NormalTok{x}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] -0.56047565 -0.23017749  1.55870831  0.07050839  0.12928774  1.71506499
##  [7]  0.46091621 -1.26506123 -0.68685285 -0.44566197
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{diff}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  0.33029816  1.78888580 -1.48819992  0.05877934  1.58577725 -1.25414878
## [7] -1.72597744  0.57820838  0.24119088
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{length}\NormalTok{(}\FunctionTok{diff}\NormalTok{(x))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 9
\end{verbatim}

\hypertarget{string-manipulation-functions}{%
\section{String Manipulation functions}\label{string-manipulation-functions}}

\hypertarget{concatenate-strings-with-paste-and-paste0}{%
\subsubsection*{\texorpdfstring{Concatenate strings with \texttt{paste()} and \texttt{paste0()}}{Concatenate strings with paste() and paste0()}}\label{concatenate-strings-with-paste-and-paste0}}
\addcontentsline{toc}{subsubsection}{Concatenate strings with \texttt{paste()} and \texttt{paste0()}}

R's inbuilt function \texttt{paste()} concatenates each element of one or more vectors given as argument. Argument \texttt{sep} is used to provide separator is any, which by default is a space i.e.~\texttt{"\ "}. On the other \texttt{sep} argument is not available in \texttt{paste0} which thus concatenates elements without any separator.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{paste}\NormalTok{(LETTERS, letters)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "A a" "B b" "C c" "D d" "E e" "F f" "G g" "H h" "I i" "J j" "K k" "L l"
## [13] "M m" "N n" "O o" "P p" "Q q" "R r" "S s" "T t" "U u" "V v" "W w" "X x"
## [25] "Y y" "Z z"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{paste0}\NormalTok{(letters, }\StringTok{\textquotesingle{}\_\textquotesingle{}}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{26}\NormalTok{) }\CommentTok{\# check replication here}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "a_1"  "b_2"  "c_3"  "d_4"  "e_5"  "f_6"  "g_7"  "h_8"  "i_9"  "j_10"
## [11] "k_11" "l_12" "m_13" "n_14" "o_15" "p_16" "q_17" "r_18" "s_19" "t_20"
## [21] "u_21" "v_22" "w_23" "x_24" "y_25" "z_26"
\end{verbatim}

\emph{Note:} that both \texttt{paste} and \texttt{paste0} returns vector with length equal to length of larger vector. \textbf{Thus if the requirement is to concatenate each of the element in the given vector(s), use another argument \texttt{collapse}. See this example.}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{paste0}\NormalTok{(letters, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{26}\NormalTok{, }\AttributeTok{collapse =} \StringTok{\textquotesingle{}+\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "a1+b2+c3+d4+e5+f6+g7+h8+i9+j10+k11+l12+m13+n14+o15+p16+q17+r18+s19+t20+u21+v22+w23+x24+y25+z26"
\end{verbatim}

\hypertarget{functions-startswith-endswith}{%
\subsubsection*{\texorpdfstring{Functions \texttt{startsWith()} / \texttt{endsWith()}}{Functions startsWith() / endsWith()}}\label{functions-startswith-endswith}}
\addcontentsline{toc}{subsubsection}{Functions \texttt{startsWith()} / \texttt{endsWith()}}

To check whether the given string vector say \texttt{x} start or end with string (entries of) \texttt{prefix} or \texttt{suffix} we can use \texttt{startsWith(x,\ prefix)} or \texttt{endsWith(x,\ suffix)} respectively. E.g.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}apples\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}oranges\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}apples and oranges\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}oranges and apples\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}apricots\textquotesingle{}}\NormalTok{)}
\FunctionTok{startsWith}\NormalTok{(x, }\StringTok{\textquotesingle{}apples\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  TRUE FALSE  TRUE FALSE FALSE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{startsWith}\NormalTok{(x, }\StringTok{\textquotesingle{}ap\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  TRUE FALSE  TRUE FALSE  TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{endsWith}\NormalTok{(x, }\StringTok{\textquotesingle{}oranges\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] FALSE  TRUE  TRUE FALSE FALSE
\end{verbatim}

Note that both these functions return logical vectors having same length as \texttt{x}.

\hypertarget{check-number-of-characters-in-string-vector-using-nchar}{%
\subsubsection*{\texorpdfstring{Check number of characters in string vector using \texttt{nchar()}}{Check number of characters in string vector using nchar()}}\label{check-number-of-characters-in-string-vector-using-nchar}}
\addcontentsline{toc}{subsubsection}{Check number of characters in string vector using \texttt{nchar()}}

To count the number of characters in each of the element in string vector, say \texttt{x}, we can use \texttt{nchar(x)} which will return a vector of integer types. E.g.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{nchar}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  6  7 18 18  8
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{} \textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}   \textquotesingle{}}\NormalTok{, }\ConstantTok{NA}\NormalTok{)}
\FunctionTok{nchar}\NormalTok{(y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  0  1  3 NA
\end{verbatim}

\hypertarget{change-case-using-toupper-tolower}{%
\subsubsection*{\texorpdfstring{Change case using \texttt{toupper()} / \texttt{tolower()}}{Change case using toupper() / tolower()}}\label{change-case-using-toupper-tolower}}
\addcontentsline{toc}{subsubsection}{Change case using \texttt{toupper()} / \texttt{tolower()}}

Changes the case of given vector to all UPPER or lower case respectively.
Example-

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}Andrew\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Bob\textquotesingle{}}\NormalTok{)}
\FunctionTok{tolower}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "andrew" "bob"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{toupper}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "ANDREW" "BOB"
\end{verbatim}

\hypertarget{extract-a-portion-of-string-using-substr}{%
\subsubsection*{\texorpdfstring{Extract a portion of string using \texttt{substr()}}{Extract a portion of string using substr()}}\label{extract-a-portion-of-string-using-substr}}
\addcontentsline{toc}{subsubsection}{Extract a portion of string using \texttt{substr()}}

To extract the characters from a given vector say \texttt{x} from a given \texttt{start} position to \texttt{stop} position (both being integers) we will use \texttt{substr(x,\ start,\ stop)}. E.g.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{substr}\NormalTok{(x, }\DecValTok{2}\NormalTok{, }\DecValTok{8}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "ndrew" "ob"
\end{verbatim}

\hypertarget{split-a-character-vector-using-strsplit}{%
\subsubsection*{\texorpdfstring{Split a character vector using \texttt{strsplit()}}{Split a character vector using strsplit()}}\label{split-a-character-vector-using-strsplit}}
\addcontentsline{toc}{subsubsection}{Split a character vector using \texttt{strsplit()}}

To split the elements of a character vector \texttt{x} into sub-strings according to the matches to sub-string \texttt{split} within them. E.g.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{strsplit}\NormalTok{(x, }\AttributeTok{split =} \StringTok{\textquotesingle{} \textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [[1]]
## [1] "Andrew"
## 
## [[2]]
## [1] "Bob"
\end{verbatim}

\textbf{Notice that output will be of \texttt{list} type.}

\hypertarget{replace-portions-of-string-vectors-sub-gsub}{%
\subsubsection*{\texorpdfstring{Replace portions of string vectors \texttt{sub()} / \texttt{gsub()}}{Replace portions of string vectors sub() / gsub()}}\label{replace-portions-of-string-vectors-sub-gsub}}
\addcontentsline{toc}{subsubsection}{Replace portions of string vectors \texttt{sub()} / \texttt{gsub()}}

These two functions are used to perform replacement of the first and all matches respectively. E.g.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Replace only first match}
\FunctionTok{sub}\NormalTok{(}\AttributeTok{pattern =} \StringTok{\textquotesingle{}B\textquotesingle{}}\NormalTok{, }\AttributeTok{replacement =} \StringTok{\textquotesingle{}12\textquotesingle{}}\NormalTok{, x, }\AttributeTok{ignore.case =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Andrew" "12ob"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Replace all matches}
\FunctionTok{gsub}\NormalTok{(}\AttributeTok{pattern =} \StringTok{\textquotesingle{}B\textquotesingle{}}\NormalTok{, }\AttributeTok{replacement =} \StringTok{\textquotesingle{}12\textquotesingle{}}\NormalTok{, x, }\AttributeTok{ignore.case =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Andrew" "12o12"
\end{verbatim}

\hypertarget{match-patterns-using-grep-grepl-regexpr-gregexpr}{%
\subsubsection*{\texorpdfstring{Match patterns using \texttt{grep()} / \texttt{grepl()} / \texttt{regexpr()} / \texttt{gregexpr()}}{Match patterns using grep() / grepl() / regexpr() / gregexpr()}}\label{match-patterns-using-grep-grepl-regexpr-gregexpr}}
\addcontentsline{toc}{subsubsection}{Match patterns using \texttt{grep()} / \texttt{grepl()} / \texttt{regexpr()} / \texttt{gregexpr()}}

These functions are used to match string passed as argument \texttt{pattern} under a string vector. These four however, differ in output/results. E.g.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{grep}\NormalTok{(}\AttributeTok{pattern =} \StringTok{\textquotesingle{}an\textquotesingle{}}\NormalTok{, x) }\CommentTok{\# will give indices.  }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## integer(0)
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#                         Output will be integer vector and length may be shorter than that of \textasciigrave{}x\textasciigrave{}}
\FunctionTok{grepl}\NormalTok{(}\AttributeTok{pattern =} \StringTok{\textquotesingle{}an\textquotesingle{}}\NormalTok{, x) }\CommentTok{\# will give a logical vector of same length as \textasciigrave{}x\textasciigrave{}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] FALSE FALSE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{regexpr}\NormalTok{(}\AttributeTok{pattern =} \StringTok{\textquotesingle{}an\textquotesingle{}}\NormalTok{, x) }\CommentTok{\# output will have multiple attributes}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -1 -1
## attr(,"match.length")
## [1] -1 -1
## attr(,"index.type")
## [1] "chars"
## attr(,"useBytes")
## [1] TRUE
\end{verbatim}

Note that \texttt{regexpr()} outputs the character position of first instance of pattern match within the elements of given vector.
\texttt{gregexpr()} is same as \texttt{regexpr()} but finds all instances of pattern. Output will be in \texttt{list} format. E.g.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gregexpr}\NormalTok{(}\AttributeTok{pattern =} \StringTok{\textquotesingle{}an\textquotesingle{}}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [[1]]
## [1] -1
## attr(,"match.length")
## [1] -1
## attr(,"index.type")
## [1] "chars"
## attr(,"useBytes")
## [1] TRUE
## 
## [[2]]
## [1] -1
## attr(,"match.length")
## [1] -1
## attr(,"index.type")
## [1] "chars"
## attr(,"useBytes")
## [1] TRUE
\end{verbatim}

\hypertarget{other-functions}{%
\section{Other functions}\label{other-functions}}

\hypertarget{transpose-a-matrix-using-t}{%
\subsubsection*{\texorpdfstring{Transpose a matrix using \texttt{t()}}{Transpose a matrix using t()}}\label{transpose-a-matrix-using-t}}
\addcontentsline{toc}{subsubsection}{Transpose a matrix using \texttt{t()}}

Used to return transpose of given matrix. E.g.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mat }\OtherTok{\textless{}{-}} \FunctionTok{outer}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{, }\AttributeTok{FUN =}\NormalTok{ \textbackslash{}(x, y) }\FunctionTok{paste0}\NormalTok{(}\StringTok{\textquotesingle{}A\textquotesingle{}}\NormalTok{, x, y))}
\NormalTok{mat}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1]  [,2]  [,3]  [,4]  [,5] 
## [1,] "A11" "A12" "A13" "A14" "A15"
## [2,] "A21" "A22" "A23" "A24" "A25"
## [3,] "A31" "A32" "A33" "A34" "A35"
## [4,] "A41" "A42" "A43" "A44" "A45"
## [5,] "A51" "A52" "A53" "A54" "A55"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{t}\NormalTok{(mat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1]  [,2]  [,3]  [,4]  [,5] 
## [1,] "A11" "A21" "A31" "A41" "A51"
## [2,] "A12" "A22" "A32" "A42" "A52"
## [3,] "A13" "A23" "A33" "A43" "A53"
## [4,] "A14" "A24" "A34" "A44" "A54"
## [5,] "A15" "A25" "A35" "A45" "A55"
\end{verbatim}

\hypertarget{generate-a-frequency-table-using-table}{%
\subsubsection*{\texorpdfstring{Generate a frequency table using \texttt{table()}}{Generate a frequency table using table()}}\label{generate-a-frequency-table-using-table}}
\addcontentsline{toc}{subsubsection}{Generate a frequency table using \texttt{table()}}

Returns a frequency/contingency table of the counts at each combination of factor levels. E.g.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(LETTERS[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{], }\DecValTok{100}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{table}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## x
##  A  B  C  D  E 
## 21 20 23 17 19
\end{verbatim}

If more than one argument is passed-

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1234}\NormalTok{)}
\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{State\_code =}\NormalTok{ x,}
                 \AttributeTok{Code2 =} \FunctionTok{sample}\NormalTok{(LETTERS[}\DecValTok{11}\SpecialCharTok{:}\DecValTok{15}\NormalTok{], }\DecValTok{100}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{))}
\NormalTok{my\_table }\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{State\_code, df}\SpecialCharTok{$}\NormalTok{Code2)}
\NormalTok{my\_table}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    
##     K L M N O
##   A 5 5 4 4 3
##   B 4 3 6 2 5
##   C 6 3 3 6 5
##   D 2 2 4 6 3
##   E 2 6 4 4 3
\end{verbatim}

\hypertarget{generate-proportion-of-frequencies-using-prop.table}{%
\subsubsection*{\texorpdfstring{Generate proportion of frequencies using \texttt{prop.table()}}{Generate proportion of frequencies using prop.table()}}\label{generate-proportion-of-frequencies-using-prop.table}}
\addcontentsline{toc}{subsubsection}{Generate proportion of frequencies using \texttt{prop.table()}}

This function takes a table object as input and calculate the proportion of frequencies.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{prop.table}\NormalTok{(my\_table)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    
##        K    L    M    N    O
##   A 0.05 0.05 0.04 0.04 0.03
##   B 0.04 0.03 0.06 0.02 0.05
##   C 0.06 0.03 0.03 0.06 0.05
##   D 0.02 0.02 0.04 0.06 0.03
##   E 0.02 0.06 0.04 0.04 0.03
\end{verbatim}

\hypertarget{column-wise-or-row-wise-sums-using-colsums-rowsums}{%
\subsubsection*{\texorpdfstring{Column-wise or Row-wise sums using \texttt{colSums()} / \texttt{rowSums()}}{Column-wise or Row-wise sums using colSums() / rowSums()}}\label{column-wise-or-row-wise-sums-using-colsums-rowsums}}
\addcontentsline{toc}{subsubsection}{Column-wise or Row-wise sums using \texttt{colSums()} / \texttt{rowSums()}}

Used to sum rows/columns in a matrix/data.frame. E.g.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Row sums}
\FunctionTok{rowSums}\NormalTok{(my\_table)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  A  B  C  D  E 
## 21 20 23 17 19
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Col sums}
\FunctionTok{colSums}\NormalTok{(my\_table)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  K  L  M  N  O 
## 19 19 21 22 19
\end{verbatim}

Note Similar to \texttt{colSums()}/ \texttt{rowSums()} we also have \texttt{colMeans()} and \texttt{rowMeans()}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rowMeans}\NormalTok{(my\_table)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   A   B   C   D   E 
## 4.2 4.0 4.6 3.4 3.8
\end{verbatim}

\hypertarget{extract-unique-values-using-unique}{%
\subsubsection*{\texorpdfstring{Extract unique values using \texttt{unique()}}{Extract unique values using unique()}}\label{extract-unique-values-using-unique}}
\addcontentsline{toc}{subsubsection}{Extract unique values using \texttt{unique()}}

Used to extract only unique values/elements from the given vector. E.g.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{unique}\NormalTok{(x) }\CommentTok{\# note the output}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "C" "B" "E" "D" "A"
\end{verbatim}

\hypertarget{check-if-two-vectors-are-identical-using-identical}{%
\subsubsection*{\texorpdfstring{Check if two vectors are identical using \texttt{identical()}}{Check if two vectors are identical using identical()}}\label{check-if-two-vectors-are-identical-using-identical}}
\addcontentsline{toc}{subsubsection}{Check if two vectors are identical using \texttt{identical()}}

Used to check whether two given vectors/objects are identical.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{identical}\NormalTok{(}\FunctionTok{unique}\NormalTok{(x), LETTERS)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] FALSE
\end{verbatim}

\hypertarget{retreive-duplicate-items-in-a-vector-using-duplicated}{%
\subsubsection*{\texorpdfstring{Retreive duplicate items in a vector using \texttt{duplicated()}}{Retreive duplicate items in a vector using duplicated()}}\label{retreive-duplicate-items-in-a-vector-using-duplicated}}
\addcontentsline{toc}{subsubsection}{Retreive duplicate items in a vector using \texttt{duplicated()}}

Used to check which elements have already appeared in the vector and are thus duplicate.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(LETTERS[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{], }\DecValTok{8}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{x}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "C" "C" "B" "B" "C" "E" "D" "A"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{duplicated}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] FALSE  TRUE FALSE  TRUE  TRUE FALSE FALSE FALSE
\end{verbatim}

\hypertarget{generate-sequences-using-other-objects-with-seq_len-seq_along}{%
\subsubsection*{\texorpdfstring{Generate sequences using other objects with \texttt{seq\_len()} / \texttt{seq\_along()}}{Generate sequences using other objects with seq\_len() / seq\_along()}}\label{generate-sequences-using-other-objects-with-seq_len-seq_along}}
\addcontentsline{toc}{subsubsection}{Generate sequences using other objects with \texttt{seq\_len()} / \texttt{seq\_along()}}

Used to generate sequence of given integer length starting with 1, or with length equal to given vector, respectively. E.g.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{seq\_len}\NormalTok{(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1 2 3 4 5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}Andrew\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Bob\textquotesingle{}}\NormalTok{)}
\FunctionTok{seq\_along}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1 2
\end{verbatim}

\hypertarget{divide-a-vector-into-categories-factor-using-cut}{%
\subsubsection*{\texorpdfstring{Divide a vector into categories (factor) using \texttt{cut()}}{Divide a vector into categories (factor) using cut()}}\label{divide-a-vector-into-categories-factor-using-cut}}
\addcontentsline{toc}{subsubsection}{Divide a vector into categories (factor) using \texttt{cut()}}

The function divides the range of \texttt{x} into intervals and codes the values in \texttt{x} according to which interval they fall. The leftmost interval corresponds to level one, the next leftmost to level two and so on. The output vector will be of type \texttt{factor.}

Example-1:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{5}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{5}\NormalTok{,}\DecValTok{6}\NormalTok{,}\DecValTok{7}\NormalTok{)}
\FunctionTok{cut}\NormalTok{(x, }\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] (0.994,3] (0.994,3] (0.994,3] (3,5]     (3,5]     (0.994,3] (0.994,3]
##  [8] (3,5]     (3,5]     (5,7.01]  (5,7.01] 
## Levels: (0.994,3] (3,5] (5,7.01]
\end{verbatim}

Example-2:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cut}\NormalTok{(x, }\DecValTok{3}\NormalTok{, }\AttributeTok{dig.lab =} \DecValTok{1}\NormalTok{, }\AttributeTok{ordered\_result =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] (1,3] (1,3] (1,3] (3,5] (3,5] (1,3] (1,3] (3,5] (3,5] (5,7] (5,7]
## Levels: (1,3] < (3,5] < (5,7]
\end{verbatim}

\textbf{Note:} that the output \texttt{factor} above is ordered.

\hypertarget{scale-the-columns-of-a-matrix-using-scale}{%
\subsubsection*{\texorpdfstring{Scale the columns of a matrix using \texttt{scale()}}{Scale the columns of a matrix using scale()}}\label{scale-the-columns-of-a-matrix-using-scale}}
\addcontentsline{toc}{subsubsection}{Scale the columns of a matrix using \texttt{scale()}}

Used to scale the columns of a numeric matrix.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{, }\AttributeTok{ncol =} \DecValTok{2}\NormalTok{)}
\NormalTok{x}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2]
## [1,]    1    6
## [2,]    2    7
## [3,]    3    8
## [4,]    4    9
## [5,]    5   10
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{scale}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##            [,1]       [,2]
## [1,] -1.2649111 -1.2649111
## [2,] -0.6324555 -0.6324555
## [3,]  0.0000000  0.0000000
## [4,]  0.6324555  0.6324555
## [5,]  1.2649111  1.2649111
## attr(,"scaled:center")
## [1] 3 8
## attr(,"scaled:scale")
## [1] 1.581139 1.581139
\end{verbatim}

\textbf{Note:} The output will always be of a matrix type with two more attributes. See this example

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{scale}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##            [,1]
## [1,] -1.2649111
## [2,] -0.6324555
## [3,]  0.0000000
## [4,]  0.6324555
## [5,]  1.2649111
## attr(,"scaled:center")
## [1] 3
## attr(,"scaled:scale")
## [1] 1.581139
\end{verbatim}

\hypertarget{cat}{%
\subsubsection*{\texorpdfstring{Output the results using \texttt{cat()}}{Output the results using cat()}}\label{cat}}
\addcontentsline{toc}{subsubsection}{Output the results using \texttt{cat()}}

Outputs the objects, concatenating the representations. \texttt{cat} performs much less conversion than \texttt{print}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}ABCD\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## ABCD
\end{verbatim}

\textbf{Note:} that indices are now \emph{not printed.} \texttt{cat} may print objects also. Example-2:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(month.name)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## January February March April May June July August September October November December
\end{verbatim}

\texttt{cat} is useful to print \emph{special characters.} Example-3:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}Budget Allocation is \textbackslash{}u20b91.5 crore\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Budget Allocation is ₹1.5 crore
\end{verbatim}

\hypertarget{sort-a-vector-using-sort}{%
\subsubsection*{\texorpdfstring{Sort a vector using \texttt{sort()}}{Sort a vector using sort()}}\label{sort-a-vector-using-sort}}
\addcontentsline{toc}{subsubsection}{Sort a vector using \texttt{sort()}}

Used to \textbf{sort} the given vector. Example-1:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{vec }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{6}\NormalTok{)}
\FunctionTok{sort}\NormalTok{(vec)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1 4 5 6 8
\end{verbatim}

Argumemt \texttt{decreasing\ =\ TRUE} is used to sort the vector in descending order instead of default ascending order.
Example-2:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sort}\NormalTok{(vec, }\AttributeTok{decreasing =}  \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 8 6 5 4 1
\end{verbatim}

\hypertarget{arrange-the-elements-of-a-vector-using-order}{%
\subsubsection*{\texorpdfstring{Arrange the elements of a vector using \texttt{order()}}{Arrange the elements of a vector using order()}}\label{arrange-the-elements-of-a-vector-using-order}}
\addcontentsline{toc}{subsubsection}{Arrange the elements of a vector using \texttt{order()}}

In contrast to \texttt{sort()} explained above, \texttt{order()} returns the indices of given vector in ascending order. Example

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{order}\NormalTok{(vec)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 4 3 1 5 2
\end{verbatim}

Thus, \texttt{sort(vec)} will essentially perform the same operations as \texttt{vec{[}order(vec){]}}. We may check-

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{identical}\NormalTok{(vec[}\FunctionTok{order}\NormalTok{(vec)], }\FunctionTok{sort}\NormalTok{(vec))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

\hypertarget{check-structure-using-str}{%
\subsubsection*{\texorpdfstring{Check structure using \texttt{str()}}{Check structure using str()}}\label{check-structure-using-str}}
\addcontentsline{toc}{subsubsection}{Check structure using \texttt{str()}}

The short \texttt{str} is not to be confused with strings as it instead is short for \texttt{structure}. Thus, \texttt{str} returns structure of given object. Example

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(vec)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  num [1:5] 5 8 4 1 6
\end{verbatim}

Extremely useful when we need to inspect data frames.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(iris)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 'data.frame':    150 obs. of  5 variables:
##  $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...
##  $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...
##  $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...
##  $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...
##  $ Species     : Factor w/ 3 levels "setosa","versicolor",..: 1 1 1 1 1 1 1 1 1 1 ...
\end{verbatim}

\hypertarget{generate-a-summary-using-summary}{%
\subsubsection*{\texorpdfstring{Generate a summary using \texttt{summary()}}{Generate a summary using summary()}}\label{generate-a-summary-using-summary}}
\addcontentsline{toc}{subsubsection}{Generate a summary using \texttt{summary()}}

In addition to \texttt{str} explained above, \texttt{summary()} is also useful is getting result summaries of given objects. Example-1: When given object is vector

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(vec)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##     1.0     4.0     5.0     4.8     6.0     8.0
\end{verbatim}

We observe that when numeric vector is passed, it produces quantile summary. Example-2: When input object is data frame.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(iris)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   
##  Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  
##  1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  
##  Median :5.800   Median :3.000   Median :4.350   Median :1.300  
##  Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  
##  3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  
##  Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  
##        Species  
##  setosa    :50  
##  versicolor:50  
##  virginica :50  
##                 
##                 
## 
\end{verbatim}

\hypertarget{pipes}{%
\section{Pipes}\label{pipes}}

Now here I would like to introduce you with the concept of pipes\index{pipes} in R. There are two types of pipes used-

\begin{itemize}
\tightlist
\item
  \texttt{\textbar{}\textgreater{}} is native pipe of R. It was introduced in R version 4.1
\item
  \texttt{\%\textgreater{}\%} pipe introduced in \texttt{magrittr} package\citep{R-magrittr}, now part of \texttt{tidyverse} which we will use extensively in our data analysis tasks.
\end{itemize}

\begin{figure}

{\centering \includegraphics[width=0.99\linewidth]{images/pipemag} 

}

\caption{Magrittr, the R package, is named after the surrealist painter Rene' Magritte. His painting was self captioned, 'This is not a pipe.'}\label{fig:pipe2}
\end{figure}

Actually \texttt{\%\textgreater{}\%} is predecessor to native R's pipe \texttt{\textbar{}\textgreater{}}. The pipes are powerful tools for clearly expressing a sequence of operations that transform an object, without the need of actually creating that object in each step. Let us understand this concept with the following example. Suppose, we have to three functions say \texttt{FIRST} , \texttt{SECOND} and \texttt{THIRD} to an object \texttt{OBJ} in sequence. So the order of operations would either be like-

\begin{verbatim}
THIRD(SECOND(FIRST(OBJ)))
\end{verbatim}

or with creating intermediate objects, when instead we actually do not need those intermediate objects.

\begin{verbatim}
OBJ1 <- FIRST(OBJ)
OBJ2 <- SECOND(OBJ1)
OBJ3 <- THIRD(OBJ2)
\end{verbatim}

Here actually we do not require \texttt{OBJ1} and \texttt{OBJ2}. So in these cases we either have to compromise with the readability of code i.e.~inside out or have to create unwanted objects. Pipes actually mitigate both these issues simultaneously. With pipes we can write above operations as either of these -

\begin{verbatim}
OBJ1 |> FIRST() |> SECOND() |> THIRD()
OBJ1 %>% FIRST() %>% SECOND() %>% THIRD()
\end{verbatim}

A diagrammatic representation is given in figure \ref{fig:pipe}.

\begin{figure}

{\centering \includegraphics[width=0.99\linewidth]{images/pipe} 

}

\caption{A diagrammtic illustation of Pipe concept in base R and tidyverse}\label{fig:pipe}
\end{figure}

Now two questions may arise here-

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What if there are multiple arguments to be passed in any of the operations?
\item
  Is there any difference between the two pipes? If yes, which is better OR what are the pros and cons of each?
\end{enumerate}

To answer these questions, we will discuss both pipes separately.

\begin{figure}

{\centering \includegraphics[width=0.99\linewidth]{images/pipes in R} 

}

\caption{Using pipes in R}\label{fig:pipe3}
\end{figure}

\hypertarget{magrittrdplyr-pipe}{%
\subsection{\texorpdfstring{Magrittr/Dplyr pipe \texttt{\%\textgreater{}\%}}{Magrittr/Dplyr pipe \%\textgreater\%}}\label{magrittrdplyr-pipe}}

Pipes usually pass result of previous operation silently into first argument of next/right expression. So \texttt{data\ \%\textgreater{}\%\ filter(col\ ==\ \textquotesingle{}A\textquotesingle{})} means \texttt{filter(data,\ col==\textquotesingle{}A\textquotesingle{})}. But there may be cases when result of previous (LHS) expression is required to be passed on second or other argument in RHS expression. A simple example may be of function \texttt{lm}, where \texttt{data} argument is second argument. In such cases we can make use special placeholder \texttt{.} as result of LHS specifically. In other words aforesaid filter example can be written with placeholder as \texttt{data\ \%\textgreater{}\%\ filter(.\ ,\ col\ ==\ \textquotesingle{}A\textquotesingle{})}. Now using this placeholder we can use result of LHS wherever we want. See this example

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{lm}\NormalTok{(Sepal.Length }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Sepal.Width, }\AttributeTok{data =}\NormalTok{ .)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = Sepal.Length ~ Sepal.Width, data = .)
## 
## Coefficients:
## (Intercept)  Sepal.Width  
##      6.5262      -0.2234
\end{verbatim}

Thus \texttt{x\ \%\textgreater{}\%\ f(y)} is equivalent to \texttt{f(x,\ y)} but \texttt{x\ \%\textgreater{}\%\ f(y,\ .)} is equivalent to \texttt{f(y,\ x)}.

\hypertarget{base-r-pipe-version-4.2.0}{%
\subsection{\texorpdfstring{Base R pipe \texttt{\textbar{}\textgreater{}} (Version 4.2.0 +)}{Base R pipe \textbar\textgreater{} (Version 4.2.0 +)}}\label{base-r-pipe-version-4.2.0}}

R version 4.2.0 introduced concept of placeholder \texttt{\_} similar to dplyr/magrittr, but with a few differences.

\begin{itemize}
\tightlist
\item
  The argument where \texttt{\_} is to be used, must be named. So \texttt{f(y,\ z\ =\ x)} can be written as \texttt{x\ \textbar{}\textgreater{}\ f(y,\ z=\ \_)}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris }\SpecialCharTok{|\textgreater{}} \FunctionTok{lm}\NormalTok{(Sepal.Length }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Sepal.Width, }\AttributeTok{data =}\NormalTok{ \_) }\SpecialCharTok{|\textgreater{}} \FunctionTok{summary}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = Sepal.Length ~ Sepal.Width, data = iris)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.5561 -0.6333 -0.1120  0.5579  2.2226 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept)   6.5262     0.4789   13.63   <2e-16 ***
## Sepal.Width  -0.2234     0.1551   -1.44    0.152    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.8251 on 148 degrees of freedom
## Multiple R-squared:  0.01382,    Adjusted R-squared:  0.007159 
## F-statistic: 2.074 on 1 and 148 DF,  p-value: 0.1519
\end{verbatim}

The requirement of named argument is not there in dplyr pipe. So essentially, \texttt{iris\ \%\textgreater{}\%\ lm(Sepal.Length\ \textasciitilde{}\ Sepal.Width,\ .)} will also work. But in base R \texttt{iris\ \textbar{}\textgreater{}\ lm(Sepal.Length\ \textasciitilde{}\ Sepal.Width,\ \_)} would not work and throw an error. Thus, in cases where the argument of placeholder is not named, we have to use anonymous function. See these-

\begin{verbatim}
# placeholder without named argument
iris |> lm(Sepal.Length ~ Sepal.Width, _)
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Correct way to use unnamed argument}
\NormalTok{iris }\SpecialCharTok{|\textgreater{}}\NormalTok{ \{\textbackslash{}(.x) }\FunctionTok{lm}\NormalTok{(Sepal.Length }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Sepal.Width, .x)\}() }\SpecialCharTok{|\textgreater{}} \FunctionTok{summary}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = Sepal.Length ~ Sepal.Width, data = .x)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.5561 -0.6333 -0.1120  0.5579  2.2226 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept)   6.5262     0.4789   13.63   <2e-16 ***
## Sepal.Width  -0.2234     0.1551   -1.44    0.152    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.8251 on 148 degrees of freedom
## Multiple R-squared:  0.01382,    Adjusted R-squared:  0.007159 
## F-statistic: 2.074 on 1 and 148 DF,  p-value: 0.1519
\end{verbatim}

Type \texttt{?\textasciigrave{}\textbar{}\textgreater{}\textasciigrave{}} in console and see help page for more details.

\hypertarget{control-statements}{%
\chapter{Control statements}\label{control-statements}}

In the previous chapter we learnt about many of the useful pre-built functions in R. In this chapter we will learn how to create customized functions suited to our needs.

\textbf{\emph{Though these are core concepts of a programming language, yet a reading to this chapter is advised for better understanding and better application while using r for data analytics.}}

\hypertarget{control-flowloops}{%
\section{Control flow/Loops}\label{control-flowloops}}

\hypertarget{if-else}{%
\subsection*{\texorpdfstring{\texttt{if\ else}}{if else}}\label{if-else}}
\addcontentsline{toc}{subsection}{\texttt{if\ else}}

The basic form(s) of \texttt{if\ else} \index{if else statement}statement in R, are-

\begin{verbatim}
if (test) do_this_if_true
if (test) do_this_if_true else else_do_this
\end{verbatim}

So, if \texttt{test} is true, \texttt{do\_this\_if\_true} will be performed and optionally if \texttt{test} is not true \texttt{else\_do\_this} will be performed. See this example-

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OtherTok{\textless{}{-}} \DecValTok{50}
\ControlFlowTok{if}\NormalTok{(x }\SpecialCharTok{\textless{}} \DecValTok{10}\NormalTok{)\{}
  \StringTok{\textquotesingle{}Smaller than 10\textquotesingle{}}
\NormalTok{\} }\ControlFlowTok{else}\NormalTok{ \{}
  \StringTok{\textquotesingle{}10 or more\textquotesingle{}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "10 or more"
\end{verbatim}

\textbf{Note} that the \texttt{if}/\texttt{if\ else} are evaluated for a single \texttt{TRUE} or \texttt{FALSE} i.e.~this control flow is \textbf{not vectorised} as we have in the case of \texttt{ifelse()} function which was vectorised.

\hypertarget{for-loop}{%
\subsection*{\texorpdfstring{\texttt{for} loop}{for loop}}\label{for-loop}}
\addcontentsline{toc}{subsection}{\texttt{for} loop}

The \texttt{for} loops \index{for loops}in r are used to iterate over \emph{given} items. So, the basic structure of these loops are -

\begin{verbatim}
for(item in vector) perform_some_action

# OR

for(item in vector) {
  perform_some_action
}
\end{verbatim}

Thus, for each item in \texttt{vector}, \texttt{perform\_some\_action} is called once; updating the value of item each time. This can be understood by the following simple example-

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{)\{}
  \FunctionTok{print}\NormalTok{(i)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1
## [1] 2
## [1] 3
\end{verbatim}

Conventionally \texttt{i} has been used in above example to iterate over given vector \texttt{1:3}, however any other symbol may also be used.

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{(item }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{)\{}
  \FunctionTok{print}\NormalTok{(item)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1
## [1] 2
## [1] 3
\end{verbatim}

If we use the name of any existing variable as \texttt{item} to iterate over the given object, \texttt{for\ loop} assigns the \texttt{item} to the current environment, overwriting any existing variable with the same name. See this example -

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OtherTok{\textless{}{-}} \DecValTok{500}
\ControlFlowTok{for}\NormalTok{(x }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{)\{}
  \CommentTok{\# do nothing}
\NormalTok{\}}
\NormalTok{x}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3
\end{verbatim}

\begin{figure}

{\centering \includegraphics[width=0.99\linewidth]{images/for_loop} 

}

\caption{A Diagrammatic representation of For Loop}\label{fig:unnamed-chunk-168}
\end{figure}

The idea can also used to iterate over any object any \emph{number of times} as we want. See these two examples.

Example-1

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_names }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}Andrew\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Bob\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Charles\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Dylan\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Edward\textquotesingle{}}\NormalTok{)}
\CommentTok{\# If we want first 4 elements}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{)\{}
  \FunctionTok{print}\NormalTok{(my\_names[i])}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Andrew"
## [1] "Bob"
## [1] "Charles"
## [1] "Dylan"
\end{verbatim}

Example-2

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# if we want all elements}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \FunctionTok{seq\_along}\NormalTok{(my\_names))\{}
  \FunctionTok{print}\NormalTok{(my\_names[i])}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Andrew"
## [1] "Bob"
## [1] "Charles"
## [1] "Dylan"
## [1] "Edward"
\end{verbatim}

There are 2 ways to terminate any \texttt{for\ loop} early-

\begin{itemize}
\tightlist
\item
  \texttt{next} which exits the current iteration only\index{next}
\item
  \texttt{break} which breaks the entire loop.\index{break}
\end{itemize}

See these examples.

Example-1

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{)\{}
  \ControlFlowTok{if}\NormalTok{ (i }\SpecialCharTok{==} \DecValTok{4}\NormalTok{)\{}
    \ControlFlowTok{next}
\NormalTok{  \}}
  \FunctionTok{print}\NormalTok{(i)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1
## [1] 2
## [1] 3
## [1] 5
\end{verbatim}

Example-2

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{)\{}
  \ControlFlowTok{if}\NormalTok{ (i }\SpecialCharTok{==} \DecValTok{4}\NormalTok{)\{}
    \ControlFlowTok{break}
\NormalTok{  \}}
  \FunctionTok{print}\NormalTok{(i)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1
## [1] 2
## [1] 3
\end{verbatim}

\hypertarget{while-loop}{%
\subsection*{\texorpdfstring{\texttt{while} loop}{while loop}}\label{while-loop}}
\addcontentsline{toc}{subsection}{\texttt{while} loop}

We have seen that \texttt{for} loop is used to iterate over a set of \texttt{known\ values} or at least known number of times. If however, we want to perform some iterative action unknown number of times, we may use \texttt{while} loop\index{while loop} which iterates till a given \texttt{condition} is \texttt{TRUE}. Another option is to have \texttt{repeat} loop\index{repeat loop} which can be used to iterate any number of times till it encounters a \texttt{break}.

The basic syntax of \texttt{while} loop is-

\begin{verbatim}
while (condition) action
\end{verbatim}

See these examples-

Example-1

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{i }\OtherTok{\textless{}{-}} \DecValTok{1}
\ControlFlowTok{while}\NormalTok{(i }\SpecialCharTok{\textless{}=}\DecValTok{4}\NormalTok{)\{}
  \FunctionTok{print}\NormalTok{(LETTERS[i])}
\NormalTok{  i }\OtherTok{\textless{}{-}}\NormalTok{ i}\SpecialCharTok{+}\DecValTok{1}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "A"
## [1] "B"
## [1] "C"
## [1] "D"
\end{verbatim}

We may check the value of \texttt{i} here after executing the loop

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{i}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 5
\end{verbatim}

Example-2:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{i }\OtherTok{\textless{}{-}} \DecValTok{4}
\ControlFlowTok{while}\NormalTok{(i }\SpecialCharTok{\textgreater{}=}\DecValTok{0}\NormalTok{)\{}
  \FunctionTok{print}\NormalTok{(LETTERS[i])}
\NormalTok{  i }\OtherTok{\textless{}{-}}\NormalTok{ i}\DecValTok{{-}1}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "D"
## [1] "C"
## [1] "B"
## [1] "A"
## character(0)
\end{verbatim}

\textbf{Note:} We have to make sure that the statements inside the brackets modify the \texttt{while\ condition} so that sooner or later the given condition is no longer \texttt{TRUE} otherwise the loop will never end and will go on forever.

\begin{figure}

{\centering \includegraphics[width=0.99\linewidth]{images/while_loop} 

}

\caption{Author's illustration of While Loop}\label{fig:unnamed-chunk-176}
\end{figure}

\begin{quote}
\textbf{\emph{Looping in R can be inefficient and time consuming when you're processing the rows or columns of large data-sets. Even one of greatest feature in R is its parallel processing of vector objects. Thus, whenever possible, it's better to use R's built-in numerical and character functions in conjunction with the \texttt{apply} family of functions.(We will discuss these in detail in the chapter related to functional programming)}}
\end{quote}

\hypertarget{file}{%
\chapter{File handling operations in R}\label{file}}

In chapter \ref{read} we have already learned about reading and writing data from/to files. In this section, we will learn about some other functions that are useful while reading and writing data, such as - changing directory, creating a file, renaming a file, check the existence of the file, listing all files in the working directory, copying files and creating directories.

\hypertarget{handling-files}{%
\section{Handling files}\label{handling-files}}

\hypertarget{creating-a-file-within-r-using-file.create}{%
\subsection{\texorpdfstring{Creating a file within R, using \texttt{file.create()}}{Creating a file within R, using file.create()}}\label{creating-a-file-within-r-using-file.create}}

Using \texttt{file.create()} function\index{file.create() function}, we can create a new file from console. If the file already exists it truncates. The function returns a \texttt{TRUE} logical value if file is created otherwise, returns \texttt{FALSE}.

Example - The following command will create a blank text file in the current working directory.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{file.create}\NormalTok{(}\StringTok{"my\_new\_text\_file.txt"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

\hypertarget{checking-whether-a-file-exists-using-file.exists}{%
\subsection{\texorpdfstring{Checking whether a file exists, using \texttt{file.exists()}}{Checking whether a file exists, using file.exists()}}\label{checking-whether-a-file-exists-using-file.exists}}

Similar to above, we can check whether a file with given name exists, using function\index{file.exists() function} \texttt{file.exists()}. Example-

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{file.exists}\NormalTok{(}\StringTok{"my\_new\_text\_file.txt"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

\hypertarget{renaming-file-with-file.rename}{%
\subsection{\texorpdfstring{Renaming file with \texttt{file.rename()}}{Renaming file with file.rename()}}\label{renaming-file-with-file.rename}}

The file name can be changed within the R console using, function\index{file.rename() function} \texttt{file.rename()}. Basic syntax is \texttt{file.rename(from\ =\ "old\_name",\ to\ =\ "new\_name")}. The function will return \texttt{TRUE} or \texttt{FALSE} depending upon the successful execution. See example

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{file.rename}\NormalTok{(}\AttributeTok{from =} \StringTok{"my\_new\_text\_file.txt"}\NormalTok{, }\AttributeTok{to =} \StringTok{"my\_renamed\_file.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Check whether old file exists}
\FunctionTok{file.exists}\NormalTok{(}\StringTok{"my\_new\_text\_file.txt"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] FALSE
\end{verbatim}

\hypertarget{copying-file-with-file.copy-function}{%
\subsection{\texorpdfstring{Copying file with \texttt{file.copy()} function}{Copying file with file.copy() function}}\label{copying-file-with-file.copy-function}}

Using \texttt{file.copy(from\ =\ "old\_path",\ to\ =\ "new\_path")} syntax \index{file.copy() function} files can be copied from one directory to another.

\hypertarget{deleting-file-with-file.remove}{%
\subsection{\texorpdfstring{Deleting file with \texttt{file.remove()}}{Deleting file with file.remove()}}\label{deleting-file-with-file.remove}}

The syntax for function\index{file.remove() function}, that removes a file with given name, is also very simple. Example-

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{file.remove}\NormalTok{(}\StringTok{"my\_renamed\_file.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

Check whether the file has been really deleted.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{file.exists}\NormalTok{(}\StringTok{"my\_renamed\_file.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] FALSE
\end{verbatim}

\hypertarget{handling-directories}{%
\section{Handling directories}\label{handling-directories}}

\hypertarget{getset-path-of-current-working-directory-using-getwd-setwd}{%
\subsection{\texorpdfstring{Get/Set path of current working directory using \texttt{getwd()}/ \texttt{setwd()}}{Get/Set path of current working directory using getwd()/ setwd()}}\label{getset-path-of-current-working-directory-using-getwd-setwd}}

We can check/get the path of current working directory (wd in short) as a character vector, using \texttt{getwd()} \index{getwd() function} function.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{getwd}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "G:/OneDrive - A c GeM CAG of INDIA (1)/new book"
\end{verbatim}

Similarly, using \texttt{setwd("given\textbackslash{}\textbackslash{}path\textbackslash{}\textbackslash{}here")} we can change the current working directory.\index{setwd() function}

\begin{quote}
Two things to be noted here - Either the path is to be given using forward slash \texttt{/} or if backslash \texttt{\textbackslash{}} is used these need to be escaped, using an extra \texttt{\textbackslash{}} as \texttt{\textbackslash{}} is itself an escape character in R.
\end{quote}

\hypertarget{create-new-directory-using-dir.create-and-other-operations}{%
\subsection{\texorpdfstring{Create new directory using \texttt{dir.create()} and other operations}{Create new directory using dir.create() and other operations}}\label{create-new-directory-using-dir.create-and-other-operations}}

A new directory can be created using function\index{dir.create() function} \texttt{dir.create()}. Example- the command below will create a new directory named `new\_dir' in the current working directory. If \texttt{TRUE} is returned, directory with given name is created.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{dir.create}\NormalTok{(}\StringTok{"new\_dir"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We can check whether any directory named `new\_dir' exists in current working directory, using function\index{dir.exists() function} \texttt{dir.exists()} function. Function will return either \texttt{TRUE} or \texttt{FALSE}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{dir.exists}\NormalTok{(}\StringTok{"new\_dir"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

We can also check all files that exists in current working directory/any other directory using \texttt{list.files()}\index{list.files() function} function.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{any}\NormalTok{(}\FunctionTok{list.files}\NormalTok{() }\SpecialCharTok{==} \StringTok{\textquotesingle{}new\_dir\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

A given directory can be removed using \texttt{unlink()} function\index{unlink() function} be specifically setting argument \texttt{recursive} to \texttt{TRUE}. Example

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{unlink}\NormalTok{(}\StringTok{"new\_dir"}\NormalTok{, }\AttributeTok{recursive =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{dir.exists}\NormalTok{(}\StringTok{"new\_dir"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] FALSE
\end{verbatim}

\hypertarget{an-important-function-for-opening-a-dialog-box-for-selecting-files-and-folder}{%
\section{An important function for opening a dialog box for selecting files and folder}\label{an-important-function-for-opening-a-dialog-box-for-selecting-files-and-folder}}

We may use either of \texttt{choose.dir()}\index{choose.dir() function} or \texttt{file.choose()}\index{file.choose() function}, to let the user select directory or file of her/his choice respectively.

Try these in your console

\begin{verbatim}
list.files(choose.dir())
file.copy(from = file.choose(), to = "new_name")
\end{verbatim}

\hypertarget{other-useful-functions-for-listingremoving-variables}{%
\section{Other useful functions for listing/removing variables}\label{other-useful-functions-for-listingremoving-variables}}

We can list all the variables available in current environment using function \texttt{ls()}\index{ls() function}. Another function \texttt{rm()}\index{rm() function} will remove the given variables. So a command like \texttt{rm(lm())} will remove all the available variables from the environment(Use this will caution as it will erase all the saved variables/data).

\hypertarget{using-save-to-save-objectscollection-of-objects}{%
\section{\texorpdfstring{Using \texttt{save()} to save objects/collection of objects}{Using save() to save objects/collection of objects}}\label{using-save-to-save-objectscollection-of-objects}}

We can save objects using function \texttt{save()} \index{save() function}which saves the objects on disk for later usage. The saved objects can be retrieved using \texttt{load()}\index{load() function} function. See this example-

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{h }\OtherTok{\textless{}{-}} \FunctionTok{hist}\NormalTok{(Nile)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{save}\NormalTok{(h, }\AttributeTok{file=}\StringTok{"nile\_hist"}\NormalTok{)}
\FunctionTok{rm}\NormalTok{(h)}
\FunctionTok{any}\NormalTok{(}\FunctionTok{ls}\NormalTok{() }\SpecialCharTok{==} \StringTok{\textquotesingle{}h\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] FALSE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{load}\NormalTok{(}\StringTok{"nile\_hist"}\NormalTok{)}
\FunctionTok{any}\NormalTok{(}\FunctionTok{ls}\NormalTok{() }\SpecialCharTok{==} \StringTok{\textquotesingle{}h\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

\hypertarget{projects}{%
\section{Projects}\label{projects}}

While working on a project, often a requirement is to keep all scripts, data, results, charts, figures, etc. at a single place. R studio has thus, a concept of working in projects\index{Projects in R Studio}, which associates a specific directory with a specific project and creates a specific file with extension \texttt{.Rproj}, which can reopen the complete scripts/ data/ etc. associated with that project.

To open a new project in Rstudio, click \texttt{file} menu then \texttt{New\ Project}. Check the following screenshots-

\begin{figure}

{\centering \includegraphics[width=0.49\linewidth,height=0.49\textheight]{images/Rproj1} \includegraphics[width=0.49\linewidth,height=0.49\textheight]{images/Rproj2} 

}

\caption{Creating Projects in R Studio}\label{fig:unnamed-chunk-188}
\end{figure}

After Creating the projects you will notice that a file with an extension \texttt{.Rproj} has been created by R Studio in the selected directory/location of project.

To resume working in the same project directory, either double click the file, or open the project using file menu i.e.~\texttt{file} \(->\) \texttt{Open\ Project}.

\hypertarget{functional-programming}{%
\chapter{Functional Programming}\label{functional-programming}}

\hypertarget{what-is-functional-programming}{%
\section{What is functional programming?}\label{what-is-functional-programming}}

Conceptually functional programming philosophy is based on lambda calculus. Lambda calculus is a framework developed by Alonzo Church\footnote{Alan Turing, who created Turing machine which in turn laid the foundation of imperative programming style, was a student of \textbf{Alonzo Church}.} to study computations with functions.

Functional programming is a programming paradigm in which we try to bind everything in pure mathematical functions style. It is a declarative type of programming style. Its main focus is on \textbf{what to solve} in contrast to an imperative style where the main focus is \textbf{how to solve}. For a more elaborated definition readers may see this wikipedia \href{https://en.wikipedia.org/wiki/Functional_programming}{link}. Simply putting functional programming is like doing something repeatedly but in declarative style. Here, functions are the primary method with which we carry out tasks. All actions are just implementations of functions we are using.

Functional programming use high order functions. A high order function is actually a function that accepts a function as an argument, or returns a function; in short, function that operates upon a function. We have already seen one such example may be without noticing it, \texttt{args()} function take a function as an argument and in turn return its arguments.

Let us learn a bit more here.

\hypertarget{usage-of-functional-programming-in-r}{%
\subsection{Usage of functional programming in R}\label{usage-of-functional-programming-in-r}}

Strictly speaking R is not a functional programming language. But we have already seen that one of the greatest strengths of R is parallel operations on vectors. In fact we need functional programming where concurrency or parallelism is required. Till now we have seen that most of the functions work on all atomic objects (vectors, matrices, arrays, etc.), but what about working of these functions on recursive objects i.e.~lists? Check this example (in your console)-

\begin{verbatim}
list1 <- list(50000, 5000, 56)
sum(list1)
\end{verbatim}

\begin{quote}
Of course, we can solve the above problem by using for loops. See
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{list1 }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\DecValTok{50000}\NormalTok{, }\DecValTok{5000}\NormalTok{, }\DecValTok{56}\NormalTok{)}
\CommentTok{\# for loop strategy}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{()}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \FunctionTok{seq\_along}\NormalTok{(list1))\{}
\NormalTok{  x[i] }\OtherTok{\textless{}{-}}\NormalTok{ list1[[i]]}
\NormalTok{\}}
\FunctionTok{sum}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 55056
\end{verbatim}

Consider another list, where we want to calculate mean of each element of that list.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{list2 }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}
  \DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{,}
  \DecValTok{11}\SpecialCharTok{:}\DecValTok{20}\NormalTok{,}
  \DecValTok{21}\SpecialCharTok{:}\DecValTok{30}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Of course, we may use a for loop again, but in R these operations can be done easily with \emph{apply} group of functions, which are one of the most famous and most used features in R.

\hypertarget{apply-family-of-functions}{%
\section{\texorpdfstring{\texttt{apply} family of functions}{apply family of functions}}\label{apply-family-of-functions}}

First of these functions is \texttt{apply()} which works on matrices/data frames.

\hypertarget{function-apply}{%
\subsection{\texorpdfstring{Function \texttt{apply()}}{Function apply()}}\label{function-apply}}

The basic syntax of \texttt{apply} is

\begin{verbatim}
apply(m, MARGIN, FUN, f_args)
\end{verbatim}

where

\begin{itemize}
\tightlist
\item
  \texttt{m} is the matrix
\item
  \texttt{MARGIN} is the dimension. If we want to \emph{apply} function to each row then use \texttt{1} or else if it is to be column-wise use \texttt{2}
\item
  \texttt{FUN} is the desired function which we want to apply
\item
  \texttt{f\_args} are the optional set of arguments, if needed to be supplied to \texttt{fun}.
\end{itemize}

An illustrative construction of \texttt{apply} function can be seen in \ref{fig:applyimage}.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/apply} 

}

\caption{Illustration of function apply}\label{fig:applyimage}
\end{figure}

Check this example

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(mat }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{, }\AttributeTok{nrow =} \DecValTok{5}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2]
## [1,]    1    6
## [2,]    2    7
## [3,]    3    8
## [4,]    4    9
## [5,]    5   10
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{apply}\NormalTok{(mat, }\DecValTok{1}\NormalTok{, mean)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3.5 4.5 5.5 6.5 7.5
\end{verbatim}

\textbf{Note:} \texttt{rowMeans(mat)} in above example would have given similar results, but for sake of simplicity we have provided a simplest example.

\textbf{Further note that we may also write our own customised function in the argument.} See this another example, where we will take sum of squares of each row. We may define our own custom function for the purpose and then \texttt{apply} it.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_fun }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x)\{}
  \FunctionTok{sum}\NormalTok{(x}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\NormalTok{\}}
\FunctionTok{apply}\NormalTok{(mat, }\DecValTok{1}\NormalTok{, my\_fun)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  37  53  73  97 125
\end{verbatim}

The need to writing a custom function before hand may be eliminated if the function so defined is not be used further. We may write anonymous function directly in the \texttt{apply} syntax -

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{apply}\NormalTok{(mat, }\DecValTok{1}\NormalTok{, }\AttributeTok{FUN =} \ControlFlowTok{function}\NormalTok{(x) }\FunctionTok{sum}\NormalTok{(x}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  37  53  73  97 125
\end{verbatim}

\textbf{In R version 4.1 and onwards R has devised shorthand style of defining inline custom functions, where we can write backslash i.e.~\texttt{\textbackslash{}} instead of writing \texttt{function}.} We could have written above expression as-

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{apply}\NormalTok{(mat, }\DecValTok{1}\NormalTok{, }\AttributeTok{FUN =}\NormalTok{ \textbackslash{}(x) }\FunctionTok{sum}\NormalTok{(x}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{))}
\DocumentationTok{\#\# [1]  37  53  73  97 125}
\end{Highlighting}
\end{Shaded}

\hypertarget{apply-need-not-necessarily-output-vectors-only}{%
\subsubsection*{\texorpdfstring{\texttt{apply()} need not necessarily output vectors only}{apply() need not necessarily output vectors only}}\label{apply-need-not-necessarily-output-vectors-only}}
\addcontentsline{toc}{subsubsection}{\texttt{apply()} need not necessarily output vectors only}

If \texttt{FUN} applied on rows/columns of matrix outputs vector of length more than 1, the output will be in matrix format. But the thing to note here is that matrix will be displayed columnwise always irrespective of fact whether \texttt{MARGIN} is \texttt{1} or \texttt{2}. As an easy example we could have shown this using function like \texttt{sqrt}, but \texttt{apply(matrix,\ MARGIN,\ sqrt)} will work like \texttt{sqrt(matrix)} only. So let's take a different example. Suppose we want to calculate \texttt{column-wise} cumulative sum in a given matrix.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{apply}\NormalTok{(mat, }\DecValTok{2}\NormalTok{, cumsum)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2]
## [1,]    1    6
## [2,]    3   13
## [3,]    6   21
## [4,]   10   30
## [5,]   15   40
\end{verbatim}

The output here is eaxctly what was desired. But what if, our requirement was to take \texttt{row-wise} cumulative sum?

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{apply}\NormalTok{(mat, }\DecValTok{1}\NormalTok{, cumsum)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2] [,3] [,4] [,5]
## [1,]    1    2    3    4    5
## [2,]    7    9   11   13   15
\end{verbatim}

It may now be noticed that the output is actually \emph{transpose} of what we were expecting. Actually the output of each iteration of \texttt{apply} function is displayed in one column always. Let us check our understanding with one more example taking function which may give output that is not dependent on input vector length.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\FunctionTok{apply}\NormalTok{(mat, }\DecValTok{1}\NormalTok{, sample, }\DecValTok{4}\NormalTok{, }\ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2] [,3] [,4] [,5]
## [1,]    1    7    8    4   10
## [2,]    6    2    8    4   10
## [3,]    1    2    3    4   10
## [4,]    1    2    3    9    5
\end{verbatim}

Thus we may conclude that-

\begin{itemize}
\tightlist
\item
  If \texttt{FUN} outputs a scalar, the output of \texttt{apply} will be a vector of \textbf{length} equal to

  \begin{itemize}
  \tightlist
  \item
    number of \texttt{rows} in input matrix given that \texttt{MARGIN} selected in 1,
  \item
    number of \texttt{columns} in input matrix given that \texttt{MARGIN} selected in 2.
  \end{itemize}
\item
  if \texttt{FUN} outputs a vector(of length \textgreater1) then output of \texttt{apply} will be a matrix having \textbf{number of columns} equal to -

  \begin{itemize}
  \tightlist
  \item
    number of \texttt{rows} in input matrix given that \texttt{MARGIN} selected in 1,
  \item
    number of \texttt{columns} in input matrix given that \texttt{MARGIN} selected in 2.
  \end{itemize}
\end{itemize}

These have been tabulated in table \ref{tab:apply}.

\begin{longtable}[]{@{}lcc@{}}
\caption{\label{tab:apply} Relation between input and output data structure in \texttt{apply}}\tabularnewline
\toprule\noalign{}
Input matrix \(m*n\) & \texttt{MARGIN\ =\ 1} & \texttt{MARGIN\ =\ 2} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
Input matrix \(m*n\) & \texttt{MARGIN\ =\ 1} & \texttt{MARGIN\ =\ 2} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
FUN gives scalar & Vector size \(m\) & Vector size \(n\) \\
FUN gives vector size \(p\) & Matrix \(p*m\) & Matrix \(p*n\) \\
\end{longtable}

We may thus have to be careful while getting the output from \texttt{apply} function as it may lead to introduction of bug in our code.

\hypertarget{apply-on-data-frames}{%
\subsubsection*{\texorpdfstring{\texttt{apply()} on data frames}{apply() on data frames}}\label{apply-on-data-frames}}
\addcontentsline{toc}{subsubsection}{\texttt{apply()} on data frames}

Now we know that data frames despite being special type of lists also behave like matrices, we may use \texttt{apply} on data frames too. See this example.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(my\_df }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(mat))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   V1 V2
## 1  1  6
## 2  2  7
## 3  3  8
## 4  4  9
## 5  5 10
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{apply}\NormalTok{(my\_df, }\DecValTok{2}\NormalTok{, sum)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## V1 V2 
## 15 40
\end{verbatim}

\hypertarget{function-lapply}{%
\subsection{\texorpdfstring{Function \texttt{lapply()}}{Function lapply()}}\label{function-lapply}}

Another cousin of apply is \texttt{lapply} which can thought of \texttt{apply} to \texttt{l}ists. So as the name suggests it is applied on lists instead of matrices. Now since \texttt{data\ frame} is also a list \texttt{lapply} can be applied on these. The basic syntax of \texttt{lapply()} is -

\begin{verbatim}
lapply(l, FUN, f_args)
\end{verbatim}

where

\begin{itemize}
\tightlist
\item
  \texttt{l} is the list
\item
  \texttt{FUN} is the desired function which we want to apply
\item
  \texttt{f\_args} are the optional set of arguments, if needed to be supplied to \texttt{fun}.
\end{itemize}

It may be noted that \texttt{MRAGIN} argument is not available here. See these examples.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{lapply}\NormalTok{(my\_df, sum)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $V1
## [1] 15
## 
## $V2
## [1] 40
\end{verbatim}

Now you may have noticed two things here -

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The output is of \texttt{list} type.
\item
  Unlike \texttt{apply} as \texttt{MARGIN} is not passed/available here, it applies \texttt{FUN} to every element of list. When we consider any \texttt{data.frame} as a list its each column is a separate element of that list. So \texttt{FUN} cannot be applied to \texttt{rows} in a \texttt{data.frame}.
\end{enumerate}

Thus \texttt{lapply()} -

\begin{itemize}
\tightlist
\item
  loops over a list, iterating over each element in that list
\item
  then \emph{applies} the function \texttt{FUN} to each element
\item
  and then returns a list.
\end{itemize}

Example-2: Let's try to find type of each column in a given data frame.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{lapply}\NormalTok{(iris, typeof)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $Sepal.Length
## [1] "double"
## 
## $Sepal.Width
## [1] "double"
## 
## $Petal.Length
## [1] "double"
## 
## $Petal.Width
## [1] "double"
## 
## $Species
## [1] "integer"
\end{verbatim}

Similar to \texttt{apply} we can define \texttt{FUN} inline here (anonymously) also. Example-3:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{lapply}\NormalTok{(my\_df, \textbackslash{}(a) a}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $V1
## [1]  1  4  9 16 25
## 
## $V2
## [1]  36  49  64  81 100
\end{verbatim}

Example-4:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\FunctionTok{lapply}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{, runif, }\AttributeTok{min=}\DecValTok{0}\NormalTok{, }\AttributeTok{max=}\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [[1]]
## [1] 2.655087
## 
## [[2]]
## [1] 3.721239 5.728534
## 
## [[3]]
## [1] 9.082078 2.016819 8.983897
## 
## [[4]]
## [1] 9.4467527 6.6079779 6.2911404 0.6178627
\end{verbatim}

\textbf{Note} that even if \texttt{lapply} is applied over a vector, it returns a list only.

\hypertarget{function-sapply}{%
\subsection{\texorpdfstring{Function \texttt{sapply()}}{Function sapply()}}\label{function-sapply}}

There is not much of the difference between \texttt{lapply()} and \texttt{sapply()}, as \texttt{sapply} is actually \texttt{s}implified l\texttt{apply}. It simplifies the argument as much as possible.

Example:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sapply}\NormalTok{(my\_df, sum)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## V1 V2 
## 15 40
\end{verbatim}

\hypertarget{other-loop-functions}{%
\section{Other loop functions}\label{other-loop-functions}}

\hypertarget{function-replicate}{%
\subsection{\texorpdfstring{Function \texttt{replicate()}}{Function replicate()}}\label{function-replicate}}

Function \texttt{replicate()}\index(replicate) is used for repeated evaluation of an expression. Syntax is

\begin{verbatim}
replicate(n, expr, simplify = "array")
\end{verbatim}

where -

\begin{itemize}
\tightlist
\item
  \texttt{n} is integer denoting the number of replications
\item
  \texttt{expr} is the expression to evaluate repeatedly
\item
  \texttt{simplify} takes either `character' or `logical' to value to indicate whether the results should be simplified.
\end{itemize}

Example:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\CommentTok{\# Default value of simplify will simplify the results as much possible}
\FunctionTok{replicate}\NormalTok{(}\DecValTok{5}\NormalTok{, }\FunctionTok{runif}\NormalTok{(}\DecValTok{3}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           [,1]      [,2]      [,3]      [,4]      [,5]
## [1,] 0.2875775 0.8830174 0.5281055 0.4566147 0.6775706
## [2,] 0.7883051 0.9404673 0.8924190 0.9568333 0.5726334
## [3,] 0.4089769 0.0455565 0.5514350 0.4533342 0.1029247
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Notice the difference with simplify=FALSE}
\FunctionTok{replicate}\NormalTok{(}\DecValTok{3}\NormalTok{, }\FunctionTok{runif}\NormalTok{(}\DecValTok{5}\NormalTok{), }\AttributeTok{simplify =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [[1]]
## [1] 0.89982497 0.24608773 0.04205953 0.32792072 0.95450365
## 
## [[2]]
## [1] 0.8895393 0.6928034 0.6405068 0.9942698 0.6557058
## 
## [[3]]
## [1] 0.7085305 0.5440660 0.5941420 0.2891597 0.1471136
\end{verbatim}

\hypertarget{function-split}{%
\subsection{\texorpdfstring{Function \texttt{split()}}{Function split()}}\label{function-split}}

The \texttt{split()}\index(split() function) function takes object (vector or other) and splits it into groups determined by a given factor. The basic syntax is-

\begin{verbatim}
split(x, f, drop=FALSE, ...)
\end{verbatim}

where

\begin{itemize}
\tightlist
\item
  \texttt{x} is input object - \texttt{vector} or \texttt{list} or \texttt{data.frame}
\item
  \texttt{f} is a factor or a list of factors. If a factor is not provided, it will be coerced to factor.
\item
  \texttt{drop} argument indicates whether empty factors should be dropped.
\end{itemize}

Example: (To divide the given list by alternate elements)-

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{split}\NormalTok{(LETTERS, }\FunctionTok{rep}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{, }\DecValTok{13}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $`1`
##  [1] "A" "C" "E" "G" "I" "K" "M" "O" "Q" "S" "U" "W" "Y"
## 
## $`2`
##  [1] "B" "D" "F" "H" "J" "L" "N" "P" "R" "T" "V" "X" "Z"
\end{verbatim}

Example-2: Find out sum of every odd and even number from 1:100-

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{split}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{100}\NormalTok{, (}\DecValTok{1}\SpecialCharTok{:}\DecValTok{100}\NormalTok{) }\SpecialCharTok{\%\%} \DecValTok{2}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \FunctionTok{lapply}\NormalTok{(sum)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $`0`
## [1] 2550
## 
## $`1`
## [1] 2500
\end{verbatim}

Example-3: Find out mean of \texttt{mpg} column splitting the \texttt{mtcars} data by \texttt{cyl}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{split}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{mpg, mtcars}\SpecialCharTok{$}\NormalTok{cyl) }\SpecialCharTok{|\textgreater{}} \FunctionTok{sapply}\NormalTok{(mean)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        4        6        8 
## 26.66364 19.74286 15.10000
\end{verbatim}

\hypertarget{tapply}{%
\subsection{\texorpdfstring{\texttt{tapply()}}{tapply()}}\label{tapply}}

The \texttt{tapply()} function\index(tapply() function) can be thought of combination of \texttt{split} and \texttt{sapply} for vectors, exactly as used in above example. It actually applies the function over subsets of a given vector. The basic syntax is-

\begin{verbatim}
tapply(X, INDEX, FUN = NULL, ..., default = NA, simplify = TRUE)
\end{verbatim}

Where -

\begin{itemize}
\tightlist
\item
  \texttt{X} is a vector
\item
  \texttt{INDEX} is factor or list of factors
\item
  \texttt{FUN} is function to be applied
\item
  \texttt{...} are other arguments, if any, of \texttt{FUN} to be passed
\item
  \texttt{simplify} if TRUE simplifies the result.
\end{itemize}

See this example

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{tapply}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{mpg, mtcars}\SpecialCharTok{$}\NormalTok{cyl, mean)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        4        6        8 
## 26.66364 19.74286 15.10000
\end{verbatim}

Needless to say if \texttt{simplify} is \texttt{FALSE} the results will not be simplified. See this example-

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# month{-}wise mean of temperatures from \textasciigrave{}airquality\textasciigrave{} data}
\FunctionTok{tapply}\NormalTok{(airquality}\SpecialCharTok{$}\NormalTok{Temp, airquality}\SpecialCharTok{$}\NormalTok{Month, mean, }\AttributeTok{simplify =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $`5`
## [1] 65.54839
## 
## $`6`
## [1] 79.1
## 
## $`7`
## [1] 83.90323
## 
## $`8`
## [1] 83.96774
## 
## $`9`
## [1] 76.9
\end{verbatim}

\hypertarget{by-function}{%
\subsection{\texorpdfstring{\texttt{by()} function}{by() function}}\label{by-function}}

This\index(by() function) works something like \texttt{tapply} but with the difference that input object here is \texttt{data.frame}. See this example

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Split the data by \textasciigrave{}cyl\textasciigrave{} column and subset first six rows only}
\FunctionTok{by}\NormalTok{(mtcars, mtcars}\SpecialCharTok{$}\NormalTok{cyl, head)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## mtcars$cyl: 4
##                 mpg cyl  disp hp drat    wt  qsec vs am gear carb
## Datsun 710     22.8   4 108.0 93 3.85 2.320 18.61  1  1    4    1
## Merc 240D      24.4   4 146.7 62 3.69 3.190 20.00  1  0    4    2
## Merc 230       22.8   4 140.8 95 3.92 3.150 22.90  1  0    4    2
## Fiat 128       32.4   4  78.7 66 4.08 2.200 19.47  1  1    4    1
## Honda Civic    30.4   4  75.7 52 4.93 1.615 18.52  1  1    4    2
## Toyota Corolla 33.9   4  71.1 65 4.22 1.835 19.90  1  1    4    1
## ------------------------------------------------------------ 
## mtcars$cyl: 6
##                 mpg cyl  disp  hp drat    wt  qsec vs am gear carb
## Mazda RX4      21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4
## Mazda RX4 Wag  21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4
## Hornet 4 Drive 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1
## Valiant        18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1
## Merc 280       19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4
## Merc 280C      17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4
## ------------------------------------------------------------ 
## mtcars$cyl: 8
##                     mpg cyl  disp  hp drat   wt  qsec vs am gear carb
## Hornet Sportabout  18.7   8 360.0 175 3.15 3.44 17.02  0  0    3    2
## Duster 360         14.3   8 360.0 245 3.21 3.57 15.84  0  0    3    4
## Merc 450SE         16.4   8 275.8 180 3.07 4.07 17.40  0  0    3    3
## Merc 450SL         17.3   8 275.8 180 3.07 3.73 17.60  0  0    3    3
## Merc 450SLC        15.2   8 275.8 180 3.07 3.78 18.00  0  0    3    3
## Cadillac Fleetwood 10.4   8 472.0 205 2.93 5.25 17.98  0  0    3    4
\end{verbatim}

\hypertarget{specifying-the-output-type-with-vapply}{%
\subsection{\texorpdfstring{Specifying the output type with \texttt{vapply()}}{Specifying the output type with vapply()}}\label{specifying-the-output-type-with-vapply}}

Function \texttt{vapply()}\index(vapply() function) works exactly like \texttt{sapply()} described above, with only difference that type of return value (output) has to be specifically provided through \texttt{FUN.VALUE} argument. Its syntax is -

\begin{verbatim}
vapply(X, FUN, FUN.VALUE, ..., USE.NAMES = TRUE)
\end{verbatim}

\begin{itemize}
\tightlist
\item
  In the argument \texttt{FUN.VALUE} we have to provide the format type of output. See this example.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{vapply}\NormalTok{(mtcars, max, }\AttributeTok{FUN.VALUE =} \FunctionTok{double}\NormalTok{(}\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     mpg     cyl    disp      hp    drat      wt    qsec      vs      am    gear 
##  33.900   8.000 472.000 335.000   4.930   5.424  22.900   1.000   1.000   5.000 
##    carb 
##   8.000
\end{verbatim}

Through \texttt{FUN.VALUE\ =\ double(1)} we have specifically provided that our output should be of \texttt{double} type with length \texttt{1}. So in case we have to find out \texttt{range} of each column-

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{vapply}\NormalTok{(mtcars, range, }\AttributeTok{FUN.VALUE =} \FunctionTok{double}\NormalTok{(}\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       mpg cyl  disp  hp drat    wt qsec vs am gear carb
## [1,] 10.4   4  71.1  52 2.76 1.513 14.5  0  0    3    1
## [2,] 33.9   8 472.0 335 4.93 5.424 22.9  1  1    5    8
\end{verbatim}

If we will try this function on a dataset having mixed type columns like \texttt{iris} dataset, \texttt{vapply} will throw an error.

\begin{verbatim}
vapply(iris, range, FUN.VALUE = double())
\end{verbatim}

\hypertarget{purrr}{%
\section{\texorpdfstring{Functional Programming in \texttt{purrr}}{Functional Programming in purrr}}\label{purrr}}

Package \texttt{purrr}\footnote{\url{https://purrr.tidyverse.org/}}, which is part of core \texttt{tidyverse}, enhances R's functional programming (FP) toolkit by providing a complete and consistent set of tools for working with functions and vectors.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(purrr)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Attaching package: 'purrr'
\end{verbatim}

\begin{verbatim}
## The following object is masked from 'package:magrittr':
## 
##     set_names
\end{verbatim}

\hypertarget{iterate-over-single-listvector-with-map_-family-of-functions}{%
\subsection{\texorpdfstring{Iterate over single list/vector with \texttt{map\_*()} family of functions}{Iterate over single list/vector with map\_*() family of functions}}\label{iterate-over-single-listvector-with-map_-family-of-functions}}

This package has many families of functions; and most primary family is \texttt{map} family of functions. \texttt{map\_*()} works nearly similar to \texttt{vapply} where we can control the type of output. The syntax style of each of these functions is nearly same, where these accept one object (list or vector) as \texttt{.x} argument, one function (or alternatively a formula) as \texttt{.f} argument; and outputs an object of specified type.

Example

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{map}\NormalTok{(mtcars, }\AttributeTok{.f =}\NormalTok{ sum)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $mpg
## [1] 642.9
## 
## $cyl
## [1] 198
## 
## $disp
## [1] 7383.1
## 
## $hp
## [1] 4694
## 
## $drat
## [1] 115.09
## 
## $wt
## [1] 102.952
## 
## $qsec
## [1] 571.16
## 
## $vs
## [1] 14
## 
## $am
## [1] 13
## 
## $gear
## [1] 118
## 
## $carb
## [1] 90
\end{verbatim}

Note that output type is list. If the output can be simplified to an atomic vector we can use either of these functions depending upon the output type of that vector.

\begin{itemize}
\tightlist
\item
  \texttt{map\_lgl} for \texttt{logical} format
\item
  \texttt{map\_int} for \texttt{integer} format
\item
  \texttt{map\_dbl} for \texttt{double} format
\item
  \texttt{map\_chr} for \texttt{character} format
\end{itemize}

\texttt{map} always return a list.
See these further examples.

Example-1:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{map\_dbl}\NormalTok{(mtcars, max)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     mpg     cyl    disp      hp    drat      wt    qsec      vs      am    gear 
##  33.900   8.000 472.000 335.000   4.930   5.424  22.900   1.000   1.000   5.000 
##    carb 
##   8.000
\end{verbatim}

\hypertarget{iterate-over-two-or-more-listsvectors-using-map2_-pmap_-family}{%
\subsection{\texorpdfstring{Iterate over two or more lists/vectors using \texttt{map2\_*()}/ \texttt{pmap\_*()} family}{Iterate over two or more lists/vectors using map2\_*()/ pmap\_*() family}}\label{iterate-over-two-or-more-listsvectors-using-map2_-pmap_-family}}

So far we have seen that \texttt{map\_*()} family of functions are used to iterate over elements of a list. Even if extra lists/vectors are provided as extra arguments, these are used as it is, in each iteration, as can be seen in first illustration in figure\footnote{Source: \href{https://adv-r.hadley.nz/functionals.html}{Advanced R} by Hadley Wickham} \ref{fig:map2}.

\begin{figure}

{\centering \includegraphics[width=0.49\linewidth]{images/map1} \includegraphics[width=0.49\linewidth]{images/map2} 

}

\caption{Working of map vs map2 family of functions \hspace{\textwidth} Source Advanced R by Hadley Wickham}\label{fig:map2}
\end{figure}

In order to iterate over two vectors/lists, we will however, need \texttt{map2\_*()} family of functions (Refer second illustration in figure \ref{fig:map2}).

See the following example

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\DecValTok{11}\NormalTok{, }\DecValTok{12}\NormalTok{, }\DecValTok{13}\NormalTok{)}
\FunctionTok{map2}\NormalTok{(x, y, }\StringTok{\textasciigrave{}}\AttributeTok{*}\StringTok{\textasciigrave{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [[1]]
## [1] 11
## 
## [[2]]
## [1] 24
## 
## [[3]]
## [1] 39
\end{verbatim}

Similarly, to iterate over multiple lists we will use \texttt{pmap\_*()}, with the only difference being that here all the vectors/list should be collectively passed on to \texttt{pmap} in a list. This can be better understood with the illustration used by Hadley Wickham in his \href{https://adv-r.hadley.nz/functionals.html}{book}. For reference see figure \ref{fig:pmap}.

\begin{figure}

{\centering \includegraphics[width=0.49\linewidth]{images/pmap} \includegraphics[width=0.49\linewidth]{images/pmap-arg} 

}

\caption{Working of pmap family of functions \hspace{\textwidth} Source Advanced R by Hadley Wickham}\label{fig:pmap}
\end{figure}

\hypertarget{read}{%
\chapter{Getting data in and out of R}\label{read}}

Till now, we have created our own datasets or we have used sample datasets available in R. In practical usage, there will hardly be any case when we get the data imported in R by itself. So we have to import and load our data sets in R, for our analytics tasks. Even after completing the analytics, the summarised data, other reports, charts, etc. need to be exported. This chapter is intended to do all these tasks.

First section deals about functions related to reading external data i.e.~importing objects into R. This is followed by another section dealing with writing data to external files i.e.~exporting data out of R.

\hypertarget{importing-external-data-in-r---base-r-methods}{%
\section{Importing external data in R - Base R methods}\label{importing-external-data-in-r---base-r-methods}}

Base R has many functions which can fulfill nearly any of our jobs to import external data into R. However, there are packages which are customised to do certain tasks in easier way and thus, we will also learn two of the packages from \texttt{tidyverse} also.

There are some important functions in base R, which are used frequently to import external data into R environment. Let us discuss these one by one.

\hypertarget{reading-tables-through-read.table-andor-read.csv}{%
\subsection*{\texorpdfstring{Reading tables through \texttt{read.table()} and/or \texttt{read.csv()}}{Reading tables through read.table() and/or read.csv()}}\label{reading-tables-through-read.table-andor-read.csv}}
\addcontentsline{toc}{subsection}{Reading tables through \texttt{read.table()} and/or \texttt{read.csv()}}

Basically these two functions are most commonly used functions in R, to get tabular data out of flat files. The two functions namely \texttt{read.table()} and \texttt{read.csv()} are used respectively to read tabular data out of flat files having simple text format (.txt) and comma separated values (.csv) formats respectively. There are three more cousins to these functions.

\begin{itemize}
\tightlist
\item
  \texttt{read.csv2()} to read csv files where \texttt{;} is used as delimiter and \texttt{,} is used for decimals instead.
\item
  \texttt{read.delim()} to read delimited files where \texttt{tab\ character} has been used as delimiter and \texttt{.} as decimal.
\item
  \texttt{read.delim2()}, similarly to read delimited files where \texttt{tab\ character} has been used as delimiter and \texttt{,} as decimal.
\end{itemize}

Most important arguments to these functions are -

\begin{itemize}
\tightlist
\item
  \texttt{file} the name of the file along with complete path as string\footnote{If backward slash \texttt{\textbackslash{}} is used in file paths, these must be \texttt{escaped} as R recognises \texttt{\textbackslash{}} as escape character itself. So, \texttt{"my/location/here/file.txt"} and \texttt{"my\textbackslash{}\textbackslash{}file\textbackslash{}\textbackslash{}here\textbackslash{}\textbackslash{}file.txt"} are the correct way of giving file name}.
\item
  \texttt{header} a logical value indicating if first line of the file has to be read as header or not.
\item
  \texttt{sep} a string indicating a separator value to separate columns.
\item
  \texttt{colClasses} a character vector indicating type of the columns if these are to be read explicitly in these types/formats only.
\item
  \texttt{skip} an integer, indicating how many rows (from beginning) are to be skipped.
\end{itemize}

Readers may check results of \texttt{?read.table()} to get a complete list of arguments of these functions.

Example: Let us try to download data related to \emph{World Happiness Report 201=21} which is available on data.world portal.

\begin{verbatim}
wh2021 <- read.csv("https://query.data.world/s/qbsbmxlfj54sl4mq3y6uxsr3pkhhmo")
# Check dimensions
dim(wh2021)
# Check column names
colnames(wh2021)
\end{verbatim}

We can see that dataset named \texttt{wh2021} having \texttt{20} columns and \texttt{149} rows is now available in our environment.

\begin{quote}
\textbf{Tip: Use \texttt{"clipboard"} in file argument for pasting copied data into R. E.g. Copy a few rows and cells from excel spreadsheet and run this command \texttt{read.delim("clipboard",\ header\ =\ FALSE)}.}
\end{quote}

\hypertarget{read-data-into-a-vector-through-scan-or-readline}{%
\subsection*{\texorpdfstring{Read data into a vector through \texttt{scan()} or \texttt{readline()}}{Read data into a vector through scan() or readline()}}\label{read-data-into-a-vector-through-scan-or-readline}}
\addcontentsline{toc}{subsection}{Read data into a vector through \texttt{scan()} or \texttt{readline()}}

As afore-mentioned functions \texttt{read.table()} \emph{et al} are used in reading data into tables, we may also require reading data into a vector or simple list. The two functions \texttt{scan()} and \texttt{readline()} are used for these purposes.

The basic syntax of \texttt{scan()} is -

\begin{verbatim}
scan(file = "",
     what = double(),
     nmax = -1,
     ...
)
\end{verbatim}

Where -

\begin{itemize}
\tightlist
\item
  \texttt{file} is name of the file, or link
\item
  \texttt{what} is format/data type to be read. Default type is double
\item
  \texttt{nmax} is mux number of data values to be read, default is \texttt{-1} which means all.
\end{itemize}

There are many other usefule arguments, for which please check results of \texttt{?scan} in your console.

Example -

\begin{verbatim}
scan("http://mattmahoney.net/iq/digits.txt", nmax = 10)
\end{verbatim}

This function is sometimes useful to read data from keyboard into a vector. Just use a blank string \texttt{""} in file name. See this example

\includegraphics{images/scan.png}

Function \texttt{readline()} on the other hand does similar job, but with a prompt. See this example

\includegraphics{images/readline.png}

\hypertarget{reading-text-files-through-readlines}{%
\subsection*{\texorpdfstring{Reading text files through \texttt{readLines()}}{Reading text files through readLines()}}\label{reading-text-files-through-readlines}}
\addcontentsline{toc}{subsection}{Reading text files through \texttt{readLines()}}

Function \texttt{readLines()} is used to read text lines from a file (or connection). To see this in action, prepare a text file (say \texttt{"txt.txt"}) and try reading it using \texttt{readLines("txt.txt")}.

\hypertarget{exporting-data-out-of-r---base-r-methods}{%
\section{Exporting data out of R - Base R methods}\label{exporting-data-out-of-r---base-r-methods}}

Since the nature of most of the data anaytic jobs carried out in R, will be followed after reading the external data which will be followed by wrangling, transformation, modelling, etc., all in R. Exporting files will not be used as much as reading external data. Still, there will be times, when wrangled data tables need to be exported out of R. For each of the different use cases, the following functions will almost complete our export requirements.

Let's learn these.

\hypertarget{writing-tabular-data-through-write.table-andor-write.csv}{%
\subsection*{\texorpdfstring{Writing tabular data through \texttt{write.table()} and/or \texttt{write.csv()}}{Writing tabular data through write.table() and/or write.csv()}}\label{writing-tabular-data-through-write.table-andor-write.csv}}
\addcontentsline{toc}{subsection}{Writing tabular data through \texttt{write.table()} and/or \texttt{write.csv()}}

Exporting data frames, whether after cleaning, wrangling, or transformation, etc., can be exported using these functions. Latter will be used to write data frames in csv formats specifically. The syntax is -

\begin{verbatim}
write.table(x, file = "", sep = " ", ...)
write.csv(x, file = "", ...)
write.csv2(x, file = "", ...)
\end{verbatim}

where -

\begin{itemize}
\tightlist
\item
  \texttt{x} is the data frame object to be exported
\item
  \texttt{file} is used to give file name (along with path)
\item
  \texttt{sep} is separator
\item
  \texttt{...} - there are many more arguments which are used to cutomised export needs. See \texttt{?write.table()} for full details.
\end{itemize}

E.g. - The following command will export \texttt{iris} data frame as \texttt{iris.csv} file in the current working directory.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{write.csv}\NormalTok{(iris, }\StringTok{\textquotesingle{}iris.csv\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{writing-character-data-line-by-line-to-a-file-through-writelines}{%
\subsection*{\texorpdfstring{Writing character data line by line to a file through \texttt{writeLines()}}{Writing character data line by line to a file through writeLines()}}\label{writing-character-data-line-by-line-to-a-file-through-writelines}}
\addcontentsline{toc}{subsection}{Writing character data line by line to a file through \texttt{writeLines()}}

Similar to \texttt{readLines}, function \texttt{writeLines()} will write the text data into a file with given file name. Type the following code in your console and check that a new file with name \texttt{my\_new\_file.txt} has been created with the given contents in your current working directory.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{writeLines}\NormalTok{(}\StringTok{"Andrew 25}
\StringTok{           Bob 45}
\StringTok{           Charles 56"}\NormalTok{, }\StringTok{"my\_new\_file.txt"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{using-dput-to-get-a-code-representation-of-r-object}{%
\subsection*{\texorpdfstring{Using \texttt{dput()} to get a code representation of R object}{Using dput() to get a code representation of R object}}\label{using-dput-to-get-a-code-representation-of-r-object}}
\addcontentsline{toc}{subsection}{Using \texttt{dput()} to get a code representation of R object}

This function will output the code representation of the given R object. This function is particularly useful, when you are searching for help online and you need to give some sample data to reproduce the problem. E.g. on \href{https://stackoverflow.com/}{\emph{Stack Overflow}} when asking for a solution to a specific problem, \href{https://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example}{reproducible data} needs to be furnished. Please also refer to section \ref{reprex} for more.

Example-1:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_data }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{Name =} \FunctionTok{c}\NormalTok{(}\StringTok{"Andrew"}\NormalTok{, }\StringTok{"Bob"}\NormalTok{, }\StringTok{"Charles"}\NormalTok{),}
                      \AttributeTok{Age =} \FunctionTok{c}\NormalTok{(}\DecValTok{25}\NormalTok{, }\DecValTok{45}\NormalTok{, }\DecValTok{56}\NormalTok{))}
\FunctionTok{dput}\NormalTok{(my\_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## structure(list(Name = c("Andrew", "Bob", "Charles"), Age = c(25, 
## 45, 56)), class = "data.frame", row.names = c(NA, -3L))
\end{verbatim}

And while reproducing someone else's dput-

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{now\_mydata }\OtherTok{\textless{}{-}} \FunctionTok{structure}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\AttributeTok{Name =} \FunctionTok{c}\NormalTok{(}\StringTok{"Andrew"}\NormalTok{, }\StringTok{"Bob"}\NormalTok{, }\StringTok{"Charles"}\NormalTok{), }\AttributeTok{Age =} \FunctionTok{c}\NormalTok{(}\DecValTok{25}\NormalTok{, }
\DecValTok{45}\NormalTok{, }\DecValTok{56}\NormalTok{)), }\AttributeTok{class =} \StringTok{"data.frame"}\NormalTok{, }\AttributeTok{row.names =} \FunctionTok{c}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\SpecialCharTok{{-}}\NormalTok{3L))}

\NormalTok{now\_mydata}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      Name Age
## 1  Andrew  25
## 2     Bob  45
## 3 Charles  56
\end{verbatim}

\hypertarget{using-external-packages-for-readingwriting-data}{%
\section{Using external packages for reading/writing data}\label{using-external-packages-for-readingwriting-data}}

\hypertarget{package-readr}{%
\subsection{\texorpdfstring{Package \texttt{readr}}{Package readr}}\label{package-readr}}

The \texttt{readr} package \citep{R-readr} is part of core \texttt{tidyverse} and is loaded directly when we load it through \texttt{library(tidyverse)}. It provides a range of analogous functions for each of the reading functions in base R.

\begin{longtable}[]{@{}ccl@{}}
\toprule\noalign{}
Base R & readr & Uses \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
read.table & read\_table & Reading table \\
read.csv & read\_csv & Reading CSV file with comma as sep \\
read.csv2 & read\_csv2 & Reading CSV file with semi-colon as sep \\
read.delim & read\_delim & Reading files with any delimiter \\
read.fwf & read\_fwf & Reading fixed width files \\
read.tsv & read\_tsv & Reading tab delimited file \\
-\/- & write\_delim & Writing files with any delimiter \\
write.csv & write\_csv & Writing files with comma delimiter \\
write.csv2 & write\_csv2 & Writing files with semi-colon delimiter \\
-\/- & write\_tsv & writing a tab delimited file \\
\end{longtable}

So a question may be asked here that what's the difference between these two sets of functions. Firstly, \texttt{readr} alternatives are much faster than their base R counterparts. Secondly, it provides an informative problem report when parsing leads to unexpected results.

For these, check results of these examples-

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{read\_csv}\NormalTok{(}\FunctionTok{readr\_example}\NormalTok{(}\StringTok{"chickens.csv"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Rows: 5 Columns: 4
## -- Column specification --------------------------------------------------------
## Delimiter: ","
## chr (3): chicken, sex, motto
## dbl (1): eggs_laid
## 
## i Use `spec()` to retrieve the full column specification for this data.
## i Specify the column types or set `show_col_types = FALSE` to quiet this message.
\end{verbatim}

\begin{verbatim}
## # A tibble: 5 x 4
##   chicken                 sex     eggs_laid motto                               
##   <chr>                   <chr>       <dbl> <chr>                               
## 1 Foghorn Leghorn         rooster         0 That's a joke, ah say, that's a jok~
## 2 Chicken Little          hen             3 The sky is falling!                 
## 3 Ginger                  hen            12 Listen. We'll either die free chick~
## 4 Camilla the Chicken     hen             7 Bawk, buck, ba-gawk.                
## 5 Ernie The Giant Chicken rooster         0 Put Captain Solo in the cargo hold.
\end{verbatim}

Note that the column types, while parsing the data frame, have now been printed. These column types have been guessed by \texttt{readr} actually. If the column types are not what were actually required to be parsed, then argument \texttt{col\_types} may be used. We can also use \texttt{spec()} function to retrieve the data types guessed by \texttt{readr} later-on, so these can be modified and used again in \texttt{col\_types} argument. See this example

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{write.csv}\NormalTok{(iris, }\StringTok{"iris.csv"}\NormalTok{) }\CommentTok{\#write a dummy data}
\FunctionTok{spec}\NormalTok{(}\FunctionTok{read\_csv}\NormalTok{(}\StringTok{"iris.csv"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## New names:
## Rows: 150 Columns: 6
## -- Column specification
## -------------------------------------------------------- Delimiter: "," chr
## (1): Species dbl (5): ...1, Sepal.Length, Sepal.Width, Petal.Length,
## Petal.Width
## i Use `spec()` to retrieve the full column specification for this data. i
## Specify the column types or set `show_col_types = FALSE` to quiet this message.
## * `` -> `...1`
\end{verbatim}

\begin{verbatim}
## cols(
##   ...1 = col_double(),
##   Sepal.Length = col_double(),
##   Sepal.Width = col_double(),
##   Petal.Length = col_double(),
##   Petal.Width = col_double(),
##   Species = col_character()
## )
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{read\_csv}\NormalTok{(}\StringTok{"iris.csv"}\NormalTok{,}
         \AttributeTok{col\_select =} \DecValTok{2}\SpecialCharTok{:}\DecValTok{6}\NormalTok{,}
         \AttributeTok{col\_types =} \FunctionTok{cols}\NormalTok{(}
           \AttributeTok{Sepal.Length =} \FunctionTok{col\_double}\NormalTok{(),}
           \AttributeTok{Sepal.Width =} \FunctionTok{col\_double}\NormalTok{(),}
           \AttributeTok{Petal.Length =} \FunctionTok{col\_double}\NormalTok{(),}
           \AttributeTok{Petal.Width =} \FunctionTok{col\_double}\NormalTok{(),}
           \AttributeTok{Species =} \FunctionTok{col\_factor}\NormalTok{(}\AttributeTok{levels =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}setosa\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}versicolor\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}virginica\textquotesingle{}}\NormalTok{))}
\NormalTok{         )}
\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{head}\NormalTok{(}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## New names:
## * `` -> `...1`
\end{verbatim}

\begin{verbatim}
## # A tibble: 2 x 5
##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
##          <dbl>       <dbl>        <dbl>       <dbl> <fct>  
## 1          5.1         3.5          1.4         0.2 setosa 
## 2          4.9         3            1.4         0.2 setosa
\end{verbatim}

\hypertarget{package-readxl}{%
\subsection{\texorpdfstring{Package \texttt{readxl}}{Package readxl}}\label{package-readxl}}

This package is also part of \texttt{tidyverse} but this one has to be loaded specifically using \texttt{library(readxl)}. As the name suggests, it has functions which are useful to read/write data to/from excel files. Excel files (having extension .xls or .xlsx) are slightly different in a way that these may contain several \emph{sheets} of data at once. The functions \texttt{read\_excel()}, \texttt{read\_xlsx}, and \texttt{read\_xls} have been designed for reading sheets from excel files. The syntax is

\begin{verbatim}
read_excel(path, sheet = NULL, range = NULL)
read_xlsx(path, sheet = NULL, range = NULL)
read_xls(path, sheet = NULL, range = NULL)
\end{verbatim}

To get the names of sheets in an excel file we can use \texttt{excel\_sheets()} function from the library.

\begin{verbatim}
excel_sheets(path)
\end{verbatim}

\hypertarget{reprex}{%
\subsection{\texorpdfstring{Package \texttt{reprex}}{Package reprex}}\label{reprex}}

This is again part of tidyverse, but has to be loaded specifically by calling \texttt{library(reprex)}. The name \texttt{reprex} is actually short for reproducible example. This is useful particularly when we are stuck in some problem and seek for online help on some forum such as \href{https://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example}{Stack Overflow}, \href{https://community.rstudio.com}{R Studio Community}, etc.

As an example do this in your console

\begin{verbatim}
library(reprex)

x <- dput(head(iris))
x
\end{verbatim}

Thereafter run \texttt{reprex()}, a small window in the \texttt{Viewer} tab will be opened like this. Moreover, the code has been copied on the clipboard.

\includegraphics{images/reprex.png}

For more reading please refer \href{https://www.tidyverse.org/help/}{this page.}\footnote{\url{https://www.tidyverse.org/help/}}

\hypertarget{data-cleaning-in-r}{%
\chapter{Data Cleaning in R}\label{data-cleaning-in-r}}

Data cleansing is one of the important steps in data analysis. Multiple packages are available in r to clean the data sets. One of such packages is \texttt{janitor} which we will be using in this chapter along with few other packages.

Let's load it

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(janitor)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Attaching package: 'janitor'
\end{verbatim}

\begin{verbatim}
## The following objects are masked from 'package:stats':
## 
##     chisq.test, fisher.test
\end{verbatim}

\hypertarget{cleaning-column-names.}{%
\section{Cleaning Column names.}\label{cleaning-column-names.}}

We know that names of objects in R follow certain conventions like we may not have certain special characters in names. If a space has been used that is to be quoted under a pair of backticks `. But generally when we read data from files in excel, we can have some `dirty' names, which we should clean before proceeding. In such \texttt{clean\_names()} come handy. E.g.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a data.frame with dirty names}
\NormalTok{test\_df }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{matrix}\NormalTok{(}\AttributeTok{ncol =} \DecValTok{6}\NormalTok{))}

\FunctionTok{names}\NormalTok{(test\_df) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"firstName"}\NormalTok{, }\StringTok{"ábc@!*"}\NormalTok{, }\StringTok{"\% successful (2009)"}\NormalTok{,}
                    \StringTok{"REPEAT VALUE"}\NormalTok{, }\StringTok{"REPEAT VALUE"}\NormalTok{, }\StringTok{""}\NormalTok{)}
\CommentTok{\# View this data}
\NormalTok{test\_df}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   firstName ábc@!* % successful (2009) REPEAT VALUE REPEAT VALUE   
## 1        NA     NA                  NA           NA           NA NA
\end{verbatim}

Using \texttt{clean\_names()} which is also pipe friendly, we can clean names in one step. (Results will be in snake case)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{test\_df }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{clean\_names}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   first_name abc percent_successful_2009 repeat_value repeat_value_2  x
## 1         NA  NA                      NA           NA             NA NA
\end{verbatim}

It -

\begin{itemize}
\tightlist
\item
  Parses letter cases and separators to a consistent format.
\item
  Default is to \texttt{snake\_case}, but other cases like \texttt{camelCase} are available
\item
  Handles special characters and spaces, including transliterating characters like \texttt{œ} to \texttt{oe}.
\item
  Appends numbers to duplicated names
\item
  Converts \texttt{“\%”} to ``percent'' and \texttt{“\#”} to \texttt{“number”} to retain meaning
\item
  Spacing (or lack thereof) around numbers is preserved
\end{itemize}

\hypertarget{handling-duplicate-records}{%
\section{Handling duplicate records}\label{handling-duplicate-records}}

In \texttt{janitor} package, we have a ready to use function \texttt{get\_dupes()}. It allows us to find ``similar'' observations in a data set based on certain characteristics. Syntax is pretty simple, and function is pipe friendly too. Suppose we have to find out duplicate in \texttt{mtcars} dataset on each combination of \texttt{wt} and \texttt{cyl}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mtcars }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{get\_dupes}\NormalTok{(wt, cyl)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     wt cyl dupe_count  mpg  disp  hp drat  qsec vs am gear carb
## 1 3.44   6          2 19.2 167.6 123 3.92 18.30  1  0    4    4
## 2 3.44   6          2 17.8 167.6 123 3.92 18.90  1  0    4    4
## 3 3.57   8          2 14.3 360.0 245 3.21 15.84  0  0    3    4
## 4 3.57   8          2 15.0 301.0 335 3.54 14.60  0  1    5    8
\end{verbatim}

We can see that it returns all duplicate records with an additional column \texttt{dupe\_count} so that these duplicates can be analysed separately.

\hypertarget{remove-constant-redundant-columns}{%
\section{Remove Constant (Redundant) columns}\label{remove-constant-redundant-columns}}

Dropping columns from a \texttt{data.frame} that contain only a single constant value throughout is again easy through \texttt{janitor::remove\_constant()}.

\hypertarget{remove-empty-rows-andor-columns}{%
\section{Remove empty rows and/or columns}\label{remove-empty-rows-andor-columns}}

While importing messy data from excel files, we may get some empty rows and/or columns. Sorting out this issue, is easy using \texttt{janitor::remove\_empty()}.

\hypertarget{fix-excel-dates-stored-as-serial-numbers}{%
\section{Fix excel dates stored as serial numbers}\label{fix-excel-dates-stored-as-serial-numbers}}

While loading excel files in R, we may have sometimes noticed \texttt{41590} instead of having a \texttt{date\ format}. Sorting out this issue is again easy in \texttt{janitor} as we have a function \texttt{excel\_numeric\_to\_date()} for this. Example

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{janitor}\SpecialCharTok{::}\FunctionTok{excel\_numeric\_to\_date}\NormalTok{(}\DecValTok{41590}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "2013-11-12"
\end{verbatim}

\hypertarget{convert-a-mix-of-date-and-datetime-formats-to-date}{%
\section{Convert a mix of date and datetime formats to date}\label{convert-a-mix-of-date-and-datetime-formats-to-date}}

Similar to above, we can also sort out, if we have a column mix of different date formats, using \texttt{janitor::convert\_to\_date()} or \texttt{janitor::convert\_to\_datetime()}. See Examples-

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{unsorted\_dates }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}2018{-}05{-}31\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}41590\textquotesingle{}}\NormalTok{, }\DecValTok{41590}\NormalTok{)}
\NormalTok{janitor}\SpecialCharTok{::}\FunctionTok{convert\_to\_date}\NormalTok{(unsorted\_dates)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "2018-05-31" "2013-11-12" "2013-11-12"
\end{verbatim}

\textbf{Note in above example, we have created a heterogeneous vector, but implicit coercion rules of R have converted all forms to character only.}

In real world examples, where data is entered through multiple machines/data points simultaneously, we may a column mix of date formats. In that case, we may use \texttt{parse\_date\_time()} function in \texttt{lubridate} package. To allow different formats we have use \texttt{order} agument in this function. Example

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mixed\_dates }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"13{-}11{-}1991"}\NormalTok{, }\StringTok{"13{-}Sep{-}22"}\NormalTok{, }
                 \StringTok{"20 August 2000"}\NormalTok{, }\StringTok{"15 August 87"}\NormalTok{, }
                 \StringTok{"03/31/23"}\NormalTok{, }\StringTok{"12{-}31{-}2022"}\NormalTok{)}

\NormalTok{lubridate}\SpecialCharTok{::}\FunctionTok{parse\_date\_time}\NormalTok{(mixed\_dates,}
                           \AttributeTok{orders =} \FunctionTok{c}\NormalTok{(}\StringTok{"d m y"}\NormalTok{, }\StringTok{"d B Y"}\NormalTok{, }\StringTok{"m/d/y"}\NormalTok{, }\StringTok{"d B y"}\NormalTok{),}
                           \AttributeTok{locale =} \StringTok{"eng"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "1991-11-13 UTC" "2022-09-13 UTC" "2000-08-20 UTC" "1987-08-15 UTC"
## [5] "2023-03-31 UTC" "2022-12-31 UTC"
\end{verbatim}

\hypertarget{merging-large-number-of-similar-datasets-into-one}{%
\chapter{Merging large number of similar datasets into one}\label{merging-large-number-of-similar-datasets-into-one}}

Data preparation for performing analytics is an important task and may require more time than actual analytics because we rarely have data in ideal format. Importing csv or flat files is rather an easy job. However, considering large popularity of MS Excel, we at times have our data saved in excel files.

Sometimes, one single data frame/table is divided into multiple sheets in one excel file whereas sometimes these tables are divided in multiple files. Here we are discussing few of these cases, where we can reduce our data preparation time by effectively writing the code for import of such data into our environment.

\hypertarget{case-1-merging-multiple-excel-sheets-into-one-data-frame}{%
\section{\texorpdfstring{\textbf{Case-1}: Merging multiple excel sheets into one data frame}{Case-1: Merging multiple excel sheets into one data frame}}\label{case-1-merging-multiple-excel-sheets-into-one-data-frame}}

As an example, let's say we have multiple States' data saved on a different sheet in one excel file say \texttt{Daur.xlsx}. See preview in fig \ref{fig:prevexcel}.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/excel} 

}

\caption{Preview of example excel file}\label{fig:prevexcel}
\end{figure}

We will use library \texttt{readxl} to read excel files. This library is bundled with \texttt{tidyverse} but is not part of core tidyverse, so it has to be loaded explicitly, though explicit download is not required if \texttt{tidyverse} is installed in the system.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(readxl)}
\end{Highlighting}
\end{Shaded}

The following steps are used-

Step-1: Read path

Step-2: Collect Names of all sheets

Step-3: Set names of elements of above vector onto itself

Step-4: Read and combine all tables into one. We will use \texttt{purrr::map\_dfr} for this.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Step{-}1}
\NormalTok{path }\OtherTok{\textless{}{-}} \StringTok{"data/daur1.xlsx"}
\CommentTok{\# Step{-}2}
\NormalTok{states\_data }\OtherTok{\textless{}{-}} \FunctionTok{excel\_sheets}\NormalTok{(path) }\SpecialCharTok{\%\textgreater{}\%} 
\CommentTok{\# step{-}3}
    \FunctionTok{set\_names}\NormalTok{(., .) }\SpecialCharTok{\%\textgreater{}\%} 
\CommentTok{\# step{-}4}
    \FunctionTok{map\_dfr}\NormalTok{(read\_excel, }\AttributeTok{path=}\NormalTok{path, }\AttributeTok{.id =} \StringTok{\textquotesingle{}State\_name\textquotesingle{}}\NormalTok{)}
\CommentTok{\# print file}
\NormalTok{states\_data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 18 x 4
##    State_name     Year    Metric_1 Metric_2
##    <chr>          <chr>      <dbl>    <dbl>
##  1 Andhra Pradesh 2015-16      119      121
##  2 Andhra Pradesh 2016-17      114      134
##  3 Andhra Pradesh 2017-18      115      122
##  4 Andhra Pradesh 2018-19      129      137
##  5 Andhra Pradesh 2019-20      149      129
##  6 Andhra Pradesh 2020-21      104      124
##  7 Assam          2015-16      104      145
##  8 Assam          2016-17      114      144
##  9 Assam          2017-18      116      116
## 10 Assam          2018-19      129      130
## 11 Assam          2019-20      144      134
## 12 Assam          2020-21      124      116
## 13 Bihar          2015-16      109      148
## 14 Bihar          2016-17      134      106
## 15 Bihar          2017-18      131      133
## 16 Bihar          2018-19      131      100
## 17 Bihar          2019-20      131      127
## 18 Bihar          2020-21      128      103
\end{verbatim}

We can see that data from all the sheets have been merged into table and one extra column has been created using sheet name. The name of that column has been provided through \texttt{.id} argument. If the new column is not required, simply don't use this argument.

\hypertarget{case-2-merging-multiple-files-into-one-data-frame}{%
\section{\texorpdfstring{\textbf{Case-2}: Merging multiple files into one data frame}{Case-2: Merging multiple files into one data frame}}\label{case-2-merging-multiple-files-into-one-data-frame}}

Often we have our source data split into multiple files, which we will have to compile in one single data frame before proceeding In this case, we may collect all such files in one directory and follow these steps-

Step-1: Store all file names using \texttt{list.files()}

Step-2: We may read all files in one list using either \texttt{lapply} or \texttt{purrr::map}.

Step-3: If data structures in all the files are same, we can directly use \texttt{purrr::map\_dfr} which will read all files and give us a data frame. If however, the structure of data in all files are not same, we may convert all columns into character type before merging these files. We can thereafter proceed for merging all data using either \texttt{purrr::map\_dfr} or \texttt{lapply} in combination with \texttt{do.call}.

\hypertarget{case-3-split-and-save-one-data-frame-into-multiple-excelcsv-files-simultaneously.}{%
\section{\texorpdfstring{\textbf{Case-3}: Split and save one data frame into multiple excel/csv files simultaneously.}{Case-3: Split and save one data frame into multiple excel/csv files simultaneously.}}\label{case-3-split-and-save-one-data-frame-into-multiple-excelcsv-files-simultaneously.}}

As an example will use \texttt{states\_data} created in case-1. We can use the following algorithm

Step-1: Create a vector of file names using \texttt{paste0}
Step-2: Split data frame into a list with separate dataframe for each state
Step-3: Write to a separate file using \texttt{purrr::walk2()}

The complete algorithm is

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# step{-}1 : create a vector of file names (output)}
\NormalTok{file\_names }\OtherTok{\textless{}{-}} \FunctionTok{paste0}\NormalTok{(}\StringTok{"data/"}\NormalTok{, }\FunctionTok{unique}\NormalTok{(states\_data}\SpecialCharTok{$}\NormalTok{State\_name), }\StringTok{".csv"}\NormalTok{)}

\NormalTok{states\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_split}\NormalTok{(State\_name) }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  purrr}\SpecialCharTok{::}\FunctionTok{walk2}\NormalTok{(file\_names, write.csv)}
\end{Highlighting}
\end{Shaded}

We can check that 3 new files with state\_names as filenames have been created in the \texttt{data} folder/directory as desired.

\hypertarget{case-4-splitting-one-data-into-muliple-files-having-multiple-sheets}{%
\section{\texorpdfstring{\textbf{Case-4}: Splitting one data into muliple files having multiple sheets}{Case-4: Splitting one data into muliple files having multiple sheets}}\label{case-4-splitting-one-data-into-muliple-files-having-multiple-sheets}}

Sometimes, we may require to split a file not only into multiple files, but simultaeously require to split each file into multiple excel sheets. E.g. A data having States and districts is to be split into State-wise files having a separate sheet for each district.

This can be achieved using \texttt{writexl} library. In this case, we may write a custom function which can do our job easily.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(writexl)}

\NormalTok{book\_and\_sheets }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(df, x, y)\{}
\NormalTok{  df\_by\_x }\OtherTok{\textless{}{-}}\NormalTok{ df }\SpecialCharTok{\%\textgreater{}\%} 
    \FunctionTok{split}\NormalTok{(.[[x]])}
  
\NormalTok{  save\_to\_excel }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(a, b)\{}
\NormalTok{    a }\SpecialCharTok{\%\textgreater{}\%} 
      \FunctionTok{split}\NormalTok{(.[[y]]) }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{      writexl}\SpecialCharTok{::}\FunctionTok{write\_xlsx}\NormalTok{(}
        \AttributeTok{path =} \FunctionTok{paste0}\NormalTok{(}\StringTok{"data/data\_by\_"}\NormalTok{, b,}\StringTok{"\_"}\NormalTok{,x, }\StringTok{".xlsx"}\NormalTok{)}
\NormalTok{      )}
    
\NormalTok{  \}}
  
  \FunctionTok{imap}\NormalTok{(df\_by\_x, save\_to\_excel)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

The function \texttt{book\_and\_sheets} designed in above code helps us to write a data say \texttt{df} into separate files based on column \texttt{x} and each of these files is further divided into sheets based on column \texttt{y}. Only thing to be remembered is that we have to pass, \texttt{x} and \texttt{y} arguments as character strings; and \texttt{df} as variable.

Example - splitting \texttt{mtcars} into files based on \texttt{cyl} and sheets based on \texttt{gear}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{book\_and\_sheets}\NormalTok{(mtcars, }\StringTok{\textquotesingle{}cyl\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}gear\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $`4`
## [1] "G:\\OneDrive - A c GeM CAG of INDIA (1)\\new book\\Data\\data_by_4_cyl.xlsx"
## 
## $`6`
## [1] "G:\\OneDrive - A c GeM CAG of INDIA (1)\\new book\\Data\\data_by_6_cyl.xlsx"
## 
## $`8`
## [1] "G:\\OneDrive - A c GeM CAG of INDIA (1)\\new book\\Data\\data_by_8_cyl.xlsx"
\end{verbatim}

We can check that three excel files have been created in directory \texttt{data/}.

\hypertarget{part-ii-exploratory-data-analysis}{%
\chapter*{Part-II: Exploratory Data Analysis}\label{part-ii-exploratory-data-analysis}}
\addcontentsline{toc}{chapter}{Part-II: Exploratory Data Analysis}

\hypertarget{visualisations-in-base-r}{%
\chapter{Visualisations in Base R}\label{visualisations-in-base-r}}

It is a common practice to visualize data as soon as we start analysing it. While small data can be easily visualised in data format, the most common being opening it in MS Excel format, it becomes a hurdle to look at it when the data is bigger. Undoubtedly, visualization is an essential tool for data analysis because it enables us to see patterns and relationships that may not be apparent from a simple numerical summary. For example, a histogram can show the distribution of a variable, which can help identify outliers or skewness that may be hidden in the summary statistics. Similarly, a scatter plot can show the relationship between two variables, which can help identify correlation or causation that may not be apparent from summary statistics.

Visualization also plays a crucial role in communicating data analysis results to others. A well-designed graph or chart can convey information more effectively than a table of numbers or a lengthy report. It can also help identify errors or anomalies in the data and facilitate better decision-making.

In R, the base graphics system provides a powerful set of functions for creating various types of graphs and plots. In this chapter, we will cover the most commonly used functions for creating visualizations in base R, including \texttt{plot()}, \texttt{boxplot()}, \texttt{barplot()}, \texttt{hist()}, \texttt{pie()}, \texttt{dotchart()} and kernel density plots.

\begin{quote}
Note: In next chapter we will cover visulaisation through \texttt{ggplot2} library which is very extensive, and is developed on theortical framework known as `grammar of graphics'. This grammar, created by Leland Wilkinson, has been implemented in a variety of data visualisation software like Tableau\footnote{\url{https://statmodeling.stat.columbia.edu/2021/05/14/tableau-and-the-grammar-of-graphics/}}, Plotly, etc.
\end{quote}

\hypertarget{using-plot}{%
\section{\texorpdfstring{Using \texttt{plot()}}{Using plot()}}\label{using-plot}}

The \texttt{plot()} function in R is a very versatile function that can create a wide variety of visualizations. Its primary purpose is to create scatterplots, but it can also create line plots, bar plots, box plots, and many other types of plots. The ``plot'' function can be customized in many ways, allowing users to create high-quality visualizations that are tailored to their specific needs.

The basic syntax of the ``plot'' function is as follows:

\begin{verbatim}
plot(x, y, ...)
\end{verbatim}

The \texttt{x} and \texttt{y} arguments specify the variables to be plotted on the x-axis and y-axis, respectively. The \texttt{...} argument can include a wide range of optional arguments that customize the plot. Some of the most commonly used optional arguments include:

\begin{itemize}
\tightlist
\item
  \texttt{xlab}: Specifies the label for the x-axis.
\item
  \texttt{ylab}: Specifies the label for the y-axis.
\item
  \texttt{main}: Specifies the main title for the plot.
\item
  \texttt{xlim}: Specifies the limits for the x-axis.
\item
  \texttt{ylim}: Specifies the limits for the y-axis.
\item
  \texttt{type}: Specifies the type of plot to be created (e.g., \texttt{"p"} for points, \texttt{"l"} for lines,\texttt{"b"} for both points and lines, etc.).
\item
  \texttt{col}: Specifies the color of the plot elements (e.g., points, lines, etc.).
\item
  \texttt{pch}: Specifies the symbol used for the points in the plot.
\item
  \texttt{cex}: Specifies the size of the plot elements (e.g., points, lines, etc.).
\end{itemize}

\begin{quote}
Note: Here \texttt{x} and \texttt{y} refer to vectors instead of column names, as we will see in next chapter on ggplot2 where we will use column names, instead of individual vectors. You'll understand the difference in the following example.
\end{quote}

Example of scatterplot: (This will create a scatterplot with ``wt'' on the x-axis and ``mpg'' on the y-axis, with axis labels and a main title.)

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{wt, }
\NormalTok{     mtcars}\SpecialCharTok{$}\NormalTok{mpg, }
     \AttributeTok{xlab =} \StringTok{"Car Weight (1000 lbs)"}\NormalTok{, }
     \AttributeTok{ylab =} \StringTok{"Miles per Gallon"}\NormalTok{, }
     \AttributeTok{main =} \StringTok{"Scatterplot of MPG vs Car Weight"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[height=0.55\textheight]{DauR_files/figure-latex/p1-1} \end{center}

Example-2: Lineplot (This will create a line plot with ``pressure''\footnote{The ``pressure'' dataset contains measurements of vapor pressure of mercury as a function of temperature, measured at 10 different times. The line plot will show the trend of vapor pressure over time.} on the y-axis and ``time'' on the x-axis, with axis labels and a main title.)

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(pressure, }
     \AttributeTok{type =} \StringTok{"l"}\NormalTok{, }
     \AttributeTok{xlab =} \StringTok{"Time"}\NormalTok{, }
     \AttributeTok{ylab =} \StringTok{"Pressure"}\NormalTok{, }
     \AttributeTok{main =} \StringTok{"Line Plot of Pressure vs Time"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[height=0.55\textheight]{DauR_files/figure-latex/p2-1} \end{center}

Note in above example that, the ``type'' argument is set to ``l'' to specify that a line plot should be created.

Example-3: Bar-Plot (bar plot of the number of cars by ``cyl'' i.e.~number of cylinders)

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{barplot}\NormalTok{(}\FunctionTok{table}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{cyl), }
        \AttributeTok{xlab =} \StringTok{"Number of Cylinders"}\NormalTok{, }
        \AttributeTok{ylab =} \StringTok{"Frequency"}\NormalTok{, }
        \AttributeTok{main =} \StringTok{"Barplot of Number of Cars by Cylinders"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[height=0.55\textheight]{DauR_files/figure-latex/p3-1} \end{center}

Note: In above example we have used \texttt{table()} function to get the frequency table of \texttt{mtcars\$cyl}.

Example-4: Boxplot ()

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a boxplot of "Sepal.Length" by "Species" in the iris dataset}
\FunctionTok{plot}\NormalTok{(}\AttributeTok{x =}\NormalTok{ iris}\SpecialCharTok{$}\NormalTok{Species, }
     \AttributeTok{y =}\NormalTok{ iris}\SpecialCharTok{$}\NormalTok{Sepal.Length, }
     \AttributeTok{main =} \StringTok{"Boxplot of Sepal Length by Species"}\NormalTok{,}
     \AttributeTok{xlab =} \StringTok{"Species"}\NormalTok{, }
     \AttributeTok{ylab =} \StringTok{"Sepal Length"}\NormalTok{, }
     \AttributeTok{col =} \StringTok{"darkgray"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[height=0.55\textheight]{DauR_files/figure-latex/p4-1} \end{center}

Example-5: density plot

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a density plot of "Sepal.Length" in the iris dataset}
\FunctionTok{plot}\NormalTok{(}\FunctionTok{density}\NormalTok{(iris}\SpecialCharTok{$}\NormalTok{Sepal.Length), }\AttributeTok{main =} \StringTok{"Density Plot of Sepal Length in the iris dataset"}\NormalTok{,}
     \AttributeTok{xlab =} \StringTok{"Sepal Length"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"Density"}\NormalTok{, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[height=0.55\textheight]{DauR_files/figure-latex/p5-1} \end{center}

Note that we have used \texttt{density()} function within \texttt{plot()} to create a density plot.

Let us learn about other plotting functions in base R.

\hypertarget{bar-plots-using-barplot}{%
\section{\texorpdfstring{Bar plots using \texttt{barplot()}}{Bar plots using barplot()}}\label{bar-plots-using-barplot}}

As the name suggests \texttt{barplot()} function is used to create bar charts in R. The basic syntax is \texttt{barplot(height,\ ...)} where \texttt{height} should be a vector providing heights of each bar. Other arguments in ellipsis \texttt{...} can be checked by using \texttt{?barplot()}.

Here are a few examples of how to use the function:

Example: Basic Bar Chart

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a basic bar chart of the "mpg" dataset}

\FunctionTok{barplot}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{mpg, }\AttributeTok{names.arg =} \FunctionTok{rownames}\NormalTok{(mtcars), }\AttributeTok{main =} \StringTok{"Miles Per gallon {-} mtcars"}\NormalTok{, }\AttributeTok{xlab =} \StringTok{"Car Model"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"MPG"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[height=0.55\textheight]{DauR_files/figure-latex/p6-1} \end{center}

Example- Bar chart with summarised data

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a summarised bar chart of the "PlantGrowth" dataset}
\FunctionTok{data}\NormalTok{(PlantGrowth)}
\FunctionTok{barplot}\NormalTok{(}\AttributeTok{height =} \FunctionTok{t}\NormalTok{(}\FunctionTok{tapply}\NormalTok{(PlantGrowth}\SpecialCharTok{$}\NormalTok{weight, }
                          \FunctionTok{list}\NormalTok{(PlantGrowth}\SpecialCharTok{$}\NormalTok{group), }
\NormalTok{                          mean)), }
        \AttributeTok{main =} \StringTok{"Mean Weight of Plants by Group"}\NormalTok{, }
        \AttributeTok{ylab =} \StringTok{"Group"}\NormalTok{, }
        \AttributeTok{xlab =} \StringTok{"Weight"}\NormalTok{, }
        \AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"red"}\NormalTok{, }\StringTok{"green"}\NormalTok{, }\StringTok{"blue"}\NormalTok{), }
        \AttributeTok{beside =} \ConstantTok{TRUE}\NormalTok{, }
        \AttributeTok{horiz =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[height=0.55\textheight]{DauR_files/figure-latex/p7-1} \end{center}

Example- Grouped Bar Chart

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a grouped bar chart of the "ChickWeight" dataset}
\FunctionTok{data}\NormalTok{(ChickWeight)}
\FunctionTok{barplot}\NormalTok{(}\AttributeTok{height =} \FunctionTok{t}\NormalTok{(}\FunctionTok{tapply}\NormalTok{(ChickWeight}\SpecialCharTok{$}\NormalTok{weight, }
                          \FunctionTok{list}\NormalTok{(ChickWeight}\SpecialCharTok{$}\NormalTok{Diet, ChickWeight}\SpecialCharTok{$}\NormalTok{Time), }
\NormalTok{                          mean)), }
        \AttributeTok{main =} \StringTok{"Mean Weight of Chicks by Diet and Time"}\NormalTok{, }
        \AttributeTok{xlab =} \StringTok{"Diet and Time"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"Weight"}\NormalTok{, }
        \AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"red"}\NormalTok{, }\StringTok{"green"}\NormalTok{, }\StringTok{"blue"}\NormalTok{, }\StringTok{"yellow"}\NormalTok{), }
        \AttributeTok{beside =} \ConstantTok{TRUE}\NormalTok{,}
        \AttributeTok{legend.text =} \FunctionTok{c}\NormalTok{(}\StringTok{"Diet 1"}\NormalTok{, }\StringTok{"Diet 2"}\NormalTok{, }\StringTok{"Diet 3"}\NormalTok{, }\StringTok{"Diet 4"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[height=0.55\textheight]{DauR_files/figure-latex/p8-1} \end{center}

Example: Stacked Bar Chart

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a stacked bar chart of the "VADeaths" dataset}
\FunctionTok{data}\NormalTok{(VADeaths)}
\FunctionTok{barplot}\NormalTok{(}\FunctionTok{as.matrix}\NormalTok{(VADeaths), }
        \AttributeTok{main =} \StringTok{"Death Rates by Age Group and Gender"}\NormalTok{, }
        \AttributeTok{xlab =} \StringTok{"Age Group"}\NormalTok{, }
        \AttributeTok{ylab =} \StringTok{"Death Rate"}\NormalTok{, }
        \AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"red"}\NormalTok{, }\StringTok{"green"}\NormalTok{, }\StringTok{"blue"}\NormalTok{, }\StringTok{"yellow"}\NormalTok{, }\StringTok{"purple"}\NormalTok{), }
        \AttributeTok{legend.text =} \FunctionTok{c}\NormalTok{(}\StringTok{"Females"}\NormalTok{, }\StringTok{"Males"}\NormalTok{), }
        \AttributeTok{beside =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[height=0.55\textheight]{DauR_files/figure-latex/p9-1} \end{center}

In fact, we can also create barplots using formula directly. See this example

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{barplot}\NormalTok{(GNP }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Year, }\AttributeTok{data =}\NormalTok{ longley)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[height=0.55\textheight]{DauR_files/figure-latex/p10-1} \end{center}

\hypertarget{histograms-using-hist}{%
\section{\texorpdfstring{Histograms using \texttt{hist()}}{Histograms using hist()}}\label{histograms-using-hist}}

The \texttt{hist()} function in R is used to create a histogram, which is a graphical representation of the distribution of a numeric variable. The basic syntax for \texttt{hist()} is as follows:

\begin{verbatim}
hist(x, breaks = "Sturges", freq = TRUE, main = NULL,
     xlab = NULL, ylab = "Frequency", ...)
\end{verbatim}

where -

\begin{itemize}
\tightlist
\item
  \texttt{x}: The data to be plotted, which should be a numeric vector or a matrix.
\item
  \texttt{breaks}: The number of bins to use in the histogram. By default, R uses the Sturges formula to determine the number of bins, but you can also specify a different number of bins or a vector of breakpoints.
\item
  \texttt{freq}: A logical value indicating whether to plot the frequency or density of the data. If \texttt{TRUE}, the y-axis represents the number of observations in each bin. If \texttt{FALSE}, the y-axis represents the density of the data.
\item
  \texttt{main}: A character string specifying the title of the histogram.
\item
  \texttt{xlab}: A character string specifying the label of the x-axis.
\item
  \texttt{ylab}: A character string specifying the label of the y-axis.
\item
  \texttt{...}: Additional arguments passed to the \texttt{plot()} function, such as \texttt{col}, \texttt{border}, and \texttt{xlim}.
\end{itemize}

Example -

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(iris}\SpecialCharTok{$}\NormalTok{Sepal.Length, }\AttributeTok{breaks =} \DecValTok{10}\NormalTok{, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{,}
     \AttributeTok{xlab =} \StringTok{"Sepal Length"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"Frequency"}\NormalTok{,}
     \AttributeTok{main =} \StringTok{"Histogram of Sepal Length in Iris Dataset"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[height=0.55\textheight]{DauR_files/figure-latex/p11-1} \end{center}

Another example using two layers

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{mpg,}
     \AttributeTok{freq=}\ConstantTok{FALSE}\NormalTok{,}
     \AttributeTok{breaks=}\DecValTok{12}\NormalTok{,}
     \AttributeTok{col=}\StringTok{"red"}\NormalTok{,}
     \AttributeTok{xlab=}\StringTok{"Miles Per Gallon"}\NormalTok{,}
     \AttributeTok{main=}\StringTok{"Histogram, with density curve"}\NormalTok{)}
\CommentTok{\# add density curve}
\FunctionTok{lines}\NormalTok{(}\FunctionTok{density}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{mpg), }\AttributeTok{col=}\StringTok{"blue"}\NormalTok{, }\AttributeTok{lwd=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[height=0.55\textheight]{DauR_files/figure-latex/p12-1} \end{center}

In above example we have used \texttt{lines()} function to add another layer to existing plot.

\hypertarget{boxplots-and-variants-using-boxplot}{%
\section{\texorpdfstring{Boxplot(s) and variants using \texttt{boxplot()}}{Boxplot(s) and variants using boxplot()}}\label{boxplots-and-variants-using-boxplot}}

A Box-plot is also called box and whiskers plot and is used to show distribution of a numerical variable using graphical summaries. Deciphering box plot can be understood using the following illustration.

\begin{figure}

{\centering \includegraphics[width=9.35in,height=0.45\textheight]{images/boxplots} 

}

\caption{Understanding boxplots}\label{fig:boxplots}
\end{figure}

Using \texttt{boxplot()} function is simple, as the syntax is \texttt{boxplot(x,\ ...)} where \texttt{x} is a numeric vector or a list of numeric vectors to be plotted, and \texttt{...} represents any additional arguments that can be used to customize the appearance of the box plot.

Example-

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{boxplot}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{mpg)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[height=0.55\textheight]{DauR_files/figure-latex/p13-1} \end{center}

However, the boxplots are particularly useful when drawn in parallel over several categories. To draw these, we can directly use formula as we did in example above. E.g. the following code will create box-plots of for each type of spray, from InsectSprays\footnote{a base R data-set having the counts of insects in agricultural experimental units treated with different insecticides.} data.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{boxplot}\NormalTok{(count }\SpecialCharTok{\textasciitilde{}}\NormalTok{ spray, }
        \AttributeTok{data =}\NormalTok{ InsectSprays, }
        \AttributeTok{col =} \StringTok{"lightgray"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[height=0.55\textheight]{DauR_files/figure-latex/p14-1} \end{center}

\hypertarget{saving-and-exporting-charts}{%
\section{Saving and exporting charts}\label{saving-and-exporting-charts}}

See last section of next chapter.

\hypertarget{use-of-par}{%
\section{\texorpdfstring{Use of \texttt{par()}}{Use of par()}}\label{use-of-par}}

One of the useful functions while using base R's graphics is \texttt{par()} which is used to set or query graphical \textbf{par}ameters. See the following example, where one of its argument \texttt{mfrow\ =\ c(nr,\ nc)} has been used inside this function to draw subsequent plots in an \texttt{nr}-by-\texttt{nc} array on the device.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\NormalTok{purrr}\SpecialCharTok{::}\FunctionTok{walk}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{, }
     \SpecialCharTok{\textasciitilde{}}\FunctionTok{plot}\NormalTok{(anscombe[[}\FunctionTok{paste0}\NormalTok{(}\StringTok{\textquotesingle{}x\textquotesingle{}}\NormalTok{, .x)]], }
\NormalTok{           anscombe[[}\FunctionTok{paste0}\NormalTok{(}\StringTok{\textquotesingle{}y\textquotesingle{}}\NormalTok{, .x)]], }
           \AttributeTok{xlab =} \FunctionTok{paste0}\NormalTok{(}\StringTok{\textquotesingle{}x\textquotesingle{}}\NormalTok{, .x),}
           \AttributeTok{ylab =} \FunctionTok{paste0}\NormalTok{(}\StringTok{\textquotesingle{}y\textquotesingle{}}\NormalTok{, .x),}
           \AttributeTok{main =} \FunctionTok{paste}\NormalTok{(}\StringTok{"Anscombe\textquotesingle{}s Quartet Chart No."}\NormalTok{, .x))}
\NormalTok{     )}
\end{Highlighting}
\end{Shaded}

\includegraphics{DauR_files/figure-latex/unnamed-chunk-237-1.pdf}

\hypertarget{visualising-data-with-ggplot2}{%
\chapter{\texorpdfstring{Visualising data with \texttt{ggplot2}}{Visualising data with ggplot2}}\label{visualising-data-with-ggplot2}}

\hypertarget{core-concepts-of-grammar-of-graphics}{%
\section{\texorpdfstring{Core concepts of \textbf{grammar of graphics}}{Core concepts of grammar of graphics}}\label{core-concepts-of-grammar-of-graphics}}

\textbf{ggplot2}\footnote{\url{https://ggplot2.tidyverse.org/}} \citep{R-ggplot2} is the package developed by Hadley Wickham, which is based on concepts laid (2005) down by Leland Wilkinson in his \emph{The Grammar of Graphics}.\footnote{\url{https://link.springer.com/book/10.1007/0-387-28695-0}} Basically, a grammar of graphics is a framework which follows a layered approach to describe and construct visualizations or graphics in a structured manner. Even the letters \texttt{gg} in ggplot2 stand for \texttt{g}rammar of \texttt{g}raphics.

Hadley Wickham, in his paper titled \textbf{A Layered Grammar of Graphics}\footnote{\url{http://vita.had.co.nz/papers/layered-grammar.pdf}}(2010) \citep{layered-grammar} proposed his idea of layered grammar of graphics in detail and simultaneously put forward his idea of \emph{ggplot2} as an open source implementation framework for building graphics. Readers/Users are advised to check the paper as it describes the concept of grammar of graphics in detail. By the end of the decade the package progressed\footnote{Version 3.3.0 was released in March 2020} to one of the most used and popular packages in R.

The relationship between the components explained in both the grammars can be illustrated with the following figure\footnote{Source: Hadley Wickham's paper on \emph{the layered grammar of graphics}}. The components on the left have been put forward by Wilkinson whereas those on right were proposed by Wickham. It may be seen that \texttt{TRANS} has no relation in \texttt{ggplot2} as its role is played by in-built features of R.

\begin{figure}

{\centering \includegraphics[width=5.25in,height=0.75\textheight]{images/layers_gg} 

}

\caption{Layers in Grammar of Graphics mapped in GGPLOT2}\label{fig:unnamed-chunk-238}
\end{figure}

Thus, to build a graphic having one or more dimensions, from a given data, we use \emph{seven} major components -

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Data:} Unarguably, a graphic/visualisation should start with a data. It is also the first argument in most important function in the package i.e.~\texttt{ggplot(data\ =)}.
\item
  \textbf{Aesthetics:} or \texttt{aes()} in short, provide a mapping of various data dimensions to axes so as to provide positions to various data points in the output plot/graphic.
\item
  \textbf{Geometries:} or \texttt{geoms} for short, are used to provide the \emph{geometries} so that data points may take a concrete shape on the visualisation. For e.g.~the data points should be depicted as bars or scatter points or else are decided by the provided \texttt{geoms.}
\item
  \textbf{Statistics:} or \texttt{stat} for short, provides the statistics to show in the visualisation like measures of central tendency, etc.
\item
  \textbf{Scale:} This component is used to decide whether any dimension needs some scaling like logrithmic transformation, etc.
\item
  \textbf{Coordinate System:} Though most of the time \emph{cartesian coordinate system} is used, yet there are times when \emph{polar coordinate system} (e.g.~pie chart) or \emph{spherical coordinate system} (e.g.~geographical maps) are used.
\item
  \textbf{Facets:} Used when based on certain dimension, the plot is divided into further sub-plots.
\end{enumerate}

\textbf{Prerequisites}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}
\end{Highlighting}
\end{Shaded}

\hypertarget{ggplot2-in-action}{%
\section*{GGPLOT2 in action}\label{ggplot2-in-action}}
\addcontentsline{toc}{section}{GGPLOT2 in action}

\begin{quote}
Out of the afore-mentioned components, three are to be explicitly provided and thus can be understood as mandatoty components. These three componenets are \texttt{data}, \texttt{aesthetics} and \texttt{geometries}. Whilst these three compoenents are mandatorily provided, it is not that others are not mandatory. basically other componenets have their defaults (e.g.~default coordinate system is cartesian coordinate system). Let us dive into these three essential components and build a plot using these.
\end{quote}

\hypertarget{building-a-basic-plot}{%
\section{Building a basic plot}\label{building-a-basic-plot}}

We will use \texttt{mtcars} datasets, a default dataset to learn the concepts.

See what happens when \texttt{data} is provided to \texttt{ggplot} function-

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data=}\NormalTok{mtcars)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{DauR_files/figure-latex/fig_blank-1} \end{center}

We can see that a blank chart/plot space has been created as our data \texttt{mtcars} has now mapped with ggplot2. Now let us provide aesthetic mappings to this using function \texttt{aes()}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ mtcars, }\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{wt, }\AttributeTok{y=}\NormalTok{mpg))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{DauR_files/figure-latex/fig_gg_2-1} \end{center}

You may now notice, apart from creating a blank space for plot, the two dimensions provided, i.e.~\texttt{wt} and \texttt{mpg} have been \emph{mapped} with \texttt{x} and \texttt{y} axes respectively. Since no geometry has been provided, the plot area is still blank. Now we will provide geometry to our dimension say \emph{point}. To do this we will use another layer of function \texttt{geom\_*} (\texttt{geom\_point()} in this case specifically).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(mtcars, }\FunctionTok{aes}\NormalTok{(wt, mpg)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{DauR_files/figure-latex/fig_3-1} \end{center}

Notice that another layer has been added to function \texttt{ggplot()} using a \texttt{+} sign here.

We could have used another geometry say boxplot here.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(mtcars, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ wt)) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{DauR_files/figure-latex/fig_4-1} \end{center}

That's basic architecture of this package. Now lets discuss more on \texttt{aesthetics} and \texttt{geometries} before moving on to another compoenents.

\hypertarget{more-on-aesthetics}{%
\section{More on Aesthetics}\label{more-on-aesthetics}}

Now what if color is provided inside \texttt{geom\_*} function.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(mtcars, }\FunctionTok{aes}\NormalTok{(wt, mpg)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{color=}\StringTok{\textquotesingle{}red\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{DauR_files/figure-latex/fig_5-1} \end{center}

As the argument \texttt{color=\textquotesingle{}red\textquotesingle{}} was mentioned inside the \texttt{geom\_point()} function, it turned every point to red. But if we have to pass a vector/column based on which the points should be colored, it should be wrapped within aesthetics function \texttt{aes()} -

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(mtcars, }\FunctionTok{aes}\NormalTok{(wt, mpg)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{color=}\NormalTok{cyl))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{DauR_files/figure-latex/fig_6-1} \end{center}

Since the \texttt{cyl} column was a numeric column, ggplot2 thought it to be a \texttt{continuos} column and thus produced a color scale instead of a legend. We however, know that this is a categorical column here, and thus if we want to produce a color legend we will have to convert it to a factor first. See now the changes-

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(mtcars, }\FunctionTok{aes}\NormalTok{(wt, mpg)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{color=}\FunctionTok{as.factor}\NormalTok{(cyl)))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{DauR_files/figure-latex/fig_7-1} \end{center}

One more thing - \texttt{aes()} function wrapped in \texttt{geom\_point()} function could have been wrapped in \texttt{ggplot()} also. So basically the following code will also produce exactly the same chart-

\begin{verbatim}
ggplot(mtcars, aes(wt, mpg, color = as.factor(cyl))) +
  geom_point()
\end{verbatim}

Two questions arise here -

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Is there any difference between the two?
\end{enumerate}

\textbf{Ans:} Yes, basically aesthetics if provided under the \texttt{geoms}, will override those aesthetics which are already provided under \texttt{ggplot} function. See the result of following command in your console-

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(mtcars, }\FunctionTok{aes}\NormalTok{(wt, mpg, }\AttributeTok{color =} \FunctionTok{as.factor}\NormalTok{(cyl))) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{color=}\StringTok{\textquotesingle{}red\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{DauR_files/figure-latex/fig_8-1} \end{center}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  What if \texttt{color=\textquotesingle{}red\textquotesingle{}} (or blue) is passed inside \texttt{aes()}?
\end{enumerate}

\textbf{Ans:} In this case ggplot will try to map it some aesthetics called `blue'. Let's see

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(mtcars, }\FunctionTok{aes}\NormalTok{(wt, mpg)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{color=}\StringTok{\textquotesingle{}blue\textquotesingle{}}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{DauR_files/figure-latex/fig_9-1} \end{center}

Interesting! GGPLOT2 has not only mapped a dummy variable called \texttt{\textquotesingle{}blue\textquotesingle{}} with color of points, but also created a legend. More interestingly the color is not what we wanted.

Different types of aesthetic attributes work better with different types of variables. For example, \texttt{color} and \texttt{shape} work well with discreet variables, while \texttt{size} or \texttt{alpha} (transparency) works well for continuous variables. In your console run the following command and check results

\begin{verbatim}
ggplot(mtcars, aes(wt, mpg, shape=as.factor(cyl))) +
  geom_point()
# OR
ggplot(mtcars, aes(wt, mpg, size=cyl)) +
  geom_point()
# OR
ggplot(mtcars, aes(wt, mpg, alpha=cyl)) +
  geom_point()
\end{verbatim}

Multiple aesthetics can be mapped simultaneously, as per requirement. See this example-

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(mtcars, }\FunctionTok{aes}\NormalTok{(wt, mpg, }\AttributeTok{shape=}\FunctionTok{as.factor}\NormalTok{(cyl), }\AttributeTok{color=}\FunctionTok{as.factor}\NormalTok{(gear), }\AttributeTok{alpha=}\NormalTok{wt)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{DauR_files/figure-latex/fig_10-1} \end{center}

Some commonly used aesthetics are -

\begin{itemize}
\tightlist
\item
  \texttt{shape} = Display a point with \texttt{geom\_point()} as a dot, star, triangle, or square
\item
  \texttt{fill} = The interior color (e.g.~of a bar or boxplot)
\item
  \texttt{color} = The exterior line of a \texttt{bar}, \texttt{boxplot}, etc., or the point color if using \texttt{geom\_point()}
\item
  \texttt{size} = Size (e.g.~line thickness, point size)
\item
  \texttt{alpha} = Transparency (\texttt{1\ =\ opaque}, \texttt{0\ =\ invisible})
\item
  \texttt{binwidth} = Width of histogram bins
\item
  \texttt{width} = Width of ``bar plot'' columns
\item
  \texttt{linetype} = Line type (e.g.~solid, dashed, dotted)
\end{itemize}

\textbf{A few shapes available in \texttt{shape} aesthetics}

\begin{figure}

{\centering \includegraphics[width=0.99\linewidth]{shapes} 

}

\caption{Some Shapes available in GGplot}\label{fig:shapes}
\end{figure}

\hypertarget{geoms}{%
\section{More on Geoms}\label{geoms}}

In previous section we have seen that as soon as we passed a \texttt{geom\_*} function/layer to \texttt{data} \& \texttt{aesthetics} layers, the chart/graph was constructed. Actually, \texttt{geom\_point()} function, in the background added three more layers i.e.~\texttt{stat}, \texttt{geom} and \texttt{position}. Why? The answer is simple, \texttt{geom\_*} are generally shortcuts, which add these three layers. So in our example, \texttt{ggplot(mtcars,\ aes(wt,\ mpg))\ +\ geom\_point()} is actually equivalent to -

\begin{verbatim}
ggplot(mpg, aes(displ, hwy)) +
  layer(
  mapping = NULL, 
  data = NULL,
  geom = "point", 
  stat = "identity",
  position = "identity"
)
\end{verbatim}

A complete list of \texttt{geoms} available in ggplot2 is given in Annex-. Some common geoms are listed below:

\begin{itemize}
\tightlist
\item
  Histograms - \texttt{geom\_histogram()}
\item
  Bar charts - \texttt{geom\_bar()} or \texttt{geom\_col()}
\item
  Box plots - \texttt{geom\_boxplot()}
\item
  Points (e.g.~scatter plots) - \texttt{geom\_point()}
\item
  Line graphs - \texttt{geom\_line()} or \texttt{geom\_path()}
\item
  Trend lines - \texttt{geom\_smooth()}
\end{itemize}

\textbf{Note that in ggplot2 \texttt{color} aesthetic represent border color of geometry and \texttt{fill} aesthetic represent color used to be fill that geometry.}

\hypertarget{list-of-geoms-available-in-ggplot2}{%
\subsection{\texorpdfstring{List of geoms available in \texttt{ggplot2}}{List of geoms available in ggplot2}}\label{list-of-geoms-available-in-ggplot2}}

Table: List of GEOMS available in \texttt{ggplot2} package

\begin{tabular}{l|l}
\hline
Topic & Title\\
\hline
geom\_abline & Reference lines: horizontal, vertical, and diagonal\\
\hline
geom\_bar & Bar charts\\
\hline
geom\_bin\_2d & Heatmap of 2d bin counts\\
\hline
geom\_blank & Draw nothing\\
\hline
geom\_boxplot & A box and whiskers plot (in the style of Tukey)\\
\hline
geom\_contour & 2D contours of a 3D surface\\
\hline
geom\_count & Count overlapping points\\
\hline
geom\_density & Smoothed density estimates\\
\hline
geom\_density\_2d & Contours of a 2D density estimate\\
\hline
geom\_dotplot & Dot plot\\
\hline
geom\_errorbarh & Horizontal error bars\\
\hline
geom\_function & Draw a function as a continuous curve\\
\hline
geom\_hex & Hexagonal heatmap of 2d bin counts\\
\hline
geom\_freqpoly & Histograms and frequency polygons\\
\hline
geom\_jitter & Jittered points\\
\hline
geom\_crossbar & Vertical intervals: lines, crossbars \& errorbars\\
\hline
geom\_map & Polygons from a reference map\\
\hline
geom\_path & Connect observations\\
\hline
geom\_point & Points\\
\hline
geom\_polygon & Polygons\\
\hline
geom\_qq\_line & A quantile-quantile plot\\
\hline
geom\_quantile & Quantile regression\\
\hline
geom\_ribbon & Ribbons and area plots\\
\hline
geom\_rug & Rug plots in the margins\\
\hline
geom\_segment & Line segments and curves\\
\hline
geom\_smooth & Smoothed conditional means\\
\hline
geom\_spoke & Line segments parameterised by location, direction and distance\\
\hline
geom\_label & Text\\
\hline
geom\_raster & Rectangles\\
\hline
geom\_violin & Violin plot\\
\hline
CoordSf & Visualise sf objects\\
\hline
update\_geom\_defaults & Modify geom/stat aesthetic defaults for future plots\\
\hline
\end{tabular}

\hypertarget{faceting}{%
\section{Faceting}\label{faceting}}

The amount of data also makes a difference: if there is a lot of data it can be hard to distinguish different groups. An alternative solution is to use faceting, as described next. Facets, or ``small-multiples'', are used to split one plot into a multi-panel figure, with one panel (``facet'') per group of data. The same type of plot is created multiple times, each one using a sub-group of the same dataset.

In \texttt{ggplot2} faceting can be acheived using either of the functions -

\begin{itemize}
\tightlist
\item
  \texttt{facet\_grid()} creates a grid of plots, with each plot showing a subset of the data. We may also specify the number of columns to use in the grid using the \texttt{ncol} argument.
\item
  \texttt{facet\_wrap()} creates a grid of plots with different variables on each axis. We may also specify the scales to use for each axis using the \texttt{scales} argument.
\end{itemize}

Let us understand this, with these examples.

Example-1

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(mpg, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ displ, }\AttributeTok{y =}\NormalTok{ hwy)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ class, }\AttributeTok{ncol =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{DauR_files/figure-latex/unnamed-chunk-241-1.pdf}

Example-2

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(mpg, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ displ, }\AttributeTok{y =}\NormalTok{ hwy)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{facet\_grid}\NormalTok{(year }\SpecialCharTok{\textasciitilde{}}\NormalTok{ class)}
\end{Highlighting}
\end{Shaded}

\includegraphics{DauR_files/figure-latex/unnamed-chunk-242-1.pdf}

Notice that \texttt{facet\_grid()} arranges the plots in a grid with different variables on each axis. We specify the variables to use for faceting using the \texttt{\textasciitilde{}} operator. For example, \texttt{facet\_grid(variable1\ \textasciitilde{}\ variable2)} will create a grid of plots with \texttt{variable1} on the y-axis and \texttt{variable2} on the x-axis. This is useful when we want to compare the relationship between two variables across different levels of a third variable.

On the other hand, \texttt{facet\_wrap()} creates a grid of plots, each showing a subset of your data based on a single variable. We specify the variable to use for faceting using the same \texttt{\textasciitilde{}} operator here too. For example, \texttt{facet\_wrap(\textasciitilde{}\ variable)} will create a grid of plots, each showing a different level of the variable. This is useful when you have a single categorical variable that you want to use for faceting.

\hypertarget{labels}{%
\section{Labels}\label{labels}}

Labeling is an essential aspect of data visualization because it provides context and information about the data being presented. Labels can include titles, axis labels, legends, and annotations that describe the data and provide important information that helps the viewer understand what they are looking at. Proper labeling can help to make the data more understandable, clear, and accessible, which enhances its overall value and impact.

\hypertarget{labeling-data-points}{%
\subsection{Labeling Data points}\label{labeling-data-points}}

To label data points in ggplot2, we can use the \texttt{geom\_text()} function. This function adds text to the plot at the specified x and y coordinates. Moreover, we can customize the appearance of the labels by adding additional arguments to \texttt{geom\_text()} -

\begin{itemize}
\tightlist
\item
  \texttt{size} to set font size
\item
  \texttt{color} to color the fonts
\item
  \texttt{hjust} or \texttt{vjust} to adjust the labels vertically or horizontally, respectively.
\end{itemize}

Example-

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(mtcars, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ hp, }\AttributeTok{y =}\NormalTok{ mpg)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_text}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{label =} \FunctionTok{rownames}\NormalTok{(mtcars)),}
            \AttributeTok{size =} \DecValTok{3}\NormalTok{,}
            \AttributeTok{color =} \StringTok{"red"}\NormalTok{,}
            \AttributeTok{vjust =} \SpecialCharTok{{-}}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{DauR_files/figure-latex/unnamed-chunk-243-1.pdf}

\textbf{TIP: Use package \texttt{ggrepel} to show all labels, when needed, without overlapping and in a better way.} E.g.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggrepel)}
\FunctionTok{options}\NormalTok{(}\AttributeTok{ggrepel.max.overlaps =} \ConstantTok{Inf}\NormalTok{)}

\FunctionTok{ggplot}\NormalTok{(mtcars, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ hp, }\AttributeTok{y =}\NormalTok{ mpg)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_text\_repel}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{label =} \FunctionTok{rownames}\NormalTok{(mtcars)),}
            \AttributeTok{size =} \DecValTok{3}\NormalTok{,}
            \AttributeTok{color =} \StringTok{"red"}\NormalTok{,}
            \AttributeTok{vjust =} \SpecialCharTok{{-}}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{DauR_files/figure-latex/unnamed-chunk-244-1.pdf}

\hypertarget{labeling-charts}{%
\subsection{Labeling Charts}\label{labeling-charts}}

There are several ways to add labels to ggplot2 charts, but we will focus on using the \texttt{labs()} function, which allows us to add \texttt{titles}, \texttt{subtitles}, \texttt{axis\ labels}, and other annotations like \texttt{caption}, etc. to the plot. Example -

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(mtcars, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ hp, }\AttributeTok{y =}\NormalTok{ mpg)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Scatter plot of mpg vs. hp"}\NormalTok{,}
       \AttributeTok{subtitle =} \StringTok{"Data from mtcars dataset"}\NormalTok{,}
       \AttributeTok{x =} \StringTok{"Horsepower"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Miles per gallon"}\NormalTok{,}
       \AttributeTok{caption =} \StringTok{"Source: R datasets"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{DauR_files/figure-latex/unnamed-chunk-245-1.pdf}

We can also customize the appearance of the labels by using the \texttt{theme()} function, which allows us to modify the font size, font family, and other visual properties of the labels. Example-

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(mtcars, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ hp, }\AttributeTok{y =}\NormalTok{ mpg)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Scatter plot of mpg vs. hp"}\NormalTok{,}
       \AttributeTok{x =} \StringTok{"Horsepower"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Miles per gallon"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{plot.title =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \DecValTok{20}\NormalTok{, }\AttributeTok{color =} \StringTok{\textquotesingle{}seagreen\textquotesingle{}}\NormalTok{),}
        \AttributeTok{axis.title =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \DecValTok{16}\NormalTok{, }\AttributeTok{color =} \StringTok{"blue"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{DauR_files/figure-latex/unnamed-chunk-246-1.pdf}

\hypertarget{modifying-scales}{%
\section{Modifying scales}\label{modifying-scales}}

Several times the requirement is to modify x or/and y axis minimum and/or maximum values i.e.~axis limits; or otherwise the axis itself is to be transformed. For these requirements, we have \texttt{sacle\_*\_*()} group of functions in \texttt{ggplot2}.

For example we have these two functions for continuos axis/variables.

\begin{verbatim}
scale_x_continuous(name, breaks, labels, limits, trans)
scale_y_continuous(name, breaks, labels, limits, trans)
\end{verbatim}

In arguments to above functions, we can see that axis title (name), axis breaks, axis labels, axis limits, and transformations can be dealt with.

Example-

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Basic Scatter Plot}
\FunctionTok{ggplot}\NormalTok{(cars, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ speed, }\AttributeTok{y =}\NormalTok{ dist)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{()}
\CommentTok{\# Modifying scales both axis title and axis limits}
\FunctionTok{ggplot}\NormalTok{(cars, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ speed, }\AttributeTok{y =}\NormalTok{ dist)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{scale\_x\_continuous}\NormalTok{(}\AttributeTok{name=}\StringTok{"Speed of cars"}\NormalTok{, }\AttributeTok{limits=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{30}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{name=}\StringTok{"Stopping distance"}\NormalTok{, }\AttributeTok{limits=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{150}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.49\linewidth]{DauR_files/figure-latex/scale1-1} \includegraphics[width=0.49\linewidth]{DauR_files/figure-latex/scale1-2} 

}

\caption{Modifying Scales in GGplot2}\label{fig:scale1}
\end{figure}

As for transformation we can use \texttt{trans} argument

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(cars, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ speed, }\AttributeTok{y =}\NormalTok{ dist)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{()}\SpecialCharTok{+}
  \FunctionTok{scale\_x\_continuous}\NormalTok{(}\AttributeTok{trans=}\StringTok{\textquotesingle{}log10\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{trans=}\StringTok{\textquotesingle{}log10\textquotesingle{}}\NormalTok{)}

\NormalTok{state.x77 }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{as.data.frame}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(Area, Illiteracy}\SpecialCharTok{/}\DecValTok{100}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_continuous}\NormalTok{(}\AttributeTok{name =} \StringTok{"Area in Square Miles"}\NormalTok{, }\AttributeTok{labels =}\NormalTok{ scales}\SpecialCharTok{::}\NormalTok{comma) }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{name =} \StringTok{"Illiteracy as \% of Population"}\NormalTok{, }\AttributeTok{labels =}\NormalTok{ scales}\SpecialCharTok{::}\NormalTok{percent)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.49\linewidth]{DauR_files/figure-latex/scale2-1} \includegraphics[width=0.49\linewidth]{DauR_files/figure-latex/scale2-2} 

}

\caption{Transforming Axes in GGplot2}\label{fig:scale2}
\end{figure}

\hypertarget{themes}{%
\section{Themes}\label{themes}}

We can customize the appearance of plots, such as the axis labels, titles, background colors, and font sizes by applying themes to the plot. In the above, notice that we have used \texttt{theme()} function to modify font etc. of labels in the plot. Now see the following example-

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(mtcars, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ wt, }\AttributeTok{y =}\NormalTok{ mpg)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"MPG vs. Weight"}\NormalTok{,}
       \AttributeTok{x =} \StringTok{"Weight (1000 lbs)"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Miles per Gallon"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{()}

\FunctionTok{ggplot}\NormalTok{(mtcars, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ wt, }\AttributeTok{y =}\NormalTok{ mpg)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"MPG vs. Weight"}\NormalTok{,}
       \AttributeTok{x =} \StringTok{"Weight (1000 lbs)"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Miles per Gallon"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_void}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.49\linewidth]{DauR_files/figure-latex/theme1-1} \includegraphics[width=0.49\linewidth]{DauR_files/figure-latex/theme1-2} 

}

\caption{Modifying Themes in GGplot2 theme bw (left) and theme void (right)}\label{fig:theme1}
\end{figure}

In this example, we've used the \texttt{theme\_bw()} and \texttt{theme\_void()} functions to apply a black-and-white theme to the plot. Some theme functions that can be used to modify plot themes are -

\begin{itemize}
\tightlist
\item
  \texttt{theme\_bw()}: A black and white theme that is useful when you want a simple, clean plot.
\item
  \texttt{theme\_classic()}: A classic theme that adds gray borders and gridlines to the plot.
\item
  \texttt{theme\_void()}: A theme with a transparent background and no gridlines or borders.
\item
  \texttt{theme\_minimal()}: A minimalistic theme that removes the gridlines and reduces the size of the axis labels.
\item
  \texttt{theme(axis.title\ =\ element\_text(size\ =\ 16),\ plot.title\ =\ element\_text(size\ =\ 20))}: This code sets the font size of the axis and plot titles to 16 and 20, respectively.
\item
  \texttt{theme(panel.background\ =\ element\_rect(fill\ =\ "gray90"))}: This code sets the background color of the plot to a light gray color.
\end{itemize}

We can also customize specific elements of the plot using \texttt{element\_*()} functions. For example:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(mtcars, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ wt, }\AttributeTok{y =}\NormalTok{ mpg)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"MPG vs. Weight"}\NormalTok{,}
       \AttributeTok{x =} \StringTok{"Weight (1000 lbs)"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Miles per Gallon"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{plot.title =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{color =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{size =} \DecValTok{20}\NormalTok{, }\AttributeTok{face =} \StringTok{"bold"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.49\linewidth]{DauR_files/figure-latex/theme2-1} 

}

\caption{Customising Themes in GGplot2}\label{fig:theme2}
\end{figure}

We can combine multiple customization options together to create a customized theme that fits our specific needs. The possibilities for customization are endless, so feel free to experiment and create your own unique theme!

\hypertarget{savingexporting-plots}{%
\section{Saving/exporting plots}\label{savingexporting-plots}}

Of course, after creating charts/plots we would like to save them for further usage in our reports/documents, etc. Though there may be many options to save a plot to disk, we will be focussing on three different methods.

\hypertarget{saving-through-rstudio-menu}{%
\subsection*{Saving through Rstudio menu}\label{saving-through-rstudio-menu}}
\addcontentsline{toc}{subsection}{Saving through Rstudio menu}

To save a graph using the RStudio menus, go to the Plots tab and choose Export.

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{images/export} 

}

\caption{Exporting Charts}\label{fig:export1}
\end{figure}

Three options are available here.

\begin{itemize}
\tightlist
\item
  Save as Image
\item
  Save as PDF
\item
  Copy to clipboard.
\end{itemize}

\hypertarget{saving-through-code}{%
\subsection*{Saving through code}\label{saving-through-code}}
\addcontentsline{toc}{subsection}{Saving through code}

We may also save our plots using function \texttt{ggsave()} here. Its syntax is simple

\begin{verbatim}
ggsave(
  filename,
  plot = last_plot(),
  device = NULL,
  path = NULL,
  scale = 1,
  width = NA,
  height = NA,
  units = c("in", "cm", "mm", "px"),
  dpi = 300,
  limitsize = TRUE,
  bg = NULL,
  ...
)
\end{verbatim}

All arguments are simple to understand. Thus for example if we need to save the following violin plot,

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{DauR_files/figure-latex/export2-1} 

}

\caption{Violin Plot}\label{fig:export2}
\end{figure}

we can use this code

\begin{verbatim}
ggsave('violin.png', base_plot, height = 10, width = 8)
\end{verbatim}

\hypertarget{graphics-devices-base-r-plots}{%
\subsection*{Graphics Devices (Base R Plots)}\label{graphics-devices-base-r-plots}}
\addcontentsline{toc}{subsection}{Graphics Devices (Base R Plots)}

If we create plots outside of ggplot (with \texttt{plot()}, \texttt{hist()}, \texttt{boxplot()}, etc.), we cannot use \texttt{ggsave()} to save our plots since it only supports plots made with ggplot.

Base R provides a way to save these plots with its graphic device functions. There are three steps involved in this process-

\begin{itemize}
\tightlist
\item
  Specify the file extension and properties (size, resolution, etc.) along with units
\item
  create the plot, in base R or/and ggplot2
\item
  Signal that the plot is finished and save it by running \texttt{dev.off()}. Thus, using this way we can insert as many charts in a single pdf without turning off the device till our pdf is ready.
\end{itemize}

Example-

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Creates a png file}
\FunctionTok{png}\NormalTok{(}
  \AttributeTok{filename =} \StringTok{"scatter.png"}\NormalTok{,}
  \AttributeTok{width =} \DecValTok{5}\NormalTok{,}
  \AttributeTok{height =} \DecValTok{3}\NormalTok{,}
  \AttributeTok{units =} \StringTok{"in"}\NormalTok{,}
  \AttributeTok{res =} \DecValTok{300}
\NormalTok{)}
\CommentTok{\# Prints a ggplot2 in it}
\FunctionTok{ggplot}\NormalTok{(mtcars, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ wt, }\AttributeTok{y =}\NormalTok{ mpg)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_abline}\NormalTok{(}\AttributeTok{intercept =} \DecValTok{5}\NormalTok{,}
              \AttributeTok{slope =} \DecValTok{3}\NormalTok{,}
              \AttributeTok{color =} \StringTok{"seagreen"}\NormalTok{)}
\CommentTok{\# Device is off}
\FunctionTok{dev.off}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## pdf 
##   2
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Creates a new PDF file}
\FunctionTok{pdf}\NormalTok{(}\AttributeTok{file =} \StringTok{"two\_page.pdf"}\NormalTok{,}
    \AttributeTok{width =} \DecValTok{6}\NormalTok{,}
    \AttributeTok{height =} \DecValTok{4}\NormalTok{)}
\CommentTok{\#first plot}
\FunctionTok{plot}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{wt, mtcars}\SpecialCharTok{$}\NormalTok{mpg)}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{a =} \DecValTok{5}\NormalTok{, }\AttributeTok{b =} \DecValTok{3}\NormalTok{, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{)}
\CommentTok{\# Second Plot}
\FunctionTok{ggplot}\NormalTok{(mtcars, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ wt, }\AttributeTok{y =}\NormalTok{ mpg)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_abline}\NormalTok{(}\AttributeTok{intercept =} \DecValTok{5}\NormalTok{,}
              \AttributeTok{slope =} \DecValTok{3}\NormalTok{,}
              \AttributeTok{color =} \StringTok{"seagreen"}\NormalTok{)}
\CommentTok{\# Device Off}
\FunctionTok{dev.off}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## pdf 
##   2
\end{verbatim}

\hypertarget{ggplot2-tips-and-other-extensionssupporting-libraries}{%
\section*{GGPLOT2 Tips and other extensions/supporting libraries}\label{ggplot2-tips-and-other-extensionssupporting-libraries}}
\addcontentsline{toc}{section}{GGPLOT2 Tips and other extensions/supporting libraries}

\begin{itemize}
\tightlist
\item
  \href{https://exts.ggplot2.tidyverse.org/gallery/}{Extensions}
\item
  \href{https://r-graph-gallery.com/ggplot2-package.html}{R graph Gallery}
\end{itemize}

\hypertarget{more-to-read}{%
\section*{More to read}\label{more-to-read}}
\addcontentsline{toc}{section}{More to read}

\begin{itemize}
\tightlist
\item
  Book \citep{ggplot22016}
\item
  \href{https://posit.co/wp-content/uploads/2022/10/data-visualization-1.pdf}{ggplot2 cheatsheet}
\item
  \href{http://r-statistics.co/ggplot2-cheatsheet.html}{Another Cheatsheet}
\end{itemize}

\hypertarget{data-transformation-in-dplyr}{%
\chapter{\texorpdfstring{Data Transformation in \texttt{dplyr}}{Data Transformation in dplyr}}\label{data-transformation-in-dplyr}}

\textbf{Prerequisites}
Obviously \texttt{dplyr} \citep{R-dplyr} will be needed. This package also comes with matrittr pipe i.e.~\texttt{\%\textgreater{}\%} and therefore in dplyr syntax we will be using these pipes. \texttt{library(magrittr)} is not needed.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(dplyr)}
\FunctionTok{library}\NormalTok{(knitr)}
\end{Highlighting}
\end{Shaded}

The package \texttt{dplyr} (\ref{fig:dplyrr}) calls its functions as `verbs' because these are actually doing some action. So \texttt{dplyr\ verbs} can be divided in three classifications depending upon where they operate -

\begin{itemize}
\tightlist
\item
  `Row' verbs that operate on Rows
\item
  `Column' verbs
\item
  `group' verbs that operate on table split into different groups.
\end{itemize}

\begin{figure}

{\centering \includegraphics[width=0.99\linewidth]{images/dplyr} 

}

\caption{Package Dplyr}\label{fig:dplyrr}
\end{figure}

Let's learn each of these -

\hypertarget{column-verbs}{%
\section{Column verbs}\label{column-verbs}}

\hypertarget{select}{%
\subsection{\texorpdfstring{\texttt{select()}}{select()}}\label{select}}

In real world data sets we will often come across with data frames having numerous columns. However for many of the data analysis tasks, most of these columns are not needed. As already stated \texttt{select} (figure \ref{fig:selectr}) operates on columns. Like \texttt{SELECT} in \texttt{SQL}, it just \emph{select} the column(s) from the given data frame. The basic syntax is - \texttt{select(data\_frame,\ column\_names,\ ...)}. So with pipes the same syntax goes like this

\begin{verbatim}
data_frame %>% 
  select(column_name)
\end{verbatim}

\begin{figure}

{\centering \includegraphics[width=0.99\linewidth]{images/select_dplyr} 

}

\caption{Illustration of dplyr::select()}\label{fig:selectr}
\end{figure}

For example, let's try our hands on \texttt{mtcars} dataset.\\
Example-1

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mtcars }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(mpg)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                      mpg
## Mazda RX4           21.0
## Mazda RX4 Wag       21.0
## Datsun 710          22.8
## Hornet 4 Drive      21.4
## Hornet Sportabout   18.7
## Valiant             18.1
## Duster 360          14.3
## Merc 240D           24.4
## Merc 230            22.8
## Merc 280            19.2
## Merc 280C           17.8
## Merc 450SE          16.4
## Merc 450SL          17.3
## Merc 450SLC         15.2
## Cadillac Fleetwood  10.4
## Lincoln Continental 10.4
## Chrysler Imperial   14.7
## Fiat 128            32.4
## Honda Civic         30.4
## Toyota Corolla      33.9
## Toyota Corona       21.5
## Dodge Challenger    15.5
## AMC Javelin         15.2
## Camaro Z28          13.3
## Pontiac Firebird    19.2
## Fiat X1-9           27.3
## Porsche 914-2       26.0
## Lotus Europa        30.4
## Ford Pantera L      15.8
## Ferrari Dino        19.7
## Maserati Bora       15.0
## Volvo 142E          21.4
\end{verbatim}

\textbf{Note} that output is still a data frame unlike the \texttt{mtcars{[}{[}\textquotesingle{}mpg\textquotesingle{}{]}{]}} which returns a vector. We can subset multiple columns here. Example-2

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mtcars }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(mpg, qsec) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                    mpg  qsec
## Mazda RX4         21.0 16.46
## Mazda RX4 Wag     21.0 17.02
## Datsun 710        22.8 18.61
## Hornet 4 Drive    21.4 19.44
## Hornet Sportabout 18.7 17.02
## Valiant           18.1 20.22
\end{verbatim}

We can also provide column numbers instead of names. Example-3

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mtcars }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{6}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{tail}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                 hp    wt
## Porsche 914-2   91 2.140
## Lotus Europa   113 1.513
## Ford Pantera L 264 3.170
## Ferrari Dino   175 2.770
## Maserati Bora  335 3.570
## Volvo 142E     109 2.780
\end{verbatim}

We can also use \texttt{select} to reorder the columns in output, by using a \texttt{dplyr} \emph{helping verb} \texttt{everything()} which is basically \emph{everything else.} See this example-

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mtcars }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(qsec, mpg, }\FunctionTok{everything}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{names}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "qsec" "mpg"  "cyl"  "disp" "hp"   "drat" "wt"   "vs"   "am"   "gear"
## [11] "carb"
\end{verbatim}

We may also use mix and match of \emph{column names} and \emph{column numbers}. See

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mtcars }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{7}\NormalTok{, mpg, }\FunctionTok{everything}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{names}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "drat" "qsec" "mpg"  "cyl"  "disp" "hp"   "wt"   "vs"   "am"   "gear"
## [11] "carb"
\end{verbatim}

Operator \texttt{:} can also be used with column names. Ex-

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mtcars }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(mpg}\SpecialCharTok{:}\NormalTok{drat) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{head}\NormalTok{(}\AttributeTok{n=}\DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                 mpg cyl disp  hp drat
## Mazda RX4      21.0   6  160 110 3.90
## Mazda RX4 Wag  21.0   6  160 110 3.90
## Datsun 710     22.8   4  108  93 3.85
## Hornet 4 Drive 21.4   6  258 110 3.08
\end{verbatim}

\textbf{Other helping verbs}
There are other helping verbs, apart from \texttt{everything()} that can be used within \texttt{select()} just to eliminate need to type the column names and select columns based on some conditions. These verbs are self explanatory-

\begin{itemize}
\tightlist
\item
  \texttt{starts\_with(\textquotesingle{}ABC\textquotesingle{})} will select all columns the names of which \textbf{starts with} string \texttt{ABC}
\item
  \texttt{ends\_with(\textquotesingle{}ABC\textquotesingle{})} will select all columns the names of which \textbf{ends with} string \texttt{ABC}
\item
  \texttt{contains(\textquotesingle{}ABC\textquotesingle{})} will select all columns the names of which \textbf{contains} string \texttt{ABC}
\item
  \texttt{num\_range(\textquotesingle{}A\textquotesingle{},\ 1:3)} will select all columns named \texttt{A1}, \texttt{A2} and \texttt{A3}
\end{itemize}

Some Examples.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  This syntax will select all columns which ends with \texttt{"color"} in column names.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{starwars }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(}\FunctionTok{ends\_with}\NormalTok{(}\StringTok{\textquotesingle{}color\textquotesingle{}}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 87 x 3
##    hair_color    skin_color  eye_color
##    <chr>         <chr>       <chr>    
##  1 blond         fair        blue     
##  2 <NA>          gold        yellow   
##  3 <NA>          white, blue red      
##  4 none          white       yellow   
##  5 brown         light       brown    
##  6 brown, grey   light       blue     
##  7 brown         light       blue     
##  8 <NA>          white, red  red      
##  9 black         light       brown    
## 10 auburn, white fair        blue-gray
## # i 77 more rows
\end{verbatim}

Example-2: This syntax will select all columns contains characters \texttt{"or"} in column names.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{starwars }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(}\FunctionTok{contains}\NormalTok{(}\StringTok{\textquotesingle{}or\textquotesingle{}}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 87 x 4
##    hair_color    skin_color  eye_color homeworld
##    <chr>         <chr>       <chr>     <chr>    
##  1 blond         fair        blue      Tatooine 
##  2 <NA>          gold        yellow    Tatooine 
##  3 <NA>          white, blue red       Naboo    
##  4 none          white       yellow    Tatooine 
##  5 brown         light       brown     Alderaan 
##  6 brown, grey   light       blue      Tatooine 
##  7 brown         light       blue      Tatooine 
##  8 <NA>          white, red  red       Tatooine 
##  9 black         light       brown     Tatooine 
## 10 auburn, white fair        blue-gray Stewjon  
## # i 77 more rows
\end{verbatim}

\textbf{Using \texttt{where()}}: This selection helper selects the variables for which a \emph{function} when applied to column names, returns \texttt{TRUE}.

Example-3: This syntax will select all numeric columns from the dataset.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{starwars }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(}\FunctionTok{where}\NormalTok{(is.numeric))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 87 x 3
##    height  mass birth_year
##     <int> <dbl>      <dbl>
##  1    172    77       19  
##  2    167    75      112  
##  3     96    32       33  
##  4    202   136       41.9
##  5    150    49       19  
##  6    178   120       52  
##  7    165    75       47  
##  8     97    32       NA  
##  9    183    84       24  
## 10    182    77       57  
## # i 77 more rows
\end{verbatim}

\hypertarget{mutate}{%
\subsection{\texorpdfstring{\texttt{mutate()}}{mutate()}}\label{mutate}}

This perhaps is one of the most important functions in \texttt{dplyr} kitty. It enables us to create new column(s) that are functions of one or more existing columns. Refer figure \ref{fig:mutater}

\begin{figure}

{\centering \includegraphics[width=0.99\linewidth]{images/mutate_dplyr} 

}

\caption{Illustration of dplyr::mutate()}\label{fig:mutater}
\end{figure}

More than one column can be added simultaneously. Newly created column may also be used for creation of another new column. See example.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{starwars }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(name}\SpecialCharTok{:}\NormalTok{mass) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{name\_upper =} \FunctionTok{toupper}\NormalTok{(name),}
         \AttributeTok{BMI =}\NormalTok{ mass}\SpecialCharTok{/}\NormalTok{(height}\SpecialCharTok{/}\DecValTok{100}\NormalTok{)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 87 x 5
##    name               height  mass name_upper           BMI
##    <chr>               <int> <dbl> <chr>              <dbl>
##  1 Luke Skywalker        172    77 LUKE SKYWALKER      26.0
##  2 C-3PO                 167    75 C-3PO               26.9
##  3 R2-D2                  96    32 R2-D2               34.7
##  4 Darth Vader           202   136 DARTH VADER         33.3
##  5 Leia Organa           150    49 LEIA ORGANA         21.8
##  6 Owen Lars             178   120 OWEN LARS           37.9
##  7 Beru Whitesun Lars    165    75 BERU WHITESUN LARS  27.5
##  8 R5-D4                  97    32 R5-D4               34.0
##  9 Biggs Darklighter     183    84 BIGGS DARKLIGHTER   25.1
## 10 Obi-Wan Kenobi        182    77 OBI-WAN KENOBI      23.2
## # i 77 more rows
\end{verbatim}

By default the new column will be added to the last of data frame. As shown in above example, more operations can be combined in one using \texttt{\%\textgreater{}\%}. There is a cousin \texttt{transmute()} of \emph{mutate} which drops all the old columns and keeps only newly created columns. Example

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{starwars }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{transmute}\NormalTok{(}\AttributeTok{name\_upper =} \FunctionTok{toupper}\NormalTok{(name))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 87 x 1
##    name_upper        
##    <chr>             
##  1 LUKE SKYWALKER    
##  2 C-3PO             
##  3 R2-D2             
##  4 DARTH VADER       
##  5 LEIA ORGANA       
##  6 OWEN LARS         
##  7 BERU WHITESUN LARS
##  8 R5-D4             
##  9 BIGGS DARKLIGHTER 
## 10 OBI-WAN KENOBI    
## # i 77 more rows
\end{verbatim}

\textbf{Other useful dplyr functions} Another good use of \texttt{mutate} is to generate summarised result and display it corresponding to each row in data. For example if the requirement is to calculate proportion of say \texttt{wt} column in \texttt{mtcars} data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mtcars }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{head}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(wt) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{total\_wt =} \FunctionTok{sum}\NormalTok{(wt),}
         \AttributeTok{wt\_proportion =}\NormalTok{ wt}\SpecialCharTok{*}\DecValTok{100}\SpecialCharTok{/}\NormalTok{total\_wt) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                      wt total_wt wt_proportion
## Mazda RX4         2.620    17.93      14.61238
## Mazda RX4 Wag     2.875    17.93      16.03458
## Datsun 710        2.320    17.93      12.93921
## Hornet 4 Drive    3.215    17.93      17.93084
## Hornet Sportabout 3.440    17.93      19.18572
## Valiant           3.460    17.93      19.29727
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{n()} is used to count number of rows
\item
  \texttt{n\_distinct()} is used to count number of distinct values for the given variable
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mtcars }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{total\_cars =} \FunctionTok{n}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                    mpg cyl disp  hp drat total_cars
## Mazda RX4         21.0   6  160 110 3.90         32
## Mazda RX4 Wag     21.0   6  160 110 3.90         32
## Datsun 710        22.8   4  108  93 3.85         32
## Hornet 4 Drive    21.4   6  258 110 3.08         32
## Hornet Sportabout 18.7   8  360 175 3.15         32
## Valiant           18.1   6  225 105 2.76         32
\end{verbatim}

\hypertarget{rename}{%
\subsection{\texorpdfstring{\texttt{rename()}}{rename()}}\label{rename}}

It is used to \emph{rename} the column names. Refer figure \ref{fig:renamer} for illustration.

\begin{figure}

{\centering \includegraphics[width=0.99\linewidth]{images/rename_dplyr} 

}

\caption{Illustration of dplyr::rename()}\label{fig:renamer}
\end{figure}

See this example

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mtcars }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{miles\_per\_gallon =}\NormalTok{ mpg) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{head}\NormalTok{(}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##               miles_per_gallon cyl disp  hp drat    wt  qsec vs am gear carb
## Mazda RX4                 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4
## Mazda RX4 Wag             21.0   6  160 110 3.90 2.875 17.02  0  1    4    4
## Datsun 710                22.8   4  108  93 3.85 2.320 18.61  1  1    4    1
\end{verbatim}

\textbf{Note} that \texttt{select} can also rename the columns but will drop all unselected columns. Check this

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mtcars }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(}\AttributeTok{miles\_per\_gallon =}\NormalTok{ mpg) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{head}\NormalTok{(}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##               miles_per_gallon
## Mazda RX4                 21.0
## Mazda RX4 Wag             21.0
## Datsun 710                22.8
\end{verbatim}

\hypertarget{relocate}{%
\subsection{\texorpdfstring{\texttt{relocate()}}{relocate()}}\label{relocate}}

It \emph{relocates} column or block of columns simultaneosly either before the column mentioned in argument \texttt{.before} or after mentioned in \texttt{.after}. See the example-

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{starwars }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{relocate}\NormalTok{(}\FunctionTok{ends\_with}\NormalTok{(}\StringTok{\textquotesingle{}color\textquotesingle{}}\NormalTok{), }\AttributeTok{.after =}\NormalTok{ name) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{head}\NormalTok{(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 5 x 14
##   name      hair_color skin_color eye_color height  mass birth_year sex   gender
##   <chr>     <chr>      <chr>      <chr>      <int> <dbl>      <dbl> <chr> <chr> 
## 1 Luke Sky~ blond      fair       blue         172    77       19   male  mascu~
## 2 C-3PO     <NA>       gold       yellow       167    75      112   none  mascu~
## 3 R2-D2     <NA>       white, bl~ red           96    32       33   none  mascu~
## 4 Darth Va~ none       white      yellow       202   136       41.9 male  mascu~
## 5 Leia Org~ brown      light      brown        150    49       19   fema~ femin~
## # i 5 more variables: homeworld <chr>, species <chr>, films <list>,
## #   vehicles <list>, starships <list>
\end{verbatim}

\hypertarget{row-verbs}{%
\section{Row verbs}\label{row-verbs}}

\hypertarget{filter}{%
\subsection{\texorpdfstring{\texttt{filter}}{filter}}\label{filter}}

This verb/function is used to subset the data, or in other words filter rows of data frame based on certain condition. Refer figure \ref{fig:filterr} for illustration.

\begin{figure}

{\centering \includegraphics[width=0.99\linewidth]{images/filter_dplyr} 

}

\caption{Illustration of dplyr::filter()}\label{fig:filterr}
\end{figure}

See this example-

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{starwars }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(eye\_color }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}red\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}yellow\textquotesingle{}}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 16 x 14
##    name     height  mass hair_color skin_color eye_color birth_year sex   gender
##    <chr>     <int> <dbl> <chr>      <chr>      <chr>          <dbl> <chr> <chr> 
##  1 C-3PO       167    75 <NA>       gold       yellow         112   none  mascu~
##  2 R2-D2        96    32 <NA>       white, bl~ red             33   none  mascu~
##  3 Darth V~    202   136 none       white      yellow          41.9 male  mascu~
##  4 R5-D4        97    32 <NA>       white, red red             NA   none  mascu~
##  5 Palpati~    170    75 grey       pale       yellow          82   male  mascu~
##  6 IG-88       200   140 none       metal      red             15   none  mascu~
##  7 Bossk       190   113 none       green      red             53   male  mascu~
##  8 Nute Gu~    191    90 none       mottled g~ red             NA   male  mascu~
##  9 Watto       137    NA black      blue, grey yellow          NA   male  mascu~
## 10 Darth M~    175    80 none       red        yellow          54   male  mascu~
## 11 Dud Bolt     94    45 none       blue, grey yellow          NA   male  mascu~
## 12 Ki-Adi-~    198    82 white      pale       yellow          92   male  mascu~
## 13 Yarael ~    264    NA none       white      yellow          NA   male  mascu~
## 14 Poggle ~    183    80 none       green      yellow          NA   male  mascu~
## 15 Zam Wes~    168    55 blonde     fair, gre~ yellow          NA   fema~ femin~
## 16 Dexter ~    198   102 none       brown      yellow          NA   male  mascu~
## # i 5 more variables: homeworld <chr>, species <chr>, films <list>,
## #   vehicles <list>, starships <list>
\end{verbatim}

Multiple conditions can be passed simultaneously. Example

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{starwars }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(skin\_color }\SpecialCharTok{==} \StringTok{\textquotesingle{}white\textquotesingle{}}\NormalTok{,}
\NormalTok{         height }\SpecialCharTok{\textgreater{}=} \DecValTok{150}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 2 x 14
##   name      height  mass hair_color skin_color eye_color birth_year sex   gender
##   <chr>      <int> <dbl> <chr>      <chr>      <chr>          <dbl> <chr> <chr> 
## 1 Darth Va~    202   136 none       white      yellow          41.9 male  mascu~
## 2 Yarael P~    264    NA none       white      yellow          NA   male  mascu~
## # i 5 more variables: homeworld <chr>, species <chr>, films <list>,
## #   vehicles <list>, starships <list>
\end{verbatim}

\textbf{Note} that these conditions act simultaneously as in operator \texttt{AND} is used. So if \texttt{OR} is to be used, use \texttt{\textbar{}} explicitly

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{starwars }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(skin\_color }\SpecialCharTok{==} \StringTok{\textquotesingle{}white\textquotesingle{}} \SpecialCharTok{|}\NormalTok{ height }\SpecialCharTok{\textgreater{}=} \DecValTok{150}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{nrow}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 71
\end{verbatim}

\hypertarget{slice_func}{%
\subsection{\texorpdfstring{\texttt{slice()} / \texttt{slice\_*()}}{slice() / slice\_*()}}\label{slice_func}}

\texttt{slice()} and its cousins also filters rows but based on rows placement. So, \texttt{data\_fr\ \%\textgreater{}\%\ slice(1:5)} will filter out first five rows of the \texttt{data\_fr}. See example

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{starwars }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{slice}\NormalTok{(}\DecValTok{4}\SpecialCharTok{:}\DecValTok{10}\NormalTok{) }\CommentTok{\# filter 4 to 10th row}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 7 x 14
##   name      height  mass hair_color skin_color eye_color birth_year sex   gender
##   <chr>      <int> <dbl> <chr>      <chr>      <chr>          <dbl> <chr> <chr> 
## 1 Darth Va~    202   136 none       white      yellow          41.9 male  mascu~
## 2 Leia Org~    150    49 brown      light      brown           19   fema~ femin~
## 3 Owen Lars    178   120 brown, gr~ light      blue            52   male  mascu~
## 4 Beru Whi~    165    75 brown      light      blue            47   fema~ femin~
## 5 R5-D4         97    32 <NA>       white, red red             NA   none  mascu~
## 6 Biggs Da~    183    84 black      light      brown           24   male  mascu~
## 7 Obi-Wan ~    182    77 auburn, w~ fair       blue-gray       57   male  mascu~
## # i 5 more variables: homeworld <chr>, species <chr>, films <list>,
## #   vehicles <list>, starships <list>
\end{verbatim}

Other \texttt{slice()} cousins -

\begin{itemize}
\tightlist
\item
  \texttt{slice\_head(5)} will slice out first 5 rows
\item
  \texttt{slice\_tail(10)} will slice out last 10 rows
\item
  \texttt{slice\_min()} or \texttt{slice\_max()} will slice rows with highest or lowest values of given variable. The full syntax is \texttt{slice\_max(.data,\ order\_by,\ ...,\ n,\ prop,\ with\_ties\ =\ TRUE)} or equivalent
\item
  \texttt{slice\_sample()} will randomly select the rows. Its syntax is \texttt{slice\_sample(.data,\ ...,\ n,\ prop,\ weight\_by\ =\ NULL,\ replace\ =\ FALSE)}
\end{itemize}

Example-1:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{starwars }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{slice\_min}\NormalTok{(height, }\AttributeTok{n=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3 x 14
##   name      height  mass hair_color skin_color eye_color birth_year sex   gender
##   <chr>      <int> <dbl> <chr>      <chr>      <chr>          <dbl> <chr> <chr> 
## 1 Yoda          66    17 white      green      brown            896 male  mascu~
## 2 Ratts Ty~     79    15 none       grey, blue unknown           NA male  mascu~
## 3 Wicket S~     88    20 brown      brown      brown              8 male  mascu~
## # i 5 more variables: homeworld <chr>, species <chr>, films <list>,
## #   vehicles <list>, starships <list>
\end{verbatim}

Example-2:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{2022}\NormalTok{)}
\NormalTok{starwars }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{slice\_sample}\NormalTok{(}\AttributeTok{prop =} \FloatTok{0.1}\NormalTok{) }\CommentTok{\#sample 10\% rows}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 8 x 14
##   name      height  mass hair_color skin_color eye_color birth_year sex   gender
##   <chr>      <int> <dbl> <chr>      <chr>      <chr>          <dbl> <chr> <chr> 
## 1 Ki-Adi-M~    198  82   white      pale       yellow            92 male  mascu~
## 2 Grievous     216 159   none       brown, wh~ green, y~         NA male  mascu~
## 3 Saesee T~    188  NA   none       pale       orange            NA male  mascu~
## 4 Wat Tamb~    193  48   none       green, gr~ unknown           NA male  mascu~
## 5 Jango Fe~    183  79   black      tan        brown             66 male  mascu~
## 6 Owen Lars    178 120   brown, gr~ light      blue              52 male  mascu~
## 7 Luminara~    170  56.2 black      yellow     blue              58 fema~ femin~
## 8 BB8           NA  NA   none       none       black             NA none  mascu~
## # i 5 more variables: homeworld <chr>, species <chr>, films <list>,
## #   vehicles <list>, starships <list>
\end{verbatim}

\hypertarget{arrange}{%
\subsection{\texorpdfstring{\texttt{arrange()}}{arrange()}}\label{arrange}}

This verb also act upon rows and it actually \emph{rearranges} them on the basis of some condition. Refer figure \ref{fig:arranger} for illustration.

\begin{figure}

{\centering \includegraphics[width=0.99\linewidth]{images/arrange_dplyr} 

}

\caption{Illustration of dplyr::arrange()}\label{fig:arranger}
\end{figure}

Example-

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{starwars }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{arrange}\NormalTok{(height) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{slice}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 5 x 14
##   name      height  mass hair_color skin_color eye_color birth_year sex   gender
##   <chr>      <int> <dbl> <chr>      <chr>      <chr>          <dbl> <chr> <chr> 
## 1 Yoda          66    17 white      green      brown            896 male  mascu~
## 2 Ratts Ty~     79    15 none       grey, blue unknown           NA male  mascu~
## 3 Wicket S~     88    20 brown      brown      brown              8 male  mascu~
## 4 Dud Bolt      94    45 none       blue, grey yellow            NA male  mascu~
## 5 R2-D2         96    32 <NA>       white, bl~ red               33 none  mascu~
## # i 5 more variables: homeworld <chr>, species <chr>, films <list>,
## #   vehicles <list>, starships <list>
\end{verbatim}

\hypertarget{group-verbs}{%
\section{Group verbs}\label{group-verbs}}

\hypertarget{group_by}{%
\subsection{\texorpdfstring{\texttt{group\_by()}}{group\_by()}}\label{group_by}}

A data analyst will be hard to find who is not using \texttt{group\_by}. It basically groups the rows on the basis of values of a given variable or block of variables. The returned result is still a data frame (and one too) but now the rows are grouped. Refer figure \ref{fig:groupby} for illustration. So any of the above functions we learnt above will give a different result after group by.

\begin{figure}

{\centering \includegraphics[width=0.99\linewidth]{images/groupby_dplyr} 

}

\caption{Illustration of Grouped Operations in dplyr}\label{fig:groupby}
\end{figure}

Note the output of this simple example

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{starwars }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(sex)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 87 x 14
## # Groups:   sex [5]
##    name     height  mass hair_color skin_color eye_color birth_year sex   gender
##    <chr>     <int> <dbl> <chr>      <chr>      <chr>          <dbl> <chr> <chr> 
##  1 Luke Sk~    172    77 blond      fair       blue            19   male  mascu~
##  2 C-3PO       167    75 <NA>       gold       yellow         112   none  mascu~
##  3 R2-D2        96    32 <NA>       white, bl~ red             33   none  mascu~
##  4 Darth V~    202   136 none       white      yellow          41.9 male  mascu~
##  5 Leia Or~    150    49 brown      light      brown           19   fema~ femin~
##  6 Owen La~    178   120 brown, gr~ light      blue            52   male  mascu~
##  7 Beru Wh~    165    75 brown      light      blue            47   fema~ femin~
##  8 R5-D4        97    32 <NA>       white, red red             NA   none  mascu~
##  9 Biggs D~    183    84 black      light      brown           24   male  mascu~
## 10 Obi-Wan~    182    77 auburn, w~ fair       blue-gray       57   male  mascu~
## # i 77 more rows
## # i 5 more variables: homeworld <chr>, species <chr>, films <list>,
## #   vehicles <list>, starships <list>
\end{verbatim}

\textbf{Note} that output now has 5 groups, though nothing different is seen in the displayed data.

This operation/verb is thus more useful if used in combination with other verbs.

Example-1: How many total characters are with same skin\_color?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{starwars }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(name, skin\_color) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(skin\_color) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{total\_with\_s\_c =} \FunctionTok{n}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 87 x 3
## # Groups:   skin_color [31]
##    name               skin_color  total_with_s_c
##    <chr>              <chr>                <int>
##  1 Luke Skywalker     fair                    17
##  2 C-3PO              gold                     1
##  3 R2-D2              white, blue              2
##  4 Darth Vader        white                    2
##  5 Leia Organa        light                   11
##  6 Owen Lars          light                   11
##  7 Beru Whitesun Lars light                   11
##  8 R5-D4              white, red               1
##  9 Biggs Darklighter  light                   11
## 10 Obi-Wan Kenobi     fair                    17
## # i 77 more rows
\end{verbatim}

Example- 2: Sample 2 rows of each \texttt{cyl} size from \texttt{mtcars}?

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{mtcars }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(cyl) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{slice\_sample}\NormalTok{(}\AttributeTok{n=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 11
## # Groups:   cyl [3]
##     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb
##   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>
## 1  22.8     4  141.    95  3.92  3.15  22.9     1     0     4     2
## 2  21.4     4  121    109  4.11  2.78  18.6     1     1     4     2
## 3  21       6  160    110  3.9   2.88  17.0     0     1     4     4
## 4  19.7     6  145    175  3.62  2.77  15.5     0     1     5     6
## 5  10.4     8  472    205  2.93  5.25  18.0     0     0     3     4
## 6  13.3     8  350    245  3.73  3.84  15.4     0     0     3     4
\end{verbatim}

Also note that \texttt{grouped} varaible(s) will always be available in the output.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mtcars }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(cyl) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(drat) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# despite not selecting cyl}
  \FunctionTok{head}\NormalTok{() }\CommentTok{\# it is available in output}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Adding missing grouping variables: `cyl`
\end{verbatim}

\begin{verbatim}
## # A tibble: 6 x 2
## # Groups:   cyl [3]
##     cyl  drat
##   <dbl> <dbl>
## 1     6  3.9 
## 2     6  3.9 
## 3     4  3.85
## 4     6  3.08
## 5     8  3.15
## 6     6  2.76
\end{verbatim}

\hypertarget{summarise}{%
\subsection{\texorpdfstring{\texttt{summarise()}}{summarise()}}\label{summarise}}

This verb creates a summary row for each group if grouped data frame is in input, otherwise one single for complete operation.\\
Example-1:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mtcars }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{total\_wt =} \FunctionTok{sum}\NormalTok{(wt))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   total_wt
## 1  102.952
\end{verbatim}

Example-2:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mtcars }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(cyl) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{total\_wt =} \FunctionTok{sum}\NormalTok{(wt))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3 x 2
##     cyl total_wt
##   <dbl>    <dbl>
## 1     4     25.1
## 2     6     21.8
## 3     8     56.0
\end{verbatim}

\hypertarget{ungroup}{%
\subsection{\texorpdfstring{\texttt{ungroup()}}{ungroup()}}\label{ungroup}}

But now, it may have been noticed that grouping the data with \texttt{group\_by} and thereafter performing some operation like \texttt{mutate} or \texttt{slice} returns the grouped data. But our requirement in next steps would be of ungrouped data. so there is specially designed function in dplyr which ungroups the grouped data.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{mtcars }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(cyl) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{slice\_sample}\NormalTok{(}\AttributeTok{n=}\DecValTok{2}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ungroup}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 11
##     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb
##   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>
## 1  22.8     4  141.    95  3.92  3.15  22.9     1     0     4     2
## 2  21.4     4  121    109  4.11  2.78  18.6     1     1     4     2
## 3  21       6  160    110  3.9   2.88  17.0     0     1     4     4
## 4  19.7     6  145    175  3.62  2.77  15.5     0     1     5     6
## 5  10.4     8  472    205  2.93  5.25  18.0     0     0     3     4
## 6  13.3     8  350    245  3.73  3.84  15.4     0     0     3     4
\end{verbatim}

\hypertarget{by-argument-in-mutate-summarise-etc.-functions}{%
\subsection{\texorpdfstring{\texttt{.by} argument in \texttt{mutate}, \texttt{summarise}, etc. functions}{.by argument in mutate, summarise, etc. functions}}\label{by-argument-in-mutate-summarise-etc.-functions}}

Since grouping the data in one step, then performing required step and then again ungrouping it is somewhat cumbersome, coming dplyr 1.1.0 version, the functions \texttt{mutate()}, \texttt{summarise}, \texttt{slice} etc. have gained an additional argument \texttt{.by} wherein the variable names for performing grouped operations may be provided directly, thus eliminating the need of both grouping and ungrouping it. So the following two syntax would produce exactly same results.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Syntax {-} 1 (old style)}

\NormalTok{mtcars }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(cyl) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{slice}\NormalTok{(}\DecValTok{2}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ungroup}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3 x 11
##     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb
##   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>
## 1  24.4     4  147.    62  3.69  3.19  20       1     0     4     2
## 2  21       6  160    110  3.9   2.88  17.0     0     1     4     4
## 3  14.3     8  360    245  3.21  3.57  15.8     0     0     3     4
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Syntax 2 (Dplyr 1.1.0 +)}

\NormalTok{mtcars }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{slice}\NormalTok{(}\DecValTok{2}\NormalTok{, }\AttributeTok{.by =}\NormalTok{ cyl)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                mpg cyl  disp  hp drat    wt  qsec vs am gear carb
## Mazda RX4 Wag 21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4
## Merc 240D     24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2
## Duster 360    14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4
\end{verbatim}

\hypertarget{reframe}{%
\subsection{\texorpdfstring{\texttt{reframe()}}{reframe()}}\label{reframe}}

\hypertarget{other-useful-functions-in-dplyr}{%
\section{\texorpdfstring{Other Useful functions in \texttt{dplyr}}{Other Useful functions in dplyr}}\label{other-useful-functions-in-dplyr}}

\hypertarget{if_else}{%
\subsection*{\texorpdfstring{\texttt{if\_else()}}{if\_else()}}\label{if_else}}
\addcontentsline{toc}{subsection}{\texttt{if\_else()}}

This function operates nearly as similar to base R's \texttt{ifelse()} with two exceptions-

\begin{itemize}
\tightlist
\item
  There is an extra argument to provide values when missing values are encountered. (See example-1)
\item
  \texttt{NA} will have to be provided specifically. (See Example-2)
\end{itemize}

See these examples. Example-1:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{2}\SpecialCharTok{:}\DecValTok{2}\NormalTok{, }\ConstantTok{NA}\NormalTok{)}
\FunctionTok{if\_else}\NormalTok{(x}\SpecialCharTok{\textgreater{}=}\DecValTok{0}\NormalTok{, }\StringTok{"positive"}\NormalTok{, }\StringTok{"negative"}\NormalTok{, }\StringTok{"missing"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "negative" "negative" "positive" "positive" "positive" "missing"
\end{verbatim}

Example-2:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{2}\SpecialCharTok{:}\DecValTok{2}\NormalTok{, }\ConstantTok{NA}\NormalTok{)}
\FunctionTok{if\_else}\NormalTok{(x}\SpecialCharTok{\textgreater{}=}\DecValTok{0}\NormalTok{, }\StringTok{"positive"}\NormalTok{, }\StringTok{"negative"}\NormalTok{, }\ConstantTok{NA\_character\_}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "negative" "negative" "positive" "positive" "positive" NA
\end{verbatim}

Due to the additional restrictions, this function is sometimes faster than its base R alternative and may also be useful in prevention of bugs in code as the output will be known beforehand.

\hypertarget{case_when}{%
\subsection*{\texorpdfstring{\texttt{case\_when()}}{case\_when()}}\label{case_when}}
\addcontentsline{toc}{subsection}{\texttt{case\_when()}}

Though both \texttt{ifelse} and \texttt{if\_else} variants provide for nesting multiple conditions, yet \texttt{case\_when} provides a simpler alternative in these conditions as here multiple conditions can be provided simultaneously. Syntax follows this style-

\begin{verbatim}
case_when(
  condition1 ~ value_if_true,
  condition2 ~ value_if_true,
  ...,
  TRUE ~ value_if_all_above_are_false
)
\end{verbatim}

See this example.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\CommentTok{\# Generate Incomes from random (uniform) distribution}
\NormalTok{income }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(}\DecValTok{7}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{9}\NormalTok{)}\SpecialCharTok{*}\DecValTok{100000}
\NormalTok{income}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 330062.0 730644.1 427181.5 806413.9 852373.8 136445.2 522484.4
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# tax brackets say 0\% upto 2 lakh, then 10\% upto 5 Lakh}
\CommentTok{\# then 20\% upto 7.5 lakh otherwise 30\%}
\NormalTok{tax\_slab }\OtherTok{\textless{}{-}} \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{  income }\SpecialCharTok{\textless{}=} \DecValTok{200000} \SpecialCharTok{\textasciitilde{}} \DecValTok{0}\NormalTok{,}
\NormalTok{  income }\SpecialCharTok{\textless{}=} \DecValTok{500000} \SpecialCharTok{\textasciitilde{}} \DecValTok{10}\NormalTok{,}
\NormalTok{  income }\SpecialCharTok{\textless{}=} \DecValTok{750000} \SpecialCharTok{\textasciitilde{}} \DecValTok{20}\NormalTok{,}
  \ConstantTok{TRUE} \SpecialCharTok{\textasciitilde{}} \DecValTok{30}
\NormalTok{)}

\CommentTok{\# check tax\_slab}
\FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{income=}\NormalTok{income,}
  \AttributeTok{tax\_slab =}\NormalTok{ tax\_slab}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     income tax_slab
## 1 330062.0       10
## 2 730644.1       20
## 3 427181.5       10
## 4 806413.9       30
## 5 852373.8       30
## 6 136445.2        0
## 7 522484.4       20
\end{verbatim}

\hypertarget{case_match}{%
\subsection*{\texorpdfstring{\texttt{case\_match()}}{case\_match()}}\label{case_match}}
\addcontentsline{toc}{subsection}{\texttt{case\_match()}}

This function is somewhat similar to \texttt{case\_when}. While \texttt{case\_when()} uses logical expressions on the left-hand side of the formula, \texttt{case\_match()} uses values to match with. Syntax is like

\begin{verbatim}
case_match(
  .x,  # A vector whose values are to be matched
  ..., # Basically for a condition like .x %in% Y, only Y has to be specified
  .default = NULL, # Default values to be specified for if no match is found
)
\end{verbatim}

So, above example could be rewritten as

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tax\_slab }\OtherTok{\textless{}{-}} \FunctionTok{case\_match}\NormalTok{(}
  \FunctionTok{floor}\NormalTok{(income), }\CommentTok{\# Vector to be checked}
  \DecValTok{0}\SpecialCharTok{:}\DecValTok{200000} \SpecialCharTok{\textasciitilde{}} \DecValTok{0}\NormalTok{, }\CommentTok{\# slab 1}
  \DecValTok{200001}\SpecialCharTok{:}\DecValTok{500000} \SpecialCharTok{\textasciitilde{}} \DecValTok{10}\NormalTok{, }\CommentTok{\# slab 2}
  \DecValTok{500001}\SpecialCharTok{:}\DecValTok{750000} \SpecialCharTok{\textasciitilde{}} \DecValTok{20}\NormalTok{, }\CommentTok{\# slab 3}
  \AttributeTok{.default =} \DecValTok{30} \CommentTok{\# Slab 4}
\NormalTok{)}

\CommentTok{\# check tax\_slab}
\FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{income=}\NormalTok{income,}
  \AttributeTok{tax\_slab =}\NormalTok{ tax\_slab}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     income tax_slab
## 1 330062.0       10
## 2 730644.1       20
## 3 427181.5       10
## 4 806413.9       30
## 5 852373.8       30
## 6 136445.2        0
## 7 522484.4       20
\end{verbatim}

\hypertarget{window-functionsoperations}{%
\section{Window functions/operations}\label{window-functionsoperations}}

We learnt that by using \texttt{group\_by} function we can create windows in data and we can make our calculations in each separate window specifically.

Dplyr provides us with some useful window functions which will operate on these windows.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{row\_number()} can be used to generate row number
\item
  \texttt{dense\_rank} / \texttt{min\_rank} / \texttt{percent\_rank()} / \texttt{ntile()} / \texttt{cume\_dist()} are other windowed functions in dplyr. Check \texttt{?dplyr::ranking} for complete reference.
\item
  \texttt{lead()} and \texttt{lag()} will give leading/lagging value in that window.
\item
  \texttt{consecutive\_id()} generates a unique identifier that increments every time a variable (or combination of variables) changes.
\end{enumerate}

These functions can be very helpful while analysing time series data.

Example-1: Generating row numbers and ranks

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# example data}
\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{val =} \FunctionTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\ConstantTok{NA}\NormalTok{)}
\NormalTok{)}

\NormalTok{df }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{row =} \FunctionTok{row\_number}\NormalTok{(),}
    \AttributeTok{min\_rank =} \FunctionTok{min\_rank}\NormalTok{(val),}
    \AttributeTok{dense\_rank =} \FunctionTok{dense\_rank}\NormalTok{(val),}
    \AttributeTok{perc\_rank =} \FunctionTok{percent\_rank}\NormalTok{(val),}
    \AttributeTok{cume\_dist =} \FunctionTok{cume\_dist}\NormalTok{(val)}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   val row min_rank dense_rank perc_rank cume_dist
## 1  10   1        4          3 1.0000000      1.00
## 2   2   2        1          1 0.0000000      0.50
## 3   3   3        3          2 0.6666667      0.75
## 4   2   4        1          1 0.0000000      0.50
## 5  NA   5       NA         NA        NA        NA
\end{verbatim}

Example-2: Using previous and next values

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Orange }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(Tree) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{prev\_circ =} \FunctionTok{lag}\NormalTok{(circumference),}
         \AttributeTok{next\_circ =} \FunctionTok{lead}\NormalTok{(circumference))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 35 x 5
## # Groups:   Tree [5]
##    Tree    age circumference prev_circ next_circ
##    <ord> <dbl>         <dbl>     <dbl>     <dbl>
##  1 1       118            30        NA        58
##  2 1       484            58        30        87
##  3 1       664            87        58       115
##  4 1      1004           115        87       120
##  5 1      1231           120       115       142
##  6 1      1372           142       120       145
##  7 1      1582           145       142        NA
##  8 2       118            33        NA        69
##  9 2       484            69        33       111
## 10 2       664           111        69       156
## # i 25 more rows
\end{verbatim}

Example-3: generating run length encoding/consecutive ID

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{grp =} \FunctionTok{c}\NormalTok{(}\StringTok{"A"}\NormalTok{, }\StringTok{"B"}\NormalTok{, }\StringTok{"B"}\NormalTok{, }\StringTok{"A"}\NormalTok{, }\StringTok{"A"}\NormalTok{, }\StringTok{"B"}\NormalTok{),}
  \AttributeTok{value =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{ , }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{)}
\NormalTok{)}

\NormalTok{df }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{RLE =} \FunctionTok{consecutive\_id}\NormalTok{(grp))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   grp value RLE
## 1   A     1   1
## 2   B     2   2
## 3   B     3   2
## 4   A     4   3
## 5   A     5   3
## 6   B     6   4
\end{verbatim}

\hypertarget{combining-tablestabular-data}{%
\chapter{Combining Tables/tabular data}\label{combining-tablestabular-data}}

\begin{figure}

{\centering \includegraphics[width=0.75\linewidth,height=0.75\textheight]{images/conf} 

}

\caption{Most of times, joining two or more tables will be required to perform analytics}\label{fig:unnamed-chunk-287}
\end{figure}

In real world scenarios, there may hardly be a case when we have to analyse one single table. There may be cases when we have to either join tables split into multiple smaller tables (e.g.~we can have smaller tables split States-wise), or the tables may be divided into various smaller master and transaction tables (relational databases).

We may thus divide the data tables joining requirements into three broad categories-

\begin{itemize}
\tightlist
\item
  Simple joins or concatenation
\item
  Relational Joins
\item
  Filtering Joins
\end{itemize}

Let us discuss each of these with examples.

\hypertarget{simple-joinsconcatenation}{%
\section{Simple joins/concatenation}\label{simple-joinsconcatenation}}

Many times tables split into smaller tables have to be joined back before proceeding further for data analytics. We may have to join two or more tables either columnwise (e.g.~some of the features for all rows have been split into a separate table) or row wise (e.g.~all the fields/columns are split into smaller tables like a separate table for each State). Diagramatically these joins may be depicted as shown in figure \ref{fig:joins}.

\begin{figure}

{\centering \includegraphics[width=0.99\linewidth]{images/concat_joins} 

}

\caption{Illustration of Simple joins/concatenation}\label{fig:joins}
\end{figure}

\hypertarget{column-binding}{%
\subsection{Column binding}\label{column-binding}}

As we have already seen that data frames act like matrices in many ways except to that fact that these support heterogeneous data unlike matrices. We have also discussed the ways two matrices can be joined. Base R has two dedicated functions i.e.~\texttt{cbind()} and \texttt{rbind()} for these operations.

In \emph{tidyverse} (dplyr specifically) we have two similar functions \texttt{bind\_cols()} and \texttt{bind\_rows} respectively which provide us better functionality for these use cases. The syntax for finction \texttt{bind\_cols()} used for concatenating two or more tables \emph{column wise} is -

\begin{verbatim}
bind_cols(
  ...,
  .name_repair = c("unique", "universal", "check_unique")
)
\end{verbatim}

Where -

\begin{itemize}
\tightlist
\item
  \texttt{...} represent data frames to be combined
\item
  \texttt{.name\_repair} argument chooses method to rename duplicate column names, if any.
\end{itemize}

Example-1:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df1 }\OtherTok{\textless{}{-}}\NormalTok{ iris[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{, }\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{5}\NormalTok{)]}
\NormalTok{df2 }\OtherTok{\textless{}{-}}\NormalTok{ iris[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{, }\DecValTok{3}\SpecialCharTok{:}\DecValTok{5}\NormalTok{]}

\FunctionTok{bind\_cols}\NormalTok{(df1, df2, }\AttributeTok{.name\_repair =} \StringTok{\textquotesingle{}universal\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## New names:
## * `Species` -> `Species...3`
## * `Species` -> `Species...6`
\end{verbatim}

\begin{verbatim}
##   Sepal.Length Sepal.Width Species...3 Petal.Length Petal.Width Species...6
## 1          5.1         3.5      setosa          1.4         0.2      setosa
## 2          4.9         3.0      setosa          1.4         0.2      setosa
## 3          4.7         3.2      setosa          1.3         0.2      setosa
\end{verbatim}

\begin{quote}
Note: The data frames to be merged should be row-consistent for column binding. Try this \texttt{bind\_cols(iris{[}1:3,\ 1:2{]},\ iris{[}1:4,\ 3:4{]})} and see the results.
\end{quote}

\hypertarget{row-binding}{%
\subsection{Row binding}\label{row-binding}}

The syntax used for appending rows of one or more tables together in one table is -

\begin{verbatim}
bind_rows(..., .id = NULL)
\end{verbatim}

where -

\begin{itemize}
\tightlist
\item
  \texttt{...} represent data frames to be combined
\item
  \texttt{.id} argument creates a new column of identifiers.
\end{itemize}

To understand it better, let us see this example

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{setosa }\OtherTok{\textless{}{-}}\NormalTok{ iris[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{]}
\NormalTok{versicolor }\OtherTok{\textless{}{-}}\NormalTok{ iris[}\DecValTok{51}\SpecialCharTok{:}\DecValTok{53}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{]}
\NormalTok{virginica }\OtherTok{\textless{}{-}}\NormalTok{ iris[}\DecValTok{101}\SpecialCharTok{:}\DecValTok{103}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{]}

\FunctionTok{bind\_rows}\NormalTok{(setosa, versicolor, virginica, }\AttributeTok{.id =} \StringTok{\textquotesingle{}groups\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   groups Sepal.Length Sepal.Width Petal.Length Petal.Width
## 1      1          5.1         3.5          1.4         0.2
## 2      1          4.9         3.0          1.4         0.2
## 3      1          4.7         3.2          1.3         0.2
## 4      2          7.0         3.2          4.7         1.4
## 5      2          6.4         3.2          4.5         1.5
## 6      2          6.9         3.1          4.9         1.5
## 7      3          6.3         3.3          6.0         2.5
## 8      3          5.8         2.7          5.1         1.9
## 9      3          7.1         3.0          5.9         2.1
\end{verbatim}

\begin{quote}
Note: In the above example if the requirement is to store data tables names into the new \emph{identifier} column just list convert the databases into a list and \texttt{.id} will take element\_names as the values in identifiers. Try this \texttt{bind\_rows(list(setosa=setosa,\ versicolor=versicolor,\ virginica=virginica),\ .id\ =\ \textquotesingle{}Species\textquotesingle{})}
\end{quote}

\hypertarget{relational-joins}{%
\section{Relational joins}\label{relational-joins}}

Relational Joins are usually needed to join multiple tables based on \emph{primary key} and \emph{secondary keys}. The joins may either be \emph{one-to-one} key join or \emph{one-to-many} key joins. Broadly these can either be \emph{inner joins} or \emph{outer joins}. Diagrammatically these may be represented as shown in figure \ref{fig:joinm}.

\begin{figure}

{\centering \includegraphics[width=0.99\linewidth]{images/mutating_joins} 

}

\caption{Illustration of Mutating Joins in dplyr}\label{fig:joinm}
\end{figure}

The syntax of all these joins is nearly same-

\begin{verbatim}
*_join(x, y, by = NULL, copy = FALSE, suffix = c('.x', '.y'), ... , keep = FALSE)
\end{verbatim}

where -

\begin{itemize}
\tightlist
\item
  \texttt{x} and \texttt{y} are data frames to be joined
\item
  \texttt{by} is a character vector of column names to be joined by
\item
  \texttt{suffix} argument provides suffixes to be added to column names, if any of those are duplicate
\item
  \texttt{keep} argument decides whether the join keys from both x and y be preserved in the output?
\end{itemize}

Let's discuss each of these joins individually.

\hypertarget{inner-joins}{%
\subsection{Inner Joins}\label{inner-joins}}

Inner Joins keeps only those rows where matching keys are present in both the data frames.

Example-1:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{band\_members}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3 x 2
##   name  band   
##   <chr> <chr>  
## 1 Mick  Stones 
## 2 John  Beatles
## 3 Paul  Beatles
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{band\_instruments}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3 x 2
##   name  plays 
##   <chr> <chr> 
## 1 John  guitar
## 2 Paul  bass  
## 3 Keith guitar
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{inner\_join}\NormalTok{(band\_members, band\_instruments)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Joining with `by = join_by(name)`
\end{verbatim}

\begin{verbatim}
## # A tibble: 2 x 3
##   name  band    plays 
##   <chr> <chr>   <chr> 
## 1 John  Beatles guitar
## 2 Paul  Beatles bass
\end{verbatim}

\hypertarget{left-joins}{%
\subsection{Left Joins}\label{left-joins}}

Left Joins on the other hand preserves all rows of data frame passed as \texttt{x} i.e.~first argument irrespective of the fact that matching key record is available in second data table or not.

Example-1:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{left\_join}\NormalTok{(band\_members, band\_instruments)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Joining with `by = join_by(name)`
\end{verbatim}

\begin{verbatim}
## # A tibble: 3 x 3
##   name  band    plays 
##   <chr> <chr>   <chr> 
## 1 Mick  Stones  <NA>  
## 2 John  Beatles guitar
## 3 Paul  Beatles bass
\end{verbatim}

\hypertarget{right-joins}{%
\subsection{Right Joins}\label{right-joins}}

Right join, is similar to left join and preserves all rows of data frame passed as \texttt{y} i.e.~second argument irrespective of the fact that matching key record is available in first data table or not.

Example-1:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{right\_join}\NormalTok{(band\_members, band\_instruments)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Joining with `by = join_by(name)`
\end{verbatim}

\begin{verbatim}
## # A tibble: 3 x 3
##   name  band    plays 
##   <chr> <chr>   <chr> 
## 1 John  Beatles guitar
## 2 Paul  Beatles bass  
## 3 Keith <NA>    guitar
\end{verbatim}

\hypertarget{full-joins}{%
\subsection{Full Joins}\label{full-joins}}

Full join returns all the rows of both the data tables despite non-availability of matching key in either of the tables.

Example-1:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{full\_join}\NormalTok{(band\_members, band\_instruments)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Joining with `by = join_by(name)`
\end{verbatim}

\begin{verbatim}
## # A tibble: 4 x 3
##   name  band    plays 
##   <chr> <chr>   <chr> 
## 1 Mick  Stones  <NA>  
## 2 John  Beatles guitar
## 3 Paul  Beatles bass  
## 4 Keith <NA>    guitar
\end{verbatim}

You must have noticed that each of the examples shown above has thrown a warning that join has been performed on variable \texttt{name}. We may override this warning by specifically providing the joining \emph{key} column name(s) in \texttt{by} argument i.e.~\texttt{by\ =\ "name"}.

There may be cases when the joining \emph{key} column(s) in the two data frames are of different names. These cases can also be handled by using \texttt{by} argument.

Example-

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{band\_instruments2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3 x 2
##   artist plays 
##   <chr>  <chr> 
## 1 John   guitar
## 2 Paul   bass  
## 3 Keith  guitar
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{left\_join}\NormalTok{(band\_members, band\_instruments2, }\AttributeTok{by =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}name\textquotesingle{}} \OtherTok{=} \StringTok{\textquotesingle{}artist\textquotesingle{}}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3 x 3
##   name  band    plays 
##   <chr> <chr>   <chr> 
## 1 Mick  Stones  <NA>  
## 2 John  Beatles guitar
## 3 Paul  Beatles bass
\end{verbatim}

\begin{quote}
Note: Each of the \texttt{*\_join()} can be joined on multiple keys/columns (i.e.~more than one) using \texttt{by} argument as explained above.
\end{quote}

\hypertarget{many-to-many-joins}{%
\subsection*{Many-to-many Joins}\label{many-to-many-joins}}
\addcontentsline{toc}{subsection}{Many-to-many Joins}

Example of four of the above-mentioned joins are many to many joins and thus need to be used carefully. See the following example

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df1 }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\ConstantTok{NA}\NormalTok{),}
  \AttributeTok{y =} \FunctionTok{c}\NormalTok{(}\DecValTok{11}\NormalTok{, }\DecValTok{12}\NormalTok{, }\DecValTok{21}\NormalTok{)}
\NormalTok{)}
\NormalTok{df1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    x  y
## 1  1 11
## 2  1 12
## 3 NA 21
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df2 }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\ConstantTok{NA}\NormalTok{, }\ConstantTok{NA}\NormalTok{),}
  \AttributeTok{z =} \FunctionTok{c}\NormalTok{(}\DecValTok{101}\NormalTok{, }\DecValTok{102}\NormalTok{, }\DecValTok{201}\NormalTok{, }\DecValTok{202}\NormalTok{)}
\NormalTok{)}
\NormalTok{df2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    x   z
## 1  1 101
## 2  1 102
## 3 NA 201
## 4 NA 202
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df1 }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{left\_join}\NormalTok{(df2, }\AttributeTok{by =} \StringTok{\textquotesingle{}x\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in left_join(., df2, by = "x"): Detected an unexpected many-to-many relationship between `x` and `y`.
## i Row 1 of `x` matches multiple rows in `y`.
## i Row 1 of `y` matches multiple rows in `x`.
## i If a many-to-many relationship is expected, set `relationship =
##   "many-to-many"` to silence this warning.
\end{verbatim}

\begin{verbatim}
##    x  y   z
## 1  1 11 101
## 2  1 11 102
## 3  1 12 101
## 4  1 12 102
## 5 NA 21 201
## 6 NA 21 202
\end{verbatim}

In fact, we can use argument \texttt{relationship} explicitly to silence this warning and avoid errors. This argument can take one of these values

\begin{itemize}
\tightlist
\item
  \texttt{NULL} (default)
\item
  \texttt{"one-to-one"}
\item
  \texttt{"one-to-many"}
\item
  \texttt{"many-to-one"}
\item
  \texttt{"many-to-many"}
\end{itemize}

Example above re-coded

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df1 }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{left\_join}\NormalTok{(df2, }\AttributeTok{by =} \StringTok{\textquotesingle{}x\textquotesingle{}}\NormalTok{, }\AttributeTok{relationship =} \StringTok{"many{-}to{-}many"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    x  y   z
## 1  1 11 101
## 2  1 11 102
## 3  1 12 101
## 4  1 12 102
## 5 NA 21 201
## 6 NA 21 202
\end{verbatim}

\hypertarget{filtering-joins}{%
\section{Filtering Joins}\label{filtering-joins}}

The other joins available in \texttt{dplyr} are basically filtering joins.

\hypertarget{semi-joins}{%
\subsection{Semi Joins}\label{semi-joins}}

First of these is \texttt{semi\_join} which essentially filters those rows from a data frame, which are based on another data frame. See this example-

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df1 }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\ConstantTok{NA}\NormalTok{),}
  \AttributeTok{y =} \FunctionTok{c}\NormalTok{(}\DecValTok{11}\NormalTok{, }\DecValTok{21}\NormalTok{, }\DecValTok{100}\NormalTok{)}
\NormalTok{)}
\NormalTok{df1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    x   y
## 1  1  11
## 2  2  21
## 3 NA 100
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df2 }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\ConstantTok{NA}\NormalTok{),}
  \AttributeTok{z =} \FunctionTok{c}\NormalTok{(}\DecValTok{101}\NormalTok{, }\DecValTok{301}\NormalTok{, }\DecValTok{401}\NormalTok{, }\DecValTok{501}\NormalTok{)}
\NormalTok{)}
\NormalTok{df2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    x   z
## 1  1 101
## 2  3 301
## 3  4 401
## 4 NA 501
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df1 }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{semi\_join}\NormalTok{(df2, }\AttributeTok{by =} \StringTok{\textquotesingle{}x\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    x   y
## 1  1  11
## 2 NA 100
\end{verbatim}

\hypertarget{anti-joins}{%
\subsection{Anti Joins}\label{anti-joins}}

The \texttt{anti\_join} is basically opposite to that of \texttt{semi\_join}. It keeps only those records from left data-frame which are not available in right data-frame. Example

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df1 }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{anti\_join}\NormalTok{(df2, }\AttributeTok{by =} \StringTok{\textquotesingle{}x\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   x  y
## 1 2 21
\end{verbatim}

\hypertarget{relational-but-non-equi-joins}{%
\section{Relational, but non-equi joins}\label{relational-but-non-equi-joins}}

Till now, we have learnt joins that use syntax of dplyr version before 1.1.0. However, in dplyr newest version, 1.1.0, released in January 2023, a new helper function \texttt{join\_by()} has been introduced. It actually, constructs a specification that describes how to join two tables using a small domain specific language. The result can be supplied as the by argument to any of the join functions that we have learnt above. For equal joins, we have learnt above, there are two slight but easier to implement changes if we use \texttt{join\_by()} -

\begin{itemize}
\tightlist
\item
  we may pass variable names in by argument instead of using characters as described above.
\item
  instead of using \texttt{=} we have to use \texttt{==} which is actually the equality operator as opposed to assignment operator we were using earlier.
\end{itemize}

So our syntax for \texttt{left\_join} example is

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{band\_instruments2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3 x 2
##   artist plays 
##   <chr>  <chr> 
## 1 John   guitar
## 2 Paul   bass  
## 3 Keith  guitar
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{left\_join}\NormalTok{(band\_members, band\_instruments2, }\AttributeTok{by =} \FunctionTok{join\_by}\NormalTok{(name }\SpecialCharTok{==}\NormalTok{ artist))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3 x 3
##   name  band    plays 
##   <chr> <chr>   <chr> 
## 1 Mick  Stones  <NA>  
## 2 John  Beatles guitar
## 3 Paul  Beatles bass
\end{verbatim}

Simple, isn't it. But now using the potential of this new helper function, we can on the other hand join two (or more data frames) using inequality conditions.

\hypertarget{inequality-joins}{%
\subsection{Inequality Joins}\label{inequality-joins}}

Inequality joins match on an inequality, such as \texttt{\textgreater{}}, \texttt{\textgreater{}=}, \texttt{\textless{}}, or \texttt{\textless{}=}, and are common in time series analysis and genomics. To construct an inequality join using \texttt{join\_by()}, we have to supply two column names separated by one of the above mentioned inequalities. See the following example for better understanding of inequality joins.

Example- Suppose we have two tables, one containing product-wise \texttt{sales} and another containing \texttt{rates} as and when these are revised. So, in order to have total sales amount, we can use inequality join.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# sales}
\NormalTok{sales }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{product.id =} \FunctionTok{c}\NormalTok{(}\StringTok{"A"}\NormalTok{, }\StringTok{"A"}\NormalTok{, }\StringTok{"A"}\NormalTok{, }\StringTok{"A"}\NormalTok{, }\StringTok{"A"}\NormalTok{, }\StringTok{"B"}\NormalTok{, }\StringTok{"B"}\NormalTok{),}
  \AttributeTok{sales.date =} \FunctionTok{as.Date}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"01{-}01{-}2022"}\NormalTok{,}
                 \StringTok{"02{-}02{-}2022"}\NormalTok{,}\StringTok{"05{-}11{-}2022"}\NormalTok{,}\StringTok{"05{-}04{-}2022"}\NormalTok{,}
                 \StringTok{"05{-}10{-}2022"}\NormalTok{,}\StringTok{"01{-}02{-}2022"}\NormalTok{,}\StringTok{"01{-}01{-}2023"}\NormalTok{), }\AttributeTok{format =} \StringTok{"\%d{-}\%m{-}\%Y"}\NormalTok{),}
  \AttributeTok{quantity =} \FunctionTok{c}\NormalTok{(5L, 10L, 15L, 5L, 15L, 1L, 2L)}
\NormalTok{)}
\CommentTok{\# Print it}
\NormalTok{sales}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   product.id sales.date quantity
## 1          A 2022-01-01        5
## 2          A 2022-02-02       10
## 3          A 2022-11-05       15
## 4          A 2022-04-05        5
## 5          A 2022-10-05       15
## 6          B 2022-02-01        1
## 7          B 2023-01-01        2
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# rates}
\NormalTok{rates }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{product.id =} \FunctionTok{c}\NormalTok{(}\StringTok{"A"}\NormalTok{, }\StringTok{"A"}\NormalTok{, }\StringTok{"A"}\NormalTok{, }\StringTok{"B"}\NormalTok{, }\StringTok{"B"}\NormalTok{),}
  \AttributeTok{revision.date =} \FunctionTok{as.Date}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"01{-}02{-}2022"}\NormalTok{,}
                    \StringTok{"03{-}04{-}2022"}\NormalTok{,}\StringTok{"05{-}10{-}2022"}\NormalTok{,}\StringTok{"01{-}01{-}2022"}\NormalTok{,}
                    \StringTok{"05{-}10{-}2022"}\NormalTok{), }\AttributeTok{format =} \StringTok{"\%d{-}\%m{-}\%Y"}\NormalTok{),}
  \AttributeTok{rate =} \FunctionTok{c}\NormalTok{(50L, 60L, 70L, 500L, 1500L)}
\NormalTok{)}
\CommentTok{\#print it}
\NormalTok{rates}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   product.id revision.date rate
## 1          A    2022-02-01   50
## 2          A    2022-04-03   60
## 3          A    2022-10-05   70
## 4          B    2022-01-01  500
## 5          B    2022-10-05 1500
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# total sale(1st attempt)}
\NormalTok{sales }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{left\_join}\NormalTok{(rates,}
            \AttributeTok{by =} \FunctionTok{join\_by}\NormalTok{(product.id,}
\NormalTok{                         sales.date }\SpecialCharTok{\textgreater{}=}\NormalTok{ revision.date))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    product.id sales.date quantity revision.date rate
## 1           A 2022-01-01        5          <NA>   NA
## 2           A 2022-02-02       10    2022-02-01   50
## 3           A 2022-11-05       15    2022-02-01   50
## 4           A 2022-11-05       15    2022-04-03   60
## 5           A 2022-11-05       15    2022-10-05   70
## 6           A 2022-04-05        5    2022-02-01   50
## 7           A 2022-04-05        5    2022-04-03   60
## 8           A 2022-10-05       15    2022-02-01   50
## 9           A 2022-10-05       15    2022-04-03   60
## 10          A 2022-10-05       15    2022-10-05   70
## 11          B 2022-02-01        1    2022-01-01  500
## 12          B 2023-01-01        2    2022-01-01  500
## 13          B 2023-01-01        2    2022-10-05 1500
\end{verbatim}

We got the results, but the results/rates are obtained fixed on all earlier dates. To solve this issue, we have rolling joins. See next section.

\hypertarget{rolling-joins}{%
\subsection{Rolling Joins}\label{rolling-joins}}

Rolling joins are a variant of inequality joins that limit the results returned from an inequality join condition. They are useful for ``rolling'' the closest match forward/backwards when there isn't an exact match. To construct a rolling join, we have to wrap inequality condition with \texttt{closest()}.

Above example solved by wrapping date condition in \texttt{closest()}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sales }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{left\_join}\NormalTok{(rates,}
            \AttributeTok{by =} \FunctionTok{join\_by}\NormalTok{(product.id,}
                         \FunctionTok{closest}\NormalTok{(sales.date }\SpecialCharTok{\textgreater{}=}\NormalTok{ revision.date)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   product.id sales.date quantity revision.date rate
## 1          A 2022-01-01        5          <NA>   NA
## 2          A 2022-02-02       10    2022-02-01   50
## 3          A 2022-11-05       15    2022-10-05   70
## 4          A 2022-04-05        5    2022-04-03   60
## 5          A 2022-10-05       15    2022-10-05   70
## 6          B 2022-02-01        1    2022-01-01  500
## 7          B 2023-01-01        2    2022-10-05 1500
\end{verbatim}

We may see that we do not have rates for product \texttt{A} for sales made on \texttt{01-01-2022}.

\hypertarget{overlap-joins}{%
\subsection{Overlap Joins}\label{overlap-joins}}

Overlap joins are a special case of inequality joins involving one or two columns from the left-hand table overlapping a range defined by two columns from the right-hand table. There are three helpers that \texttt{join\_by()} recognizes to assist with constructing overlap joins, all of which can be constructed from simpler inequalities.

\begin{itemize}
\item
  \texttt{between(x,\ y\_lower,\ y\_upper,\ ...,\ bounds\ =\ "{[}{]}")} which is just short for \texttt{x\ \textgreater{}=\ y\_lower,\ x\ \textless{}=\ y\_upper}
\item
  \texttt{within(x\_lower,\ x\_upper,\ y\_lower,\ y\_upper)} which is just short for \texttt{x\_lower\ \textgreater{}=\ y\_lower,\ x\_upper\ \textless{}=\ y\_upper}
\item
  \texttt{overlaps(x\_lower,\ x\_upper,\ y\_lower,\ y\_upper,\ ...,\ bounds\ =\ "{[}{]}")} which is short for \texttt{x\_lower\ \textless{}=\ y\_upper,\ x\_upper\ \textgreater{}=\ y\_lower}
\end{itemize}

We can make use of these functions as per our need.

Example-2: Suppose we have \texttt{orders} data where we have customer-wise orders, corresponsing order dates and ship dates. Let us suppose we want to know about customers who placed another order, without waiting for shipping of any of their previous/earlier order.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{orders }\OtherTok{\textless{}{-}} \FunctionTok{structure}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\AttributeTok{customer\_id =} \FunctionTok{c}\NormalTok{(}\StringTok{"A"}\NormalTok{, }\StringTok{"A"}\NormalTok{, }\StringTok{"A"}\NormalTok{, }\StringTok{"B"}\NormalTok{, }\StringTok{"B"}\NormalTok{, }\StringTok{"C"}\NormalTok{, }
\StringTok{"C"}\NormalTok{), }\AttributeTok{order\_id =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{7}\NormalTok{, }\AttributeTok{order\_date =} \FunctionTok{c}\NormalTok{(}\StringTok{"2019{-}01{-}01"}\NormalTok{, }\StringTok{"2019{-}01{-}05"}\NormalTok{, }
\StringTok{"2019{-}01{-}16"}\NormalTok{, }\StringTok{"2019{-}01{-}05"}\NormalTok{, }\StringTok{"2019{-}01{-}06"}\NormalTok{, }\StringTok{"2019{-}01{-}07"}\NormalTok{, }\StringTok{"2019{-}01{-}09"}
\NormalTok{), }\AttributeTok{ship\_date =} \FunctionTok{c}\NormalTok{(}\StringTok{"2019{-}01{-}30"}\NormalTok{, }\StringTok{"2019{-}02{-}10"}\NormalTok{, }\StringTok{"2019{-}01{-}18"}\NormalTok{, }\StringTok{"2019{-}01{-}08"}\NormalTok{, }
\StringTok{"2019{-}01{-}08"}\NormalTok{, }\StringTok{"2019{-}01{-}08"}\NormalTok{, }\StringTok{"2019{-}01{-}10"}\NormalTok{)), }\AttributeTok{row.names =} \FunctionTok{c}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }
\SpecialCharTok{{-}}\NormalTok{7L), }\AttributeTok{class =} \StringTok{"data.frame"}\NormalTok{)}

\CommentTok{\# print }
\NormalTok{orders}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   customer_id order_id order_date  ship_date
## 1           A        1 2019-01-01 2019-01-30
## 2           A        2 2019-01-05 2019-02-10
## 3           A        3 2019-01-16 2019-01-18
## 4           B        4 2019-01-05 2019-01-08
## 5           B        5 2019-01-06 2019-01-08
## 6           C        6 2019-01-07 2019-01-08
## 7           C        7 2019-01-09 2019-01-10
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# customers who placed the orders without}
\CommentTok{\# waiting to ship their earlier order}
\NormalTok{orders }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{inner\_join}\NormalTok{(orders,}
             \AttributeTok{by =} \FunctionTok{join\_by}\NormalTok{(customer\_id,}
                          \FunctionTok{overlaps}\NormalTok{(order\_date, ship\_date, order\_date, ship\_date),}
\NormalTok{                          order\_id }\SpecialCharTok{\textless{}}\NormalTok{ order\_id)) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   customer_id order_id.x order_date.x ship_date.x order_id.y order_date.y
## 1           A          1   2019-01-01  2019-01-30          2   2019-01-05
## 2           A          1   2019-01-01  2019-01-30          3   2019-01-16
## 3           A          2   2019-01-05  2019-02-10          3   2019-01-16
## 4           B          4   2019-01-05  2019-01-08          5   2019-01-06
##   ship_date.y
## 1  2019-02-10
## 2  2019-01-18
## 3  2019-01-18
## 4  2019-01-08
\end{verbatim}

\hypertarget{cross-joins}{%
\subsection{Cross Joins}\label{cross-joins}}

This join matches everything with everything and therefore can be thought of as a cross product of two data frames. We can use \texttt{cross\_join} for this special join. If we have \texttt{m} and \texttt{n} rows in two data frames, then as a result of cross join we will have \texttt{m*n} rows.

Example-

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{products =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}A\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}B\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}C\textquotesingle{}}\NormalTok{))}

\NormalTok{df }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{cross\_join}\NormalTok{(df)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   products.x products.y
## 1          A          A
## 2          A          B
## 3          A          C
## 4          B          A
## 5          B          B
## 6          B          C
## 7          C          A
## 8          C          B
## 9          C          C
\end{verbatim}

\hypertarget{data-wrangling-in-tidyr}{%
\chapter{\texorpdfstring{Data Wrangling in \texttt{tidyr}}{Data Wrangling in tidyr}}\label{data-wrangling-in-tidyr}}

In this chapter we will learn about reshaping data to the format most suitable for our data analysis work. To reshape the data, we will use \texttt{tidyr} package \citep{R-tidyr} which is part of core \texttt{tidyverse} and can be loaded either by calling \texttt{library(tidyr)} or \texttt{library(tidyverse)}.

\textbf{Prerequisites}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

\hypertarget{concepts-of-tidy-data}{%
\section{Concepts of tidy data}\label{concepts-of-tidy-data}}

\href{https://hadley.nz/}{Hadley Wickham}, the chief scientist behind development of RStudio, tidyverse, and much more, introduced the concept of tidy data in a paper\footnote{\url{https://www.jstatsoft.org/article/view/v059i10}} published in the Journal of Statistical Software \citep{JSSv059i10}. Tidy data is a framework to structure data sets so they can be easily analyzed and visualized. It can be thought of as a goal one should aim for when cleaning data. Once we understand what tidy data is, that knowledge will make our data analysis, visualization, and collection much easier. \citep{JSSv021i12}

A tidy data-set has the following properties:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Each variable forms a column.
\item
  Each observation forms a row.
\item
  Each type of observational unit forms a table.
\end{enumerate}

Diagrammatically\footnote{Image taken from Hadley Wickham's book \href{https://r4ds.had.co.nz/}{R for data science}} this can be represented as in figure \ref{fig:tidy}.

\begin{figure}

{\centering \includegraphics[width=0.99\linewidth]{images/tidy-1} 

}

\caption{Diagrammatic representation of tidy data}\label{fig:tidy}
\end{figure}

Once a dataset is tidy, it can be used as input into a variety of other functions that may transform, model, or visualize the data.

Consider these five examples\footnote{all taken from package \texttt{tidyr}}. All these examples\footnote{These data-tables display the number of TB cases documented by the World Health Organization in Afghanistan, Brazil, and China between 1999 and 2000. The data contains values associated with four variables (country, year, cases, and population), but each table organizes the values in a different layout.} represent same data but shown in different formats-

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Example{-}1}
\NormalTok{tidyr}\SpecialCharTok{::}\NormalTok{table1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 4
##   country      year  cases population
##   <chr>       <dbl>  <dbl>      <dbl>
## 1 Afghanistan  1999    745   19987071
## 2 Afghanistan  2000   2666   20595360
## 3 Brazil       1999  37737  172006362
## 4 Brazil       2000  80488  174504898
## 5 China        1999 212258 1272915272
## 6 China        2000 213766 1280428583
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Example{-}2}
\NormalTok{tidyr}\SpecialCharTok{::}\NormalTok{table2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 12 x 4
##    country      year type            count
##    <chr>       <dbl> <chr>           <dbl>
##  1 Afghanistan  1999 cases             745
##  2 Afghanistan  1999 population   19987071
##  3 Afghanistan  2000 cases            2666
##  4 Afghanistan  2000 population   20595360
##  5 Brazil       1999 cases           37737
##  6 Brazil       1999 population  172006362
##  7 Brazil       2000 cases           80488
##  8 Brazil       2000 population  174504898
##  9 China        1999 cases          212258
## 10 China        1999 population 1272915272
## 11 China        2000 cases          213766
## 12 China        2000 population 1280428583
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Example{-}3}
\NormalTok{tidyr}\SpecialCharTok{::}\NormalTok{table3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 3
##   country      year rate             
##   <chr>       <dbl> <chr>            
## 1 Afghanistan  1999 745/19987071     
## 2 Afghanistan  2000 2666/20595360    
## 3 Brazil       1999 37737/172006362  
## 4 Brazil       2000 80488/174504898  
## 5 China        1999 212258/1272915272
## 6 China        2000 213766/1280428583
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Example{-}4 (Same data in 2 data tables now)}
\NormalTok{tidyr}\SpecialCharTok{::}\NormalTok{table4a}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3 x 3
##   country     `1999` `2000`
##   <chr>        <dbl>  <dbl>
## 1 Afghanistan    745   2666
## 2 Brazil       37737  80488
## 3 China       212258 213766
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tidyr}\SpecialCharTok{::}\NormalTok{table4b}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3 x 3
##   country         `1999`     `2000`
##   <chr>            <dbl>      <dbl>
## 1 Afghanistan   19987071   20595360
## 2 Brazil       172006362  174504898
## 3 China       1272915272 1280428583
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Example{-}5}
\NormalTok{tidyr}\SpecialCharTok{::}\NormalTok{table5}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 4
##   country     century year  rate             
##   <chr>       <chr>   <chr> <chr>            
## 1 Afghanistan 19      99    745/19987071     
## 2 Afghanistan 20      00    2666/20595360    
## 3 Brazil      19      99    37737/172006362  
## 4 Brazil      20      00    80488/174504898  
## 5 China       19      99    212258/1272915272
## 6 China       20      00    213766/1280428583
\end{verbatim}

Let us discuss these example one by one -

\begin{itemize}
\tightlist
\item
  \texttt{table1} fulfills all three rules stated above and is thus, in tidy format. Notice that every observation has its own row, and each variable is stored in a separate column.
\item
  \texttt{table2} stores one observation in two columns (separately for cases and population) and is thus not tidy.
\item
  \texttt{table3} stores two variables in one column (cases and population together) and is thus not tidy.
\item
  \texttt{table4a} and \texttt{table4b} clearly stores one observation in two different tables and is thus not tidy. We may further notice that both these tables use values as column headers, which also violate rule no.3 stated above.
\item
  \texttt{table5} again stores one variable i.e.~year in two separate columns, and thus does not follow rule no.2 stated above.
\end{itemize}

\hypertarget{reshaping-data}{%
\section{Reshaping data}\label{reshaping-data}}

In real world problems we will mostly come across data-sets that are not in tidy formats and for data analysis, visualisation we will need \texttt{tidying} the datasets. To do this we will first need to understand what actually is a value, a variable/field and an observation. As a second step of \emph{tidying} we will require to reshape the data by either -

\begin{itemize}
\tightlist
\item
  re-organising the observation originally spread into multiple rows (e.g.~\texttt{table2}), in one row; OR
\item
  re-organising the variable spread into multiple columns (e.g.~\texttt{table3}, etc.), in one single column.
\end{itemize}

To perform this \emph{tidying} exercise, we will need two most important functions from \texttt{tidyr} i.e.~\texttt{pivot\_longer} and \texttt{pivot\_wider}. So let us understand the functioning of these.

\hypertarget{longer-format-through-function-pivot_longer}{%
\subsection{\texorpdfstring{LONGER format through function \texttt{pivot\_longer()}}{LONGER format through function pivot\_longer()}}\label{longer-format-through-function-pivot_longer}}

Often we will come across data sets that will have values (instead of actual variable name) as column headers. Let us take the example of \texttt{table4a} or \texttt{table4b} shown above. Both these tables have values of variable \texttt{year} as column names. So we need to re-structure these tables into a longer format where these values form part of columns instead of column names. We will use \texttt{pivot\_longer()} function for this. Th basic syntax is-

\begin{verbatim}
pivot_longer(
  data,
  cols,
  names_to = "name",
  values_to = "value",
  ...
)
\end{verbatim}

\emph{Note that there many more useful arguments to this function, but first let us consider on these only.}

\begin{itemize}
\tightlist
\item
  \texttt{cols} indicate names of columns (as a character vector) to be converted into longer format
\item
  \texttt{names\_to\ =\ "name"} argument will actually convert values used as column headers back to a column with given ``name''
\item
  \texttt{values\_to\ =\ "value"} argument will convert values of all those columns back into one column with given name ``value'' (e.g.~population in table4b)
\end{itemize}

Basic functionality can be understood using the following example-

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris\_summary }\OtherTok{\textless{}{-}}\NormalTok{ iris }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(Species) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{mean =} \FunctionTok{mean}\NormalTok{(Sepal.Width),}
            \AttributeTok{st\_dev =} \FunctionTok{sd}\NormalTok{(Sepal.Width))}
\NormalTok{iris\_summary}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3 x 3
##   Species     mean st_dev
##   <fct>      <dbl>  <dbl>
## 1 setosa      3.43  0.379
## 2 versicolor  2.77  0.314
## 3 virginica   2.97  0.322
\end{verbatim}

Using \texttt{pivot\_wider} we can convert column headers into values. Check the diagram in figure \ref{fig:plong}.

\begin{figure}

{\centering \includegraphics[width=0.99\linewidth]{images/pivot_longer} 

}

\caption{Diagrammatic representation of pivot\_longer}\label{fig:plong}
\end{figure}

Now we are ready to convert \texttt{table4a} into a tidier format.

\hypertarget{case-i-when-values-are-in-column-headers}{%
\subsubsection*{Case-I when values are in column headers}\label{case-i-when-values-are-in-column-headers}}
\addcontentsline{toc}{subsubsection}{Case-I when values are in column headers}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pivot\_longer}\NormalTok{(}
\NormalTok{  table4a, }
  \AttributeTok{cols =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}1999\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}2000\textquotesingle{}}\NormalTok{), }
  \AttributeTok{names\_to =} \StringTok{\textquotesingle{}year\textquotesingle{}}\NormalTok{, }
  \AttributeTok{values\_to =} \StringTok{\textquotesingle{}cases\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 3
##   country     year   cases
##   <chr>       <chr>  <dbl>
## 1 Afghanistan 1999     745
## 2 Afghanistan 2000    2666
## 3 Brazil      1999   37737
## 4 Brazil      2000   80488
## 5 China       1999  212258
## 6 China       2000  213766
\end{verbatim}

With \emph{pipes} and a little tweaking, the above syntax could have been written as-

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tidyr}\SpecialCharTok{::}\NormalTok{table4a }\SpecialCharTok{\%\textgreater{}\%}                         \CommentTok{\# first argument passed through pipe }
  \FunctionTok{pivot\_longer}\NormalTok{(}\AttributeTok{cols =} \SpecialCharTok{{-}}\NormalTok{country,     }\CommentTok{\# all columns except country}
               \AttributeTok{names\_to =} \StringTok{"year"}\NormalTok{,}
               \AttributeTok{values\_to =} \StringTok{"cases"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{case-ii-when-both-variables-and-variable-names-are-combined-together-as-column-names}{%
\subsubsection*{Case-II when both variables and variable names are combined together as column names}\label{case-ii-when-both-variables-and-variable-names-are-combined-together-as-column-names}}
\addcontentsline{toc}{subsubsection}{Case-II when both variables and variable names are combined together as column names}

We have seen a simple case to tidy the table when the values (e.g.~years) were depicted as column names instead of variables i.e.~actual data. There may be cases when column names are a combination of both.

Example - Say we have a table \texttt{table6} as

\begin{verbatim}
## # A tibble: 3 x 5
##   country     cases_1999 cases_2000   pop_1999   pop_2000
##   <chr>            <dbl>      <dbl>      <dbl>      <dbl>
## 1 Afghanistan        745       2666   19987071   20595360
## 2 Brazil           37737      80488  172006362  174504898
## 3 China           212258     213766 1272915272 1280428583
\end{verbatim}

We may use \texttt{names\_sep} argument in this case, which will separate the combined variables from the column names -

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{table6 }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_longer}\NormalTok{(}\AttributeTok{cols =} \SpecialCharTok{!}\NormalTok{country,}
               \AttributeTok{names\_sep =} \StringTok{"\_"}\NormalTok{,}
               \AttributeTok{names\_to =} \FunctionTok{c}\NormalTok{(}\StringTok{"count\_type"}\NormalTok{, }\StringTok{"year"}\NormalTok{),}
               \AttributeTok{values\_to =} \StringTok{"count"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 12 x 4
##    country     count_type year       count
##    <chr>       <chr>      <chr>      <dbl>
##  1 Afghanistan cases      1999         745
##  2 Afghanistan cases      2000        2666
##  3 Afghanistan pop        1999    19987071
##  4 Afghanistan pop        2000    20595360
##  5 Brazil      cases      1999       37737
##  6 Brazil      cases      2000       80488
##  7 Brazil      pop        1999   172006362
##  8 Brazil      pop        2000   174504898
##  9 China       cases      1999      212258
## 10 China       cases      2000      213766
## 11 China       pop        1999  1272915272
## 12 China       pop        2000  1280428583
\end{verbatim}

\begin{quote}
Note that we have two column names in argument \texttt{names\_to}.
\end{quote}

Though the above table is still not in tidy format, yet the example was taken to show the functioning of other arguments of the \texttt{pivot\_longer}. We provided two static column names to the related argument and the variables were created after splitting the headers with sep \texttt{\_}. We actually require one dynamic value to be retained as column name (\texttt{cases} and \texttt{pop} here) but need to convert \texttt{year} to variables.

To do so we will use special value \texttt{".value"} in the related argument. See

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{table6 }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_longer}\NormalTok{(}\AttributeTok{cols =} \SpecialCharTok{!}\NormalTok{country,}
               \AttributeTok{names\_sep =} \StringTok{"\_"}\NormalTok{,}
               \AttributeTok{names\_to =} \FunctionTok{c}\NormalTok{(}\StringTok{".value"}\NormalTok{, }\StringTok{"year"}\NormalTok{),}
               \AttributeTok{values\_to =} \StringTok{"count"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 4
##   country     year   cases        pop
##   <chr>       <chr>  <dbl>      <dbl>
## 1 Afghanistan 1999     745   19987071
## 2 Afghanistan 2000    2666   20595360
## 3 Brazil      1999   37737  172006362
## 4 Brazil      2000   80488  174504898
## 5 China       1999  212258 1272915272
## 6 China       2000  213766 1280428583
\end{verbatim}

\begin{quote}
Note that by using \texttt{.value} the argument \texttt{values\_to} becomes meaning less.
\end{quote}

\hypertarget{wider-format-through-pivot_wider}{%
\subsection{\texorpdfstring{WIDER format through \texttt{pivot\_wider()}}{WIDER format through pivot\_wider()}}\label{wider-format-through-pivot_wider}}

As the name suggests, \texttt{pivot\_wider()} does exactly opposite to what a \texttt{pivot\_longer} does. Additionally, this function is used to create summary reports, as pivot functionality in MS Excel, through \texttt{values\_fn} argument. Diagrammatically this can be represented as in figure \ref{fig:pwide}. The basic syntax (with commonly used arguments) is-

\begin{verbatim}
pivot_wider(
  data,
  id_cols = NULL,
  names_from = name,
  values_from = value,
  values_fill = NULL,
  values_fn = NULL,
  ...
)
\end{verbatim}

where-

\begin{itemize}
\tightlist
\item
  \texttt{id\_cols} is a vector of columns that uniquely identifies each observation
\item
  \texttt{names\_from} is a vector of columns to get the name of the output column
\item
  \texttt{values\_from} similarly provides columns to get the cell values from
\item
  \texttt{values\_fill} provides what each value should be filled in with when missing
\item
  \texttt{values\_fn} is a named list - to apply different aggregations to different \texttt{values\_from} columns
\end{itemize}

\begin{quote}
Also note that there are many other arguments for this function, which may be used to deal with complicated tables.
\end{quote}

\begin{figure}

{\centering \includegraphics[width=0.99\linewidth]{images/pivot_wider} 

}

\caption{Diagrammatic representation of pivot\_wider}\label{fig:pwide}
\end{figure}

Example-1:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tidyr}\SpecialCharTok{::}\NormalTok{table2 }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_wider}\NormalTok{(}\AttributeTok{names\_from =} \StringTok{"type"}\NormalTok{,}
              \AttributeTok{values\_from =} \StringTok{"count"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 4
##   country      year  cases population
##   <chr>       <dbl>  <dbl>      <dbl>
## 1 Afghanistan  1999    745   19987071
## 2 Afghanistan  2000   2666   20595360
## 3 Brazil       1999  37737  172006362
## 4 Brazil       2000  80488  174504898
## 5 China        1999 212258 1272915272
## 6 China        2000 213766 1280428583
\end{verbatim}

Example-2:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tidyr}\SpecialCharTok{::}\NormalTok{table2 }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_wider}\NormalTok{(}\AttributeTok{names\_from =}\NormalTok{ year,}
              \AttributeTok{values\_from =}\NormalTok{ count)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 4
##   country     type           `1999`     `2000`
##   <chr>       <chr>           <dbl>      <dbl>
## 1 Afghanistan cases             745       2666
## 2 Afghanistan population   19987071   20595360
## 3 Brazil      cases           37737      80488
## 4 Brazil      population  172006362  174504898
## 5 China       cases          212258     213766
## 6 China       population 1272915272 1280428583
\end{verbatim}

Example-3: Summarisation

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tidyr}\SpecialCharTok{::}\NormalTok{table2 }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_wider}\NormalTok{(}\AttributeTok{id\_cols =}\NormalTok{ country,}
              \AttributeTok{names\_from =}\NormalTok{ type,}
              \AttributeTok{values\_from =}\NormalTok{ count,}
              \AttributeTok{values\_fn =}\NormalTok{ mean)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3 x 3
##   country       cases  population
##   <chr>         <dbl>       <dbl>
## 1 Afghanistan   1706.   20291216.
## 2 Brazil       59112.  173255630 
## 3 China       213012  1276671928.
\end{verbatim}

Example-4: Summarisation with different id\_cols

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tidyr}\SpecialCharTok{::}\NormalTok{table2 }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_wider}\NormalTok{(}\AttributeTok{id\_cols =}\NormalTok{ year,}
              \AttributeTok{names\_from =}\NormalTok{ type,}
              \AttributeTok{values\_from =}\NormalTok{ count,}
              \AttributeTok{values\_fn =}\NormalTok{ sum)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 2 x 3
##    year  cases population
##   <dbl>  <dbl>      <dbl>
## 1  1999 250740 1464908705
## 2  2000 296920 1475528841
\end{verbatim}

Example-5: Use of multiple columns in \texttt{names\_from} argument

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tidyr}\SpecialCharTok{::}\NormalTok{table2 }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_wider}\NormalTok{(}\AttributeTok{names\_from =} \FunctionTok{c}\NormalTok{(year, type),}
               \AttributeTok{values\_from =}\NormalTok{ count)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3 x 5
##   country     `1999_cases` `1999_population` `2000_cases` `2000_population`
##   <chr>              <dbl>             <dbl>        <dbl>             <dbl>
## 1 Afghanistan          745          19987071         2666          20595360
## 2 Brazil             37737         172006362        80488         174504898
## 3 China             212258        1272915272       213766        1280428583
\end{verbatim}

What if order is reversed in \texttt{names\_from} arg. See Example-6:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tidyr}\SpecialCharTok{::}\NormalTok{table2 }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_wider}\NormalTok{(}\AttributeTok{names\_from =} \FunctionTok{c}\NormalTok{(type, year),}
               \AttributeTok{values\_from =}\NormalTok{ count)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3 x 5
##   country     cases_1999 population_1999 cases_2000 population_2000
##   <chr>            <dbl>           <dbl>      <dbl>           <dbl>
## 1 Afghanistan        745        19987071       2666        20595360
## 2 Brazil           37737       172006362      80488       174504898
## 3 China           212258      1272915272     213766      1280428583
\end{verbatim}

Example-7: Multiple columns in \texttt{values\_from}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tidyr}\SpecialCharTok{::}\NormalTok{table1 }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_wider}\NormalTok{(}\AttributeTok{names\_from =}\NormalTok{ year,}
              \AttributeTok{values\_from =} \FunctionTok{c}\NormalTok{(cases, population))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3 x 5
##   country     cases_1999 cases_2000 population_1999 population_2000
##   <chr>            <dbl>      <dbl>           <dbl>           <dbl>
## 1 Afghanistan        745       2666        19987071        20595360
## 2 Brazil           37737      80488       172006362       174504898
## 3 China           212258     213766      1272915272      1280428583
\end{verbatim}

Example-8: Use of names\_vary argument to control the order of output columns

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tidyr}\SpecialCharTok{::}\NormalTok{table1 }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_wider}\NormalTok{(}\AttributeTok{names\_from =}\NormalTok{ year,}
              \AttributeTok{values\_from =} \FunctionTok{c}\NormalTok{(cases, population),}
              \AttributeTok{names\_vary =} \StringTok{"slowest"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3 x 5
##   country     cases_1999 population_1999 cases_2000 population_2000
##   <chr>            <dbl>           <dbl>      <dbl>           <dbl>
## 1 Afghanistan        745        19987071       2666        20595360
## 2 Brazil           37737       172006362      80488       174504898
## 3 China           212258      1272915272     213766      1280428583
\end{verbatim}

For more details please refer to \href{https://tidyr.tidyverse.org/articles/pivot.html}{package vignette} or \href{https://r4ds.had.co.nz/tidy-data.html}{Chapter-12 of R for Data Science book}.

\hypertarget{separate-columns-into-multiple-columns-join-columns-into-one-column}{%
\subsection{Separate Column(s) into multiple columns/ Join columns into one column}\label{separate-columns-into-multiple-columns-join-columns-into-one-column}}

\hypertarget{separate-a-character-column-into-multiple-with-separate}{%
\subsubsection*{\texorpdfstring{Separate a character column into multiple with \texttt{separate()}}{Separate a character column into multiple with separate()}}\label{separate-a-character-column-into-multiple-with-separate}}
\addcontentsline{toc}{subsubsection}{Separate a character column into multiple with \texttt{separate()}}

As the name suggests, \texttt{separate()} function is used to separate a given character column into multiple columns either using a regular expression or a vector of character positions.
The syntax is -

\begin{verbatim}
separate(
  data,
  col,
  into,
  sep = "[^[:alnum:]]+",
  remove = TRUE,
  convert = FALSE,
  extra = "warn",
  fill = "warn",
  ...
)
\end{verbatim}

Explanation of purpose of different arguments in above syntax -

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{data} is as usual name of the data frame
\item
  \texttt{col} is the name of the column which is required to be separated.
\item
  \texttt{into} should be a character vector, which usually should be equal length of maximum number of new columns which will be created out of such separation. (Refer examples nos. 1)
\item
  \texttt{sep} provides a separator value. (Refer Example -3 below).
\item
  \texttt{remove} if \texttt{FALSE}, the original column is not removed from the output. (Refer Example -3 below).
\item
  \texttt{convert} if \texttt{TRUE}, the component columns are converted to double/integer/logical/NA, if possible. This is useful if the component columns are integer, numeric or logical. (Refer Example-1 below).
\item
  \texttt{extra} argument is used to control when number of desired component columns are less than the maximum possible count. (Refer Example-4 below).
\item
  \texttt{fill} argument is on the other hand, useful when the number of components are different for each row. (Refer Example-2 below)
\end{enumerate}

Example-1:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tidyr}\SpecialCharTok{::}\NormalTok{table3 }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{separate}\NormalTok{(rate, }\AttributeTok{into =} \FunctionTok{c}\NormalTok{(}\StringTok{"cases"}\NormalTok{, }\StringTok{"population"}\NormalTok{),}
           \AttributeTok{convert =} \ConstantTok{TRUE}\NormalTok{) }\CommentTok{\# optional {-} will convert the values}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 4
##   country      year  cases population
##   <chr>       <dbl>  <int>      <int>
## 1 Afghanistan  1999    745   19987071
## 2 Afghanistan  2000   2666   20595360
## 3 Brazil       1999  37737  172006362
## 4 Brazil       2000  80488  174504898
## 5 China        1999 212258 1272915272
## 6 China        2000 213766 1280428583
\end{verbatim}

Example-2:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\StringTok{"a"}\NormalTok{, }\StringTok{"a+b"}\NormalTok{, }\StringTok{"c+d+e"}\NormalTok{)}
\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{separate}\NormalTok{(x,}
           \AttributeTok{into=}\FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}X1\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}X2\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}X3\textquotesingle{}}\NormalTok{),}
           \AttributeTok{fill =} \StringTok{"left"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     X1   X2 X3
## 1 <NA> <NA>  a
## 2 <NA>    a  b
## 3    c    d  e
\end{verbatim}

Example-3:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\StringTok{"A$B"}\NormalTok{, }\StringTok{"C+D"}\NormalTok{, }\StringTok{"E{-}F"}\NormalTok{)}
\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{separate}\NormalTok{(x, }
           \AttributeTok{sep =} \StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{{-}|}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{$"}\NormalTok{,}
           \AttributeTok{into =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}X1\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}X2\textquotesingle{}}\NormalTok{),}
           \AttributeTok{remove =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: Expected 2 pieces. Missing pieces filled with `NA` in 1 rows [2].
\end{verbatim}

\begin{verbatim}
##     x  X1   X2
## 1 A$B   A    B
## 2 C+D C+D <NA>
## 3 E-F   E    F
\end{verbatim}

Example-4:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\StringTok{"a"}\NormalTok{, }\StringTok{"a+b"}\NormalTok{, }\StringTok{"c+d+e"}\NormalTok{)}
\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{separate}\NormalTok{(x,}
           \AttributeTok{into=}\FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}X1\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}X2\textquotesingle{}}\NormalTok{),}
           \AttributeTok{extra =} \StringTok{"merge"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: Expected 2 pieces. Missing pieces filled with `NA` in 1 rows [1].
\end{verbatim}

\begin{verbatim}
##   X1   X2
## 1  a <NA>
## 2  a    b
## 3  c  d+e
\end{verbatim}

\hypertarget{unite-multiple-character-columns-into-one-using-unite}{%
\subsubsection*{\texorpdfstring{Unite multiple character columns into one using \texttt{unite()}}{Unite multiple character columns into one using unite()}}\label{unite-multiple-character-columns-into-one-using-unite}}
\addcontentsline{toc}{subsubsection}{Unite multiple character columns into one using \texttt{unite()}}

It complements \texttt{separate} by uniting the columns into one. Its syntax is

\begin{verbatim}
unite(data, 
      col, 
      ..., 
      sep = "_", 
      remove = TRUE, 
      na.rm = FALSE)
\end{verbatim}

Explanation of arguments in the above syntax-

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{data} is as usual name of the data frame.
\item
  \texttt{col} should be the name of new column to be formed (should be a string),
\item
  \texttt{...} the names of columns to be united should be provided
\item
  \texttt{sep} is separator to be used for uniting
\item
  \texttt{remove} if \texttt{FALSE}, will not remove original component columns
\item
  \texttt{na.rm} if \texttt{TRUE}, the missing values will be removed beforehand.
\end{enumerate}

Example-1a:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tidyr}\SpecialCharTok{::}\NormalTok{table5 }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{unite}\NormalTok{(}\StringTok{"Year"}\NormalTok{,}
        \FunctionTok{c}\NormalTok{(}\StringTok{"century"}\NormalTok{, }\StringTok{"year"}\NormalTok{), }
        \AttributeTok{sep =} \StringTok{""}\NormalTok{,}
        \AttributeTok{remove =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 5
##   country     Year  century year  rate             
##   <chr>       <chr> <chr>   <chr> <chr>            
## 1 Afghanistan 1999  19      99    745/19987071     
## 2 Afghanistan 2000  20      00    2666/20595360    
## 3 Brazil      1999  19      99    37737/172006362  
## 4 Brazil      2000  20      00    80488/174504898  
## 5 China       1999  19      99    212258/1272915272
## 6 China       2000  20      00    213766/1280428583
\end{verbatim}

Example-1b:
We may complete the tidying process in the next step

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tidyr}\SpecialCharTok{::}\NormalTok{table5 }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{unite}\NormalTok{(}\StringTok{"Year"}\NormalTok{,}
        \FunctionTok{c}\NormalTok{(}\StringTok{"century"}\NormalTok{, }\StringTok{"year"}\NormalTok{), }
        \AttributeTok{sep =} \StringTok{""}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{separate}\NormalTok{(rate, }
           \AttributeTok{into =} \FunctionTok{c}\NormalTok{(}\StringTok{"cases"}\NormalTok{, }\StringTok{"population"}\NormalTok{),}
           \AttributeTok{convert =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 4
##   country     Year   cases population
##   <chr>       <chr>  <int>      <int>
## 1 Afghanistan 1999     745   19987071
## 2 Afghanistan 2000    2666   20595360
## 3 Brazil      1999   37737  172006362
## 4 Brazil      2000   80488  174504898
## 5 China       1999  212258 1272915272
## 6 China       2000  213766 1280428583
\end{verbatim}

Example-

\hypertarget{separate-rows-into-multiple-rows}{%
\subsection{Separate row(s) into multiple rows}\label{separate-rows-into-multiple-rows}}

\hypertarget{split-data-into-multiple-rows-with-separate_rows}{%
\subsubsection*{\texorpdfstring{Split data into multiple rows with \texttt{separate\_rows()}}{Split data into multiple rows with separate\_rows()}}\label{split-data-into-multiple-rows-with-separate_rows}}
\addcontentsline{toc}{subsubsection}{Split data into multiple rows with \texttt{separate\_rows()}}

This function is used to separate delimited values placed in one single cell/column into multiple rows (as against in rows using \texttt{separate}).
See Example-

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tidyr}\SpecialCharTok{::}\NormalTok{table3 }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{separate\_rows}\NormalTok{(}
\NormalTok{    rate,}
    \AttributeTok{sep =} \StringTok{"/"}\NormalTok{,}
    \AttributeTok{convert =} \ConstantTok{TRUE}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 12 x 3
##    country      year       rate
##    <chr>       <dbl>      <int>
##  1 Afghanistan  1999        745
##  2 Afghanistan  1999   19987071
##  3 Afghanistan  2000       2666
##  4 Afghanistan  2000   20595360
##  5 Brazil       1999      37737
##  6 Brazil       1999  172006362
##  7 Brazil       2000      80488
##  8 Brazil       2000  174504898
##  9 China        1999     212258
## 10 China        1999 1272915272
## 11 China        2000     213766
## 12 China        2000 1280428583
\end{verbatim}

To create \texttt{type} we have to however, add an extra step to the above example-

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tidyr}\SpecialCharTok{::}\NormalTok{table3 }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{separate\_rows}\NormalTok{(}
\NormalTok{    rate,}
    \AttributeTok{sep =} \StringTok{"/"}\NormalTok{,}
    \AttributeTok{convert =} \ConstantTok{TRUE}
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(country, year) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{type =} \FunctionTok{c}\NormalTok{(}\StringTok{"cases"}\NormalTok{, }\StringTok{"pop"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 12 x 4
## # Groups:   country, year [6]
##    country      year       rate type 
##    <chr>       <dbl>      <int> <chr>
##  1 Afghanistan  1999        745 cases
##  2 Afghanistan  1999   19987071 pop  
##  3 Afghanistan  2000       2666 cases
##  4 Afghanistan  2000   20595360 pop  
##  5 Brazil       1999      37737 cases
##  6 Brazil       1999  172006362 pop  
##  7 Brazil       2000      80488 cases
##  8 Brazil       2000  174504898 pop  
##  9 China        1999     212258 cases
## 10 China        1999 1272915272 pop  
## 11 China        2000     213766 cases
## 12 China        2000 1280428583 pop
\end{verbatim}

\hypertarget{expand-table-to-handle-missing-rowsvalues}{%
\subsection{Expand table to handle missing rows/values}\label{expand-table-to-handle-missing-rowsvalues}}

Sometimes, when we deal with missing data, we require to handle implicit missing values as well. Either we have to turn these values into explicit missing values or we have to fill appropriate values. In such cases, two functions namely \texttt{complete} and \texttt{fill} both from same package are extremely useful. Let's learn these as well.

\hypertarget{turn-implicit-missing-values-to-explicit-using-complete}{%
\subsubsection*{\texorpdfstring{Turn implicit missing values to explicit using \texttt{complete()}}{Turn implicit missing values to explicit using complete()}}\label{turn-implicit-missing-values-to-explicit-using-complete}}
\addcontentsline{toc}{subsubsection}{Turn implicit missing values to explicit using \texttt{complete()}}

As th name suggests, this function is used to turn implicit missing values (invisible rows) to explicit missing values (visible rows with \texttt{NA}).

As an example, let's suppose fuel prices are revised randomly. Say, after revision on \texttt{1\ January\ 2020} prices revise on \texttt{20\ January\ 2020}. So we will have only 2 rows in data for say \texttt{Janaury\ 2020}. Thus, there are 29 implicit missing values in the data.

The syntax is

\begin{verbatim}
complete(data, 
         ..., 
         fill = list(), 
         explicit = TRUE)
\end{verbatim}

Where -

\begin{itemize}
\tightlist
\item
  \texttt{data} is as usual argument to provide data frame
\item
  \texttt{...} are meant to provide column names to be completed
\item
  \texttt{fill} provides a list to supply a single value which can be provided instead of \texttt{NA} for missing combinations.
\end{itemize}

In the example mentioned above we can proceed as

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# First create a sample data}
\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{date =} \FunctionTok{c}\NormalTok{(}\FunctionTok{as.Date}\NormalTok{(}\StringTok{"2020{-}01{-}01"}\NormalTok{), }\FunctionTok{as.Date}\NormalTok{(}\StringTok{"2020{-}01{-}20"}\NormalTok{)),}
  \AttributeTok{price =} \FunctionTok{c}\NormalTok{(}\FloatTok{75.12}\NormalTok{, }\FloatTok{78.32}\NormalTok{)}
\NormalTok{)}
\NormalTok{df }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         date price
## 1 2020-01-01 75.12
## 2 2020-01-20 78.32
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Use tidyr::complete to see explicit missing values}
\NormalTok{df }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{complete}\NormalTok{(}\AttributeTok{date =} \FunctionTok{seq.Date}\NormalTok{(}\FunctionTok{as.Date}\NormalTok{(}\StringTok{"2020{-}01{-}01"}\NormalTok{), }\FunctionTok{as.Date}\NormalTok{(}\StringTok{"2020{-}01{-}31"}\NormalTok{), }\AttributeTok{by =} \StringTok{"day"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 31 x 2
##    date       price
##    <date>     <dbl>
##  1 2020-01-01  75.1
##  2 2020-01-02  NA  
##  3 2020-01-03  NA  
##  4 2020-01-04  NA  
##  5 2020-01-05  NA  
##  6 2020-01-06  NA  
##  7 2020-01-07  NA  
##  8 2020-01-08  NA  
##  9 2020-01-09  NA  
## 10 2020-01-10  NA  
## # i 21 more rows
\end{verbatim}

Though the above example created dates as per the criteria given, \texttt{complete} function can find all unique combinations in the set of columns provided and return complete set of observations. See this example

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Let\textquotesingle{}s create a sample data}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{df2 }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{year =} \FunctionTok{c}\NormalTok{(}\DecValTok{2020}\NormalTok{, }\DecValTok{2020}\NormalTok{, }\DecValTok{2020}\NormalTok{, }\DecValTok{2021}\NormalTok{, }\DecValTok{2021}\NormalTok{),}
  \AttributeTok{qtr =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{),}
  \AttributeTok{sales =} \FunctionTok{runif}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{100}\NormalTok{, }\DecValTok{200}\NormalTok{)}
\NormalTok{)}
\NormalTok{df2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   year qtr    sales
## 1 2020   1 128.7578
## 2 2020   3 178.8305
## 3 2020   4 140.8977
## 4 2021   2 188.3017
## 5 2021   3 194.0467
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# use complete to find all combination}
\NormalTok{df2 }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{complete}\NormalTok{(year, qtr, }\CommentTok{\# cols provided}
           \AttributeTok{fill =} \FunctionTok{list}\NormalTok{(}\AttributeTok{sales =} \DecValTok{0}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 8 x 3
##    year   qtr sales
##   <dbl> <dbl> <dbl>
## 1  2020     1  129.
## 2  2020     2    0 
## 3  2020     3  179.
## 4  2020     4  141.
## 5  2021     1    0 
## 6  2021     2  188.
## 7  2021     3  194.
## 8  2021     4    0
\end{verbatim}

\textbf{Note:} If \texttt{fill} argument would not have been used, the value of sales in missing columns would have been \texttt{NA} instead of provided value.

\hypertarget{fill-missing-values-based-on-criteria-fill}{%
\subsubsection*{\texorpdfstring{Fill missing values based on criteria \texttt{fill()}}{Fill missing values based on criteria fill()}}\label{fill-missing-values-based-on-criteria-fill}}
\addcontentsline{toc}{subsubsection}{Fill missing values based on criteria \texttt{fill()}}

This function helps in filling the missing values using previous or next entry. Think of a paper sheet where many cells in a table have been filled as \texttt{"-do-"}. Syntax is -

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{fill}\NormalTok{(data, }\CommentTok{\# used to provide data}
\NormalTok{     ..., }\CommentTok{\# provide columns to be filled}
     \AttributeTok{.direction =} \FunctionTok{c}\NormalTok{(}\StringTok{"down"}\NormalTok{, }\StringTok{"up"}\NormalTok{, }\StringTok{"downup"}\NormalTok{, }\StringTok{"updown"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

The arguments are pretty simple. Most of the time \texttt{"down"} method is used. As an example we could see the example of fuel prices mentioned above.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fill\_df }\OtherTok{\textless{}{-}}\NormalTok{ df }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{complete}\NormalTok{(}\AttributeTok{date =} \FunctionTok{seq.Date}\NormalTok{(}\FunctionTok{as.Date}\NormalTok{(}\StringTok{"2020{-}01{-}01"}\NormalTok{), }\FunctionTok{as.Date}\NormalTok{(}\StringTok{"2020{-}01{-}31"}\NormalTok{), }\AttributeTok{by =} \StringTok{"day"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{fill}\NormalTok{(price, }\AttributeTok{.direction =} \StringTok{"down"}\NormalTok{)}

\FunctionTok{head}\NormalTok{(fill\_df)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 2
##   date       price
##   <date>     <dbl>
## 1 2020-01-01  75.1
## 2 2020-01-02  75.1
## 3 2020-01-03  75.1
## 4 2020-01-04  75.1
## 5 2020-01-05  75.1
## 6 2020-01-06  75.1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{tail}\NormalTok{(fill\_df)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 2
##   date       price
##   <date>     <dbl>
## 1 2020-01-26  78.3
## 2 2020-01-27  78.3
## 3 2020-01-28  78.3
## 4 2020-01-29  78.3
## 5 2020-01-30  78.3
## 6 2020-01-31  78.3
\end{verbatim}

\hypertarget{generating-descriptive-statistics}{%
\chapter{Generating Descriptive statistics}\label{generating-descriptive-statistics}}

Exploratory Data Analysis or often abbreviated as EDA, is mostly the first and foremost step before carrying out any data analytics task, is used to analyze and investigate data sets and summarize their main characteristics, often employing data visualization methods. EDA is primarily used to see what data can reveal beyond the formal modeling or hypothesis testing task and provides a provides a better understanding of data set variables and the relationships between them. It can also help determine if the statistical techniques you are considering for data analysis are appropriate. Originally developed by American mathematician John Tukey in the 1960s, EDA techniques continue to be a widely used method in the data discovery process today.

\hypertarget{using-base-r}{%
\section{Using base R}\label{using-base-r}}

Base R provides us with two functions used ato ascertain structure and summary statistics of a data frame. First is \texttt{str} short for \textbf{structure} (and not to be confused with \textbf{str}ing) which as its full name suggests gives us structure of the data. Its usage is simple

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(iris)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 'data.frame':    150 obs. of  5 variables:
##  $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...
##  $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...
##  $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...
##  $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...
##  $ Species     : Factor w/ 3 levels "setosa","versicolor",..: 1 1 1 1 1 1 1 1 1 1 ...
\end{verbatim}

As can be seen it gives us number of variables (columns) as well as observations (rows) available in the given data. It thereafter presents us names of all the columns/variables in the data along with their types. That's not all. It also prints few first values in all of the columns. For \texttt{factor} columns it also gives us available levels in those factor variables.

Another function from base R is \texttt{summary} which can be used to generate some summary statistics from the given data frame. Let's see what we can get from this function.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(iris)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   
##  Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  
##  1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  
##  Median :5.800   Median :3.000   Median :4.350   Median :1.300  
##  Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  
##  3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  
##  Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  
##        Species  
##  setosa    :50  
##  versicolor:50  
##  virginica :50  
##                 
##                 
## 
\end{verbatim}

We can see that it nicely gives us five-point summary for all \texttt{numeric} variables and count of all values present in \texttt{factor} variables. Apart from the five point summary i.e.~(1) minimum, (2) 1st quartile, (3) Median, (4) third quartile and (5) maximum; we also get mean (arithmetic) of all numeric variables.

Before moving forward, we can discuss again \texttt{table()} function here which is used to genrate counts of factor/character variable(s) in base R.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{with}\NormalTok{(iris, }\FunctionTok{table}\NormalTok{(Species))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Species
##     setosa versicolor  virginica 
##         50         50         50
\end{verbatim}

\hypertarget{dplyr-functions}{%
\section{Dplyr functions}\label{dplyr-functions}}

For calculating other statistics we can use \texttt{dplyr::summarise} in combination with \texttt{across}. For Example to calculate \texttt{mean}, \texttt{sd}, \texttt{variance} for all numeric variables of say \texttt{iris} data, we can do-

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(dplyr)}
\NormalTok{iris }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\FunctionTok{where}\NormalTok{(is.numeric),}
                   \AttributeTok{.fns =} \FunctionTok{list}\NormalTok{(}
                     \AttributeTok{Mean =} \SpecialCharTok{\textasciitilde{}} \FunctionTok{mean}\NormalTok{(.),}
                     \AttributeTok{SD =} \SpecialCharTok{\textasciitilde{}} \FunctionTok{sd}\NormalTok{(.),}
                     \AttributeTok{Var =} \SpecialCharTok{\textasciitilde{}} \FunctionTok{var}\NormalTok{(.)}
\NormalTok{                   )))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Sepal.Length_Mean Sepal.Length_SD Sepal.Length_Var Sepal.Width_Mean
## 1          5.843333       0.8280661        0.6856935         3.057333
##   Sepal.Width_SD Sepal.Width_Var Petal.Length_Mean Petal.Length_SD
## 1      0.4358663       0.1899794             3.758        1.765298
##   Petal.Length_Var Petal.Width_Mean Petal.Width_SD Petal.Width_Var
## 1         3.116278         1.199333      0.7622377       0.5810063
\end{verbatim}

Before trying to understand the output let's learn to use \texttt{dplyr::across}. Actually \texttt{across} is used inside dplyr verbs mostly with \texttt{mutate} or \texttt{summarise} through which we can mutate/summarise multiple variables (columns) simultaneously. So, at least two arguments are needed; first variable names which can be provided through a type checking variable, str detecting function, etc.; and second argument either a function name or a list of functions together. So in above example we have summarised all numeric columns (see first argument is a function \texttt{is.numeric} which only operates on column names) and second argument is a list of three functions in lambda style notation. In our example we are having 4 numeric columns and three aggregating functions, so 12 columns we are getting in output.

We can further reshape/transform the data using \texttt{tidyr::pivot\_longer}. See

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyr)}
\NormalTok{iris }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\FunctionTok{where}\NormalTok{(is.numeric),}
                   \AttributeTok{.fns =} \FunctionTok{list}\NormalTok{(}
                     \AttributeTok{Mean =} \SpecialCharTok{\textasciitilde{}} \FunctionTok{mean}\NormalTok{(.),}
                     \AttributeTok{SD =} \SpecialCharTok{\textasciitilde{}} \FunctionTok{sd}\NormalTok{(.),}
                     \AttributeTok{Var =} \SpecialCharTok{\textasciitilde{}} \FunctionTok{var}\NormalTok{(.)}
\NormalTok{                   ))) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_longer}\NormalTok{(}\FunctionTok{everything}\NormalTok{(),}
               \AttributeTok{names\_sep =} \StringTok{"\_"}\NormalTok{,}
               \AttributeTok{names\_to =} \FunctionTok{c}\NormalTok{(}\StringTok{".value"}\NormalTok{, }\StringTok{"Function"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3 x 5
##   Function Sepal.Length Sepal.Width Petal.Length Petal.Width
##   <chr>           <dbl>       <dbl>        <dbl>       <dbl>
## 1 Mean            5.84        3.06          3.76       1.20 
## 2 SD              0.828       0.436         1.77       0.762
## 3 Var             0.686       0.190         3.12       0.581
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\FunctionTok{where}\NormalTok{(is.numeric),}
                   \AttributeTok{.fns =} \FunctionTok{list}\NormalTok{(}
                     \AttributeTok{Mean =} \SpecialCharTok{\textasciitilde{}} \FunctionTok{mean}\NormalTok{(.),}
                     \AttributeTok{SD =} \SpecialCharTok{\textasciitilde{}} \FunctionTok{sd}\NormalTok{(.),}
                     \AttributeTok{Var =} \SpecialCharTok{\textasciitilde{}} \FunctionTok{var}\NormalTok{(.)}
\NormalTok{                   ))) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_longer}\NormalTok{(}\FunctionTok{everything}\NormalTok{(),}
               \AttributeTok{names\_sep =} \StringTok{"\_"}\NormalTok{,}
               \AttributeTok{names\_to =} \FunctionTok{c}\NormalTok{(}\StringTok{"Variable"}\NormalTok{, }\StringTok{".value"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 4 x 4
##   Variable      Mean    SD   Var
##   <chr>        <dbl> <dbl> <dbl>
## 1 Sepal.Length  5.84 0.828 0.686
## 2 Sepal.Width   3.06 0.436 0.190
## 3 Petal.Length  3.76 1.77  3.12 
## 4 Petal.Width   1.20 0.762 0.581
\end{verbatim}

Let us also discuss one more data summary statistics function of \texttt{dplyr} that is \texttt{glimpse}. It is basically a pipe friendly version of \texttt{str()}. See

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Rows: 150
## Columns: 5
## $ Sepal.Length <dbl> 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.~
## $ Sepal.Width  <dbl> 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.~
## $ Petal.Length <dbl> 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.~
## $ Petal.Width  <dbl> 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.~
## $ Species      <fct> setosa, setosa, setosa, setosa, setosa, setosa, setosa, s~
\end{verbatim}

To calculate counts of factor variable (as generated by \texttt{table} in base R), we can use \texttt{dplyr::count} a pipe friendly function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{count}\NormalTok{(Species)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      Species  n
## 1     setosa 50
## 2 versicolor 50
## 3  virginica 50
\end{verbatim}

We can generate counts of multiple combinations of variables

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ggplot2}\SpecialCharTok{::}\NormalTok{diamonds }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{count}\NormalTok{(cut, color, }\AttributeTok{name =} \StringTok{"count"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 35 x 3
##    cut   color count
##    <ord> <ord> <int>
##  1 Fair  D       163
##  2 Fair  E       224
##  3 Fair  F       312
##  4 Fair  G       314
##  5 Fair  H       303
##  6 Fair  I       175
##  7 Fair  J       119
##  8 Good  D       662
##  9 Good  E       933
## 10 Good  F       909
## # i 25 more rows
\end{verbatim}

\hypertarget{using-psych}{%
\section{\texorpdfstring{Using \texttt{psych}}{Using psych}}\label{using-psych}}

There are indeed some beautiful packages in R, which creates beautiful EDA summaries for us without much ado. Package \texttt{psych} is one of these.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(psych)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Attaching package: 'psych'
\end{verbatim}

\begin{verbatim}
## The following objects are masked from 'package:ggplot2':
## 
##     %+%, alpha
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{describe}\NormalTok{(USArrests)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          vars  n   mean    sd median trimmed    mad  min   max range  skew
## Murder      1 50   7.79  4.36   7.25    7.53   5.41  0.8  17.4  16.6  0.37
## Assault     2 50 170.76 83.34 159.00  168.48 110.45 45.0 337.0 292.0  0.22
## UrbanPop    3 50  65.54 14.47  66.00   65.88  17.79 32.0  91.0  59.0 -0.21
## Rape        4 50  21.23  9.37  20.10   20.36   8.60  7.3  46.0  38.7  0.75
##          kurtosis    se
## Murder      -0.95  0.62
## Assault     -1.15 11.79
## UrbanPop    -0.87  2.05
## Rape         0.08  1.32
\end{verbatim}

Note that output is in \texttt{data.frame} format ready to use. Another function in \texttt{psych} is \texttt{describeBy} which creates grouped summaries.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{describeBy}\NormalTok{(ggplot2}\SpecialCharTok{::}\NormalTok{diamonds, }\AttributeTok{group =} \StringTok{"cut"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Descriptive statistics by group 
## cut: 1
##         vars    n    mean      sd  median trimmed     mad    min      max
## carat      1 1610    1.05    0.52    1.00    0.98    0.43   0.22     5.01
## cut        2 1610    1.00    0.00    1.00    1.00    0.00   1.00     1.00
## color      3 1610    3.85    1.71    4.00    3.85    1.48   1.00     7.00
## clarity    4 1610    3.02    1.45    3.00    2.93    1.48   1.00     8.00
## depth      5 1610   64.04    3.64   65.00   64.48    1.33  43.00    79.00
## table      6 1610   59.05    3.95   58.00   58.64    2.97  49.00    95.00
## price      7 1610 4358.76 3560.39 3282.00 3695.65 2183.13 337.00 18574.00
## x          8 1610    6.25    0.96    6.18    6.21    0.81   0.00    10.74
## y          9 1610    6.18    0.96    6.10    6.14    0.79   0.00    10.54
## z         10 1610    3.98    0.65    3.97    3.95    0.52   0.00     6.98
##            range  skew kurtosis    se
## carat       4.79  1.68     5.31  0.01
## cut         0.00   NaN      NaN  0.00
## color       6.00  0.06    -0.86  0.04
## clarity     7.00  0.68     0.14  0.04
## depth      36.00 -1.17     2.20  0.09
## table      46.00  1.34     4.83  0.10
## price   18237.00  1.78     3.07 88.73
## x          10.74  0.36     1.58  0.02
## y          10.54  0.36     1.53  0.02
## z           6.98  0.34     1.43  0.02
## ------------------------------------------------------------ 
## cut: 2
##         vars    n    mean      sd  median trimmed     mad    min      max
## carat      1 4906    0.85    0.45    0.82    0.80    0.43   0.23     3.01
## cut        2 4906    2.00    0.00    2.00    2.00    0.00   2.00     2.00
## color      3 4906    3.57    1.76    3.00    3.51    1.48   1.00     7.00
## clarity    4 4906    3.60    1.47    3.00    3.44    1.48   1.00     8.00
## depth      5 4906   62.37    2.17   63.40   62.70    0.74  54.30    67.00
## table      6 4906   58.69    2.85   58.00   58.57    2.97  51.00    66.00
## price      7 4906 3928.86 3681.59 3050.50 3251.51 2853.26 327.00 18788.00
## x          8 4906    5.84    1.06    5.98    5.80    1.10   0.00     9.44
## y          9 4906    5.85    1.05    5.99    5.82    1.08   0.00     9.38
## z         10 4906    3.64    0.65    3.70    3.62    0.68   0.00     5.79
##            range  skew kurtosis    se
## carat       2.78  1.03     1.22  0.01
## cut         0.00   NaN      NaN  0.00
## color       6.00  0.25    -0.93  0.03
## clarity     7.00  0.81     0.29  0.02
## depth      12.70 -1.20     0.17  0.03
## table      15.00  0.31    -0.64  0.04
## price   18461.00  1.72     3.04 52.56
## x           9.44  0.15    -0.15  0.02
## y           9.38  0.14    -0.17  0.02
## z           5.79  0.09     0.12  0.01
## ------------------------------------------------------------ 
## cut: 3
##         vars     n    mean      sd  median trimmed     mad   min      max
## carat      1 12082    0.81    0.46    0.71    0.75    0.46   0.2     4.00
## cut        2 12082    3.00    0.00    3.00    3.00    0.00   3.0     3.00
## color      3 12082    3.57    1.72    3.00    3.51    1.48   1.0     7.00
## clarity    4 12082    4.00    1.59    4.00    3.87    1.48   1.0     8.00
## depth      5 12082   61.82    1.38   62.10   61.95    1.48  56.8    64.90
## table      6 12082   57.96    2.12   58.00   57.88    1.48  44.0    66.00
## price      7 12082 3981.76 3935.86 2648.00 3243.22 2855.49 336.0 18818.00
## x          8 12082    5.74    1.10    5.74    5.69    1.25   0.0    10.01
## y          9 12082    5.77    1.10    5.77    5.72    1.25   0.0     9.94
## z         10 12082    3.56    0.73    3.56    3.53    0.76   0.0    31.80
##            range  skew kurtosis    se
## carat       3.80  0.99     0.89  0.00
## cut         0.00   NaN      NaN  0.00
## color       6.00  0.25    -0.89  0.02
## clarity     7.00  0.57    -0.43  0.01
## depth       8.10 -0.71    -0.30  0.01
## table      22.00  0.28     0.04  0.02
## price   18482.00  1.60     2.24 35.81
## x          10.01  0.23    -0.65  0.01
## y           9.94  0.23    -0.66  0.01
## z          31.80  4.96   183.94  0.01
## ------------------------------------------------------------ 
## cut: 4
##         vars     n    mean      sd  median trimmed     mad   min      max
## carat      1 13791    0.89    0.52    0.86    0.83    0.56   0.2     4.01
## cut        2 13791    4.00    0.00    4.00    4.00    0.00   4.0     4.00
## color      3 13791    3.70    1.71    4.00    3.67    1.48   1.0     7.00
## clarity    4 13791    3.74    1.50    4.00    3.60    1.48   1.0     8.00
## depth      5 13791   61.26    1.16   61.40   61.36    1.19  58.0    63.00
## table      6 13791   58.75    1.48   59.00   58.77    1.48  51.0    62.00
## price      7 13791 4584.26 4349.20 3185.00 3822.23 3371.43 326.0 18823.00
## x          8 13791    5.97    1.19    6.11    5.92    1.42   0.0    10.14
## y          9 13791    5.94    1.26    6.06    5.89    1.41   0.0    58.90
## z         10 13791    3.65    0.73    3.72    3.62    0.86   0.0     8.06
##            range  skew kurtosis    se
## carat       3.81  0.86     0.43  0.00
## cut         0.00   NaN      NaN  0.00
## color       6.00  0.12    -0.88  0.01
## clarity     7.00  0.69     0.06  0.01
## depth       5.00 -0.61    -0.37  0.01
## table      11.00 -0.37     1.33  0.01
## price   18497.00  1.33     1.07 37.03
## x          10.14  0.17    -0.85  0.01
## y          58.90  5.53   225.05  0.01
## z           8.06  0.11    -0.44  0.01
## ------------------------------------------------------------ 
## cut: 5
##         vars     n    mean      sd  median trimmed     mad   min      max
## carat      1 21551    0.70    0.43    0.54    0.64    0.33   0.2     3.50
## cut        2 21551    5.00    0.00    5.00    5.00    0.00   5.0     5.00
## color      3 21551    3.53    1.66    4.00    3.48    1.48   1.0     7.00
## clarity    4 21551    4.46    1.71    4.00    4.39    1.48   1.0     8.00
## depth      5 21551   61.71    0.72   61.80   61.76    0.59  43.0    66.70
## table      6 21551   55.95    1.25   56.00   55.97    1.48  43.0    63.00
## price      7 21551 3457.54 3808.40 1810.00 2656.14 1630.86 326.0 18806.00
## x          8 21551    5.51    1.06    5.25    5.41    1.19   0.0     9.65
## y          9 21551    5.52    1.07    5.26    5.42    1.19   0.0    31.80
## z         10 21551    3.40    0.66    3.23    3.34    0.73   0.0     6.03
##            range  skew kurtosis    se
## carat       3.30  1.34     1.63  0.00
## cut         0.00   NaN      NaN  0.00
## color       6.00  0.19    -0.82  0.01
## clarity     7.00  0.36    -0.71  0.01
## depth      23.70 -1.44    22.33  0.00
## table      20.00  0.20     1.70  0.01
## price   18480.00  1.84     2.98 25.94
## x           9.65  0.66    -0.42  0.01
## y          31.80  1.30    15.99  0.01
## z           6.03  0.65    -0.36  0.00
\end{verbatim}

There is one more function \texttt{describeData} is this package which also results in first as well as last four (default) values.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{describeData}\NormalTok{(ggplot2}\SpecialCharTok{::}\NormalTok{diamonds)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## n.obs =  53940 of which  53940   are complete cases.   Number of variables =  10  of which all are numeric  TRUE  
##          variable # n.obs type    H1      H2   H3      H4   T1        T2
## carat*            1 53940    4  0.23    0.21 0.23    0.29 0.72      0.70
## cut*              2 53940    4 Ideal Premium Good Premium Good Very Good
## color*            3 53940    4     E       E    E       I    D         D
## clarity*          4 53940    4   SI2     SI1  VS1     VS2  SI1       SI1
## depth*            5 53940    4  61.5    59.8 56.9    62.4 63.1      62.8
## table*            6 53940    4    55      61   65      58   55        60
## price*            7 53940    4   326     326  327     334 2757      2757
## x*                8 53940    4  3.95    3.89 4.05    4.20 5.69      5.66
## y*                9 53940    4  3.98    3.84 4.07    4.23 5.75      5.68
## z*               10 53940    4  2.43    2.31 2.31    2.63 3.61      3.56
##               T3    T4
## carat*      0.86  0.75
## cut*     Premium Ideal
## color*         H     D
## clarity*     SI2   SI2
## depth*      61.0  62.2
## table*        58    55
## price*      2757  2757
## x*          6.15  5.83
## y*          6.12  5.87
## z*          3.74  3.64
\end{verbatim}

\hypertarget{using-skimr}{%
\section{\texorpdfstring{Using \texttt{skimr}}{Using skimr}}\label{using-skimr}}

Package \texttt{skimr} generates beautiful data EDA summary reports which can be customised as per one's taste. Full descriptions of this package may be seen \href{https://cran.r-project.org/web/packages/skimr/vignettes/skimr.html}{here}. For basic purposes we can use function \texttt{skim} from this package to get data EDA summary reports.

\begin{verbatim}
library(skimr)
skim(iris)
\end{verbatim}

\includegraphics[width=0.5\linewidth]{images/skmi1}

\hypertarget{viewing-relationships-between-different-variables}{%
\section{Viewing relationships between different variables}\label{viewing-relationships-between-different-variables}}

We can use package \texttt{PerformanceAnalytics} to generate and view relationships between different variables in the data. For this purpose function \texttt{PerformanceAnalytics::chart.Correlation()} may be used as shown below.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{library}\NormalTok{(PerformanceAnalytics))}

\NormalTok{USArrests }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(}\FunctionTok{where}\NormalTok{(is.numeric)) }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  PerformanceAnalytics}\SpecialCharTok{::}\FunctionTok{chart.Correlation}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{DauR_files/figure-latex/unnamed-chunk-345-1} 

}

\caption{Viewing relationships with PerformanceAnalytics}\label{fig:unnamed-chunk-345}
\end{figure}

As can be seen that it generates visualization of a Correlation Matrix of the numeric variables in the given data.

There is one more package \texttt{GGally} which also creates beautiful charts for viewing relationships. There are two functions in this package which are particularly useful.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The \texttt{ggpairs()} function of the GGally package allows to build a great scatterplot matrix. Scatterplots of each pair of numeric variable are drawn on the left part of the figure. Pearson correlation is displayed on the right. Variable distribution is available on the diagonal.
\item
  The \texttt{ggcorr()} function allows to visualize the correlation of each pair of variable as a square. Note that the method argument allows to pick the correlation type you desire.
\end{enumerate}

See the following example-

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{library}\NormalTok{(GGally))}
\NormalTok{USArrests }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select\_if}\NormalTok{(is.numeric) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggcorr}\NormalTok{(}\AttributeTok{label =} \ConstantTok{TRUE}\NormalTok{)}

\NormalTok{USArrests }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select\_if}\NormalTok{(is.numeric) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggpairs}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{DauR_files/figure-latex/two-corr-1} \includegraphics[width=0.5\linewidth]{DauR_files/figure-latex/two-corr-2} 

}

\caption{Scatterplot Matrix (Left) and Correlation plot (Right) produced in GGally}\label{fig:two-corr}
\end{figure}

\hypertarget{part-iii-probability-and-sampling-in-r}{%
\chapter*{Part-III: Probability and Sampling in R}\label{part-iii-probability-and-sampling-in-r}}
\addcontentsline{toc}{chapter}{Part-III: Probability and Sampling in R}

\hypertarget{probability-in-r}{%
\chapter{Probability in R}\label{probability-in-r}}

We will keep short here. Instead of learning all the concepts of probability, we will see how to calculate probability, densities, quantiles for nearly any type of distribution. R's powerhorse has four types of functions for each of the distributions associated called \texttt{pqdr} functions. Actually all these are prefixes. Consider a probability function \(P(X=x) = p\) for a variable \(x\) and \(p\) be the associated probability.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.5455}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1636}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1091}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.0909}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.0909}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Distribution
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
P
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Q
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
D
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
R
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Beta & pbeta & qbeta & dbeta & rbeta \\
Binomial & pbinom & qbinom & dbinom & rbinom \\
Cauchy & pcauchy & qcauchy & dcauchy & rcauchy \\
Chi-Square & pchisq & qchisq & dchisq & rchisq \\
Exponential & pexp & qexp & dexp & rexp \\
F & pf & qf & df & rf \\
Gamma & pgamma & qgamma & dgamma & rgamma \\
Geometric & pgeom & qgeom & dgeom & rgeom \\
Hypergeometric & phyper & qhyper & dhyper & rhyper \\
Logistic & plogis & qlogis & dlogis & rlogis \\
Log Normal & plnorm & qlnorm & dlnorm & rlnorm \\
Negative Binomial & pnbinom & qnbinom & dnbinom & rnbinom \\
Normal & pnorm & qnorm & dnorm & rnorm \\
Poisson & ppois & qpois & dpois & rpois \\
Student t & pt & qt & dt & rt \\
Studentized Range & ptukey & qtukey & dtukey & rtukey \\
Uniform & punif & qunif & dunif & runif \\
Weibull & pweibull & qweibull & dweibull & rweibull \\
Wilcoxon Rank Sum Statistic & pwilcox & qwilcox & dwilcox & rwilcox \\
Wilcoxon Signed Rank Statistic & psignrank & qsignrank & dsignrank & rsignrank \\
\end{longtable}

All these functions are vectorised. Let us explore these one by one.

\hypertarget{p-set-of-functions}{%
\section{\texorpdfstring{\texttt{p*()} set of functions}{p*() set of functions}}\label{p-set-of-functions}}

These set of functions give the cumulative \textbf{p}robability distribution of that probability function.

Example-1. What is the probability of a number being less than or equal to \texttt{25} in \texttt{Normal} distribution with \texttt{mean\ =\ 50} and \texttt{sd\ =\ 10}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pnorm}\NormalTok{(}\DecValTok{25}\NormalTok{, }\AttributeTok{mean =} \DecValTok{50}\NormalTok{, }\AttributeTok{sd =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.006209665
\end{verbatim}

On the contrary, the probability of a number being greater than or equal to 25 in the above distribution is-

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Either deduct probability from 1 }
\DecValTok{1} \SpecialCharTok{{-}} \FunctionTok{pnorm}\NormalTok{(}\DecValTok{25}\NormalTok{, }\AttributeTok{mean =} \DecValTok{50}\NormalTok{, }\AttributeTok{sd =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.9937903
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Or provide FALSE to lower.tail argument}
\FunctionTok{pnorm}\NormalTok{(}\DecValTok{25}\NormalTok{, }\AttributeTok{mean =} \DecValTok{50}\NormalTok{, }\AttributeTok{sd =} \DecValTok{10}\NormalTok{, }\AttributeTok{lower.tail =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.9937903
\end{verbatim}

Example-2: What is the probability of one or more heads out of two tosses of a fair coin (binomial distribution with \texttt{p\ =\ 0.5}).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pbinom}\NormalTok{(}\DecValTok{1}\NormalTok{, }\AttributeTok{size =} \DecValTok{2}\NormalTok{, }\AttributeTok{p =} \FloatTok{0.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.75
\end{verbatim}

\hypertarget{q-set-of-functions}{%
\section{\texorpdfstring{\texttt{q*()} set of functions}{q*() set of functions}}\label{q-set-of-functions}}

These set of functions, give \textbf{q}uantile which is the inverse of cumulative probability function. So if \(f\) is cdf (cumulative distribution function) of a given probability distribution then \(F\) the quantile is inverse of \texttt{f} i.e.~\(F = f^{-1}\). These are related by

\begin{equation} 
p = f(x)
\label{eq:s1}
\end{equation}

\begin{equation} 
x = F(x) = f^{-1}(x)
\label{eq:s2}
\end{equation}

Example- In the above same normal distribution (\texttt{mean\ =\ 50} and \texttt{sd\ =\ 10}) What is number below which 90\% of population will be distributed.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{qnorm}\NormalTok{(}\FloatTok{0.9}\NormalTok{, }\AttributeTok{mean =} \DecValTok{50}\NormalTok{, }\AttributeTok{sd =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 62.81552
\end{verbatim}

Similar to \texttt{cdf} here we may use \texttt{lower.tail} argument to find the number above which a population percent is distributed.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{qnorm}\NormalTok{(}\FloatTok{0.9}\NormalTok{, }\AttributeTok{mean =} \DecValTok{50}\NormalTok{, }\AttributeTok{sd =} \DecValTok{10}\NormalTok{, }\AttributeTok{lower.tail =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 37.18448
\end{verbatim}

\hypertarget{d-set-of-functions}{%
\section{\texorpdfstring{\texttt{d*()} set of functions}{d*() set of functions}}\label{d-set-of-functions}}

We saw that \texttt{p} group denotes \texttt{cdf}, \texttt{q} group denotes \texttt{inverse\ cdf}, but \texttt{d} group actually denotes probability \textbf{d}ensity function of a given distribution. Simply stating, this returns the height of probability distribution function for a given x value.

So what is expected probability of drawing exactly 2 heads out of two tosses of a single fair coin (i.e.~from a binomial distribution with probability \texttt{p\ =\ 0.5}).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{dbinom}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\AttributeTok{prob =} \FloatTok{0.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.25
\end{verbatim}

\hypertarget{r-set-of-functions}{%
\section{\texorpdfstring{\texttt{r*()} set of functions}{r*() set of functions}}\label{r-set-of-functions}}

These set of functions are used to generate \textbf{r}andom numbers from a Statistical distribution. So to generate \texttt{10} random numbers from Normal distribution with \texttt{mean\ =\ 50} and \texttt{sd\ =\ 10}, we can use \texttt{rnorm}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rnorm}\NormalTok{(}\DecValTok{10}\NormalTok{, }\AttributeTok{mean =} \DecValTok{50}\NormalTok{, }\AttributeTok{sd =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 33.10444 62.39496 48.91034 48.82758 51.83083 62.80555 32.72729 66.90184
##  [9] 55.03812 75.28337
\end{verbatim}

We can actually check this using histogram.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1234}\NormalTok{)}
\FunctionTok{hist}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(}\DecValTok{10000}\NormalTok{, }\DecValTok{50}\NormalTok{, }\DecValTok{10}\NormalTok{), }\AttributeTok{breaks =} \DecValTok{50}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.75\linewidth]{DauR_files/figure-latex/unnamed-chunk-353-1} 

}

\caption{Histogram of Random numbers generated out of Normal distribution}\label{fig:unnamed-chunk-353}
\end{figure}

\hypertarget{random-sampling-in-r}{%
\chapter{Random sampling in R}\label{random-sampling-in-r}}

International Standard On Auditing - 530 defines\footnote{\url{https://www.ifac.org/system/files/downloads/a027-2010-iaasb-handbook-isa-530.pdf}} audit sampling as \emph{the application of audit procedures to less than 100\% of items within a population of audit relevance such that all sampling units have a chance of selection in order to provide the auditor with a reasonable basis on which to draw conclusions about the entire population.} Statistical sampling is further defines as \emph{an approach to sampling having two characteristics - random selection of samples, and the use of probability theory to evaluate sample results, including measurement of sampling risk.}

Appendix 4 of ISA 53 further prescribes different statistical methods of sample selection. We will discuss here each type of sampling methodology used to sample records for audit.

\textbf{Prerequisites}

Load \texttt{tidyverse}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

\hypertarget{simple-random-sampling-with-and-without-replacement}{%
\section{Simple Random Sampling (With and without replacement)}\label{simple-random-sampling-with-and-without-replacement}}

In this method, records are selected completely at random, by generating random numbers e.g.~using random number tables, etc. Refer figure \ref{fig:simple} for illustration. We can replicate the method of random number generation in R. Even the method of random number generation can be reproducible, by fixing the random number seed. Mainly two functions will be used here \texttt{sample()}\index{sample() function} and \texttt{set.seed()}\index{set.seed() function} already discussed in section \ref{prob}. Since \texttt{sample()} function takes a vector as input and gives vector as output again, we can make use of \texttt{dplyr::slice\_sample()} function, discussed in section \ref{prob}, which operates on data frames instead.

\begin{figure}

{\centering \includegraphics[width=0.99\linewidth]{images/simple} 

}

\caption{Illustration of Simple Random Sampling}\label{fig:simple}
\end{figure}

Let's see this sampling on \texttt{iris} data. Suppose we have to select a sample of \texttt{n=12} records, without replacement-

\begin{verbatim}
dat <- iris # input data
# set the seed
set.seed(123)
# sample n records
dat %>% 
  slice_sample(n = 12, replace = FALSE)
\end{verbatim}

\begin{tabular}{r|r|r|r|l}
\hline
Sepal.Length & Sepal.Width & Petal.Length & Petal.Width & Species\\
\hline
4.3 & 3.0 & 1.1 & 0.1 & setosa\\
\hline
5.0 & 3.3 & 1.4 & 0.2 & setosa\\
\hline
7.7 & 3.8 & 6.7 & 2.2 & virginica\\
\hline
4.4 & 3.2 & 1.3 & 0.2 & setosa\\
\hline
5.9 & 3.0 & 5.1 & 1.8 & virginica\\
\hline
6.5 & 3.0 & 5.2 & 2.0 & virginica\\
\hline
5.5 & 2.5 & 4.0 & 1.3 & versicolor\\
\hline
5.5 & 2.6 & 4.4 & 1.2 & versicolor\\
\hline
5.8 & 2.7 & 5.1 & 1.9 & virginica\\
\hline
6.1 & 3.0 & 4.6 & 1.4 & versicolor\\
\hline
6.3 & 3.4 & 5.6 & 2.4 & virginica\\
\hline
5.1 & 2.5 & 3.0 & 1.1 & versicolor\\
\hline
\end{tabular}

The syntax is simple. In the first step we have fixed the random number seed for reproducibility. Using \texttt{slice\_sample()} we have selected \texttt{n=12} records without replacement (\texttt{replace\ =\ FALSE}).

\begin{quote}
If sample size is based on some proportion, we have to use \texttt{prop\ =\ .10} (say 10\%) instead of \texttt{n} argument. Moreover, if sampling is with replacement, we have to use \texttt{replace\ =\ TRUE}.
\end{quote}

\hypertarget{srs}{%
\section{Systematic random sampling}\label{srs}}

ISA 530 defines this sampling approach as \emph{`Systematic selection, in which the number of sampling units in the population is divided by the sample size to give a sampling interval, for example 50, and having determined a starting point within the first 50, each 50th sampling unit thereafter is selected. Although the starting point may be determined haphazardly, the sample is more likely to be truly random if it is determined by use of a computerized random number generator or random number tables. When using systematic selection, the auditor would need to determine that sampling units within the population are not structured in such a way that the sampling interval corresponds with a particular pattern in the population.'} Refer figure \ref{fig:systematic} for illustration.

\begin{figure}

{\centering \includegraphics[width=0.99\linewidth]{images/systematic} 

}

\caption{Illustration of Systematic Random Sampling}\label{fig:systematic}
\end{figure}

We can replicate this approach again following two steps-

\textbf{Step-1:} Select \texttt{n} as the sample size. Then generate a maximum starting point say \texttt{s} by dividing number of rows in the data by \texttt{n}. Thereafter we have to choose a starting point from \texttt{1:s}. We can use sample function here. Let's say this starting number is \texttt{s1}. Then we have to generate an arithmetic sequence, say \texttt{rand\_seq} starting from \texttt{s1} and increasing every \texttt{s} steps thereafter with total \texttt{n} terms.

\textbf{Step-2:} In the next step we will shuffle the data by using \texttt{slice\_sample} and select a sample using function \texttt{filter}.

The methodology is replicated as

\begin{verbatim}
set.seed(123)
n <- 15 # sample size
s <- floor(nrow(dat)/n)
s1 <- sample(1:s, 1, replace = FALSE)
rand_seq <- seq(s1, by = s, length.out = n)
dat %>% 
  slice_sample(prop = 1) %>% 
  filter(row_number() %in% rand_seq)
  
\end{verbatim}

\begin{tabular}{r|r|r|r|l}
\hline
Sepal.Length & Sepal.Width & Petal.Length & Petal.Width & Species\\
\hline
7.7 & 3.8 & 6.7 & 2.2 & virginica\\
\hline
6.1 & 2.8 & 4.0 & 1.3 & versicolor\\
\hline
7.6 & 3.0 & 6.6 & 2.1 & virginica\\
\hline
7.2 & 3.2 & 6.0 & 1.8 & virginica\\
\hline
7.2 & 3.0 & 5.8 & 1.6 & virginica\\
\hline
6.3 & 2.7 & 4.9 & 1.8 & virginica\\
\hline
4.8 & 3.1 & 1.6 & 0.2 & setosa\\
\hline
6.4 & 2.8 & 5.6 & 2.1 & virginica\\
\hline
5.6 & 3.0 & 4.5 & 1.5 & versicolor\\
\hline
4.9 & 2.5 & 4.5 & 1.7 & virginica\\
\hline
5.9 & 3.2 & 4.8 & 1.8 & versicolor\\
\hline
6.3 & 3.3 & 6.0 & 2.5 & virginica\\
\hline
5.4 & 3.7 & 1.5 & 0.2 & setosa\\
\hline
6.7 & 3.1 & 4.4 & 1.4 & versicolor\\
\hline
5.2 & 3.5 & 1.5 & 0.2 & setosa\\
\hline
\end{tabular}

\hypertarget{probability-proportionate-to-size-with-or-without-replacement-a.k.a-monetary-unit-sampling}{%
\section{Probability Proportionate to size (with or without replacement) a.k.a monetary unit sampling}\label{probability-proportionate-to-size-with-or-without-replacement-a.k.a-monetary-unit-sampling}}

This sampling approach is defined in ISA-530 as \emph{``a type of value-weighted selection in which sample size, selection and evaluation results in a conclusion in monetary amounts.''}

Our methodology is not much difference from methodology adopted in section \ref{srs} except that we will make use of \texttt{weight\_by\ =} argument now.

Let's use \texttt{state.x77} data that comes with base R. Since the data is in matrix format, let's first convert it data frame using \texttt{as.data.frame()} first.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(state.x77)}
\end{Highlighting}
\end{Shaded}

Other steps are simple.

\begin{verbatim}
set.seed(123)
dat %>% 
  slice_sample(n=12, weight_by = Population)
\end{verbatim}

\begin{tabular}{l|r|r|r|r|r|r|r|r}
\hline
  & Population & Income & Illiteracy & Life Exp & Murder & HS Grad & Frost & Area\\
\hline
Pennsylvania & 11860 & 4449 & 1.0 & 70.43 & 6.1 & 50.2 & 126 & 44966\\
\hline
Kentucky & 3387 & 3712 & 1.6 & 70.10 & 10.6 & 38.5 & 95 & 39650\\
\hline
Michigan & 9111 & 4751 & 0.9 & 70.63 & 11.1 & 52.8 & 125 & 56817\\
\hline
Oregon & 2284 & 4660 & 0.6 & 72.13 & 4.2 & 60.0 & 44 & 96184\\
\hline
Utah & 1203 & 4022 & 0.6 & 72.90 & 4.5 & 67.3 & 137 & 82096\\
\hline
California & 21198 & 5114 & 1.1 & 71.71 & 10.3 & 62.6 & 20 & 156361\\
\hline
Virginia & 4981 & 4701 & 1.4 & 70.08 & 9.5 & 47.8 & 85 & 39780\\
\hline
Arizona & 2212 & 4530 & 1.8 & 70.55 & 7.8 & 58.1 & 15 & 113417\\
\hline
Georgia & 4931 & 4091 & 2.0 & 68.54 & 13.9 & 40.6 & 60 & 58073\\
\hline
Massachusetts & 5814 & 4755 & 1.1 & 71.83 & 3.3 & 58.5 & 103 & 7826\\
\hline
Hawaii & 868 & 4963 & 1.9 & 73.60 & 6.2 & 61.9 & 0 & 6425\\
\hline
New Jersey & 7333 & 5237 & 1.1 & 70.93 & 5.2 & 52.5 & 115 & 7521\\
\hline
\end{tabular}

\hypertarget{stratified-random-sampling}{%
\section{Stratified random sampling}\label{stratified-random-sampling}}

Stratification is defined in ISA-530 as \emph{the process of dividing a population into sub-populations, each of which is a group of sampling units which have similar characteristics (often monetary value).} Thus, stratified random sampling may imply any of the afore-mentioned sampling techniques applied to individual strata instead of whole population. Refer figure \ref{fig:strata} for illustration.

\begin{figure}

{\centering \includegraphics[width=0.99\linewidth]{images/stratified} 

}

\caption{Illustration of Stratified Random Sampling}\label{fig:strata}
\end{figure}

The function \texttt{dplyr::group\_by()} will be used here for stratification. Thereafter we can proceed for sampling described as above.

Example Data: - Let's include region in \texttt{state.x77} data using \texttt{dplyr::bind\_cols}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat }\OtherTok{\textless{}{-}} \FunctionTok{bind\_cols}\NormalTok{(}
  \FunctionTok{as.data.frame}\NormalTok{(state.x77),}
  \FunctionTok{as.data.frame}\NormalTok{(state.region)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Let's see first 6 rows of this data

\begin{tabular}{l|r|r|r|r|r|r|r|r|l}
\hline
  & Population & Income & Illiteracy & Life Exp & Murder & HS Grad & Frost & Area & state.region\\
\hline
Alabama & 3615 & 3624 & 2.1 & 69.05 & 15.1 & 41.3 & 20 & 50708 & South\\
\hline
Alaska & 365 & 6315 & 1.5 & 69.31 & 11.3 & 66.7 & 152 & 566432 & West\\
\hline
Arizona & 2212 & 4530 & 1.8 & 70.55 & 7.8 & 58.1 & 15 & 113417 & West\\
\hline
Arkansas & 2110 & 3378 & 1.9 & 70.66 & 10.1 & 39.9 & 65 & 51945 & South\\
\hline
California & 21198 & 5114 & 1.1 & 71.71 & 10.3 & 62.6 & 20 & 156361 & West\\
\hline
Colorado & 2541 & 4884 & 0.7 & 72.06 & 6.8 & 63.9 & 166 & 103766 & West\\
\hline
\end{tabular}

We can check a summary of number of States per region

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  tibble}\SpecialCharTok{::}\FunctionTok{rownames\_to\_column}\NormalTok{(}\StringTok{\textquotesingle{}State\textquotesingle{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# this step will not be }
                                  \CommentTok{\# used in databases without row names}
  \FunctionTok{group\_by}\NormalTok{(state.region) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{states =} \FunctionTok{n}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 4 x 2
##   state.region  states
##   <fct>          <int>
## 1 Northeast          9
## 2 South             16
## 3 North Central     12
## 4 West              13
\end{verbatim}

\textbf{Case-1:} When the sample size is constant for all strata. Say \texttt{2} records per region.

\begin{verbatim}
set.seed(123)
n <- 2
dat %>% 
  tibble::rownames_to_column('State') %>% # this step will not be used in databases without row names
  group_by(state.region) %>% 
  slice_sample(n=n) %>% 
  ungroup()
\end{verbatim}

\begin{tabular}{l|r|r|r|r|r|r|r|r|l}
\hline
State & Population & Income & Illiteracy & Life Exp & Murder & HS Grad & Frost & Area & state.region\\
\hline
Massachusetts & 5814 & 4755 & 1.1 & 71.83 & 3.3 & 58.5 & 103 & 7826 & Northeast\\
\hline
New York & 18076 & 4903 & 1.4 & 70.55 & 10.9 & 52.7 & 82 & 47831 & Northeast\\
\hline
Delaware & 579 & 4809 & 0.9 & 70.06 & 6.2 & 54.6 & 103 & 1982 & South\\
\hline
North Carolina & 5441 & 3875 & 1.8 & 69.21 & 11.1 & 38.5 & 80 & 48798 & South\\
\hline
Indiana & 5313 & 4458 & 0.7 & 70.88 & 7.1 & 52.9 & 122 & 36097 & North Central\\
\hline
Minnesota & 3921 & 4675 & 0.6 & 72.96 & 2.3 & 57.6 & 160 & 79289 & North Central\\
\hline
Utah & 1203 & 4022 & 0.6 & 72.90 & 4.5 & 67.3 & 137 & 82096 & West\\
\hline
Hawaii & 868 & 4963 & 1.9 & 73.60 & 6.2 & 61.9 & 0 & 6425 & West\\
\hline
\end{tabular}

\textbf{Case-2:} When the sample size or proportion is different among strata.
This time let us assume that column for \emph{stratum} is not directly available in the data.\\
- Say, 20\% of States having Population upto \texttt{1000};
- 30\% of States having population greater than \texttt{1000} but upto \texttt{5000} and finally;
- 50\% of states having population more than \texttt{5000} have to be sampled.

In this scenario, our strategy would be use \texttt{purrr::map2\_dfr()} function after splitting the data with \texttt{group\_split()} function.

Syntax would be

\begin{verbatim}
# define proportions
props <- c(0.2, 0.3, 0.5)

# set seed
set.seed(123)

# take data
dat %>% 
  # reduntant step where data has no column names
  tibble::rownames_to_column('State') %>%
  # create column according to stratums
  mutate(stratum = cut(Population, c(0, 1000, 5000, max(Population)),
                      labels = c("Low", "Mid", "High"))) %>% 
  # split data into groups
  group_split(stratum) %>% 
  # sample in each group
  map2_dfr(props,
           .f = function(d, w) slice_sample(d, prop = w))
\end{verbatim}

We may check the sample selected across each stratum

\begin{tabular}{l|r|r}
\hline
stratum & Total & Selected\\
\hline
Low & 12 & 2\\
\hline
Mid & 26 & 7\\
\hline
High & 12 & 6\\
\hline
\end{tabular}

\hypertarget{cluster-sampling}{%
\section{Cluster sampling}\label{cluster-sampling}}

ISA 530 does not explicitly define cluster sampling. Actually this sampling is sampling of strata and we can apply above mentioned techniques easily to sample clusters. E.g. in the sample data above, we can sample say, 2 clusters (or regions).

Thus, our strategy would be first to sample groups from unique available values and thereafter filter all the records.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# set the seed}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\CommentTok{\# sample clusters}
\NormalTok{clusters }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(}
  \FunctionTok{unique}\NormalTok{(dat}\SpecialCharTok{$}\NormalTok{state.region),}
  \AttributeTok{size =} \DecValTok{2}
\NormalTok{)}
\CommentTok{\# filter all records in above clusters}
\NormalTok{clust\_samp }\OtherTok{\textless{}{-}}\NormalTok{ dat }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(state.region }\SpecialCharTok{\%in\%}\NormalTok{ clusters)}
\CommentTok{\# check number of records}
\NormalTok{clust\_samp}\SpecialCharTok{$}\NormalTok{state.region }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{table}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## .
##     Northeast         South North Central          West 
##             9             0            12             0
\end{verbatim}

\hypertarget{part-iv-machine-learning-in-r}{%
\chapter*{Part-IV: Machine Learning in R}\label{part-iv-machine-learning-in-r}}
\addcontentsline{toc}{chapter}{Part-IV: Machine Learning in R}

\hypertarget{linear-regression}{%
\chapter{Linear Regression}\label{linear-regression}}

Regression models are a class of statistical models that let us explore the relationship between a response variable and one or more explanatory variables. If such a relationship exists and is detected, we can make predictions about the value of the response variable given some explanatory variables. As an example, if a relationship between number of employees in an office and its monthly expenses on salary is established, we can predict the monthly expenses incurred by that office on salary if we know the number of employees. That lets us do thought of experiments like asking how much the a new company had to pay say 10 employees on salary expenses monthly.

Explanatory variables are sometimes also referred to as \emph{regressors} or \emph{independent} or \emph{predictor} variables whereas response variable is also called as \emph{dependent} or \emph{outcome} variable. When the response variable is numeric and the relationship is linear, the regression is called as linear regression. Of course, there are certain other assumptions, which we will discuss later on in the chapter.

\hypertarget{basic-concepts-1}{%
\section{Basic concepts}\label{basic-concepts-1}}

Before we start creating regression models, it's always a good practice to visualize the data. It will help us ascertaining the nature of relationship between the predictors and response variable, if any. To visualize the relationship between two numeric variables, we can use a scatter plot. See the two examples in figure \ref{fig:concept}. In the first example (plot) we can see a near perfect linear relationship between GNP and Population of the countries, whereas a moderate but negative relationship between two variables is seen in second example.

\begin{figure}

{\centering \includegraphics[width=0.47\linewidth,height=0.45\textheight]{DauR_files/figure-latex/concept-1} \includegraphics[width=0.47\linewidth,height=0.45\textheight]{DauR_files/figure-latex/concept-2} 

}

\caption{Linear Regression - Intuition}\label{fig:concept}
\end{figure}

Linear regression is a perfect model to predict outcome or response variable when it has linear relationship with explanatory variables. In that case our predicted values will lie on the assumed line (in linear relationship) ideally. Maths behind estimating or predicting outcomes is thus, finding the following algebraic equation \eqref{eq:lr1}.

\begin{equation} 
y = mx + c
\label{eq:lr1}
\end{equation}

Where -

\begin{itemize}
\tightlist
\item
  \texttt{m} is the slope of the line (in linear relationship)
\item
  \texttt{c} is the intercept of Y-axis. (value of \(y\) when \(x\) is \texttt{0} )
\end{itemize}

Interpreting this equation in real world is like estimating the coefficients i.e.~both \(m\) and \(c\) in above equation. The goal of our exercise will thus be to estimate the best values of \(m\) and \(c\). These two parameters, \(m\) and \(c\) are sometimes also referred to as \(\beta_1\) and \(\beta_0\) respectively. Those familiar with mathematics behind fitting the equation of a line (in two dimensional space), may know that we require only two variables (data points i.e.~\(x\) and \(y\) value pairs) values to find these parameters. So it means, that if we have a fair amount of data points available (which will in rarest of circumstances be collinear i.e.~lying on one same line), we can actually get many such regression lines. Our goal is thus, to find best of these lines. But, how?

To answer this question, let us also understand that the values of \(x\) and \(y\) pairs in actual practice, don't lie on the regression line because these points will rarely be collinear (See plots in Figure \ref{fig:concept} and note that none of the data point actually lie on the line). So for each data point of response variable, there is an actual value and one fitted value (the one falling on the regression line). The difference between these values is called \emph{error term} or \emph{residual}. Technically these are not errors but random noise that the model is not able to explain. Mathematically, if \(\hat{y}\) is fitted value, \(y\) is actual value, the difference also called error (of prediction) term say \(\epsilon\) can be denoted as -

\begin{equation} 
\epsilon = y - \hat{y}
\label{eq:lr2}
\end{equation}

OR, we can say that

\begin{equation} 
y = \beta_0 + \beta_1x + \epsilon
\label{eq:lr3}
\end{equation}

Now, one method to find best fit regression line is to minimize the error terms. Theoretically, it means to capture pattern/relationship between data points as much as possible so that what's left behind is true random noise. One way could be to minimise the mean of these error terms. But these error terms can be both positive or negative. See figure \ref{fig:errors}. So to ensure that these are not cancelled out while taking mean, we can minimise either the mean of their absolute values or their squares. Most commonly accepted method is to take mean of squares and minimise it. One of the benefit of adopting it over another, is that while squaring errors or residuals, large residuals get higher weight than lower residuals. That's why this linear regression technique is also sometimes referred to as \textbf{Ordinary Least Squares} or \textbf{OLS} regression.

\begin{figure}

{\centering \includegraphics[height=0.35\textheight]{DauR_files/figure-latex/errors-1} 

}

\caption{Residuals - Intuition and Concept}\label{fig:errors}
\end{figure}

\hypertarget{simple-linear-regression-in-r}{%
\section{Simple Linear Regression in R}\label{simple-linear-regression-in-r}}

Don't worry, in R we do not have to do this minimisation job ourselves. In fact, base R has a fantastic function \texttt{lm} which can fit a best regression line, for a given set of variables, for us. The syntax is simple.

\begin{verbatim}
lm(formula, data, ...)
\end{verbatim}

where -

\begin{itemize}
\tightlist
\item
  \texttt{formula} an object of class ``formula'' (or one that can be coerced to that class): a symbolic description of the model to be fitted. For our example above, we can write \texttt{y\ \textasciitilde{}\ x}
\item
  \texttt{data} an optional data frame.
\end{itemize}

It actually returns a special object, which can be printed directly like other R objects. However, it is best printed with the \texttt{summary} function. Firstly we will see an example of simple linear regression which is a linear regression with only one independent variable.

Example-1: \textbf{Problem Statement:} \texttt{iris} which is a famous (Fisher's or Anderson's) data set, loaded by default in R, gives the measurements in centimeters of the variables sepal length (\texttt{Sepal.Length}) and width (\texttt{Sepal.Width}) and petal length (\texttt{Petal.Length}) and width (\texttt{Petal.Width}), respectively, for 50 flowers from each of 3 species (\texttt{Species}) of iris. The species are Iris \texttt{setosa}, \texttt{versicolor}, and \texttt{virginica}. Let us try to establish a relationship between \texttt{Sepal.Length} and \texttt{Sepal.Width} variables of \texttt{setosa} Species i.e.~iris` data-set, (first 50 records only).

As already stated above, it is always a good practice to visualize the data, if possible. So let's make a scatter-plot, as seen in Figure \ref{fig:ex1plot}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris }\SpecialCharTok{\%\textgreater{}\%} 
  \CommentTok{\# Extract First 50 records}
  \FunctionTok{head}\NormalTok{(}\DecValTok{50}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(Sepal.Length, Sepal.Width)) }\SpecialCharTok{+}
  \CommentTok{\# Scatter plot}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \CommentTok{\# adding a trend line without error bands}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{\textquotesingle{}lm\textquotesingle{}}\NormalTok{, }\AttributeTok{se=}\ConstantTok{FALSE}\NormalTok{, }\AttributeTok{formula =} \StringTok{"y\textasciitilde{}x"}\NormalTok{) }\SpecialCharTok{+}
  \CommentTok{\# Theme Black and White}
  \FunctionTok{theme\_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[height=0.3\textheight]{DauR_files/figure-latex/ex1plot-1} 

}

\caption{Relationship between sepal widths and lengths in setosa species}\label{fig:ex1plot}
\end{figure}

The relationship seem fairly linear (Figure \ref{fig:ex1plot}), so let's build the model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lin\_reg1 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ Sepal.Width }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Sepal.Length, }\AttributeTok{data =}\NormalTok{ iris[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{50}\NormalTok{,])}

\CommentTok{\# Let us print the object directly}
\NormalTok{lin\_reg1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = Sepal.Width ~ Sepal.Length, data = iris[1:50, ])
## 
## Coefficients:
##  (Intercept)  Sepal.Length  
##      -0.5694        0.7985
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Try printing it with summary()}
\FunctionTok{summary}\NormalTok{(lin\_reg1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = Sepal.Width ~ Sepal.Length, data = iris[1:50, ])
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.72394 -0.18273 -0.00306  0.15738  0.51709 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(>|t|)    
## (Intercept)   -0.5694     0.5217  -1.091    0.281    
## Sepal.Length   0.7985     0.1040   7.681 6.71e-10 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.2565 on 48 degrees of freedom
## Multiple R-squared:  0.5514, Adjusted R-squared:  0.542 
## F-statistic: 58.99 on 1 and 48 DF,  p-value: 6.71e-10
\end{verbatim}

Observing the outputs above, we can notice that simply printing the object returns coefficients whereas printing with \texttt{summary} gives us a lot of other information. But how to interpret this information? Before proceeding to interpret the output, let us understand a few more concepts which are essential here. These concepts are basically some assumptions, which we have made while finding the best fit line or in other words estimating the parameters statistically.

\hypertarget{assumptions-of-linear-regression}{%
\section{Assumptions of Linear Regression}\label{assumptions-of-linear-regression}}

Linear regression makes several assumptions about the data, such as :

\begin{itemize}
\tightlist
\item
  \emph{Linearity of the data}. The relationship between the predictor (\(x\)) and the outcome (\(y\)) is assumed to be linear. Obviously, the relationship should be linear. If we would try to fit non-linear relationship through linear regression our results wouldn't be correct. Also when we use multiple predictors, as we will see shortly, we make another assumption that the model is additive in nature besides being linear. Refer figure \ref{fig:linearity}. It is clear that if we try to establish a linear relationship (red line) when it is actually cubic (green dashed line) our model will give erroneous results.
\end{itemize}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{DauR_files/figure-latex/linearity-1} 

}

\caption{Is the relationship linear?}\label{fig:linearity}
\end{figure}

\begin{itemize}
\item
  \emph{Normality of residuals}. The residuals are assumed to be normally distributed. Actually, this assumption is followed by the assumption that our dependent variable is normally distributed and not concentrated anywhere. Thus, if dependent variable is normally distributed, and if we have been able to capture the relationship available, then what has been left must be true noise and it should be normally distributed with a mean of \(0\).
\item
  \emph{Homogeneity of residuals variance}. The residuals are assumed to have a constant variance, statistically known as homoscedasticity. It shows that residuals that are left out of regression model are true noise and not related to fitted values, which in that case would have meant that the model was insufficient to capture the actual relationship. Heteroscedasticity (the violation of homoskedasticity) is present when the size of the error term differs across values of an independent variable. This can be best understood by plots in Figures \ref{fig:homod} where residuals in left plot indicate equal variance and thus homoskedasticity whereas in the right plot heteroskedasticity is indicated clearly.
\end{itemize}

\begin{figure}

{\centering \includegraphics[width=0.47\linewidth]{DauR_files/figure-latex/homod-1} \includegraphics[width=0.47\linewidth]{DauR_files/figure-latex/homod-2} 

}

\caption{Homoskedasticity (left) Vs. Heteroskedasticity (right)
Sample data created by author for demonstration only}\label{fig:homod}
\end{figure}

\begin{itemize}
\tightlist
\item
  \emph{Independence} of residuals error terms. This assumption is also followed by original assumption that our dependent variable is independent in itself and any \(y\) value is not dependent upon another set of \(y\) values. In our example, we can understand it like that Sepal width of one sample is not affecting width of another.
\end{itemize}

\hypertarget{interpreting-the-output-of-lm}{%
\section{\texorpdfstring{Interpreting the output of \texttt{lm}}{Interpreting the output of lm}}\label{interpreting-the-output-of-lm}}

Let us discuss each of the component or section of \texttt{lm} output, hereinafter referred to as model.

\hypertarget{call}{%
\subsection{\texorpdfstring{\texttt{call}}{call}}\label{call}}

The \texttt{call} section shows us the formula that we have used to fit the regression model. So \texttt{Sepal.Width} is our dependent or response variable, \texttt{Sepal.Length} is predictor or independent variable. These variables refer to our data-set which is, first 50 rows of \texttt{iris} or \texttt{iris{[}1:50,{]}}.

\hypertarget{residuals}{%
\subsection{\texorpdfstring{\texttt{Residuals}}{Residuals}}\label{residuals}}

This depicts the quantile or five point summary of error terms or the residuals, which also as discussed, are the difference between the actual values and the predicted values. We can generate these same values by taking the actual values of \(y\) variable and subtracting these from its predicted values of the model.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(iris}\SpecialCharTok{$}\NormalTok{Sepal.Width[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{50}\NormalTok{] }\SpecialCharTok{{-}}\NormalTok{ lin\_reg1}\SpecialCharTok{$}\NormalTok{fitted.values)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. 
## -0.723945 -0.182730 -0.003062  0.000000  0.157380  0.517085
\end{verbatim}

Ideally, the median of error values/residuals should be centered around \texttt{0} thus telling us that these are somewhat symmetrical and our model is predicting fairly at both positive/higher and negative/lower side. Any skewness will thus show that errors are not normally distributed and our model may be biased towards that side.

In our example, we can observe a slight left-skewed distribution of error terms which indicates that our model is not doing that well for higher sepal lengths as it is doing for lower ones. This can also be seen in Figure \ref{fig:ex1hist} i.e.~histogram of residuals.

\begin{figure}

{\centering \includegraphics{DauR_files/figure-latex/ex1hist-1} 

}

\caption{Histogram of Resuduals}\label{fig:ex1hist}
\end{figure}

\hypertarget{coefficients}{%
\subsection{\texorpdfstring{\texttt{Coefficients}}{Coefficients}}\label{coefficients}}

Remember that these were our goals of the exercise. Here coefficients will be written as coefficient for that predictor and an intercept term, i.e.~our \(\beta_1\) and \(\beta_0\) respectively. We can easily interpret that positive coefficients means positive relation and negative coefficient means value of outcome variable will decrease as corresponding independent variable increase. So, in our example, we can deduce that -

\begin{itemize}
\tightlist
\item
  for \texttt{0} sepal length, the sepal width will be \texttt{-0.5694} (Though mathematically only as physically having \texttt{0} and negative width and lengths are not possible).
\item
  for every unit i.e.~\texttt{1} i.e.~unit increase in sepal length, width increases by \texttt{0.7985}
\end{itemize}

Thus our regression line equation is -

\begin{equation} 
Sepal.Width = -0.5694 + 0.7985 * Sepal.Length
\label{eq:lr4}
\end{equation}

\textbf{Now one thing to note here that, since we are adopting OLS approach to find out the (estimated) equation of best line, the coefficients we have arrived at, are only the estimated values of mean of these coefficients. Actually, we started (behind the scenes) with a null hypothesis that there is no linear relationship or, in other words, that the coefficients are zero. Alternate hypothesis, in this case, as you may have guessed by now, was that these coefficients are not zero. The coefficients may follow a statistical/ probabilistic distribution and thus, we may infer only its estimated (mean) value.}

\textbf{We can see the confidence intervals of each of the coefficient using function \texttt{confint}. By default, the 95\% confidence intervals may be generated. See the following.}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{confint}\NormalTok{(lin\_reg1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                   2.5 %    97.5 %
## (Intercept)  -1.6184048 0.4795395
## Sepal.Length  0.5894925 1.0075641
\end{verbatim}

Now, these estimated distribution must also have some standard error, a probability statistic and a p-value.

\begin{itemize}
\tightlist
\item
  The \emph{standard error} of the coefficient is an estimate of the standard deviation of the coefficient. It tells us how much uncertainty there is with our coefficient. We can build confidence intervals of coefficients using this statistic, as shown above.
\item
  The t-statistic is simply the estimated coefficient divided by the standard error. By now you may have understood that we are applying student's t-distribution while estimating the parameters.
\item
  Finally \texttt{p\ value} i.e.~\textbf{Pr(\textgreater\textbar t\textbar)} gives us the probability value and tells us how significant is our coefficient value.
\end{itemize}

\hypertarget{signif.-codes}{%
\subsection{\texorpdfstring{\texttt{Signif.\ codes}}{Signif. codes}}\label{signif.-codes}}

These are nothing but code legends which are simply telling us how significant out p-value may be for each case. Notice three asterisks in from of coefficient estimate of \texttt{Sepal.Length} which indicate that coefficient is extremely significant and we can reject null hypothesis that \(\beta_1\) is \(0\).

These codes give us a quick way to visually see which coefficients are significant to the model.

\hypertarget{residual-standard-error}{%
\subsection{\texorpdfstring{\texttt{Residual\ standard\ error}}{Residual standard error}}\label{residual-standard-error}}

The residual standard error is a measure, and one of the metrics, telling us how well the model fits the data. This is actually the standard deviation of all error terms with the difference that instead of taking \texttt{n} terms we are taking \texttt{degrees\ of\ freedom}.

\begin{equation} 
RSE = \sqrt{\frac{1}{(n-2)} \sum_{i=1}^n (y_i - \hat{y_i})^2}
\label{eq:lr5}
\end{equation}

Obviously \(df\) is \(n-2\), as there is one regressor and one intercept. We can verify equation \eqref{eq:lr5} by calculation.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sqrt}\NormalTok{(}\FunctionTok{sum}\NormalTok{((iris[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{50}\NormalTok{, }\StringTok{"Sepal.Width"}\NormalTok{] }\SpecialCharTok{{-}}\NormalTok{ lin\_reg1}\SpecialCharTok{$}\NormalTok{fitted.values)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}\SpecialCharTok{/}\DecValTok{48}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.2565263
\end{verbatim}

\hypertarget{r-squared-both-multiple-and-adjusted}{%
\subsection{\texorpdfstring{\texttt{R-Squared} both Multiple and Adjusted}{R-Squared both Multiple and Adjusted}}\label{r-squared-both-multiple-and-adjusted}}

\emph{Multiple R-Squared} is also called the coefficient of determination. Often this is the most cited measurement of how well the model is fitting to the data. It tells us what percentage of the variation within our dependent variable that the independent variable is explaining through the model. By looking at output we can say that about 55\% of variation is explained through the model. We will discuss it in detail, in the next section.

\emph{Adjusted R squared} on the other hand, shows us what percentage of the variation within our dependent variable that all predictors are explaining. Thus, it is helpful when there are multiple regressors i.e.~in Multiple Linear Regression. The difference between these two metrics might be subtle where we adjust for the variance attributed by adding multiple variables.

\hypertarget{f-statistic-and-p-value}{%
\subsection{\texorpdfstring{\texttt{F-Statistic} and \texttt{p-value}}{F-Statistic and p-value}}\label{f-statistic-and-p-value}}

So why a p-value again? Is there a hypothesis again? Yes, When running a regression model, a hypothesis test is being run on the global model, that there is no relationship between the dependent variable and the independent variable(s). The alternative hypothesis is that there is a relationship. In other words, alternate hypothesis means at least one coefficient of regression is non-zero. This hypothesis is tested on F-statistic and hence the two values. \texttt{p-value} in our example is very small which lead us to reject the null hypothesis and conclude that there is strong evidence that a relationship does exist between \texttt{Sepal.Length} and \texttt{Sepal.Width}.

The reason for this test is based on the fact that if we run multiple hypothesis tests on our coefficients, it is likely that a variable is included which isn't actually significant.

\hypertarget{model-evaluation-metrics}{%
\section{Model Evaluation Metrics}\label{model-evaluation-metrics}}

To evaluate the model's performance and accuracy, evaluation metrics are needed. There are several types of metrics which are used to evaluate the performance of model we have built. We will discuss a few of them here -

\hypertarget{mae---mean-absolute-error}{%
\subsection{MAE - Mean Absolute Error}\label{mae---mean-absolute-error}}

As the name suggests it is mean of absolute values of errors or residuals. The formula thus, can be written as equation \eqref{eq:lr6}.

\begin{equation} 
{MAE} = \frac{1}{N}\sum_{i = 1}^{N}{\lvert}{y_i - \hat{y_i}}{\rvert}
\label{eq:lr6}
\end{equation}

Clearly, it is average value of residuals and a larger value denotes lesser accurate model. In isolation, the MAE is not very useful, however, to compare performance of several models while fitting a best regression model, obviously we can use this metric to choose a better model.

Moreover, once we extract \texttt{\$residuals} out of the model, calculating the metric is easy. In our example-

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Mean Absolute Error}
\NormalTok{lin\_reg1}\SpecialCharTok{$}\NormalTok{residuals }\SpecialCharTok{|\textgreater{}} \FunctionTok{abs}\NormalTok{() }\SpecialCharTok{|\textgreater{}} \FunctionTok{mean}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.199243
\end{verbatim}

\hypertarget{mse---mean-square-error-and-rmse---root-mean-square-error}{%
\subsection{MSE - Mean Square Error and RMSE - Root Mean Square Error}\label{mse---mean-square-error-and-rmse---root-mean-square-error}}

Again as the name suggests, the mean of square of all residuals is mean square error or MSE. The formula may be written as in equation \eqref{eq:lr7}.

\begin{equation} 
{MSE} = \frac{1}{N}\sum_{i = 1}^{N}({y_i - \hat{y_i}})^2
\label{eq:lr7}
\end{equation}

MSE penalizes the higher residuals by squaring them. It may be thus thought as weighted average where more and more weight is allocated as the residual value rises. Similar, to MAE, we can use this metric to choose a better model out of the several validating models.

Interestingly, by definition it is also cost function in regression, as while finding parameters, we are actually minimising MSE only. Similar to MAE, calculating this require no special skills.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Mean Square Error}
\NormalTok{lin\_reg1}\SpecialCharTok{$}\NormalTok{residuals}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{|\textgreater{}} \FunctionTok{mean}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.0631735
\end{verbatim}

RMSE or root mean square error is square root of MSE.

\begin{equation} 
{RMSE} = \sqrt{\frac{1}{N}\sum_{i = 1}^{N}({y_i - \hat{y_i}})^2}
\label{eq:lr8}
\end{equation}

In our Example-

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Root Mean Square Error}
\NormalTok{lin\_reg1}\SpecialCharTok{$}\NormalTok{residuals}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{|\textgreater{}} \FunctionTok{mean}\NormalTok{() }\SpecialCharTok{|\textgreater{}} \FunctionTok{sqrt}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.2513434
\end{verbatim}

\hypertarget{mape---mean-absolute-percentage-error}{%
\subsection{MAPE - Mean Absolute Percentage Error}\label{mape---mean-absolute-percentage-error}}

This metric instead of taking residual value in isolation, takes residual value as percentage of actual values. The formula is thus,

\begin{equation} 
{MAPE} = \frac{1}{N}\sum_{i = 1}^{N}{\lvert}\frac{({y_i - \hat{y_i}})}{y_i}\cdot{100}\%{\rvert}
\label{eq:lr9}
\end{equation}

Clearly, MAPE is independent of the scale of the variables since its error estimates are in terms of percentage. In our example, we can calculate MAPE-

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Mean Absolute Percentage Error}
\NormalTok{\{lin\_reg1}\SpecialCharTok{$}\NormalTok{residuals}\SpecialCharTok{/}\NormalTok{iris}\SpecialCharTok{$}\NormalTok{Sepal.Width[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{50}\NormalTok{]\} }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  \{}\FunctionTok{abs}\NormalTok{(.)}\SpecialCharTok{*}\DecValTok{100}\NormalTok{\} }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mean}\NormalTok{(.) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{sprintf}\NormalTok{(}\StringTok{"MAPE is \%1.2f\%\%"}\NormalTok{, .)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "MAPE is 5.95%"
\end{verbatim}

\hypertarget{r---squared-and-adjusted-r-squared}{%
\subsection{R - Squared and adjusted R-squared}\label{r---squared-and-adjusted-r-squared}}

As already discussed, it is coefficient of determination or goodness of fit of regression. The can be written as equation \eqref{eq:lr10}.

\begin{equation} 
R^2 = 1 - \frac{\sum_{i=1}^{N}(y_i - \hat{y_i})^2}{\sum_{i=1}^{N}(y_i - \bar{y_i})^2}
\label{eq:lr10}
\end{equation}

The numerator in the fraction above, is also called as \(SSE\) or Sum of Squares of Errors; and denominator is also called as \(TSS\) or Total Sum of Squares. Actually the ratio (or fraction in above formula) i.e.~\(\frac{SSE}{TSS}\) denotes ratio of variance in errors to the variance (about mean) in the actual values. Thus \(R^2\) actually denotes how much variance is explained by the model; and clearly as the variance errors or \(SSE\) minimises and approaches \(0\), \(R^2\) increases and approaches \(1\) i.e.~a perfect model.

We can easily verify the formula from the results obtained.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# Sum of Squares of Errors}
\NormalTok{y\_bar }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(iris[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{50}\NormalTok{,}\StringTok{"Sepal.Width"}\NormalTok{])}
\NormalTok{(SSE }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(lin\_reg1}\SpecialCharTok{$}\NormalTok{residuals}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3.158675
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(TSS }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{((iris[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{50}\NormalTok{,}\StringTok{"Sepal.Width"}\NormalTok{] }\SpecialCharTok{{-}}\NormalTok{ y\_bar)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 7.0408
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(r\_sq }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ SSE}\SpecialCharTok{/}\NormalTok{TSS)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.5513756
\end{verbatim}

Now, we can think of this \(R^2\) in one more way, as it is simply the square of the correlation between the actual and predicted values. We can verify once again

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(}\FunctionTok{cor}\NormalTok{(iris[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{50}\NormalTok{, }\StringTok{\textquotesingle{}Sepal.Width\textquotesingle{}}\NormalTok{], lin\_reg1}\SpecialCharTok{$}\NormalTok{fitted.values))}\SpecialCharTok{\^{}}\DecValTok{2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.5513756
\end{verbatim}

Usually, when we keep on adding independent variables to our regression model \(R^2\) increases and it can easily incorporate over-fitting in itself. For this reason, sometimes adjusted r squared is used, which penalizes R squared for the number of predictors used in the model.

\begin{equation} 
{Adjusted}\;{ R^2} = 1 - \frac{(1-R^2)(N - 1)}{(N - p - 1)}
\label{eq:lr11}
\end{equation}

where \(p\) is the number of predictors included in model. The \texttt{summary} function returns both these metrics. We can verify the calculation-

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ ((}\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{r\_sq)}\SpecialCharTok{*}\NormalTok{(}\DecValTok{50{-}1}\NormalTok{)}\SpecialCharTok{/}\NormalTok{(}\DecValTok{50{-}1{-}1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.5420292
\end{verbatim}

\hypertarget{plotting-the-results-and-their-interpretion}{%
\section{Plotting the results and their interpretion}\label{plotting-the-results-and-their-interpretion}}

The output of \texttt{lm} can be plotted with \texttt{plot} command to see six diagnostics plots, one by one, which can be chosen using \texttt{which} argument. These six plots are -

\begin{itemize}
\tightlist
\item
  Residuals Vs. Fitted Values
\item
  Normal Q-Q
\item
  Scale-Location
\item
  Cook's distance
\item
  Residuals vs.~leverage
\item
  Cook's distance vs.~leverage
\end{itemize}

Let us see these, for the example above.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{), }\AttributeTok{oma =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(lin\_reg1, }\AttributeTok{which =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{, }\AttributeTok{sub.caption =} \StringTok{""}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[height=0.4\textheight]{DauR_files/figure-latex/plots1-1} 

}

\caption{First two diagnostic plots}\label{fig:plots1}
\end{figure}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Residuals Vs. Fitted Values}: This plot is used to determine if the residuals exhibit non-linear patterns. If the red line across the center of the plot is roughly horizontal then we can assume that the residuals follow a linear pattern. In our example we can see that the red line deviates from a perfect horizontal line but not severely. We would likely declare that the residuals follow a roughly linear pattern and that a linear regression model is appropriate for this data-set. This plot is useful to check first assumption of linear regression i.e.~linearity of the data.
\item
  \textbf{Normal Q-Q}: This plot is used to determine if the residuals of the regression model are normally distributed, which was our another assumption. If the points in this plot fall roughly along a straight diagonal line, then we can assume the residuals are normally distributed.
\end{enumerate}

Moreover, notice that the extreme outlier values impacting our modelling will be labeled. We can see that values from rows, 23, 33 and 42 are labeled.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{), }\AttributeTok{oma =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(lin\_reg1, }\AttributeTok{which =} \DecValTok{3}\SpecialCharTok{:}\DecValTok{4}\NormalTok{, }\AttributeTok{sub.caption =} \StringTok{""}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[height=0.4\textheight]{DauR_files/figure-latex/plots2-1} 

}

\caption{Next two diagnostic plots}\label{fig:plots2}
\end{figure}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  \textbf{Scale-Location}: This plot is used to check the assumption of equal variance, i.e.~``homoskedasticity'' among the residuals in our regression model. If the red line is roughly horizontal across the plot, then the assumption of equal variance is likely met.
\item
  \textbf{Cook's Distance}: An influential value is a value, which inclusion or exclusion can alter the results of the regression analysis. Such a value is associated with a large residual. Not all outliers (or extreme data points) are influential in linear regression analysis. A metric called Cook's distance, is used to determine the influence of a value. This metric defines influence as a combination of leverage and residual size.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{), }\AttributeTok{oma =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(lin\_reg1, }\AttributeTok{which =} \DecValTok{5}\SpecialCharTok{:}\DecValTok{6}\NormalTok{, }\AttributeTok{sub.caption =} \StringTok{""}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[height=0.4\textheight]{DauR_files/figure-latex/plots3-1} 

}

\caption{Last two diagnostic plots}\label{fig:plots3}
\end{figure}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  \textbf{Residuals vs.~leverage}: This plot is used to identify influential observations. If any points in this plot fall outside of Cook's distance (the dashed lines) then it is an influential observation. Actually, a data point has high leverage, if it has extreme predictor x values.
\item
  \textbf{Sixth plot i.e.~Cooks distance vs.~leverage} is also used to identify extreme values that may be impacting our model.
\end{enumerate}

Though the base R's command \texttt{plot} can generate all the plots, we may make use of library \texttt{performance} to generate all the relevant diagnostic plots beautifully and with small interpretation. See

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(performance)}
\FunctionTok{check\_model}\NormalTok{(lin\_reg1)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{DauR_files/figure-latex/perf-1} 

}

\caption{Output from package-performance}\label{fig:perf}
\end{figure}

The warning is obvious, we cannot have \texttt{multi-collinearity} problem as there is only one regressor. Actually multi-collinearity is about another assumption, we would have made in case there were more than one independent variables. This assumption would have been that the independent variables are not mutually collinear. We will see detailed explanation in case of multiple linear regression in the subsequent section.

\hypertarget{using-lm-for-predictions}{%
\section{\texorpdfstring{Using \texttt{lm} for predictions}{Using lm for predictions}}\label{using-lm-for-predictions}}

The output of \texttt{lm} is actually a list which contains much more information than we saw above. See which info is contained here -

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{names}\NormalTok{(lin\_reg1) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{as.data.frame}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{setNames}\NormalTok{(}\StringTok{\textquotesingle{}Objects\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          Objects
## 1   coefficients
## 2      residuals
## 3        effects
## 4           rank
## 5  fitted.values
## 6         assign
## 7             qr
## 8    df.residual
## 9        xlevels
## 10          call
## 11         terms
## 12         model
\end{verbatim}

We may extract any of the as per requirement. E.g.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lin\_reg1}\SpecialCharTok{$}\NormalTok{coefficients}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  (Intercept) Sepal.Length 
##   -0.5694327    0.7985283
\end{verbatim}

There is a package \texttt{broom} which uses \texttt{tidy} fundamentals to returns all the useful information by a single function \texttt{augment}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(broom)}
\FunctionTok{augment}\NormalTok{(lin\_reg1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 50 x 8
##    Sepal.Width Sepal.Length .fitted   .resid   .hat .sigma    .cooksd .std.resid
##          <dbl>        <dbl>   <dbl>    <dbl>  <dbl>  <dbl>      <dbl>      <dbl>
##  1         3.5          5.1    3.50 -0.00306 0.0215  0.259 0.00000160    -0.0121
##  2         3            4.9    3.34 -0.343   0.0218  0.254 0.0205        -1.35  
##  3         3.2          4.7    3.18  0.0163  0.0354  0.259 0.0000772      0.0649
##  4         3.1          4.6    3.10 -0.00380 0.0471  0.259 0.00000568    -0.0152
##  5         3.6          5      3.42  0.177   0.0200  0.258 0.00495        0.696 
##  6         3.9          5.4    3.74  0.157   0.0455  0.258 0.00940        0.628 
##  7         3.4          4.6    3.10  0.296   0.0471  0.255 0.0346         1.18  
##  8         3.4          5      3.42 -0.0232  0.0200  0.259 0.0000853     -0.0914
##  9         2.9          4.4    2.94 -0.0441  0.0803  0.259 0.00140       -0.179 
## 10         3.1          4.9    3.34 -0.243   0.0218  0.257 0.0103        -0.959 
## # i 40 more rows
\end{verbatim}

Let's also visualise the predicted values vis-a-vis actual values/residuals in Figure \ref{fig:predvsact}.

\begin{figure}

{\centering \includegraphics[width=0.48\linewidth]{DauR_files/figure-latex/predvsact-1} \includegraphics[width=0.48\linewidth]{DauR_files/figure-latex/predvsact-2} 

}

\caption{Predicted Vs. Actual Values (Left) and Residuals (Right)}\label{fig:predvsact}
\end{figure}

So if we have predict output from a new data, just ensure that data is in exactly same format as of regressor and we can use \texttt{predict} from base R directly. See this example.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{new\_vals }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{1}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{as.data.frame}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{setNames}\NormalTok{(}\StringTok{\textquotesingle{}Sepal.Length\textquotesingle{}}\NormalTok{)}
\FunctionTok{predict}\NormalTok{(lin\_reg1, new\_vals)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        1        2        3        4        5        6        7        8 
## 3.416249 3.552228 2.691731 5.185508 2.801470 3.523949 4.037588 2.183525 
##        9       10 
## 4.190880 3.667629
\end{verbatim}

\hypertarget{multiple-linear-regression}{%
\section{Multiple Linear Regression}\label{multiple-linear-regression}}

As the name suggests, multiple linear regression is the model where multiple independent variables may have linear relationship with dependent variable. In this case, the regression equation will be -

\begin{equation} 
y = \beta_0 + \beta_1x_1 + \beta_2x_2 + \beta_3x_3 + ... + \epsilon
\label{eq:lr12}
\end{equation}

where \(x_1\), \(x_2\), \(x_3\) \ldots, \(x_n\) will be \(n\) independent variables and \(y\) will be dependent variable as usual.

It may be clear that this equation represents an equation of a plane if there are two regressors. If there are \(n\) regressors, the equation will represent equation of a hyperplane of \(n-1\) dimensions. However, visualizing the variables and relation between them can be a bit tricky if there are multiple variables. We may decide about the type of the visualization will suit the requirement in that case.

Now, as already stated, there is an additional assumption, \textbf{that all the independent variables are mutually independent too i.e.~do not have multi-collinearity between them.} Let's build an example model again. We have to just add the predictors (independent variables) using the \texttt{+} operator in the formula \texttt{call}.

\textbf{Problem Statement:} Example-2. Let's try to establish the relationship between cars' mileage (\texttt{mpg} variable in \texttt{mtcars} data-set) with engine displacement \texttt{disp}, horse power \texttt{hp} and weight \texttt{wt}. First of all let's visualise the individual relationships between the variables through three plots as in Figure \ref{fig:ex2vis}.

\begin{figure}

{\centering \includegraphics[width=0.31\linewidth]{DauR_files/figure-latex/ex2vis-1} \includegraphics[width=0.31\linewidth]{DauR_files/figure-latex/ex2vis-2} \includegraphics[width=0.31\linewidth]{DauR_files/figure-latex/ex2vis-3} 

}

\caption{Relationship between regressors and outcome variables}\label{fig:ex2vis}
\end{figure}

Let's also visualise the correlation between all these variables. See Figure \ref{fig:ex2vis2}.

\begin{figure}

{\centering \includegraphics{DauR_files/figure-latex/ex2vis2-1} 

}

\caption{Correlation between the variables in Example-2}\label{fig:ex2vis2}
\end{figure}

Now let's build the model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\OtherTok{\textless{}{-}}\NormalTok{ mtcars[, }\FunctionTok{c}\NormalTok{(}\StringTok{"mpg"}\NormalTok{, }\StringTok{"disp"}\NormalTok{, }\StringTok{"hp"}\NormalTok{, }\StringTok{"wt"}\NormalTok{)]}
\NormalTok{lin\_reg2 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }
               \AttributeTok{data =}\NormalTok{ data)}
\FunctionTok{summary}\NormalTok{(lin\_reg2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = mpg ~ ., data = data)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -3.891 -1.640 -0.172  1.061  5.861 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(>|t|)    
## (Intercept) 37.105505   2.110815  17.579  < 2e-16 ***
## disp        -0.000937   0.010350  -0.091  0.92851    
## hp          -0.031157   0.011436  -2.724  0.01097 *  
## wt          -3.800891   1.066191  -3.565  0.00133 ** 
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 2.639 on 28 degrees of freedom
## Multiple R-squared:  0.8268, Adjusted R-squared:  0.8083 
## F-statistic: 44.57 on 3 and 28 DF,  p-value: 8.65e-11
\end{verbatim}

In formula call, please note that I have used \texttt{.} instead of naming all the variables. In fact \texttt{.} is a shorthand style of mentioning that all the variables except that mentioned as y variable will be our independent variables/predictors. In results, there is one coefficient for slope for each predictors and one intercept term in the output. The output equation can be written as equation \eqref{eq:lr13}.

\begin{equation} 
{mpg} = -0.000937\cdot{disp} - 0.031157\cdot{hp} - 3.800891\cdot{wt} + 37.105505
\label{eq:lr13}
\end{equation}

\textbf{Interpretation:} Notice that all slopes are in negative meaning that mileage drops by increase in each of weight, displacement and horsepower. Interpreting the equation would be on similar lines. We can now say that \emph{keeping other variables constant}, the effect of change (increase) of 1 unit in \texttt{disp} will result in decrease of mileage by 0.000937 miles per gallon. Keeping other variables constant is important here. Of course, by above equation we may deduce that if other factors change, the effect on response variable would be different.

\textbf{Results:} Analysing results, we may notice that our model is explains 83\% of variance in data. Of course, adjusted r-squared is lower which means that adding extra variables may have increased the r-squared. Global p-value is highly significant which means that at least of the coefficients is non-zero. Of course, the least significant coefficient is that of \texttt{disp} which we can remove and re-run the model to check the parameters again.

\hypertarget{including-categorical-or-factor-variables-in-lm}{%
\section{\texorpdfstring{Including categorical or factor variables in \texttt{lm}}{Including categorical or factor variables in lm}}\label{including-categorical-or-factor-variables-in-lm}}

By now we have seen that linear regression is useful for predicting numerical output and through the examples we have seen that our regressors were numerical too. But what if there's an input variable which is categorical or nominal?

In such case, we will have to ensure that categorical variable is of type \texttt{factor} before proceeding to build a model.

\textbf{Problem Statement:} Example-3. Let's try to predict \({Sepal.Width}\) from \({Species}\) in the iris data-set. This time we will take complete data-set. Since, we know that \texttt{Species} is already of factor type we need not convert it into one. Before moving on let's visualize the relation between the two variables using ggplot2. Box-plots are best suited here.

\begin{figure}

{\centering \includegraphics[height=0.3\textheight]{DauR_files/figure-latex/fact1-1} 

}

\caption{Species Vs. Sepal Width}\label{fig:fact1}
\end{figure}

Now let's build the model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm\_fact }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Sepal.Width }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Species, }\AttributeTok{data =}\NormalTok{ iris)}
\FunctionTok{summary}\NormalTok{(lm\_fact)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = Sepal.Width ~ Species, data = iris)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -1.128 -0.228  0.026  0.226  0.972 
## 
## Coefficients:
##                   Estimate Std. Error t value Pr(>|t|)    
## (Intercept)        3.42800    0.04804  71.359  < 2e-16 ***
## Speciesversicolor -0.65800    0.06794  -9.685  < 2e-16 ***
## Speciesvirginica  -0.45400    0.06794  -6.683 4.54e-10 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.3397 on 147 degrees of freedom
## Multiple R-squared:  0.4008, Adjusted R-squared:  0.3926 
## F-statistic: 49.16 on 2 and 147 DF,  p-value: < 2.2e-16
\end{verbatim}

In results we now got one intercept and two slopes for single regressor \texttt{Species}. So what happened now?

\textbf{Interpretation:} Actually, factor data type requirement for categorical regressor was due to the fact that this factor variable is encoded as dummy variable for each of the category available in it. Dummy variable is numeric and we can now run linear regression as earlier. The first category available in it will be baseline level. Since there were three levels included in \texttt{Species} it has been encoded into two dummy variables. To see what happened behind the scenes, we may use \texttt{contrasts} function.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{contrasts}\NormalTok{(iris}\SpecialCharTok{$}\NormalTok{Species)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##            versicolor virginica
## setosa              0         0
## versicolor          1         0
## virginica           0         1
\end{verbatim}

\texttt{model.matrix}

It is clear that when \texttt{versicolor} is \texttt{1} the other variable is \texttt{0} and vice versa. Obviously when both are \texttt{0} it means that \texttt{Species} is \texttt{setosa} and that's why no separate slope for that is present in output. Now we can write our regression line equation as \eqref{eq:lr14}

\begin{equation} 
{Sepal.Width} = 3.428 + (-0.658)\cdot{Speciesversicolor} + (-0.454)\cdot{Speciesvirginica}
\label{eq:lr14}
\end{equation}

Interpreting above equation is now easy. For each \texttt{versicolor} the \texttt{Sepal.Width} may be \texttt{3.428\ -\ 0.658} or 2.77. Obviously when two dummy variables are zero, the Sepal.Width would be equal to intercept; and thus, we can conclude that intercept is nothing but prediction for base-line level. In fact we can subtract a \texttt{1} to obtain these interpreted results.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm\_fact }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Sepal.Width }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Species }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\AttributeTok{data =}\NormalTok{ iris)}
\FunctionTok{summary}\NormalTok{(lm\_fact)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = Sepal.Width ~ Species - 1, data = iris)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -1.128 -0.228  0.026  0.226  0.972 
## 
## Coefficients:
##                   Estimate Std. Error t value Pr(>|t|)    
## Speciessetosa      3.42800    0.04804   71.36   <2e-16 ***
## Speciesversicolor  2.77000    0.04804   57.66   <2e-16 ***
## Speciesvirginica   2.97400    0.04804   61.91   <2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.3397 on 147 degrees of freedom
## Multiple R-squared:  0.9881, Adjusted R-squared:  0.9879 
## F-statistic:  4083 on 3 and 147 DF,  p-value: < 2.2e-16
\end{verbatim}

\textbf{Notice that other two coefficients have now been adjusted automatically.}

\textbf{Results:} Observing the figure \ref{fig:fact1}, we may notice that the coefficients are nothing but mean Sepal Widths for each Species, which is obvious and logical too. Those, who are interested in seeing equation (without intercept) can also make one, as in equation \eqref{eq:lr15}.

\begin{equation} 
{Sepal.Width} = 3.428\cdot{Speciessetosa} + (2.77)\cdot{Speciesversicolor} + (2.974)\cdot{Speciesvirginica}
\label{eq:lr15}
\end{equation}

Since, these coefficients are not slopes in true sense, we will refer these as intercepts, in next examples/sections.

\hypertarget{parallel-slopes-regression}{%
\subsection{Parallel slopes regression}\label{parallel-slopes-regression}}

To understand how categorical response variable acts, when there are other numerical variables in regression model, let us build a model step by step.

\textbf{Problem statement:} Example-4. We are taking \texttt{mpg} data-set included by default with \texttt{ggplot2} package. This data-set shows \emph{Fuel economy data from 1999 to 2008 for 38 popular models of cars}. Let us predict highway mileage \texttt{hwy} from engine displacement \texttt{displ} and year of the model \texttt{year}. Let us visualize the variables. From Figure \ref{fig:ex4vis} it is clear that highway mileage is linearly associated with displacement.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(mpg, }\FunctionTok{aes}\NormalTok{(hwy, displ)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"lm"}\NormalTok{, }\AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{formula =} \StringTok{"y \textasciitilde{} x"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(.}\SpecialCharTok{\textasciitilde{}}\NormalTok{ year) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[height=0.35\textheight]{DauR_files/figure-latex/ex4vis-1} 

}

\caption{Highway Mileage Vs. Displacement over cars manufactured in 1998 Vs. 2008}\label{fig:ex4vis}
\end{figure}

\textbf{Step-1:} Let us first include a single numerical variable \texttt{displ} to predict highway mileage \texttt{hwy}; and examine the coefficients.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{par\_slop1 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(hwy }\SpecialCharTok{\textasciitilde{}}\NormalTok{ displ, }\AttributeTok{data =}\NormalTok{ mpg)}
\FunctionTok{coef}\NormalTok{(par\_slop1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## (Intercept)       displ 
##   35.697651   -3.530589
\end{verbatim}

We got one intercept and one slope coefficient. We can even see related visualisation in figure \ref{fig:parrslop} (left).

\textbf{Step-2:} Now let us try to predict mileage on the basis of \texttt{year} of manufacture only. So as already stated, we have to convert it into factor. We can do that directly in the formula. \emph{Also note that we are subtracting \(1\) from the response variables, which actually replaces intercept with the baseline level explicitly.} Now see the coefficients-

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}
\NormalTok{par\_slop2 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(hwy }\SpecialCharTok{\textasciitilde{}} \FunctionTok{factor}\NormalTok{(year) }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{, }\AttributeTok{data =}\NormalTok{ mpg)}
\FunctionTok{coef}\NormalTok{(par\_slop2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## factor(year)1999 factor(year)2008 
##         23.42735         23.45299
\end{verbatim}

Notice that we got intercept for each of the category available in factor variable. By seeing plot in Figure \ref{fig:parrslop}-(Right) that these intercepts are nothing but means for each category.

\begin{figure}

{\centering \includegraphics[width=0.47\linewidth]{DauR_files/figure-latex/parrslop-1} \includegraphics[width=0.47\linewidth]{DauR_files/figure-latex/parrslop-2} 

}

\caption{Mileage vs. Displacement (Left) and Year (right)}\label{fig:parrslop}
\end{figure}

\textbf{Step-3:} Now we will build the complete model by including both variables together; and examine the coefficients.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{par\_slop }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(hwy }\SpecialCharTok{\textasciitilde{}}\NormalTok{ displ }\SpecialCharTok{+} \FunctionTok{factor}\NormalTok{(year) }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{, }\AttributeTok{data =}\NormalTok{ mpg)}
\FunctionTok{coef}\NormalTok{(par\_slop)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##            displ factor(year)1999 factor(year)2008 
##        -3.610986        35.275706        36.677842
\end{verbatim}

We can see one slope coefficient for numerical variable and two different intercepts for each of the Years. Try visualising these. In fact there are two different parallel lines (same slope). Refer figure \ref{fig:parrslop2}.

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth,height=0.35\textheight]{DauR_files/figure-latex/parrslop2-1} 

}

\caption{Parallel Slopes}\label{fig:parrslop2}
\end{figure}

This is also evident, if we write out the equation \eqref{eq:lr16}.

\begin{equation} 
{hwy} = -3.610986\cdot{displ} + 35.275706\cdot{year1999} + 36.677842\cdot{year2008}
\label{eq:lr16}
\end{equation}

Either one of the dummy variables will be 0 and another 1, so that variable with \texttt{1} will act as intercept term, but the slope term will remain same. In other words, whatever be the year, the mileage will vary with displacement at the same rate. This is in actual circumstances, rare. Rate of change of response variable will change as per factor variables (regressor) change their values. So how to incorporate these changes in our model? The answer is \texttt{interaction} which has been discussed in next section.

\hypertarget{extending-multiple-linear-regression-by-including-interactions}{%
\subsection{\texorpdfstring{Extending multiple linear regression by including \texttt{interactions}}{Extending multiple linear regression by including interactions}}\label{extending-multiple-linear-regression-by-including-interactions}}

The parallel slopes model, we saw in previous section enforced a common slope for each category. That's not always the best option.

\textbf{Problem Statement:} In same example-4 (earlier section) we can introduce \texttt{interaction} between two predictors using special operator OR shorthand notation \texttt{:} in formula call. See the following example-

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{new\_model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(hwy }\SpecialCharTok{\textasciitilde{}}\NormalTok{ displ }\SpecialCharTok{+} \FunctionTok{factor}\NormalTok{(year) }\SpecialCharTok{+}\NormalTok{ displ}\SpecialCharTok{:}\FunctionTok{factor}\NormalTok{(year) }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{ , }\AttributeTok{data =}\NormalTok{ mpg)}
\FunctionTok{coef}\NormalTok{(new\_model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                  displ       factor(year)1999       factor(year)2008 
##              -3.768406              35.792231              36.136730 
## displ:factor(year)2008 
##               0.305168
\end{verbatim}

Now notice an extra coefficient, though small which is change in slope when moving from baseline category to category of year- 2008. This, in fact represents that the model has a different slope for each category; refer Figure \ref{fig:parrslop3}.

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth,height=0.35\textheight]{DauR_files/figure-latex/parrslop3-1} 

}

\caption{Changing Slopes with interaction}\label{fig:parrslop3}
\end{figure}

Interpreting these models are now, not that difficult as it seem earlier. Let's generate summary first.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(new\_model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = hwy ~ displ + factor(year) + displ:factor(year) - 
##     1, data = mpg)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -7.8595 -2.4360 -0.2103  1.6037 15.3677 
## 
## Coefficients:
##                        Estimate Std. Error t value Pr(>|t|)    
## displ                   -3.7684     0.2788 -13.517   <2e-16 ***
## factor(year)1999        35.7922     0.9794  36.546   <2e-16 ***
## factor(year)2008        36.1367     1.0492  34.442   <2e-16 ***
## displ:factor(year)2008   0.3052     0.3882   0.786    0.433    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 3.784 on 230 degrees of freedom
## Multiple R-squared:  0.9759, Adjusted R-squared:  0.9755 
## F-statistic:  2332 on 4 and 230 DF,  p-value: < 2.2e-16
\end{verbatim}

We may write our equation now (equation \eqref{eq:lr17}.

\begin{equation} 
{hwy} = -3.7684\cdot{displ} + 35.7922\cdot{year1999} + 36.1367\cdot{year2008} + 0.3052\cdot{displ}\cdot{year2008}
\label{eq:lr17}
\end{equation}

Clearly, when year is 2008, the slope for \texttt{displ} changes by \texttt{0.3052}.

We can add as many interactions as we would like to, using the shorthand \texttt{:}; however, when there are many interactions we may make use of another shorthand operator \texttt{*}. So \texttt{x*z} would mean \texttt{x\ +\ z\ +\ x:z} and \texttt{x*z*w} would mean \texttt{x\ +\ z\ +\ w\ +\ x:z\ +\ x:w\ +\ z:w}. This obviously wouldn't make any difference but would save us a lot of typing.

\hypertarget{multi-collinearity-and-variance-inflation-factor-vif}{%
\section{Multi-collinearity and Variance Inflation Factor (VIF)}\label{multi-collinearity-and-variance-inflation-factor-vif}}

Multi-collinearity indicates a strong linear relationship among the predictor variables. This can create challenges in the regression analysis because it becomes difficult to determine the individual effects of each independent variable on the dependent variable accurately. Multi-collinearity can lead to unstable and unreliable coefficient estimates, making it harder to interpret the results and draw meaningful conclusions from the model. It is essential to detect and address multi-collinearity to ensure the validity and robustness of regression models.

But why it poses a problem in regression analysis. Actually, multi-collinearity means that one independent variable can be predicted from another and it in turn means that independent variables are no longer independent.

Multi-collinearity can be detected using many different methods. One of the method can be to use correlation plots, which is explained in next section. Another method is to use Variance Inflation Factor or VIF. VIF determines the strength of the correlation between the independent variables. It is predicted by taking a variable and regressing it against every other variable. In other words, VIF score of an independent variable represents how well the variable is explained by other independent variables.

\begin{equation} 
{VIF} = \frac{1}{1-R^2}
\label{eq:lr18}
\end{equation}

So the closer \(R^2\) value to \(1\) the higher the \({VIF}\).

\begin{itemize}
\tightlist
\item
  \(VIF\) starts at \(1\) and has no upper limit
\item
  \({VIF} = 1\) means there is no correlation between the independent variable and the other variables
\item
  \({VIF}\) exceeding \(5\) indicates high multi-collinearity between that independent variable and others.
\end{itemize}

Again in R, we do not have to manually calculate \({VIF}\) for each variable. Using \texttt{vif()} function, of library \texttt{car} we can calculate this. Let's use it on another model built on \texttt{mtcars} data where \texttt{mpg} variable is predicted using all other variables. For all other variables, instead of using names, we will use another shorthand operator \texttt{.}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mod }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ . , }\AttributeTok{data =}\NormalTok{ mtcars)}
\FunctionTok{library}\NormalTok{(car)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Loading required package: carData
\end{verbatim}

\begin{verbatim}
## 
## Attaching package: 'car'
\end{verbatim}

\begin{verbatim}
## The following object is masked from 'package:psych':
## 
##     logit
\end{verbatim}

\begin{verbatim}
## The following object is masked from 'package:dplyr':
## 
##     recode
\end{verbatim}

\begin{verbatim}
## The following object is masked from 'package:purrr':
## 
##     some
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{car}\SpecialCharTok{::}\FunctionTok{vif}\NormalTok{(mod)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       cyl      disp        hp      drat        wt      qsec        vs        am 
## 15.373833 21.620241  9.832037  3.374620 15.164887  7.527958  4.965873  4.648487 
##      gear      carb 
##  5.357452  7.908747
\end{verbatim}

We may notice high collinearity among some of these predictors as also indicated in above output.

\hypertarget{one-complete-example}{%
\section{One Complete Example}\label{one-complete-example}}

\textbf{Problem Statement:} Example-5. The data pertains to inter-country Life-Cycle Savings 1960-1970. Under the life-cycle savings hypothesis as developed by \emph{Franco Modigliani}, the savings ratio (aggregate personal saving divided by disposable income) \texttt{sr} is explained by per-capita disposable income \texttt{dpi}, the percentage rate of change in per-capita disposable income \texttt{ddpi}, and two demographic variables: the percentage of population less than 15 years old \texttt{pop15} and the percentage of the population over 75 years old \texttt{pop75}. The data are averaged over the decade 1960--1970 to remove the business cycle or other short-term fluctuations.

Let's try linear regression on \texttt{LifeCycleSavings} data.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Visualise first 6 rows}
\FunctionTok{head}\NormalTok{(LifeCycleSavings)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##              sr pop15 pop75     dpi ddpi
## Australia 11.43 29.35  2.87 2329.68 2.87
## Austria   12.07 23.32  4.41 1507.99 3.93
## Belgium   13.17 23.80  4.43 2108.47 3.82
## Bolivia    5.75 41.89  1.67  189.13 0.22
## Brazil    12.88 42.19  0.83  728.47 4.56
## Canada     8.79 31.72  2.85 2982.88 2.43
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Build a model}
\NormalTok{ex1 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(sr }\SpecialCharTok{\textasciitilde{}}\NormalTok{ . , }\AttributeTok{data =}\NormalTok{ LifeCycleSavings)}
\end{Highlighting}
\end{Shaded}

In \texttt{call} formula above, notice the shorthand \texttt{.} operator which here means all variable other than y variable are treated as input variables. See its output-

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(ex1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = sr ~ ., data = LifeCycleSavings)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -8.2422 -2.6857 -0.2488  2.4280  9.7509 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(>|t|)    
## (Intercept) 28.5660865  7.3545161   3.884 0.000334 ***
## pop15       -0.4611931  0.1446422  -3.189 0.002603 ** 
## pop75       -1.6914977  1.0835989  -1.561 0.125530    
## dpi         -0.0003369  0.0009311  -0.362 0.719173    
## ddpi         0.4096949  0.1961971   2.088 0.042471 *  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 3.803 on 45 degrees of freedom
## Multiple R-squared:  0.3385, Adjusted R-squared:  0.2797 
## F-statistic: 5.756 on 4 and 45 DF,  p-value: 0.0007904
\end{verbatim}

Multiple R squared is about 34\% which means nearly 34\% variability is explained by linear model. Let us see some diagnostics plots (figure \ref{fig:perf2})

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{performance}\SpecialCharTok{::}\FunctionTok{check\_model}\NormalTok{(ex1)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{DauR_files/figure-latex/perf2-1} 

}

\caption{Diagnostic Plots}\label{fig:perf2}
\end{figure}

We may notice some multi-collinearity, between two population variables. See figures and \ref{fig:perf2} and \ref{fig:multi2}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{LifeCycleSavings }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(pop15, pop75)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{()}\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{\textquotesingle{}lm\textquotesingle{}}\NormalTok{, }\AttributeTok{formula =} \StringTok{\textquotesingle{}y\textasciitilde{}x\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[height=0.3\textheight]{DauR_files/figure-latex/multi2-1} 

}

\caption{Are x and y correlated?}\label{fig:multi2}
\end{figure}

This can also be verified by corrplots in figure \ref{fig:corrp}.

\begin{figure}

{\centering \includegraphics[height=0.45\textheight]{DauR_files/figure-latex/corrp-1} 

}

\caption{Correlation Plots}\label{fig:corrp}
\end{figure}

Let us see the VIF among the predictors.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{car}\SpecialCharTok{::}\FunctionTok{vif}\NormalTok{(ex1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    pop15    pop75      dpi     ddpi 
## 5.937661 6.629105 2.884369 1.074309
\end{verbatim}

Thus, the model should be tuned better by removing this multi-collinear variable. Let us try to remove \texttt{pop75} and re-run the model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ex2 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(sr }\SpecialCharTok{\textasciitilde{}}\NormalTok{ pop15 }\SpecialCharTok{+}\NormalTok{ dpi }\SpecialCharTok{+}\NormalTok{ ddpi, }\AttributeTok{data =}\NormalTok{ LifeCycleSavings)}
\FunctionTok{summary}\NormalTok{(ex2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = sr ~ pop15 + dpi + ddpi, data = LifeCycleSavings)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -7.6889 -2.8813  0.0296  1.7989 10.4330 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(>|t|)    
## (Intercept) 19.2771687  4.3888974   4.392 6.53e-05 ***
## pop15       -0.2883861  0.0945354  -3.051  0.00378 ** 
## dpi         -0.0008704  0.0008795  -0.990  0.32755    
## ddpi         0.3929355  0.1989390   1.975  0.05427 .  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 3.862 on 46 degrees of freedom
## Multiple R-squared:  0.3026, Adjusted R-squared:  0.2572 
## F-statistic: 6.654 on 3 and 46 DF,  p-value: 0.0007941
\end{verbatim}

Notice that multiple R-squared has now reduced to 30\%. Let us try to remove the variable \texttt{dpi} the coefficient of which is not that significant.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ex3 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(sr }\SpecialCharTok{\textasciitilde{}}\NormalTok{ pop15 }\SpecialCharTok{+}\NormalTok{ ddpi, }\AttributeTok{data =}\NormalTok{ LifeCycleSavings)}
\FunctionTok{summary}\NormalTok{(ex3)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = sr ~ pop15 + ddpi, data = LifeCycleSavings)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -7.5831 -2.8632  0.0453  2.2273 10.4753 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept) 15.59958    2.33439   6.682 2.48e-08 ***
## pop15       -0.21638    0.06033  -3.586 0.000796 ***
## ddpi         0.44283    0.19240   2.302 0.025837 *  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 3.861 on 47 degrees of freedom
## Multiple R-squared:  0.2878, Adjusted R-squared:  0.2575 
## F-statistic: 9.496 on 2 and 47 DF,  p-value: 0.0003438
\end{verbatim}

Multiple R squared increased slightly i.e.~now reached around 29\%. Let us try to visualise the relationship through a scatter plot.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{LifeCycleSavings }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(pop15, sr)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{\textquotesingle{}lm\textquotesingle{}}\NormalTok{, }\AttributeTok{formula =} \StringTok{\textquotesingle{}y \textasciitilde{} x\textquotesingle{}}\NormalTok{, }\AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[height=0.3\textheight]{DauR_files/figure-latex/unnamed-chunk-381-1} 

}

\caption{Ascertaining linear relationship}\label{fig:unnamed-chunk-381}
\end{figure}

Clearly the relationship between the predictor and response variable is not that strong and hence the results.

\hypertarget{simpsons-paradox}{%
\section{Simpson's paradox}\label{simpsons-paradox}}

Edward Hugh Simpson, a statistician and former cryptanalyst at Bletchley Park, described this statistical phenomenon in a paper in 1951. It is classic example how regressions, without including necessary terms, can be misleading. At its core, the paradox arises when a trend that appears in different subgroups of data is either \emph{reversed} or \emph{disappears} when the subgroups are combined. This seemingly counter-intuitive occurrence can lead to misleading conclusions and thus underscores the importance of careful analysis and interpretation of data.

\textbf{Problem Statement:} Example-6. Here is data of \texttt{palmerpenguins} where let's try to establish relationship between penguins bills' depth and lengths i.e.~\texttt{bill\_depth\_mm} and \texttt{bill\_length\_mm}. See the plot in fig \ref{fig:ex3}.

\begin{figure}

{\centering \includegraphics[width=0.47\linewidth]{DauR_files/figure-latex/ex3-1} \includegraphics[width=0.47\linewidth]{DauR_files/figure-latex/ex3-2} 

}

\caption{Regression having variable hidden (left) and exposed (right)}\label{fig:ex3}
\end{figure}

In plot (left) a negative relationship is seen, but when the lurking variable is exposed (right), we can see a positive relationship for each of the species . Thus, species is a significant confounding variable to assess linear relationship here. Thus before finalizing the model, we have to be sure that we are not missing any important variable. To avoid incorrect results due to underlying Simpson's Paradox, we must ensure to:

\textbf{Identify Confounding Variables:} Be vigilant in identifying potential confounding variables that could affect the relationship between the variables under study.

\textbf{Consider All Levels:} Analyze data at different levels, including subgroup and aggregate levels, to gain a comprehensive understanding of the relationship.

\textbf{Utilise Statistical Techniques:} such as regression analysis or propensity score matching, to control for confounding variables and obtain more accurate insights.

\textbf{Transparent Reporting:} Clearly report the methodology, assumptions, and limitations of the analysis to ensure that others can critically evaluate the findings.

Simpson's Paradox, thus, serves as a powerful reminder that data analysis is an intricate process that requires careful consideration of underlying factors.

\hypertarget{conclusion-and-final-thoughts}{%
\section{Conclusion and Final thoughts}\label{conclusion-and-final-thoughts}}

In above sections, we learned techniques of regression analysis, which is a powerful and useful tool in data analytics while auditing. We may use regression analysis, inter alia, for -

\textbf{Detection of Anomalies and Outliers:} Regression analysis can help auditors in identifying anomalies, outliers, or unexpected patterns in financial data. Unusual relationships between variables can signal potential errors, fraud, or irregularities that require further investigation.

\textbf{Risk Assessment:} By analyzing the relationships between various financial or operational variables, regression analysis can assist auditors in assessing the level of risk associated with different aspects of an organization's operations. This helps auditors prioritize their efforts and allocate resources effectively.

\textbf{Control Testing:} Regression analysis can aid us in testing the effectiveness of internal controls within an organization. By examining the relationship between control variables and outcomes, auditors can assess whether controls are functioning as intended. We can also use regression analysis to compare an organization's financial performance against industry benchmarks or similar companies. Deviations from expected relationships can highlight areas that warrant closer examination.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{principal-component-analysis-in-r}{%
\chapter{Principal Component Analysis in R}\label{principal-component-analysis-in-r}}

\emph{The content is under development}

\hypertarget{clustering-in-r-using-kmeans-algorithm}{%
\chapter{Clustering in R (Using Kmeans algorithm)}\label{clustering-in-r-using-kmeans-algorithm}}

\emph{The content is under development}

\hypertarget{association-rule-mining-in-r-apriori}{%
\chapter{Association Rule Mining in R (Apriori)}\label{association-rule-mining-in-r-apriori}}

\emph{The content is under development}

\hypertarget{part-v-time-series-analysis}{%
\chapter*{Part-V: Time Series Analysis}\label{part-v-time-series-analysis}}
\addcontentsline{toc}{chapter}{Part-V: Time Series Analysis}

\hypertarget{manipulating-datedate-time-objects-using-lubridate}{%
\chapter{Manipulating Date/Date-time objects using lubridate}\label{manipulating-datedate-time-objects-using-lubridate}}

\emph{The content is under development}

\hypertarget{time-series-analysis}{%
\chapter{Time Series Analysis}\label{time-series-analysis}}

Time series analysis is an essential technique for forecasting and analyzing trends in data over time. It is commonly used in various fields like finance, economics, and engineering.

So a question arises, what is a time series? A time series is a sequence of data points collected over time, where the observations are recorded in chronological order. Time series analysis involves analyzing and modeling these data points to understand patterns, trends, and make predictions about future values. It is therefore used in predictive as well as descriptive analysis.

Time Series Analysis plays an important role in audit analytics as well. A few of the use cases can be-

\begin{itemize}
\tightlist
\item
  \textbf{Revenue Analysis:} An auditor may analyze revenue data over time to identify irregularities or suspicious patterns that could indicate potential fraud or misstatement. By examining the revenue time series, the auditor can look for unexpected fluctuations, unusual growth or decline trends, or abnormal seasonality. Deviations from historical patterns or industry benchmarks may indicate fraudulent activities, such as revenue manipulation, fictitious transactions, or irregular recognition practices.
\item
  \textbf{Inventory Analysis:} Auditors often analyze inventory data to assess the adequacy of inventory levels, identify potential inventory obsolescence or shrinkage, and evaluate the efficiency of inventory management. Time series analysis can be valuable in understanding inventory patterns and identifying potential risks or anomalies.
\end{itemize}

There are several time series data which are loaded by default in R; a few of these are listed below. We will use these datasets to understand the various concepts related to time series analysis.

\begin{itemize}
\tightlist
\item
  \texttt{AirPassengers} containing monthly airline passenger numbers from 1949-1960. See figure \ref{fig:tsex} (a)
\item
  \texttt{Nile} contains flow of Nile river data. See figure \ref{fig:tsex} (b)
\item
  \texttt{sunspots} containing monthly sunspot numbers from 1749-1983. See figure \ref{fig:tsex} (c)
\item
  \texttt{JohnsonJohnson} which contains quarterly earnings per Johnson \& Johnson share. See figure \ref{fig:tsex} (d)
\end{itemize}

\begin{figure}

{\centering \includegraphics{DauR_files/figure-latex/tsex-1} 

}

\caption{Few time series data sets in R}\label{fig:tsex}
\end{figure}

To analyse any time series, we have to understand its different components.

\hypertarget{components-of-time-series}{%
\section{Components of Time series}\label{components-of-time-series}}

A time series can be decomposed into several components:

\begin{itemize}
\tightlist
\item
  \textbf{Level:} The baseline value or average of the series over time. It represents the long-term behavior of the series. This is basic component, and is always present in a time series object. E.g. In a straight horizontal line only level is there which is equal to the value of y intercept.
\item
  \textbf{Trend:} The overall direction of the series. It indicates whether the series is increasing, decreasing, or staying relatively constant over time. E.g. A clear increasing trend can be seen in Johnson \& Johnson earnings, in Figure \ref{fig:tsex} (d).
\item
  \textbf{Seasonality:} The repetitive and predictable patterns within the series that occur at regular intervals. Seasonality can be daily, weekly, monthly, quarterly, or yearly, etc. E.g. In figure \ref{fig:tsex} (d) we may see seasonal patterns in earnings of Johnson\&Johnson share.
\item
  \textbf{Errors (Residuals):} The random fluctuations or noise in the series that cannot be explained by the level, trend, or seasonality. E.g. See figure \ref{fig:tsex} (c). The error component is an important aspect of time series analysis because it provides information about the uncertainty and variability of the data.
\end{itemize}

\hypertarget{additive-or-multiplicative-components}{%
\section{Additive or multiplicative components}\label{additive-or-multiplicative-components}}

In time series analysis, the trend, seasonal, and residual components can be modeled as either additive or multiplicative. The choice of the model depends on how the components combine to create the observed values of the series.

An \textbf{additive model} assumes that the components of the time series are added together to create the observed values. In other words, the value of the time series at any point in time is equal to the sum of the trend, seasonal, and residual components at that point in time. This is expressed mathematically as:

\[
y_t = T_t + S_t + e_t
\]

where \(y_t\), \(T_t\), \(S_t\) and \(e_t\) are the values of the series, trend component, seasonal component and residuals, respectively at time \(t\).

A \textbf{multiplicative model}, on the other hand, assumes that the components of the time series are multiplied together to create the observed values. In other words, the value of the time series at any point in time is equal to the product of the trend, seasonal, and residual components at that point in time. This is expressed mathematically as:

\[
y_t = T_t \times S_t \times e_t
\]

where \(y_t\), \(T_t\), \(S_t\) and \(e_t\) are defined as before. We can convert multiplicative time series into additive time series by taking \(log\). Note that seasonal component in \texttt{AirPassengers} time series depicted in \ref{fig:tsex} (a) is multiplicative. See Figure \ref{fig:logts}.

\begin{figure}

{\centering \includegraphics[width=0.49\linewidth]{DauR_files/figure-latex/logts-1} \includegraphics[width=0.49\linewidth]{DauR_files/figure-latex/logts-2} 

}

\caption{Transforming Time Series}\label{fig:logts}
\end{figure}

We will learn about decomposing time series components in next sections.

\textbf{Damped Trends:} Gardner and Mckenzie, in 1985 \citep{mcke} observed that most time series methods assume that any trend will continue unabated, regardless of the forecast lead time. They, based on empirical findings, suggested that forecast accuracy can be improved by either damping or ignoring altogether trends which have a low probability of persistence. We will learn about this concept, as well in next sections.

\hypertarget{practical-examples-in-r}{%
\section{Practical examples in R}\label{practical-examples-in-r}}

Before analysing time series, in R let's see how time series objects are dealt in R.

\hypertarget{pre-requisites}{%
\subsection*{Pre-requisites}\label{pre-requisites}}
\addcontentsline{toc}{subsection}{Pre-requisites}

Though most of the time we will be using base R, yet \texttt{forecast} is a fabulous package in R, developed by \textbf{Rob Hyndman}, which we will be using for predicting. Predicting values help us understand how well we have captured the hidden trends and patterns in any time series.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(forecast)}
\FunctionTok{library}\NormalTok{(ggplot2)}
\end{Highlighting}
\end{Shaded}

\hypertarget{creating-a-time-series-object-in-r}{%
\subsection{Creating a time series object in R}\label{creating-a-time-series-object-in-r}}

R provides us a simple function \texttt{ts()} to convert a series of observations (basically a vector) into a specific time series object. The syntax is -

\begin{verbatim}
ts(data, 
  start = 1, 
  end = numeric(length(data)), 
  frequency = 1, 
  ...)
\end{verbatim}

where:

\begin{itemize}
\tightlist
\item
  \texttt{data}: a numeric vector or matrix containing the data for the time series
\item
  \texttt{start}: the start time of the time series, represented as either a numeric or a Date or POSIXct object
\item
  \texttt{end}: the end time of the time series, represented as either a numeric or a Date or POSIXct object
\item
  \texttt{frequency}: the number of observations per unit time for the time series. For example, if the data is recorded monthly, the frequency would be 12
\item
  \texttt{...}: additional arguments that can be passed to the function, such as \texttt{names}, \texttt{delim}, or \texttt{tsp} (time series start and end points)
\end{itemize}

Example-1: Let's create a time series.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a numeric vector representing monthly sales data for a year}
\NormalTok{sales }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{25}\NormalTok{, }\DecValTok{35}\NormalTok{, }\DecValTok{40}\NormalTok{, }\DecValTok{45}\NormalTok{, }\DecValTok{50}\NormalTok{, }\DecValTok{55}\NormalTok{, }\DecValTok{60}\NormalTok{, }\DecValTok{65}\NormalTok{, }\DecValTok{70}\NormalTok{)}

\CommentTok{\# Create a time series object with monthly frequency starting from January}
\NormalTok{sales\_ts }\OtherTok{\textless{}{-}} \FunctionTok{ts}\NormalTok{(sales, }\AttributeTok{start =} \FunctionTok{c}\NormalTok{(}\DecValTok{2022}\NormalTok{, }\DecValTok{4}\NormalTok{), }\AttributeTok{frequency =} \DecValTok{12}\NormalTok{)}

\CommentTok{\# Print the time series object}
\NormalTok{sales\_ts}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec
## 2022              10  20  30  25  35  40  45  50  55
## 2023  60  65  70
\end{verbatim}

We can check its class.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{class}\NormalTok{(sales\_ts)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "ts"
\end{verbatim}

Let's also check \texttt{Nile}, which is already available in base R.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Nile}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Time Series:
## Start = 1871 
## End = 1970 
## Frequency = 1 
##   [1] 1120 1160  963 1210 1160 1160  813 1230 1370 1140  995  935 1110  994 1020
##  [16]  960 1180  799  958 1140 1100 1210 1150 1250 1260 1220 1030 1100  774  840
##  [31]  874  694  940  833  701  916  692 1020 1050  969  831  726  456  824  702
##  [46] 1120 1100  832  764  821  768  845  864  862  698  845  744  796 1040  759
##  [61]  781  865  845  944  984  897  822 1010  771  676  649  846  812  742  801
##  [76] 1040  860  874  848  890  744  749  838 1050  918  986  797  923  975  815
##  [91] 1020  906  901 1170  912  746  919  718  714  740
\end{verbatim}

\hypertarget{plotting-time-series-objects}{%
\subsection{Plotting Time Series Objects}\label{plotting-time-series-objects}}

To plot \texttt{ts} object we can simply use \texttt{plot()} command or alternatively we can use \texttt{ggplot2} as well.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Plotting in base R}
\FunctionTok{plot}\NormalTok{(sales\_ts, }\AttributeTok{main =} \StringTok{"Sales during FY 2022{-}23}\SpecialCharTok{\textbackslash{}n}\StringTok{Dummy Data by author"}\NormalTok{)}

\NormalTok{forecast}\SpecialCharTok{::}\FunctionTok{autoplot}\NormalTok{(AirPassengers) }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_log10}\NormalTok{(}\StringTok{\textquotesingle{}Logrithmic values\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{\textquotesingle{}AirPassengers converted to additive\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.48\linewidth]{DauR_files/figure-latex/tsex2-1} \includegraphics[width=0.48\linewidth]{DauR_files/figure-latex/tsex2-2} 

}

\caption{Example of time series plotting in R; Base R (Left) and ggplot2 (Right)}\label{fig:tsex2}
\end{figure}

\emph{Note: Library \texttt{forecast} in R also provides us a function \texttt{autoplot()} which plots a time series object, similar to plot(), but the plot returned here is a \texttt{ggplot2} object which can be modified/fine-tuned using \texttt{ggplot2} functions.}

\hypertarget{time-series-modelling-and-forecasting}{%
\section{Time series modelling and forecasting}\label{time-series-modelling-and-forecasting}}

As we have already discussed, forecasting future plays an important part of time series analysis; an important prerequisite is to model our data. In fact, the forecasting/modelling techniques can be broadly classified into two categories, data based techniques and model-based techniques.

\hypertarget{data-based-techniques}{%
\subsection*{Data-based techniques}\label{data-based-techniques}}
\addcontentsline{toc}{subsection}{Data-based techniques}

Data-based techniques, also known as statistical or empirical techniques, focus on analyzing the patterns and characteristics of the observed time series data directly. These techniques do not explicitly assume a specific underlying model structure. Instead, they rely on statistical properties and patterns present in the data. Examples of data-based techniques include:

\hypertarget{naive-method}{%
\subsection{Naive method}\label{naive-method}}

This method assumes that the future value will be the same as the last observed value. This is the simplest and most basic forecasting method and hence named \emph{naive}. The formula for the naive method is:

\[
\text{Naive method: }\hat{Y}_{T+1} = Y_T
\]

Here, \(\hat{Y}_{T+1}\) is the forecast value for the next time period and \(Y_T\) is the last observed value i.e.~for a series of \(T\) observations. Thus, predicting future values using naive method require no special skill.

\textbf{Problem Statement-1:} Let's predict say 5, future values of Nile flow data. In \texttt{forecast} there is a function \texttt{naive} which will do the job, once we give the value of parameter \texttt{h} which is short for forecasting \textbf{h}orizon. In Figure \ref{fig:naive}, we can see forecast values, in red, which are exactly same as previous/last value.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{forecast}\SpecialCharTok{::}\FunctionTok{naive}\NormalTok{(Nile, }\AttributeTok{h =} \DecValTok{5}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{autoplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Nile flow {-} forecast by Naive method"}\NormalTok{,}
       \AttributeTok{x =} \StringTok{"Year"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.98\linewidth]{DauR_files/figure-latex/naive-1} 

}

\caption{Naive forecasting}\label{fig:naive}
\end{figure}

If we aren't interested in visualising confidence values (by default 80\% and 95\% confidence values are shown in bands), we can tune parameter \texttt{level} accordingly.

\hypertarget{moving-averages}{%
\subsection{Moving averages}\label{moving-averages}}

In this technique, the forecast value is computed as the average of the most recent observations within a sliding window of fixed length, say \(n\).. It is a technique to smooth out a time series by averaging neighboring values. It helps in reducing noise and revealing underlying trends.

The formula for the moving average method is:

\[
\hat{Y}_{t+1} = \frac{1}{n}(Y_t + Y_{t-1} + ... + Y_{t-n+1})
\]

Here, \(\hat{Y}_{t+1}\) is the forecast value for the next time period and \(Y_t\), \(Y_{t-1}\), \ldots, \(Y_{t-n+1}\) are the past \(n\) observations.

Smoothing through moving average can either be \textbf{centre-weighted} or \textbf{tailed-weighted}. In former, \(k\) i.e.~\texttt{order} of the moving average, should be an odd number because each data point is replaced with the mean of that observation and \((k-1)/2\) observations before and \((k-1)/2\) observations after it. Smoothed time series \texttt{Nile} for different \texttt{k} can be seen in Figure \ref{fig:ma}.

\begin{figure}

{\centering \includegraphics{DauR_files/figure-latex/ma-1} 

}

\caption{Simple Moving Averages}\label{fig:ma}
\end{figure}

\textbf{Problem Statement-2:} Let's try to visualise moving average trend in Nile data. We will use \texttt{ma} function from \texttt{forecast} for this. In left side Figure of \ref{fig:ma1} we have a smoother (with k = 7) time series and 10 forecast values using last value of moving average.

We can also use \texttt{rollmean} function from \texttt{zoo} library, which is another package to analyse time series objects. Refer right-side figure in \ref{fig:ma1}. In this example, we have used \texttt{wineind} data/time series which shows \emph{Australian total wine sales by wine makers in bottles \textless= 1 litre. Jan 1980 -- Aug 1994.} One of the advantage using this method, is that we can see both original and smoothed time series.

\begin{figure}

{\centering \includegraphics[width=0.48\linewidth]{DauR_files/figure-latex/ma1-1} \includegraphics[width=0.48\linewidth]{DauR_files/figure-latex/ma1-2} 

}

\caption{Simple Moving averages}\label{fig:ma1}
\end{figure}

\hypertarget{exponential-smoothing}{%
\subsection{Exponential smoothing}\label{exponential-smoothing}}

Exponential smoothing techniques update forecasts based on weighted averages of past observations, giving more weight to recent observations. Examples include simple exponential smoothing, Holt's method, and Holt-Winters' method.

As the name suggests simple exponential smoothing or SES is simplest of these. \textbf{Simple exponential smoothing (SES)} is actually a moving weighted average where recent observations are given more weights while calculating averages. \emph{This method is suitable for forecasting data with no clear trend or seasonal pattern.}

\[
\hat{y}_{T+1} = \alpha{y_T} + \alpha(1- \alpha){y_{T-1}} + \alpha(1 - \alpha)^2{y_{T-2}} + ...
\]

The value of \(\alpha\), the smoothing parameter, in above equation should follow \(0 \le \alpha \le 1\). Clearly different values of \(\alpha\) will give different smoothing and we will have to adopt most suitable one, one such example with \(\alpha = 0.1\) is shown in the Figure \ref{fig:ses1} (Left).

Holt extended SES by introducing trend component and thus a new smoothing parameter for trend \(\beta\). The method is also named after him as \textbf{Holt's Linear Method} of exponential smoothing. Now there are are two equations, mathematically.

\[
\begin{aligned}
\hat{y}_{t+h|t} &= l_t + hb_t \\
l_t &= \alpha y_t + (1-\alpha)(l_{t-1} + b_{t-1}) \\
b_t &= \beta(l_t - l_{t-1}) + (1-\beta)b_{t-1}
\end{aligned}
\]

\(\alpha\) and \(\beta\) (again \(0 \le \beta \le 1\)) in above equations are smoothing parameters for level and trend components, respectively; and \(h\) is forecast horizon. Also, \(y_t\) is the actual value, \(l_t\) is the level (or intercept), \(b_t\) is the trend (or slope) of the time series at time \(t\). Refer \ref{fig:ses1} (Right).

To apply \emph{SES}, in R, we can use function \texttt{ses} and for applying \emph{Holt's} smoothing, we can use \texttt{holt}; both from \texttt{forecast} library. Example usages are as follows.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lynxses }\OtherTok{\textless{}{-}} \FunctionTok{ses}\NormalTok{(lynx, }\AttributeTok{h=} \DecValTok{10}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.1}\NormalTok{)}
\NormalTok{holtfit }\OtherTok{\textless{}{-}} \FunctionTok{holt}\NormalTok{(airmiles, }\AttributeTok{h =} \DecValTok{10}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.3}\NormalTok{, }\AttributeTok{beta =} \FloatTok{0.1}\NormalTok{)}

\FunctionTok{autoplot}\NormalTok{(lynxses) }\SpecialCharTok{+}
  \FunctionTok{autolayer}\NormalTok{(lynxses}\SpecialCharTok{$}\NormalTok{fitted) }\SpecialCharTok{+}
\NormalTok{  ggplot2}\SpecialCharTok{::}\FunctionTok{ggtitle}\NormalTok{(}\StringTok{\textquotesingle{}SES with alpha = 0.1 : Lynx data\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}
\NormalTok{  ggplot2}\SpecialCharTok{::}\FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+}
\NormalTok{  ggplot2}\SpecialCharTok{::}\FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \StringTok{"bottom"}\NormalTok{) }\SpecialCharTok{+}
\NormalTok{  ggplot2}\SpecialCharTok{::}\FunctionTok{labs}\NormalTok{(}\AttributeTok{color =} \StringTok{""}\NormalTok{)}

\FunctionTok{autoplot}\NormalTok{(holtfit) }\SpecialCharTok{+}
  \FunctionTok{autolayer}\NormalTok{(holtfit}\SpecialCharTok{$}\NormalTok{fitted) }\SpecialCharTok{+}
\NormalTok{  ggplot2}\SpecialCharTok{::}\FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Holt\textquotesingle{}s Linear Trend method with beta = 0.1 : AirMiles data"}\NormalTok{) }\SpecialCharTok{+}
\NormalTok{  ggplot2}\SpecialCharTok{::}\FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+}
\NormalTok{  ggplot2}\SpecialCharTok{::}\FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \StringTok{"bottom"}\NormalTok{) }\SpecialCharTok{+}
\NormalTok{  ggplot2}\SpecialCharTok{::}\FunctionTok{labs}\NormalTok{(}\AttributeTok{color =} \StringTok{""}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.49\linewidth]{DauR_files/figure-latex/ses1-1} \includegraphics[width=0.49\linewidth]{DauR_files/figure-latex/ses1-2} 

}

\caption{Simple Exponential Smoothing (Left) Vs. Holt's Linear Method (Right)}\label{fig:ses1}
\end{figure}

As we have discussed earlier, damped trends work better many times. So based on Mckenzie and gardner work, an additional dampening paramater \(\phi\) was introduced. The equations were modified as -
\[
\begin{aligned}
\hat{y}_{t+h|t} &= l_t + (\phi + \phi^2 + ... + \phi^h)b_t \\
l_t &= \alpha y_t + (1-\alpha)(l_{t-1} + \phi b_{t-1}) \\
b_t &= \beta(l_t - l_{t-1}) + (1-\beta)\phi b_{t-1}
\end{aligned}
\]

Clearly, if \(\phi = 1\), the above equations are equivalent to Holt's Linear Method. \(\phi\) for values \(0 < \phi < 1\), dampens the trend, as shown in figure \ref{fig:hl2} (Left). A comparison of both can be seen in the Figure \ref{fig:hl2} (Right).

To extend holt's linear method with \emph{damped trends} in R, we can set parameter \texttt{damped\ =\ TRUE} in \texttt{holt} function discussed above. Example code can be seen below.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}

\NormalTok{holtfit }\OtherTok{\textless{}{-}} \FunctionTok{holt}\NormalTok{(airmiles, }\AttributeTok{h =} \DecValTok{10}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.3}\NormalTok{, }\AttributeTok{beta =} \FloatTok{0.1}\NormalTok{, }\AttributeTok{damped =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{phi =} \FloatTok{0.8}\NormalTok{)}
\FunctionTok{autoplot}\NormalTok{(holtfit) }\SpecialCharTok{+}
  \FunctionTok{autolayer}\NormalTok{(holtfit}\SpecialCharTok{$}\NormalTok{fitted) }\SpecialCharTok{+}
\NormalTok{  ggplot2}\SpecialCharTok{::}\FunctionTok{ggtitle}\NormalTok{(}\StringTok{\textquotesingle{}Damped Trend Forecast with phi=0.8\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}
\NormalTok{  ggplot2}\SpecialCharTok{::}\FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+}
\NormalTok{  ggplot2}\SpecialCharTok{::}\FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \StringTok{"bottom"}\NormalTok{) }\SpecialCharTok{+}
\NormalTok{  ggplot2}\SpecialCharTok{::}\FunctionTok{labs}\NormalTok{(}\AttributeTok{color =} \StringTok{""}\NormalTok{)}


\NormalTok{holtfit }\OtherTok{\textless{}{-}} \FunctionTok{holt}\NormalTok{(airmiles,}
                \AttributeTok{h =} \DecValTok{10}\NormalTok{,}
                \AttributeTok{alpha =} \FloatTok{0.3}\NormalTok{,}
                \AttributeTok{beta =} \FloatTok{0.1}\NormalTok{)}

\NormalTok{holtfit2 }\OtherTok{\textless{}{-}} \FunctionTok{holt}\NormalTok{(}
\NormalTok{  airmiles,}
  \AttributeTok{h =} \DecValTok{10}\NormalTok{,}
  \AttributeTok{alpha =} \FloatTok{0.3}\NormalTok{,}
  \AttributeTok{beta =} \FloatTok{0.1}\NormalTok{,}
  \AttributeTok{damped =} \ConstantTok{TRUE}\NormalTok{,}
  \AttributeTok{phi =} \FloatTok{0.8}
\NormalTok{)}

\FunctionTok{autoplot}\NormalTok{(airmiles)}\SpecialCharTok{+}
  \FunctionTok{autolayer}\NormalTok{(holtfit, }\AttributeTok{series=}\StringTok{"Holt\textquotesingle{}s method"}\NormalTok{, }\AttributeTok{PI =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{autolayer}\NormalTok{(holtfit2, }\AttributeTok{series=}\StringTok{"Holt\textquotesingle{}s method with damped trend"}\NormalTok{, }\AttributeTok{PI =} \ConstantTok{FALSE}\NormalTok{)}\SpecialCharTok{+}
\NormalTok{  ggplot2}\SpecialCharTok{::}\FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Holt\textquotesingle{}s method"}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{xlab}\NormalTok{(}\StringTok{"Year"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"Revenue passenger miles"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{guides}\NormalTok{(}\AttributeTok{colour=}\FunctionTok{guide\_legend}\NormalTok{(}\AttributeTok{title=}\StringTok{"Method"}\NormalTok{)) }\SpecialCharTok{+}
\NormalTok{  ggplot2}\SpecialCharTok{::}\FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \StringTok{"bottom"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.49\linewidth]{DauR_files/figure-latex/hl2-1} \includegraphics[width=0.49\linewidth]{DauR_files/figure-latex/hl2-2} 

}

\caption{Linear methos Vs. Damped trends}\label{fig:hl2}
\end{figure}

Now to incorporate \texttt{seasonality} (with say \(m\) periods) along with trend and level, Holt and Winters suggested to incorporate additional parameter \(\gamma\) in above equations.

\[
\text{Holt-Winters method:}
\]
\[
\begin{aligned}
\hat{y}_{t+h|t} &= l_t + hb_t + s_{t+m-(m-1 \bmod m)} \\
l_t &= \alpha(y_t - s_{t-m}) + (1-\alpha)(l_{t-1} + b_{t-1}) \\
b_t &= \beta(l_t - l_{t-1}) + (1-\beta)b_{t-1} \\
s_t &= \gamma(y_t - l_{t-1} - b_{t-1}) + (1-\gamma)s_{t-m}
\end{aligned}
\]

An example can be seen in figure \ref{fig:hw1}. To apply \emph{Holt-Winters} method in R, we can use \texttt{hw} function with desired values of parameters. An example code may be as follows, where we have tried to extend \texttt{AirPassengers} time series with this method with both \texttt{additive} and \texttt{multiplicative} trends.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hw1 }\OtherTok{\textless{}{-}} \FunctionTok{hw}\NormalTok{(AirPassengers, }\AttributeTok{seasonal =} \StringTok{"additive"}\NormalTok{)}
\NormalTok{hw2 }\OtherTok{\textless{}{-}} \FunctionTok{hw}\NormalTok{(AirPassengers, }\AttributeTok{seasonal =} \StringTok{"multiplicative"}\NormalTok{)}


\FunctionTok{autoplot}\NormalTok{(hw1) }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Additive Model"}\NormalTok{)}

\FunctionTok{autoplot}\NormalTok{(hw2) }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Multiplicative Method"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.48\linewidth]{DauR_files/figure-latex/hw1-1} \includegraphics[width=0.48\linewidth]{DauR_files/figure-latex/hw1-2} 

}

\caption{Holt-Winter's Method of Forecasting}\label{fig:hw1}
\end{figure}

\hypertarget{seasonal-decomposition-methods}{%
\subsection*{Seasonal decomposition methods}\label{seasonal-decomposition-methods}}
\addcontentsline{toc}{subsection}{Seasonal decomposition methods}

Seasonal decomposition techniques decompose a time series into its different components, such as trend, seasonality, and noise. This allows for a better understanding of the individual components and their impact on the overall series. There are several methods to decompose a time series into its different components. We will however, discuss two of these.

\hypertarget{classical-seasonal-decomposition}{%
\subsection{Classical seasonal decomposition}\label{classical-seasonal-decomposition}}

First of these methods is \textbf{classical decomposition method} which decomposes a time series into its trend, seasonal, and residual components. It assumes that the seasonal component repeats identically from year to year, and the trend component changes linearly over time. The steps involved in the classical decomposition method are as follows:

\begin{itemize}
\tightlist
\item
  \textbf{Trend Component:} The trend component is estimated using a moving average or regression method.
\item
  \textbf{Seasonal Component:} The seasonal component is estimated by averaging the values across the same seasonal periods in different years.
\item
  \textbf{Residual Component:} The residual component is obtained by subtracting the trend and seasonal components from the original time series.
\end{itemize}

\textbf{Problem Statement:} Let's try to decompose a time series say \texttt{AirPassengers} to see its components. To decompose a time series in R, we will use function \texttt{decompose} as shown in the following code. Decomposing time series in R gives us four different plots each for (i) Observed i.e.~original values, (ii) trend, (iii) seasonality and (iv) random noise available.

Case-1: Additive decomposition

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{decomposed\_air\_passengers }\OtherTok{\textless{}{-}} \FunctionTok{decompose}\NormalTok{(AirPassengers)}
\FunctionTok{summary}\NormalTok{(decomposed\_air\_passengers)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          Length Class  Mode     
## x        144    ts     numeric  
## seasonal 144    ts     numeric  
## trend    144    ts     numeric  
## random   144    ts     numeric  
## figure    12    -none- numeric  
## type       1    -none- character
\end{verbatim}

Case-2: Multiplicative decomposition. In this case we can use argument \texttt{type}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{decomposed\_air\_passengers2 }\OtherTok{\textless{}{-}} \FunctionTok{decompose}\NormalTok{(AirPassengers, }\AttributeTok{type =} \StringTok{"multiplicative"}\NormalTok{)}
\FunctionTok{summary}\NormalTok{(decomposed\_air\_passengers2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          Length Class  Mode     
## x        144    ts     numeric  
## seasonal 144    ts     numeric  
## trend    144    ts     numeric  
## random   144    ts     numeric  
## figure    12    -none- numeric  
## type       1    -none- character
\end{verbatim}

A plot of decomposed \texttt{AirPassengers}, using classical approach is shown in Figure \ref{fig:stl1}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{AirPassengers }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{decompose}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{plot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{DauR_files/figure-latex/stl1-1} 

}

\caption{Seasonal Decomposition Techniques}\label{fig:stl1}
\end{figure}

\hypertarget{seasonal-and-trend-decomposition-using-loess.}{%
\subsection[Seasonal and Trend decomposition using Loess.]{\texorpdfstring{Seasonal and Trend decomposition using Loess\footnote{Loess is a method for estimating nonlinear relationships}.}{Seasonal and Trend decomposition using Loess.}}\label{seasonal-and-trend-decomposition-using-loess.}}

STL (Seasonal and Trend decomposition using Loess) is a robust and flexible method for decomposing a time series into its trend, seasonal, and residual components. It uses local regression (Loess) to estimate the trend and seasonal components. The STL algorithm is as follows:

\begin{itemize}
\tightlist
\item
  Seasonal Component: The seasonal component is estimated using Loess, which fits a smooth curve to the seasonal patterns.
\item
  Trend Component: The trend component is estimated by removing the estimated seasonal component from the original time series.
\item
  Residual Component: The residual component is obtained by subtracting the estimated trend and seasonal components from the original time series.
\end{itemize}

To decompose time series in R, using STL, we will use function \texttt{stl} as shown below. A plot of STL decomposed \texttt{AirPassengers} is shown in figure \ref{fig:stl2}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{stl}\NormalTok{(AirPassengers, }\AttributeTok{s.window =} \StringTok{"periodic"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{plot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{DauR_files/figure-latex/stl2-1} 

}

\caption{STL Decomposition}\label{fig:stl2}
\end{figure}

\hypertarget{model-based-techniques}{%
\subsection*{Model-based techniques}\label{model-based-techniques}}
\addcontentsline{toc}{subsection}{Model-based techniques}

Model-based techniques involve fitting a specific mathematical or statistical model to the observed time series data. These techniques assume a particular structure for the data and estimate model parameters based on that. Model-based techniques typically require more assumptions but can provide a more detailed understanding of the underlying dynamics. Examples of model-based techniques include:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Autoregressive Integrated Moving Average (ARIMA)}: ARIMA models capture the linear dependencies between lagged observations and differences of the time series. They are commonly used for modeling stationary time series.
\item
  \textbf{Seasonal ARIMA (SARIMA)}: SARIMA models extend the ARIMA framework to incorporate seasonality in the data. They are suitable for time series exhibiting both trend and seasonality.
\end{enumerate}

In future version of the book, we will discuss these methods.

\hypertarget{plotting-different-elements-of-time-series}{%
\section{Plotting different elements of time series}\label{plotting-different-elements-of-time-series}}

In R, we can use functions such as -

\begin{itemize}
\tightlist
\item
  \texttt{ggseasonplot()}: to create a seasonal plot
\item
  \texttt{ggsubseriesplot()}: to create mini plots for each season and show seasonal means
\item
  \texttt{gglagplot()}: Plot the time series against lags of itself
\item
  \texttt{ggAcf()}: Plot the autocorrelation function (ACF)
\end{itemize}

Example-1:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{theme\_set}\NormalTok{(}\FunctionTok{theme}\NormalTok{(}\AttributeTok{axis.text.x =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{angle =} \DecValTok{90}\NormalTok{, }
                                           \AttributeTok{vjust =} \FloatTok{0.5}\NormalTok{, }
                                           \AttributeTok{hjust=}\DecValTok{1}\NormalTok{)))}
\FunctionTok{library}\NormalTok{(patchwork)}
\NormalTok{g1 }\OtherTok{\textless{}{-}} \FunctionTok{ggseasonplot}\NormalTok{(AirPassengers) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{()}
\NormalTok{g2 }\OtherTok{\textless{}{-}} \FunctionTok{ggsubseriesplot}\NormalTok{(AirPassengers) }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{\textquotesingle{}Sub{-}Series Plot\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{()}
\NormalTok{(g1 }\SpecialCharTok{/}\NormalTok{ g2)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{DauR_files/figure-latex/unnamed-chunk-388-1} 

}

\caption{Seasonalilty}\label{fig:unnamed-chunk-388}
\end{figure}

Example-2: Lag Plot of \texttt{AirPassengers} may be seen at Figure \ref{fig:lagp}. In the figure notice that plot at lag=12 suggest that series has seasonality with 12 periods.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gglagplot}\NormalTok{(AirPassengers) }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{\textquotesingle{}Lag Plots\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \StringTok{"bottom"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{color =} \ConstantTok{NULL}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{guides}\NormalTok{(}\AttributeTok{color =} \FunctionTok{guide\_legend}\NormalTok{(}\AttributeTok{nrow =} \DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{DauR_files/figure-latex/lagp-1} 

}

\caption{Seasonalilty through Lagplots}\label{fig:lagp}
\end{figure}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{part-vi-network-analytics}{%
\chapter*{Part VI: Network Analytics}\label{part-vi-network-analytics}}
\addcontentsline{toc}{chapter}{Part VI: Network Analytics}

\hypertarget{network-analyticsgraph-theory-in-r}{%
\chapter{Network Analytics/Graph theory in R}\label{network-analyticsgraph-theory-in-r}}

\textbf{The content is under development/Finalisation.}

Network analysis has emerged as a pivotal tool in the realm of audit analytics, offering auditors a powerful method to uncover intricate relationships and patterns within vast datasets. Leveraging the rich ecosystem of packages in R, auditors can construct, visualize, and analyze networks with ease, enabling them to detect anomalies, identify key players, and assess the robustness of interconnected systems. From detecting fraud to optimizing supply chains, the application of network analysis in auditing not only enhances risk assessment but also empowers auditors to make data-driven decisions with greater precision and confidence. So let's dive in.

\hypertarget{an-introduction-to-graph-theory}{%
\section{An introduction to Graph theory}\label{an-introduction-to-graph-theory}}

Network analysis applies concepts of graph theory, which is branch of mathematics, to analyze and interpret complex systems, such as social networks, transportation networks, and biological networks, to uncover patterns, structures, and dynamics within these systems. Graph theory focuses on the properties and characteristics of graphs, such as paths, cycles, connectivity, and graph coloring.

\hypertarget{definition-of-a-graph}{%
\subsection{Definition of a graph}\label{definition-of-a-graph}}

By definition, a graph is a mathematical structure consisting of a set of vertices (or nodes or actors) and a set of edges (or connections or links) that establish relationships between these vertices. Mathematically a graph \(G = (V, E)\) consists of a set \(V\) of vertices/nodes and a set \(E\) of edges/links, as illustrated in \ref{fig:gra1}. A vertex \(V\) may represent real-world objects such as persons, computers, products, etc. An edge \(E\), on the other hand may represent the \emph{relationship} between the nodes, it connects, such as friendship between those persons, or physical connection between the computers, etc.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{DauR_files/figure-latex/gra1-1} 

}

\caption{An Example Graph}\label{fig:gra1}
\end{figure}

\hypertarget{types-of-graph}{%
\subsection*{Types of graph}\label{types-of-graph}}
\addcontentsline{toc}{subsection}{Types of graph}

Example graph shown in figure \ref{fig:gra1} is basically an \textbf{Undirected Graph}, where the edges between the nodes aren't directed. E.g. A connection between two computers. On the other hand, if the edge(s) between the nodes is following a direction or specific order, then the graph is known as \textbf{Directed Graph}. Example on twitter (now X) an user may follow other users without being followed by them in turn, as shown in figure \ref{fig:gra2}.

\begin{figure}

{\centering \includegraphics[width=1\linewidth,height=0.33\textheight]{DauR_files/figure-latex/gra2-1} 

}

\caption{An Example Directed Graph}\label{fig:gra2}
\end{figure}

Moreover, sometimes an edge may be connected with itself. In this case, the edge is called as \textbf{self-edge} and such graphs (allowing vertices to join themselves) are called as \textbf{Pseudographs}. As real-world example of pseudographs think of transactions between different firms. A firm should usually transact with other firms but sometimes it may transact within its own account maintained separately for a specific purpose. Example in figure \ref{fig:gra3} (left) node \texttt{E} is connected with itself.

Further sometimes there may be more than one edge connecting same pair of nodes (in same order). Such edges are called as \textbf{multi-edges}; e.g.~in figure \ref{fig:gra3} (right) there are two edges connecting from \texttt{B} to \texttt{E}. In real-world example we can think of multiple flights operating between same pair of airports by different carriers.

\begin{figure}

{\centering \includegraphics[width=0.48\linewidth,height=0.33\textheight]{DauR_files/figure-latex/gra3-1} \includegraphics[width=0.48\linewidth,height=0.33\textheight]{DauR_files/figure-latex/gra3-2} 

}

\caption{Examples of Self Edge and Multi Edge}\label{fig:gra3}
\end{figure}

\textbf{Complete Graphs} are graphs where all pairs of vertices are connected by an edge. Example complete graphs are shown in the figure \ref{fig:gra4}. While in real-world instances of complete graphs may be rare, it maybe possible that a few of the nodes/vertices in a bigger graph show complete mutual relationship. In other words, the graph of those vertices if taken as a sub-graph is complete, then that complete subgraph is called as \textbf{clique}. As an example think of a subgraph consisting of nodes \texttt{B,\ D,\ E\ and\ G} in graph shown in \ref{fig:gra1} is a clique.

\begin{figure}

{\centering \includegraphics[width=0.49\linewidth,height=0.31\textheight]{DauR_files/figure-latex/gra4-1} \includegraphics[width=0.49\linewidth,height=0.31\textheight]{DauR_files/figure-latex/gra4-2} 

}

\caption{Examples of Complete Graphs}\label{fig:gra4}
\end{figure}

Before moving on, let us learn about two more specific types of graph. One is known as \textbf{Bipartite Graphs}. These are actually graphs having two mutually disjoint set of vertices with condition that no pair within the same set is connected. E.g. graph in figure \ref{fig:gra5} (Left) is a bipartite graph. We can think of two departments where edges represent correspondence between officials between the two departments. Actually the idea can be extended to form k-partite graphs having \texttt{k} such disjoint sets.

Another category is \textbf{Trees}, representing hierarchical relationships among entities/individuals. In a tree, vertices are connected through edges or links, making it a type of graph. For a graph to qualify as a tree, there must be precisely one path between any pair of vertices when considered undirected. The bipartite graph illustrated in the example figure \ref{fig:gra5} (on the left) can also be interpreted as a tree because it meets this criterion. Refer figure \ref{fig:gra5} (Right) wherein the same graph has been redrawn as a tree.

\begin{figure}

{\centering \includegraphics[width=0.49\linewidth,height=0.4\textheight]{DauR_files/figure-latex/gra5-1} \includegraphics[width=0.49\linewidth,height=0.4\textheight]{DauR_files/figure-latex/gra5-2} 

}

\caption{Bipartite and Tree Graph example}\label{fig:gra5}
\end{figure}

\hypertarget{vertex-and-edge-attributes}{%
\subsection{Vertex and Edge attributes}\label{vertex-and-edge-attributes}}

Recall that a Graph basically consists of two sets \((V, E)\) wherein \(V\) is a set of \texttt{vertices} and \(E\) is set of \texttt{edges}. In their simplest form, these sets can be conceived as lists, but both can be enriched by incorporating additional attributes. E.g. if a graph consists of airports and flights operating between them, the set \(V\) representing airports could include attributes such as, (i) names, (ii) types whether domestic or international, (iii) geographical coordinates, (iv) State/Country in which located, etc. and so on. Similarly, the set of edges \(E\) could encompass additional details like (i) the count of flights operating between two airports, (ii) the airlines servicing those flights, (iii) the aerial distance between airports, and so on. An example may be seen in figure \ref{fig:gra6}.

\begin{figure}

{\centering \includegraphics[width=0.98\linewidth,height=0.32\textheight]{DauR_files/figure-latex/gra6-1} 

}

\caption{Examples of Vertex and Edge properties}\label{fig:gra6}
\end{figure}

\hypertarget{practical-approach-for-creating-graphs-in-r}{%
\section{Practical approach for creating graphs in R}\label{practical-approach-for-creating-graphs-in-r}}

\hypertarget{packages-to-use}{%
\subsection{Packages to use}\label{packages-to-use}}

One of the best packages for creating and analysing graphs in R is \texttt{igraph}. Package \texttt{igraph} was originally developed by \textbf{Gábor Csárdi} and \textbf{Tamás Nepusz}, and is written in the C programming language in order to achieve good performance.
Let's load it.

\begin{verbatim}
library(igraph)
\end{verbatim}

Apart from \texttt{igraph} which in itself is a complete package for network analysis, we will also be using \texttt{visNetwork} and \texttt{ggraph} packages. Former is used to create interactive network charts and latter is used to create \texttt{ggplot2} compatible plots so that these can be customised further in familiar environment.

Let's now proceed to learn creating graph objects in R. Actually graph objects can be created in many ways, out of which we will learn three methods. These methods will serve our purpose most of the time.

\hypertarget{creating-a-graph-from-data-frame}{%
\subsection{Creating a graph from data frame}\label{creating-a-graph-from-data-frame}}

When working with data analysis, data frames are often our primary tool. Consequently, the most common and practical approach to creating graphs in igraph involves utilizing data.frame objects. Essentially, the \texttt{data.frame} objects we use to generate graphs should consist of at least two columns, where each row represents an edge within the intended graph. The first column is interpreted as the \texttt{\textquotesingle{}from\textquotesingle{}} node, while the second column is considered the \texttt{\textquotesingle{}to\textquotesingle{}} node, regardless of their respective column names. To achieve this, we employ the \texttt{graph\_from\_data\_frame()} function from the \texttt{igraph} library. Its syntax is as follows:

\begin{verbatim}
graph_from_data_frame(
  d =         ,  # data.frame
  directed = TRUE,
  vertices = NULL # optional data.frame of vertices
)
\end{verbatim}

Example-1: Let's construct a graph object from a data frame containing four edges.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Example data frame of edges}
\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{from =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{),}
  \AttributeTok{to =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{4}\NormalTok{)}
\NormalTok{)}
\CommentTok{\# Creating graph object}
\NormalTok{gr1 }\OtherTok{\textless{}{-}} \FunctionTok{graph\_from\_data\_frame}\NormalTok{(df)}
\CommentTok{\# Let\textquotesingle{}s print it}
\NormalTok{gr1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## IGRAPH 4df74a4 DN-- 5 4 -- 
## + attr: name (v/c)
## + edges from 4df74a4 (vertex names):
## [1] 1->2 1->3 3->5 3->4
\end{verbatim}

Here, we observe that a graph object named \texttt{gr1} has been successfully generated and printed in the console, providing us with pertinent information about it.

\begin{itemize}
\tightlist
\item
  The first line always begins with \texttt{IGRAPH}, followed by seven characters, which represent the initial characters of a unique graph ID. Interested users can employ the \texttt{graph\_id()} function to retrieve the full ID if needed.
\item
  Subsequently, a four-letter character string is displayed. In this example, two are \texttt{UN}, followed by two blanks or \texttt{-\/-}. These characters signify the following:

  \begin{itemize}
  \tightlist
  \item
    The first letter distinguishes between directed (\texttt{D}) and undirected (\texttt{U}) graphs.
  \item
    The second letter, \texttt{N}, denotes named graphs, i.e., graphs with the name vertex attribute set.
  \item
    The third letter, \texttt{W}, indicates weighted graphs, i.e., graphs with the weight edge attribute set.
  \item
    The fourth letter, \texttt{B}, signifies bipartite graphs, i.e., graphs with the type vertex attribute set.
  \end{itemize}
\item
  Following this is the count of vertices and edges, separated by two dashes.
\item
  Starting from the second line, the graph's attributes are listed, separated by commas. Each attribute's type -- graph (\texttt{g}), vertex (\texttt{v}), or edge (\texttt{e}) -- and data type -- character (\texttt{c}), numeric (\texttt{n}), logical (\texttt{l}), or other (\texttt{x}) -- are specified.
\item
  In the last line a few of the edges are printed.
\end{itemize}

Readers may notice that all four edges from our df have been imported into the gr1 graph and are displayed accordingly.

Now \texttt{directed} argument by default is \texttt{TRUE} so by default the graph created is \texttt{directed} as also confirmed by first alphabet \texttt{D} in the four character string. So let's check how an undirected graph is created and printed.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Creating graph object}
\NormalTok{gr2 }\OtherTok{\textless{}{-}} \FunctionTok{graph\_from\_data\_frame}\NormalTok{(df, }\AttributeTok{directed =} \ConstantTok{FALSE}\NormalTok{)}
\CommentTok{\# Let\textquotesingle{}s print it}
\NormalTok{gr2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## IGRAPH 4df8ef6 UN-- 5 4 -- 
## + attr: name (v/c)
## + edges from 4df8ef6 (vertex names):
## [1] 1--2 1--3 3--5 3--4
\end{verbatim}

All good. First letter is now \texttt{U} representing undirected graph. Readers may also notice another change while printing the edges that these are now printed without any arrow mark. Here we created another graph object just to show how the \texttt{directed} argument is used. However, any existing \textbf{\texttt{directed} graph can be converted to an \texttt{undirected} graph} using function \texttt{as.directed()}. Check-

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{as.undirected}\NormalTok{(gr1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## IGRAPH 4dfa233 UN-- 5 4 -- 
## + attr: name (v/c)
## + edges from 4dfa233 (vertex names):
## [1] 1--3 1--2 3--5 3--4
\end{verbatim}

Now what about the last argument \texttt{vertices\ =} in the function \texttt{graph\_from\_data\_frame}? In most of the cases, the \texttt{d} argument having data of edges may be sufficient, yet sometimes a graph may contain isolates (isolated nodes not connected with any other edge). So to include those vertices, we may use \texttt{vertices} argument. Additionally to include any of the vertex property in the graph being created, we may use that as an additional columns in the data frame. Similar to this analogy, all additional columns in our edges dataset (after first two columns) will be used to set edge properties.

Example-2: Let's add an isolated edge \texttt{"6"} in our graph.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_v }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{id =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{),}
  \AttributeTok{name =} \FunctionTok{c}\NormalTok{(}\StringTok{"Ram"}\NormalTok{, }\StringTok{"Shyam"}\NormalTok{, }\StringTok{"Alex"}\NormalTok{, }\StringTok{"Bob"}\NormalTok{, }\StringTok{"Charlie"}\NormalTok{, }\StringTok{"Kumar"}\NormalTok{)}
\NormalTok{)}
\CommentTok{\# Creating graph object}
\NormalTok{gr3 }\OtherTok{\textless{}{-}} \FunctionTok{graph\_from\_data\_frame}\NormalTok{(df, }\AttributeTok{directed =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{vertices =}\NormalTok{ df\_v)}
\CommentTok{\# Let\textquotesingle{}s print it}
\NormalTok{gr3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## IGRAPH 4dfd5ea UN-- 6 4 -- 
## + attr: name (v/c)
## + edges from 4dfd5ea (vertex names):
## [1] Ram --Shyam   Ram --Alex    Alex--Charlie Alex--Bob
\end{verbatim}

Notice the change in number of edges now. Also notice that the edges printed with different names due to presence of column named \texttt{name} in vertices data frame.

Readers may try to add additional columns in edge dataset/data frame and create respective graph objects.

\hypertarget{creating-a-graph-from-edge-list}{%
\subsection{Creating a graph from Edge list}\label{creating-a-graph-from-edge-list}}

Creating graphs from edge-lists is nearly the same approach as creating graphs from data frames. Only difference is absence of \texttt{vertices} argument here. The syntax is

\begin{verbatim}
graph_from_edgelist(el, directed = TRUE)
\end{verbatim}

Here \texttt{el} should be a two column matrix, character or numeric, representing edges; and thus that's another difference here.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{edges }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{origin =} \FunctionTok{c}\NormalTok{(}\StringTok{"Ram"}\NormalTok{, }\StringTok{"Ram"}\NormalTok{, }\StringTok{"Alex"}\NormalTok{, }\StringTok{"Alex"}\NormalTok{), }
  \AttributeTok{dest =} \FunctionTok{c}\NormalTok{(}\StringTok{"Shyam"}\NormalTok{, }\StringTok{"Alex"}\NormalTok{, }\StringTok{"Charlie"}\NormalTok{, }\StringTok{"Bob"}\NormalTok{)}
\NormalTok{)}
\NormalTok{edges }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{(edges)}
\CommentTok{\# Now let\textquotesingle{}s create a graph}
\NormalTok{gr4 }\OtherTok{\textless{}{-}} \FunctionTok{graph\_from\_edgelist}\NormalTok{(edges, }\AttributeTok{directed =} \ConstantTok{FALSE}\NormalTok{)}
\CommentTok{\# And print it}
\NormalTok{gr4}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## IGRAPH 4dff2dc UN-- 5 4 -- 
## + attr: name (v/c)
## + edges from 4dff2dc (vertex names):
## [1] Ram --Shyam   Ram --Alex    Alex--Charlie Alex--Bob
\end{verbatim}

Fine enough. We have successfully created an undirected graph with \texttt{5} nodes and \texttt{4} edges.

\hypertarget{creating-a-graph-from-adjacency-matrix}{%
\subsection{Creating a graph from adjacency matrix}\label{creating-a-graph-from-adjacency-matrix}}

But what exactly is an adjacency matrix? It's a square matrix sized \(n \times n\), where both the rows and columns are indexed by \(n\) vertices. The \((i, j)\)-th entry of this matrix holds significance:

\begin{itemize}
\tightlist
\item
  In a graph devoid of any edge attribute such as \texttt{weight}, a value of \texttt{1} signifies the existence of an edge from vertex \(v_{i}\) to \(v_j\). Conversely, a value of \texttt{0} indicates the absence of an edge between those two vertices.
\item
  However, in graphs equipped with edge attributes like \texttt{weight}, the corresponding numerical value represents the weight of the edge from vertex \(v_{i}\) to \(v_j\).
\end{itemize}

Example: Let's try to create graph as shown in \ref{fig:gra6} through corresponding adjacency matrix. First, without weights.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Let\textquotesingle{}s create the matrix}
\NormalTok{adj\_mat }\OtherTok{\textless{}{-}} \FunctionTok{structure}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{), }\AttributeTok{dim =} \FunctionTok{c}\NormalTok{(4L, }
\NormalTok{4L), }\AttributeTok{dimnames =} \FunctionTok{list}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"DEL"}\NormalTok{, }\StringTok{"BNG"}\NormalTok{, }\StringTok{"BOM"}\NormalTok{, }\StringTok{"PNQ"}\NormalTok{), }\FunctionTok{c}\NormalTok{(}\StringTok{"DEL"}\NormalTok{, }
\StringTok{"BNG"}\NormalTok{, }\StringTok{"BOM"}\NormalTok{, }\StringTok{"PNQ"}\NormalTok{)))}
\NormalTok{adj\_mat}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     DEL BNG BOM PNQ
## DEL   0   1   1   1
## BNG   1   0   1   0
## BOM   1   1   0   1
## PNQ   1   0   1   0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Let\textquotesingle{}s create the graph}
\NormalTok{gr5 }\OtherTok{\textless{}{-}} \FunctionTok{graph\_from\_adjacency\_matrix}\NormalTok{(adj\_mat)}
\CommentTok{\# Let\textquotesingle{}s print it}
\NormalTok{gr5}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## IGRAPH 4e01077 DN-- 4 10 -- 
## + attr: name (v/c)
## + edges from 4e01077 (vertex names):
##  [1] DEL->BNG DEL->BOM DEL->PNQ BNG->DEL BNG->BOM BOM->DEL BOM->BNG BOM->PNQ
##  [9] PNQ->DEL PNQ->BOM
\end{verbatim}

So far so good. Directed graph (default) has been created with edges between airports both to and from separately. Now let us try to create weighted graph for the same data where weights will now be the distance between those airports.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Let\textquotesingle{}s create the matrix}
\NormalTok{adj\_mat2 }\OtherTok{\textless{}{-}} \FunctionTok{structure}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1750}\NormalTok{, }\DecValTok{1150}\NormalTok{, }\DecValTok{1200}\NormalTok{, }\DecValTok{1750}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{850}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1150}\NormalTok{, }\DecValTok{850}\NormalTok{, }
\DecValTok{0}\NormalTok{, }\DecValTok{120}\NormalTok{, }\DecValTok{1200}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{120}\NormalTok{, }\DecValTok{0}\NormalTok{), }\AttributeTok{dim =} \FunctionTok{c}\NormalTok{(4L, 4L), }\AttributeTok{dimnames =} \FunctionTok{list}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"DEL"}\NormalTok{, }
\StringTok{"BNG"}\NormalTok{, }\StringTok{"BOM"}\NormalTok{, }\StringTok{"PNQ"}\NormalTok{), }\FunctionTok{c}\NormalTok{(}\StringTok{"DEL"}\NormalTok{, }\StringTok{"BNG"}\NormalTok{, }\StringTok{"BOM"}\NormalTok{, }\StringTok{"PNQ"}\NormalTok{)))}
\NormalTok{adj\_mat2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      DEL  BNG  BOM  PNQ
## DEL    0 1750 1150 1200
## BNG 1750    0  850    0
## BOM 1150  850    0  120
## PNQ 1200    0  120    0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Let\textquotesingle{}s create the graph}
\NormalTok{gr6 }\OtherTok{\textless{}{-}} \FunctionTok{graph\_from\_adjacency\_matrix}\NormalTok{(adj\_mat2, }\AttributeTok{weighted =} \ConstantTok{TRUE}\NormalTok{)}
\CommentTok{\# Let\textquotesingle{}s print it}
\NormalTok{gr6}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## IGRAPH 4e02dc5 DNW- 4 10 -- 
## + attr: name (v/c), weight (e/n)
## + edges from 4e02dc5 (vertex names):
##  [1] DEL->BNG DEL->BOM DEL->PNQ BNG->DEL BNG->BOM BOM->DEL BOM->BNG BOM->PNQ
##  [9] PNQ->DEL PNQ->BOM
\end{verbatim}

This time notice that edge property named \texttt{weight} with \texttt{numeric} type has been added to the graph.

\hypertarget{getting-data-back-from-graph-objects}{%
\subsection{Getting data back from graph objects}\label{getting-data-back-from-graph-objects}}

Once the graph objects have been created getting the data back both in \texttt{data.frame} or adjacency \texttt{matrix} is pretty easy. For this we can use either of \texttt{igraph} functions \texttt{as\_data\_frame()} or \texttt{as\_adjacency\_matrix()}. Let check both these functions.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{as\_data\_frame()} : Since package \texttt{tibble}'s earlier versions also had a same named function, it is safe to use \texttt{igraph::as\_data\_frame()} to avoid conflict and bug in the code. It takes a igraph object and outputs a \texttt{data.frame}. For example let's get the data back from \texttt{gr6} object created in above code. Since the \texttt{gr6} was created from an adjacency matrix it will be amusing to see if the function is working correctly.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{igraph}\SpecialCharTok{::}\FunctionTok{as\_data\_frame}\NormalTok{(gr6)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    from  to weight
## 1   DEL BNG   1750
## 2   DEL BOM   1150
## 3   DEL PNQ   1200
## 4   BNG DEL   1750
## 5   BNG BOM    850
## 6   BOM DEL   1150
## 7   BOM BNG    850
## 8   BOM PNQ    120
## 9   PNQ DEL   1200
## 10  PNQ BOM    120
\end{verbatim}

From the output above, it is clear that the function worked correctly. Since the graph was \texttt{directed} the edges have been created from both sides. So let's also check the function's output in case of undirected graph.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{igraph}\SpecialCharTok{::}\FunctionTok{as\_data\_frame}\NormalTok{(}\FunctionTok{as.undirected}\NormalTok{(gr6))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   from  to weight
## 1  DEL BNG   3500
## 2  DEL BOM   2300
## 3  BNG BOM   1700
## 4  DEL PNQ   2400
## 5  BOM PNQ    240
\end{verbatim}

Absolutely fine. But notice that weights of the edges has been combined while converting directed graph to an undirected graph. That also can be tackled of while using \texttt{as.undirected()} by tweaking the argument \texttt{edge.attr.comb} which basically takes a function to combine the separate edge attributes. So,

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{as.undirected}\NormalTok{(gr6, }\AttributeTok{edge.attr.comb =} \FunctionTok{list}\NormalTok{(}\AttributeTok{weight =}\NormalTok{ mean)) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  igraph}\SpecialCharTok{::}\FunctionTok{as\_data\_frame}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   from  to weight
## 1  DEL BNG   1750
## 2  DEL BOM   1150
## 3  BNG BOM    850
## 4  DEL PNQ   1200
## 5  BOM PNQ    120
\end{verbatim}

It will not be out of place to mention here that this function has an additional argument \texttt{what}, in case requirement is to export \texttt{"vertex"} and/or \texttt{"edges"} data. If the argument is set to \texttt{"both"} the two data-sets will be returned in a list.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Vertices data only}
\NormalTok{igraph}\SpecialCharTok{::}\FunctionTok{as\_data\_frame}\NormalTok{(gr6, }\AttributeTok{what =} \StringTok{"vertices"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     name
## DEL  DEL
## BNG  BNG
## BOM  BOM
## PNQ  PNQ
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \texttt{as\_adjacency\_matrix()} : Similar to above function, her this function will take an igraph object and returns adjacency \texttt{matrix} here instead. So let's check it also.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{as.undirected}\NormalTok{(gr6, }\AttributeTok{edge.attr.comb =} \FunctionTok{list}\NormalTok{(}\AttributeTok{weight =}\NormalTok{ mean)) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  igraph}\SpecialCharTok{::}\FunctionTok{as\_adjacency\_matrix}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 4 x 4 sparse Matrix of class "dgCMatrix"
##     DEL BNG BOM PNQ
## DEL   .   1   1   1
## BNG   1   .   1   .
## BOM   1   1   .   1
## PNQ   1   .   1   .
\end{verbatim}

We may notice that adjacency matrix has been returned but without weights. So to map \texttt{weights} we may use its \texttt{attr} argument which by default is \texttt{NULL}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{as.undirected}\NormalTok{(gr6, }\AttributeTok{edge.attr.comb =} \FunctionTok{list}\NormalTok{(}\AttributeTok{weight =}\NormalTok{ mean)) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  igraph}\SpecialCharTok{::}\FunctionTok{as\_adjacency\_matrix}\NormalTok{(}\AttributeTok{attr =} \StringTok{"weight"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 4 x 4 sparse Matrix of class "dgCMatrix"
##      DEL  BNG  BOM  PNQ
## DEL    . 1750 1150 1200
## BNG 1750    .  850    .
## BOM 1150  850    .  120
## PNQ 1200    .  120    .
\end{verbatim}

\hypertarget{adding-vertex-andor-edge-attributes-to-an-existing-igraph-object}{%
\section{\texorpdfstring{Adding vertex and/or edge attributes to an existing \texttt{igraph} object}{Adding vertex and/or edge attributes to an existing igraph object}}\label{adding-vertex-andor-edge-attributes-to-an-existing-igraph-object}}

We've previously explored how vertex and edge properties can be incorporated into an \texttt{igraph} during its creation. However, there may be instances where the need arises to add vertex and/or edge properties to an \texttt{igraph} object after its creation or to an existing igraph object. Before delving into that, let's first understand how we can retrieve existing edges, nodes, or their attributes from an existing \texttt{igraph}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Getting vertices of an \texttt{igraph} using \texttt{V()}
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{V}\NormalTok{(gr6)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## + 4/4 vertices, named, from 4e02dc5:
## [1] DEL BNG BOM PNQ
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Getting edges of an \texttt{igraph} using \texttt{E()}
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{E}\NormalTok{(gr6)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## + 10/10 edges from 4e02dc5 (vertex names):
##  [1] DEL->BNG DEL->BOM DEL->PNQ BNG->DEL BNG->BOM BOM->DEL BOM->BNG BOM->PNQ
##  [9] PNQ->DEL PNQ->BOM
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Extracting edge or vertex attribute using \texttt{\$}
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Vertex Property "name"}
\FunctionTok{V}\NormalTok{(gr6)}\SpecialCharTok{$}\NormalTok{name}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "DEL" "BNG" "BOM" "PNQ"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Edge property "weight"}
\FunctionTok{E}\NormalTok{(gr6)}\SpecialCharTok{$}\NormalTok{weight}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 1750 1150 1200 1750  850 1150  850  120 1200  120
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Adding vertex or edge property similarly using \texttt{\$}
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# adding vertex property say "airport\_name"}
\FunctionTok{V}\NormalTok{(gr6)}\SpecialCharTok{$}\NormalTok{airport\_name }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"New Delhi"}\NormalTok{, }\StringTok{"Bengaluru"}\NormalTok{, }\StringTok{"Mumbai"}\NormalTok{, }\StringTok{"Pune"}\NormalTok{)}
\CommentTok{\# Single value will be replicated across all values}
\FunctionTok{V}\NormalTok{(gr6)}\SpecialCharTok{$}\NormalTok{country }\OtherTok{\textless{}{-}} \StringTok{"India"}

\DocumentationTok{\#\# Let\textquotesingle{}s check it}
\NormalTok{igraph}\SpecialCharTok{::}\FunctionTok{as\_data\_frame}\NormalTok{(gr6, }\AttributeTok{what =} \StringTok{"vertices"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     name airport_name country
## DEL  DEL    New Delhi   India
## BNG  BNG    Bengaluru   India
## BOM  BOM       Mumbai   India
## PNQ  PNQ         Pune   India
\end{verbatim}

Let's also add some edge property too.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Adding edge property "carrier"}
\FunctionTok{E}\NormalTok{(gr6)}\SpecialCharTok{$}\NormalTok{carrier }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\StringTok{"Vistara"}\NormalTok{,}\DecValTok{6}\NormalTok{), }\FunctionTok{rep}\NormalTok{(}\StringTok{"Indigo"}\NormalTok{, }\DecValTok{2}\NormalTok{), }\FunctionTok{rep}\NormalTok{(}\StringTok{"Air India"}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\CommentTok{\# Adding another edge property on the basis of condition}
\FunctionTok{E}\NormalTok{(gr6)}\SpecialCharTok{$}\NormalTok{route }\OtherTok{\textless{}{-}} \FunctionTok{case\_when}\NormalTok{(}
  \FunctionTok{E}\NormalTok{(gr6)}\SpecialCharTok{$}\NormalTok{weight }\SpecialCharTok{\textless{}=} \DecValTok{400} \SpecialCharTok{\textasciitilde{}} \StringTok{"Short"}\NormalTok{,}
  \FunctionTok{E}\NormalTok{(gr6)}\SpecialCharTok{$}\NormalTok{weight }\SpecialCharTok{\textless{}=} \DecValTok{1000} \SpecialCharTok{\textasciitilde{}} \StringTok{"Medium"}\NormalTok{,}
  \ConstantTok{TRUE} \SpecialCharTok{\textasciitilde{}} \StringTok{"Long"}
\NormalTok{)}

\DocumentationTok{\#\# Let\textquotesingle{}s check it}
\NormalTok{igraph}\SpecialCharTok{::}\FunctionTok{as\_data\_frame}\NormalTok{(gr6)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    from  to weight   carrier  route
## 1   DEL BNG   1750   Vistara   Long
## 2   DEL BOM   1150   Vistara   Long
## 3   DEL PNQ   1200   Vistara   Long
## 4   BNG DEL   1750   Vistara   Long
## 5   BNG BOM    850   Vistara Medium
## 6   BOM DEL   1150   Vistara   Long
## 7   BOM BNG    850    Indigo Medium
## 8   BOM PNQ    120    Indigo  Short
## 9   PNQ DEL   1200 Air India   Long
## 10  PNQ BOM    120 Air India  Short
\end{verbatim}

\hypertarget{visualising-graphs}{%
\section{Visualising graphs}\label{visualising-graphs}}

After covering graph definition and storage methods, let's explore techniques for visualizing them. Visualization is crucial for conveying the essence of graphs and networks, with the arrangement and style playing significant roles in communication. Apart from factors related to visual appeal, layout becomes crucial. The relative positioning of vertices greatly impacts visualization effectiveness. This is evident from the comparison of two graphs in Figure \ref{fig:gra25}, both representing the same graph depicted earlier in Figure \ref{fig:gra2}.

\begin{figure}

{\centering \includegraphics[width=0.48\linewidth,height=0.34\textheight]{DauR_files/figure-latex/gra25-1} \includegraphics[width=0.48\linewidth,height=0.34\textheight]{DauR_files/figure-latex/gra25-2} 

}

\caption{Two layouts of a same graph}\label{fig:gra25}
\end{figure}

Nowadays, there are many packages available in R to plot or visualise graph objects while analysing network data. Of these, we will learn two here, (i) one plotting with \texttt{igraph} only though the plots created will be static; and (ii) \texttt{visNetwork} which will be used to create interactive visualizations. Visualizing geographical networks have been discussed separately in an another Chapter.

\hypertarget{plotting-using-plot-in-igraph}{%
\section{\texorpdfstring{Plotting using \texttt{plot()} in \texttt{igraph}}{Plotting using plot() in igraph}}\label{plotting-using-plot-in-igraph}}

Plotting \texttt{igraph} objects in R is very simple. Just use \texttt{plot} command which basically uses \texttt{plot.igraph} method, and will plot any igraph object using default values to its other arguments. As we have already seen that layouts are important while plotting graph objects, we can set layout using argument \texttt{layout} in \texttt{plot}.

In the following example (Figure \ref{fig:gra26}), we can see two random layouts of same graph as we have seen in Figure \ref{fig:gra25}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_gr }\OtherTok{\textless{}{-}}\NormalTok{ igraph}\SpecialCharTok{::}\FunctionTok{as\_data\_frame}\NormalTok{(kite) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{slice}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{9}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{graph\_from\_data\_frame}\NormalTok{()}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{12345}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(my\_gr, }\AttributeTok{vertex.size =} \DecValTok{25}\NormalTok{, }\AttributeTok{layout =}\NormalTok{ layout\_randomly)}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{54321}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(my\_gr, }\AttributeTok{vertex.size =} \DecValTok{25}\NormalTok{, }\AttributeTok{layout =}\NormalTok{ layout\_randomly)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.48\linewidth,height=0.34\textheight]{DauR_files/figure-latex/gra26-1} \includegraphics[width=0.48\linewidth,height=0.34\textheight]{DauR_files/figure-latex/gra26-2} 

}

\caption{Two Other layouts of a same graph}\label{fig:gra26}
\end{figure}

In the above example, we have seen that same layout may generate different coordinates for plotting at each iteration. However, using specific random number seed, we can fix the random layout of graph for purpose of reproducibility.

Before moving on let's create a famous network/graph of \texttt{Zachary\ karate\ club} and learn how to plot network graph effectively. The description available on Wikipedia for this network, I am reproducing here. \emph{A social network of a karate club was studied by Wayne W. Zachary for a period of three years from 1970 to 1972.{[}2{]} The network captures 34 members of a karate club, documenting links between pairs of members who interacted outside the club. During the study a conflict arose between the administrator ``John A'' and instructor ``Mr.~Hi'' (pseudonyms), which led to the split of the club into two. Half of the members formed a new club around Mr.~Hi; members from the other part found a new instructor or gave up karate. Based on collected data Zachary correctly assigned all but one member of the club to the groups they actually joined after the split.}

This graph object is available in package \texttt{igraphdata} from which we can load it.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(igraph)}
\FunctionTok{library}\NormalTok{(igraphdata)}
\FunctionTok{data}\NormalTok{(}\StringTok{"karate"}\NormalTok{)}
\NormalTok{karate}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## This graph was created by an old(er) igraph version.
##   Call upgrade_graph() on it to use with the current igraph version
##   For now we convert it on the fly...
\end{verbatim}

\begin{verbatim}
## IGRAPH 4b458a1 UNW- 34 78 -- Zachary's karate club network
## + attr: name (g/c), Citation (g/c), Author (g/c), Faction (v/n), name
## | (v/c), label (v/c), color (v/n), weight (e/n)
## + edges from 4b458a1 (vertex names):
##  [1] Mr Hi  --Actor 2  Mr Hi  --Actor 3  Mr Hi  --Actor 4  Mr Hi  --Actor 5 
##  [5] Mr Hi  --Actor 6  Mr Hi  --Actor 7  Mr Hi  --Actor 8  Mr Hi  --Actor 9 
##  [9] Mr Hi  --Actor 11 Mr Hi  --Actor 12 Mr Hi  --Actor 13 Mr Hi  --Actor 14
## [13] Mr Hi  --Actor 18 Mr Hi  --Actor 20 Mr Hi  --Actor 22 Mr Hi  --Actor 32
## [17] Actor 2--Actor 3  Actor 2--Actor 4  Actor 2--Actor 8  Actor 2--Actor 14
## [21] Actor 2--Actor 18 Actor 2--Actor 20 Actor 2--Actor 22 Actor 2--Actor 31
## [25] Actor 3--Actor 4  Actor 3--Actor 8  Actor 3--Actor 9  Actor 3--Actor 10
## + ... omitted several edges
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{karate\_data }\OtherTok{\textless{}{-}}\NormalTok{ igraph}\SpecialCharTok{::}\FunctionTok{as\_data\_frame}\NormalTok{(karate)}
\FunctionTok{head}\NormalTok{(karate\_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    from      to weight
## 1 Mr Hi Actor 2      4
## 2 Mr Hi Actor 3      5
## 3 Mr Hi Actor 4      3
## 4 Mr Hi Actor 5      3
## 5 Mr Hi Actor 6      3
## 6 Mr Hi Actor 7      3
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(karate, }\AttributeTok{layout =}\NormalTok{ layout\_nicely)}
\end{Highlighting}
\end{Shaded}

\includegraphics{DauR_files/figure-latex/gra27-1.pdf}

\hypertarget{layouts}{%
\subsection{Layouts}\label{layouts}}

Network layouts are algorithms that return coordinates for each node in a network. The igraph library offers several built-in layouts. Learning the algorithm behind these layouts is outside the scope of the chapter.

\begin{itemize}
\tightlist
\item
  \texttt{layout\_as\_bipartite()} Minimize edge-crossings in a simple two-row (or column) layout for bipartite graphs.
\item
  \texttt{layout\_as\_star()} A simple layout generator, that places one vertex in the center of a circle and the rest of the vertices equidistantly on the perimeter.
\item
  \texttt{layout\_as\_tree()} The \textbf{Reingold-Tilford} graph layout algorithm having a tree-like layout, perfect for trees, acceptable for graphs with not too many cycles.
\item
  \texttt{layout\_in\_circle()} Places vertices on a circle, in the order of their vertex ids.
\item
  \texttt{layout\_nicely()} This function tries to choose an appropriate graph layout algorithm for the graph, automatically, based on a simple algorithm.
\item
  \texttt{layout\_on\_grid()} places vertices on a rectangular grid, in two or three dimensions
\item
  \texttt{layout\_on\_sphere()} Places vertices on a sphere, approximately uniformly, in the order of their vertex ids.
\item
  \texttt{layout\_randomly()} This function uniformly randomly places the vertices of the graph in two or three dimensions.
\item
  \texttt{layout\_with\_dh()} Places vertices of a graph on the plane, according to the simulated annealing algorithm by \textbf{Davidson and Harel}.
\item
  \texttt{layout\_with\_fr()} Places vertices on the plane using the force-directed layout algorithm by \textbf{Fruchterman and Reingold}.
\item
  \texttt{layout\_with\_gem()} Places vertices on the plane using the GEM force-directed layout algorithm.
\item
  \texttt{layout\_with\_kk()} \textbf{Kamada-Kawai} layout algorithm which places the vertices on the plane, or in 3D space, based on a physical model of springs.
\item
  \texttt{layout\_with\_sugiyama()} \textbf{Sugiyama} layout algorithm for layered directed acyclic graphs. The algorithm minimized edge crossings.
\end{itemize}

Discussing each and every layout here will be out of the scope. Readers are advised to play with these different layouts to get a fair understanding of these layouts. However, we may see a few useful layouts as example on \texttt{karate} data.

\begin{itemize}
\tightlist
\item
  Circular layout: Figure \ref{fig:gra28} Left.
\item
  Fruchterman Reingold: Figure \ref{fig:gra28} Right.
\item
  Kamada Kawai: Figure \ref{fig:gra29} Left.
\item
  Sugiyama: Figure \ref{fig:gra29} Right.
\item
  Tree layouts (two representations): Figure \ref{fig:gra30}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{igraph\_options}\NormalTok{(}\AttributeTok{vertex.size =} \DecValTok{18}\NormalTok{)}
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{))}

\FunctionTok{plot}\NormalTok{(karate, }\AttributeTok{layout =}\NormalTok{ layout\_in\_circle)}
\FunctionTok{title}\NormalTok{(}\StringTok{"Circular layout"}\NormalTok{)}

\FunctionTok{plot}\NormalTok{(karate, }\AttributeTok{layout =}\NormalTok{ layout\_with\_fr)}
\FunctionTok{title}\NormalTok{(}\StringTok{"Fruchterman {-} Reingold"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.48\linewidth,height=0.34\textheight]{DauR_files/figure-latex/gra28-1} 

}

\caption{Two layouts of Zachary Karate Club Network}\label{fig:gra28}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{))}

\FunctionTok{plot}\NormalTok{(karate, }\AttributeTok{layout =}\NormalTok{ layout\_with\_kk)}
\FunctionTok{title}\NormalTok{(}\StringTok{"Kamada Kawai"}\NormalTok{)}

\FunctionTok{plot}\NormalTok{(karate, }\AttributeTok{layout =}\NormalTok{ layout\_with\_sugiyama)}
\FunctionTok{title}\NormalTok{(}\StringTok{"Sugiyama"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.48\linewidth,height=0.34\textheight]{DauR_files/figure-latex/gra29-1} 

}

\caption{Two Other layouts of Zachary Karate Club Network}\label{fig:gra29}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{))}

\FunctionTok{plot}\NormalTok{(karate, }\AttributeTok{layout =}\NormalTok{ layout\_as\_tree)}
\FunctionTok{title}\NormalTok{(}\StringTok{"Default Tree layout"}\NormalTok{)}

\FunctionTok{plot}\NormalTok{(karate, }\AttributeTok{layout =} \FunctionTok{layout\_as\_tree}\NormalTok{(karate, }\AttributeTok{circular =} \ConstantTok{TRUE}\NormalTok{))}
\FunctionTok{title}\NormalTok{(}\StringTok{"circular Tree"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.48\linewidth,height=0.34\textheight]{DauR_files/figure-latex/gra30-1} 

}

\caption{Two Tree layouts of Zachary Karate Club Network}\label{fig:gra30}
\end{figure}

\hypertarget{displaying-vertexedge-properties}{%
\subsection{Displaying Vertex/Edge properties}\label{displaying-vertexedge-properties}}

While the layouts may be important when displaying networks, additional information such as displaying vertex or edge properties such as their categories, etc. may also play an important role. Edge/Vertex properties can both be continuous and/or discrete and visualising those properties in a network shall be through usual properties like color, size, width, shape, etc. Out \texttt{karate} graph object is already colored, so readers may be wondering how the vertices are colored in the representation.

Actually vertices, in an igraph object can be colored using its property attribute \texttt{color}. Let's retrieve it to understand.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{V}\NormalTok{(karate)}\SpecialCharTok{$}\NormalTok{color}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 1 1 1 1 1 1 1 1 2 2 1 1 1 1 2 2 1 1 2 1 2 1 2 2 2 2 2 2 2 2 2 2 2 2
\end{verbatim}

We may see some discrete integer values are stored therein. Let's reallocate these to specific colors as we want.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{V}\NormalTok{(karate)}\SpecialCharTok{$}\NormalTok{color }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"red"}\NormalTok{, }\StringTok{"dodgerblue"}\NormalTok{)[}\FunctionTok{V}\NormalTok{(karate)}\SpecialCharTok{$}\NormalTok{color]}
\FunctionTok{head}\NormalTok{(}\FunctionTok{V}\NormalTok{(karate)}\SpecialCharTok{$}\NormalTok{color)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "red" "red" "red" "red" "red" "red"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(karate, }\AttributeTok{layout =}\NormalTok{ layout\_nicely)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.98\linewidth,height=0.34\textheight]{DauR_files/figure-latex/gra32-1} 

}

\caption{Vertex coloring in Network}\label{fig:gra32}
\end{figure}

Similarly, we can use following attributes to display certain properties of vertices/edges in an igraph plot.

\begin{itemize}
\tightlist
\item
  \textbf{Vertex Properties}

  \begin{itemize}
  \tightlist
  \item
    \texttt{size} - Size of the vertex. Default is 15
  \item
    \texttt{color} - Fill color of the vertex
  \item
    \texttt{frame.color} - Border color of vertex
  \item
    \texttt{shape} - shape of vertex. Can allocate one of the following \texttt{c("circle",\ "square",\ "rectangle",\ "none")}.
  \item
    \texttt{label} - a character vector used to label the nodes
  \item
    \texttt{label.family} Font family of label. Default is \texttt{serif}
  \item
    \texttt{label.font} Font of label. \texttt{1} means plain (default), \texttt{2}: bold, \texttt{3} italic, \texttt{4} bold italic \texttt{5} symbol
  \item
    \texttt{label.cex} font size of label
  \end{itemize}
\item
  \textbf{Edge properties}

  \begin{itemize}
  \tightlist
  \item
    \texttt{color} color of edge
  \item
    \texttt{width} edge width
  \item
    \texttt{lty} line type for edges (\texttt{0} or \texttt{"blank"}; \texttt{1} or \texttt{"solid"}; \texttt{2} or \texttt{"dashed"}; \texttt{3} or \texttt{"dotted"}; \texttt{4} or \texttt{"dotdash"}, \texttt{6} or \texttt{"twodash"})
  \item
    \texttt{label} label of edges (\texttt{label.family}, \texttt{lable.font} and \texttt{label.cex} similarly for font family, font type and font size respectively)
  \item
    \texttt{curved}: Edge curvature; range is \texttt{0-1}
  \item
    \texttt{arrow.size} Arrow size (default is 1) (for directed graphs)
  \item
    \texttt{arrow.width} arrow width, default is \texttt{1}.
  \item
    \texttt{arrow.mode}: arrow mode (\texttt{0} means no arrow; \texttt{1} back, \texttt{2} forward arrow, \texttt{3} both)
  \end{itemize}
\end{itemize}

Apart from adding these properties to igraph object we can set these properties as arguments in the plot function, just by adding \texttt{edge.} or \texttt{vertex.} before the property/attribute name.

Let's see some of these through the following example.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# set reproducible layout}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{l\_s }\OtherTok{\textless{}{-}} \FunctionTok{layout\_nicely}\NormalTok{(karate)}

\CommentTok{\# Add vertex labels}
\FunctionTok{V}\NormalTok{(karate)}\SpecialCharTok{$}\NormalTok{label }\OtherTok{\textless{}{-}}\NormalTok{ stringr}\SpecialCharTok{::}\FunctionTok{str\_remove}\NormalTok{(}\FunctionTok{V}\NormalTok{(karate)}\SpecialCharTok{$}\NormalTok{name, }\StringTok{"Actor "}\NormalTok{)}

\CommentTok{\# Change shapes of two prominent actors}
\FunctionTok{V}\NormalTok{(karate)}\SpecialCharTok{$}\NormalTok{shape }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{(}\FunctionTok{V}\NormalTok{(karate)}\SpecialCharTok{$}\NormalTok{name }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"Mr Hi"}\NormalTok{, }\StringTok{"John A"}\NormalTok{), }\StringTok{"rectangle"}\NormalTok{, }\StringTok{"circle"}\NormalTok{)}

\CommentTok{\# Change Edge width as per "weight"}
\FunctionTok{E}\NormalTok{(karate)}\SpecialCharTok{$}\NormalTok{width }\OtherTok{\textless{}{-}} \FunctionTok{E}\NormalTok{(karate)}\SpecialCharTok{$}\NormalTok{weight}

\FunctionTok{plot}\NormalTok{(karate, }\AttributeTok{layout =}\NormalTok{ l\_s, }\AttributeTok{edge.label.cex =} \FloatTok{0.7}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.98\linewidth]{DauR_files/figure-latex/gra33-1} 

}

\caption{Edge and Vertex properties in Network}\label{fig:gra33}
\end{figure}

\hypertarget{using-visnetwork-for-interactive-visualizations}{%
\section{\texorpdfstring{Using \texttt{visNetwork} for interactive visualizations}{Using visNetwork for interactive visualizations}}\label{using-visnetwork-for-interactive-visualizations}}

\hypertarget{subgraphs-adding-or-deleting-edgesvertices}{%
\section{Subgraphs, adding or deleting edges/vertices}\label{subgraphs-adding-or-deleting-edgesvertices}}

\hypertarget{describing-networkgraph-for-edges-and-nodes-therein}{%
\section{Describing network/graph for edges and nodes therein}\label{describing-networkgraph-for-edges-and-nodes-therein}}

\hypertarget{applying-network-analysis-in-auditfraud-detection}{%
\chapter{Applying network analysis in audit/fraud detection}\label{applying-network-analysis-in-auditfraud-detection}}

\hypertarget{dup_net}{%
\section{Use case-1: Finding network of related entities/persons - Identity Theft}\label{dup_net}}

\textbf{The Following Content is Under Development.}

Imagine a scenario when users may have multiple IDs such as mobile numbers, email ids, and say some other ID issued by a Government Department say Income Tax Department (e.g.~PAN number in Indian Scenario). Using techniques mentioned in section \ref{dup}, we may easily find out duplicate users, i.e.~duplicates on the basis of one ID. Sometimes need arise where we have to find out network of all the duplicate users where they have changed one or two IDs but retained another. E.g. There may be a social sector scheme where any beneficiary is expected to be registered only once for getting that scheme benefits. Scheme audit(s) may require auditors to check duplicate beneficiaries using multiple IDs.

Understand this with the table \ref{tab:gra101}.

\begin{table}

\caption{\label{tab:gra101}Dummy Data}
\centering
\begin{tabular}[t]{r|l|l|l}
\hline
ID & Mobile & Email & PAN\\
\hline
1 & 9111111111 & aaaa@gmail.com & PANNO0000A\\
\hline
2 & 9222222222 & bbbb@gmail.com & PANNO0000A\\
\hline
3 & 9333333333 & cccc@gmail.com & PANNO1111B\\
\hline
4 & 9444444444 & dddd@gmail.com & PANNO2222C\\
\hline
5 & 9111111111 & eeee@gmail.com & PANNO3333D\\
\hline
6 & 9111111111 & ffff@gmail.com & PANNO4444E\\
\hline
7 & 9555555555 & gggg@gmail.com & PANNO5555F\\
\hline
8 & 9666666666 & hhhh@gmail.com & PANNO5555F\\
\hline
9 & 9333333333 & iiii@gmail.com & PANNO6666G\\
\hline
10 & 9222222222 & bbbb@gmail.com & PANNO7777H\\
\hline
\end{tabular}
\end{table}

It may be seen that out of ten persons, two with IDs 6 and 10 respectively share none of IDs out of Email, PAN and Telephone number. But if we see closely, ID-6 shares mobile number with ID-1 who in turn share PAN number with ID-2. ID-2 further shares both Email and Mobile number with ID-6 thus establishing a relation and a network between ID-6 and ID-10. This is clear in figure at \ref{fig:igraph11}. \emph{Note that we are not considering names while finding out duplicates.}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{include\_graphics}\NormalTok{(}\StringTok{"images/canvas.png"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.75\linewidth]{images/canvas} 

}

\caption{Network diagram of connected entities}\label{fig:igraph11}
\end{figure}

We may find these duplicates using a branch of mathematics called \emph{Graph Theory}.\footnote{\url{https://en.wikipedia.org/wiki/Graph_theory}} We won't be discussing any core concepts of graph theory here. There are a few packages to work with graph theory concepts in R, and we will be using \texttt{igraph} \citep{R-igraph} for our analysis here. Let's load the library.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(igraph)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{MainID =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{9}\NormalTok{,}
  \AttributeTok{Name =} \FunctionTok{c}\NormalTok{(}\StringTok{"A"}\NormalTok{, }\StringTok{"B"}\NormalTok{, }\StringTok{"C"}\NormalTok{, }\StringTok{"B"}\NormalTok{, }\StringTok{"E"}\NormalTok{, }\StringTok{"A"}\NormalTok{, }\StringTok{"F"}\NormalTok{, }\StringTok{"G"}\NormalTok{, }\StringTok{"H"}\NormalTok{),}
  \AttributeTok{ID1 =} \FunctionTok{c}\NormalTok{(}\DecValTok{11}\NormalTok{,}\DecValTok{12}\NormalTok{,}\DecValTok{13}\NormalTok{,}\DecValTok{13}\NormalTok{,}\DecValTok{14}\NormalTok{,}\DecValTok{15}\NormalTok{,}\DecValTok{16}\NormalTok{,}\DecValTok{17}\NormalTok{,}\DecValTok{17}\NormalTok{),}
  \AttributeTok{ID2 =} \FunctionTok{c}\NormalTok{(}\StringTok{"1a"}\NormalTok{, }\StringTok{"1b"}\NormalTok{,}\StringTok{"1b"}\NormalTok{, }\StringTok{"2a"}\NormalTok{, }\StringTok{"2b"}\NormalTok{, }\StringTok{"2c"}\NormalTok{, }\StringTok{"2c"}\NormalTok{, }\StringTok{"2e"}\NormalTok{, }\StringTok{"3a"}\NormalTok{),}
  \AttributeTok{ID3 =} \FunctionTok{c}\NormalTok{(}\StringTok{"AB"}\NormalTok{, }\StringTok{"AB"}\NormalTok{, }\StringTok{"BC"}\NormalTok{, }\StringTok{"CD"}\NormalTok{, }\StringTok{"EF"}\NormalTok{, }\StringTok{"GH"}\NormalTok{, }\StringTok{"HI"}\NormalTok{, }\StringTok{"HI"}\NormalTok{, }\StringTok{"JK"}\NormalTok{)}
\NormalTok{)}
\CommentTok{\# A preview of our sample data}
\NormalTok{dat}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   MainID Name ID1 ID2 ID3
## 1      1    A  11  1a  AB
## 2      2    B  12  1b  AB
## 3      3    C  13  1b  BC
## 4      4    B  13  2a  CD
## 5      5    E  14  2b  EF
## 6      6    A  15  2c  GH
## 7      7    F  16  2c  HI
## 8      8    G  17  2e  HI
## 9      9    H  17  3a  JK
\end{verbatim}

Now the complete algorithm is as under-

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{id\_cols }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"ID1"}\NormalTok{, }\StringTok{"ID2"}\NormalTok{, }\StringTok{"ID3"}\NormalTok{)}
\NormalTok{dat }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\AttributeTok{.cols =} \FunctionTok{all\_of}\NormalTok{(id\_cols), as.character)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_longer}\NormalTok{(}\AttributeTok{cols =} \FunctionTok{all\_of}\NormalTok{(id\_cols), }
               \AttributeTok{values\_drop\_na =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(MainID, value) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{graph\_from\_data\_frame}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{components}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pluck}\NormalTok{(membership) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{stack}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{set\_names}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}UNIQUE\_ID\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}MainID\textquotesingle{}}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{right\_join}\NormalTok{(dat }\SpecialCharTok{\%\textgreater{}\%} 
               \FunctionTok{mutate}\NormalTok{(}\AttributeTok{MainID =} \FunctionTok{as.factor}\NormalTok{(MainID)), }
             \AttributeTok{by =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}MainID\textquotesingle{}}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   UNIQUE_ID MainID Name ID1 ID2 ID3
## 1         1      1    A  11  1a  AB
## 2         1      2    B  12  1b  AB
## 3         1      3    C  13  1b  BC
## 4         1      4    B  13  2a  CD
## 5         2      5    E  14  2b  EF
## 6         3      6    A  15  2c  GH
## 7         3      7    F  16  2c  HI
## 8         3      8    G  17  2e  HI
## 9         3      9    H  17  3a  JK
\end{verbatim}

We may see that we have got unique ID of users based on all three IDs. Let us understand the algorithm used step by step.

\textbf{Step-1}: First we have to ensure that all the ID columns (Store names of these columns in one vector say \texttt{id\_cols}) must be of same type. Since we had a mix of character (Alphanumeric) and numeric IDs, using \texttt{dplyr::across} with \texttt{dplyr::mutate} we can convert all the three ID columns to character type. Readers may refer to section \ref{vectors} for type change, and section \ref{across} for changing data type of multiple columns simultaneously using \texttt{dplyr::across}.

Thus, first two lines of code above correspond to this step only.

\begin{verbatim}
id_cols <- c("ID1", "ID2", "ID3")
dat %>%
  mutate(across(.cols = id_cols, as.character))
\end{verbatim}

\textbf{Step-2}: Pivot all id columns to longer format so that all Ids are linked with one main ID. Now two things should be kept in mind. One that there should be a main\_Id column in the data frame. If not create one using \texttt{dplyr::row\_number()} before pivoting. Secondly, if there are \texttt{NA}s in any of the IDs these have to be removed while pivoting. Use argument \texttt{values\_drop\_na\ =\ TRUE} inside the \texttt{tidyr::pivot\_longer}. Thus, this step will correspond to this line-

\begin{verbatim}
pivot_longer(cols = all_of(id_cols), values_drop_na = TRUE)
\end{verbatim}

where - first argument data is invisibly passed through dplyr pipe i.e.~\texttt{\%\textgreater{}\%}. Upto this step, our data frame will look like -

\begin{verbatim}
## # A tibble: 27 x 4
##    MainID Name  name  value
##     <int> <chr> <chr> <chr>
##  1      1 A     ID1   11   
##  2      1 A     ID2   1a   
##  3      1 A     ID3   AB   
##  4      2 B     ID1   12   
##  5      2 B     ID2   1b   
##  6      2 B     ID3   AB   
##  7      3 C     ID1   13   
##  8      3 C     ID2   1b   
##  9      3 C     ID3   BC   
## 10      4 B     ID1   13   
## # i 17 more rows
\end{verbatim}

\textbf{Step-3:} Now we need only two columns, one is \texttt{mainID} and another is \texttt{value} which is created by pivoting all ID columns. We will use \texttt{select(MainID,\ value)} for that.

\textbf{Step-4:} Thereafter we will create a graph object from this data (output after step-3), using \texttt{igraph} package. Interested readers may see how the graph object will look like, using \texttt{plot()} function. The output is shown in figure \ref{fig:igraph2}. \textbf{However, this step is entirely optional and it may also be kept in mind that graph output of large data will be highly cluttered and may not be comprehensible at all.}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\AttributeTok{.cols =} \FunctionTok{all\_of}\NormalTok{(id\_cols), as.character)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_longer}\NormalTok{(}\AttributeTok{cols =} \FunctionTok{all\_of}\NormalTok{(id\_cols), }
               \AttributeTok{values\_drop\_na =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(MainID, value) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{graph\_from\_data\_frame}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{plot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{DauR_files/figure-latex/igraph2-1} 

}

\caption{Plot of graph object}\label{fig:igraph2}
\end{figure}

\textbf{Step-5:} This step will be a combination of three lines of codes which will number each ID based on connectivity of all components in the graph objects. Actually \texttt{components} will give us an object where \texttt{\$membership} will give us \texttt{unique\_ids} for each component in the graph.

\begin{verbatim}
## $membership
##  1  2  3  4  5  6  7  8  9 11 1a AB 12 1b 13 BC 2a CD 14 2b EF 15 2c GH 16 HI 
##  1  1  1  1  2  3  3  3  3  1  1  1  1  1  1  1  1  1  2  2  2  3  3  3  3  3 
## 17 2e 3a JK 
##  3  3  3  3 
## 
## $csize
## [1] 13  4 13
## 
## $no
## [1] 3
\end{verbatim}

Next we have to \texttt{purrr::pluck}, \texttt{\$membership} only from this object, which will return a named vector.

\begin{verbatim}
##  1  2  3  4  5  6  7  8  9 11 1a AB 12 1b 13 BC 2a CD 14 2b EF 15 2c GH 16 HI 
##  1  1  1  1  2  3  3  3  3  1  1  1  1  1  1  1  1  1  2  2  2  3  3  3  3  3 
## 17 2e 3a JK 
##  3  3  3  3
\end{verbatim}

We can then \texttt{stack} this named vector into a data frame using \texttt{stack} and \texttt{set\_names}

\begin{verbatim}
##    UNIQUE_ID MainID
## 1          1      1
## 2          1      2
## 3          1      3
## 4          1      4
## 5          2      5
## 6          3      6
## 7          3      7
## 8          3      8
## 9          3      9
## 10         1     11
## 11         1     1a
## 12         1     AB
## 13         1     12
## 14         1     1b
## 15         1     13
## 16         1     BC
## 17         1     2a
## 18         1     CD
## 19         2     14
## 20         2     2b
## 21         2     EF
## 22         3     15
## 23         3     2c
## 24         3     GH
## 25         3     16
## 26         3     HI
## 27         3     17
## 28         3     2e
## 29         3     3a
## 30         3     JK
\end{verbatim}

I suggest to purposefully name second column in the output data as \texttt{MainID} so that it can be joined with original data frame in the last step. \texttt{UNIQUE\_ID} in this data will give us the new column which will allocate same ID to all possible duplicates in network of three IDs.

\textbf{Step-6:} In the last step we have to join the data frame back to original data frame. Since the type of \texttt{MainID} is now factor type, we can convert type of this column in original data frame before \texttt{right\_join} the same. Hence the final step, \texttt{right\_join(dat\ \%\textgreater{}\%\ mutate(MainID\ =\ as.factor(MainID)),\ by\ =\ c(\textquotesingle{}MainID\textquotesingle{}))}.

\hypertarget{use-case-2-classification-using-graph-theory}{%
\section{Use case-2: Classification using graph theory}\label{use-case-2-classification-using-graph-theory}}

\emph{The Content is Under Development.}

\hypertarget{use-case-3-finding-circular-transactions}{%
\section{Use case-3: Finding circular transactions}\label{use-case-3-finding-circular-transactions}}

\emph{The Content is Under Development.}

\hypertarget{part-vii-text-analytics-in-r}{%
\chapter*{Part-VII: Text Analytics in R}\label{part-vii-text-analytics-in-r}}
\addcontentsline{toc}{chapter}{Part-VII: Text Analytics in R}

\hypertarget{string-manipulation-in-stringr}{%
\chapter{\texorpdfstring{String manipulation in \texttt{stringr}}{String manipulation in stringr}}\label{string-manipulation-in-stringr}}

\emph{The content is under development}

In earlier sections we have covered essential tools for data mining which help us in reading data, data cleaning, reshaping data as per our requirements, deriving insights and getting inferences from. However, analyzing text is a bit different as usually text data is unstructured. In data science projects, we often find data-sets with text in the form of strings. These strings often have important information, and we can get the most out of them by effectively working with and analyzing them. String manipulation techniques are essential for preparing data, creating features, text mining, and tasks in natural language processing (NLP).

In Chapter related to functions, we saw some functions from base R for string manipulation. However, \texttt{stringr} which is a part of tidyverse has a plethora of functions designed to make working with strings as easy as possible. We will learn a few of those in this chapter.

First of all, let's load it. Readers may note that we will use a special function namely \texttt{str\_view()} which is used to print the underlying representation of a string and to see how a pattern matches. In actual code this code may rarely be used.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(stringr)}
\end{Highlighting}
\end{Shaded}

Let us also create a few example strings.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{line1 }\OtherTok{\textless{}{-}} \StringTok{"I\textquotesingle{}m gonna make him an offer he can\textquotesingle{}t refuse."}
\NormalTok{line2 }\OtherTok{\textless{}{-}} \StringTok{"Carpe diem.}\SpecialCharTok{\textbackslash{}n}\StringTok{Seize the day, boys."}
\NormalTok{line3 }\OtherTok{\textless{}{-}} \StringTok{"You\textquotesingle{}ve got to ask yourself one question: }\SpecialCharTok{\textbackslash{}"}\StringTok{Do I feel lucky?}\SpecialCharTok{\textbackslash{}"}\StringTok{"}
\end{Highlighting}
\end{Shaded}

\hypertarget{printing-strings-the-way-we-want.}{%
\section{Printing strings the way we want.}\label{printing-strings-the-way-we-want.}}

Let us try printing above strings

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ex\_lines }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(line1, line2, line3)}
\FunctionTok{print}\NormalTok{(ex\_lines)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "I'm gonna make him an offer he can't refuse."                 
## [2] "Carpe diem.\nSeize the day, boys."                            
## [3] "You've got to ask yourself one question: \"Do I feel lucky?\""
\end{verbatim}

Not pretty! In earlier chapter we learnt of the function \texttt{cat} which helps us printing the strings in a way we want i.e.~avoiding escape characters and other unwanted things. So let's use that.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(line1, line2, line3)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## I'm gonna make him an offer he can't refuse. Carpe diem.
## Seize the day, boys. You've got to ask yourself one question: "Do I feel lucky?"
\end{verbatim}

Prettier! Still there's a problem. Actually, \texttt{cat()} accepts a \texttt{sep} argument by which the lines/strings will be separated. So let's use that.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(line1, line2, line3, }\AttributeTok{sep =} \StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## I'm gonna make him an offer he can't refuse.
## Carpe diem.
## Seize the day, boys.
## You've got to ask yourself one question: "Do I feel lucky?"
\end{verbatim}

Base R has another function \texttt{writeLines()} which has also been designed to print the \emph{strings} in the way we usually want, as against \texttt{cat()} which is general purpose and designed for concatenating \emph{objects} and printing them.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{writeLines}\NormalTok{(ex\_lines)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## I'm gonna make him an offer he can't refuse.
## Carpe diem.
## Seize the day, boys.
## You've got to ask yourself one question: "Do I feel lucky?"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Let\textquotesingle{}s also print some Unicode and special characters.}
\FunctionTok{writeLines}\NormalTok{(}\StringTok{"\textbackslash{}u0928\textbackslash{}u092e\textbackslash{}u0938\textbackslash{}u094d\textbackslash{}u0924\textbackslash{}u0947 }
\StringTok{         \textbackslash{}u0926\textbackslash{}u0941\textbackslash{}u0928\textbackslash{}u093f\textbackslash{}u092f\textbackslash{}u093e"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## नमस्ते 
##          दुनिया
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{writeLines}\NormalTok{(}\StringTok{"He owes me \textbackslash{}U20b9 15 lakh."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## He owes me ₹ 15 lakh.
\end{verbatim}

In this reference, let's also discuss a bit about \texttt{str\_view} from \texttt{stringr} which has been designed to view the strings and matching, as we will see in next sub-sections.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str\_view}\NormalTok{(ex\_lines)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] | I'm gonna make him an offer he can't refuse.
## [2] | Carpe diem.
##     | Seize the day, boys.
## [3] | You've got to ask yourself one question: "Do I feel lucky?"
\end{verbatim}

\hypertarget{string-concatenation-with-str_c}{%
\section{\texorpdfstring{String concatenation with \texttt{str\_c()}}{String concatenation with str\_c()}}\label{string-concatenation-with-str_c}}

We have already seen two functions \texttt{paste} and \texttt{paste0} from base R in earlier chapter. However \texttt{stringr} package has a function \texttt{str\_c} (\texttt{c} is short for concatenation) for similar purposes. But there a couple of differences.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The default \texttt{sep} is \texttt{""} here as opposed to \texttt{"\ "} in \texttt{paste()} and absence of \texttt{sep} argument in \texttt{paste0()} altogether.
\item
  Function \texttt{paste()} turns missing values into the string \texttt{“NA”}, whereas \texttt{str\_c()} propagates missing values. That means combining any strings with a missing value will result in another missing value.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{company }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Microsoft"}\NormalTok{, }\StringTok{"Salesforce"}\NormalTok{, }\ConstantTok{NA}\NormalTok{)}
\NormalTok{product }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Excel"}\NormalTok{, }\StringTok{"Tableau"}\NormalTok{, }\StringTok{"R"}\NormalTok{)}
\FunctionTok{paste}\NormalTok{(company, product)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Microsoft Excel"    "Salesforce Tableau" "NA R"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str\_c}\NormalTok{(company, product, }\AttributeTok{sep =} \StringTok{" "}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Microsoft Excel"    "Salesforce Tableau" NA
\end{verbatim}

This also ensures returning same length output as of given vectors making it especially useful while working in \texttt{dplyr::mutate}. However, if we want to flatten the given vector of strings using some separator, we use \texttt{collapse} argument of \texttt{paste} or \texttt{paste0}. Stringr has a function \texttt{str\_flatten()} designed specifically for this purpose, making it useful while working with \texttt{dplyr::summarise}. Not only that, it has an extra argument \texttt{last} which is extremely useful in flattening last piece of the vector.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fruits }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"apple"}\NormalTok{, }\StringTok{"banana"}\NormalTok{, }\StringTok{"pineapple"}\NormalTok{)}

\FunctionTok{str\_flatten}\NormalTok{(fruits, }\AttributeTok{collapse =} \StringTok{", "}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "apple, banana, pineapple"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str\_flatten}\NormalTok{(fruits, }\AttributeTok{collapse =} \StringTok{", "}\NormalTok{, }\AttributeTok{last =} \StringTok{" and "}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "apple, banana and pineapple"
\end{verbatim}

There is a special variant \texttt{str\_flatten\_comma()} wherein ``comma'' is default \texttt{collapse} argument. So we have type a bit lesser in that case.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str\_flatten\_comma}\NormalTok{(fruits)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "apple, banana, pineapple"
\end{verbatim}

In this context, we may also discuss one more function \texttt{str\_glue} which provides us a powerful and elegant syntax for interpolating strings with \texttt{\{\}}. See the following example.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Note that output will be of same length as given variable/string vector.}
\FunctionTok{str\_glue}\NormalTok{(}\StringTok{"I like \{fruits\}"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## I like apple
## I like banana
## I like pineapple
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_fruits }\OtherTok{\textless{}{-}} \FunctionTok{str\_flatten\_comma}\NormalTok{(fruits, }\AttributeTok{last =} \StringTok{" and "}\NormalTok{)}
\FunctionTok{str\_glue}\NormalTok{(}\StringTok{"I like \{my\_fruits\} in fruits."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## I like apple, banana and pineapple in fruits.
\end{verbatim}

\hypertarget{string-length-with-str_length}{%
\section{\texorpdfstring{String length with \texttt{str\_length()}}{String length with str\_length()}}\label{string-length-with-str_length}}

For counting number of characters in a string we use \texttt{nchar()} from base R. However, \texttt{str\_length()} is designed for similar purpose.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str\_length}\NormalTok{(ex\_lines)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 44 32 59
\end{verbatim}

However, it has been designed to handle factors in a better sense than \texttt{nchar()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# nchar(unique(iris$Species))}
\CommentTok{\# Returns an error}

\CommentTok{\# This will work}
\FunctionTok{str\_length}\NormalTok{(}\FunctionTok{unique}\NormalTok{(iris}\SpecialCharTok{$}\NormalTok{Species))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  6 10  9
\end{verbatim}

\hypertarget{string-extraction-with-str_sub}{%
\section{\texorpdfstring{String extraction with \texttt{str\_sub()}}{String extraction with str\_sub()}}\label{string-extraction-with-str_sub}}

Function \texttt{str\_sub()} extracts parts of strings based on their location. It takes three arguments, first argument, string, is a vector of strings. Other arguments \texttt{start} and \texttt{end} specify the boundaries of the piece to extract in characters.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Extracting first two characters}
\FunctionTok{str\_sub}\NormalTok{(fruits, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "ap" "ba" "pi"
\end{verbatim}

If you are wondering that this works similarly than \texttt{substr} then it is worthwhile to mention here that unlike \texttt{substr} from base R, it can accept negative position integers wherein the counting will be done backwards.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# Note the difference}
\FunctionTok{substr}\NormalTok{(fruits, }\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "" "" ""
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str\_sub}\NormalTok{(fruits, }\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "le" "na" "le"
\end{verbatim}

Not only that it won't fail if string falls short for the given positions.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str\_sub}\NormalTok{(fruits, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "e"  "na" "ap"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str\_sub}\NormalTok{(fruits, }\SpecialCharTok{{-}}\DecValTok{6}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "a"  "ba" "ea"
\end{verbatim}

\hypertarget{string-matching-based-on-regex-with-str_detect-str_subset-and-str_count}{%
\section{\texorpdfstring{String matching based on regex with \texttt{str\_detect()}, \texttt{str\_subset()} and \texttt{str\_count()}}{String matching based on regex with str\_detect(), str\_subset() and str\_count()}}\label{string-matching-based-on-regex-with-str_detect-str_subset-and-str_count}}

Let's search \texttt{"apple"} in all three \texttt{fruits} strings.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str\_view}\NormalTok{(fruits, }\StringTok{"apple"}\NormalTok{, }\AttributeTok{match =} \ConstantTok{NA}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] | <apple>
## [2] | banana
## [3] | pine<apple>
\end{verbatim}

There are three functions in \texttt{stringr} to do the job.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{str\_detect()} works like \texttt{grepl} and returns a logical vector.
\item
  \texttt{str\_subset()} works like \texttt{grep} with \texttt{value\ =\ TRUE} argument.
\item
  \texttt{str\_count()} will return the count of matches in each of the element of given string.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str\_detect}\NormalTok{(fruits, }\StringTok{"apple"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  TRUE FALSE  TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str\_subset}\NormalTok{(fruits, }\StringTok{"apple"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "apple"     "pineapple"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str\_count}\NormalTok{(fruits, }\StringTok{"apple"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1 0 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Let\textquotesingle{}s count character "a" in each of \textasciigrave{}fruits\textasciigrave{}}
\FunctionTok{str\_count}\NormalTok{(fruits, }\StringTok{"a"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1 3 1
\end{verbatim}

\hypertarget{changing-case-in-stringr}{%
\section{Changing case in stringr}\label{changing-case-in-stringr}}

There are four functions in \texttt{stringr} to make our life easier while changing case of the given strings.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{str\_to\_lower()} converts the string to lower case.
\item
  \texttt{str\_to\_upper()} converts the string to UPPER CASE.
\item
  \texttt{str\_to\_title()} make the given string in Title Case, wherein first alphabet of all characters is in upper case.
\item
  \texttt{str\_to\_sentence()} convert to sentence case, where only the first letter of sentence is capitalized.
\end{enumerate}

Examples.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# lower case}
\FunctionTok{str\_view}\NormalTok{(}\FunctionTok{str\_to\_lower}\NormalTok{(ex\_lines))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] | i'm gonna make him an offer he can't refuse.
## [2] | carpe diem.
##     | seize the day, boys.
## [3] | you've got to ask yourself one question: "do i feel lucky?"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# UPPER CASE}
\FunctionTok{str\_view}\NormalTok{(}\FunctionTok{str\_to\_upper}\NormalTok{(ex\_lines))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] | I'M GONNA MAKE HIM AN OFFER HE CAN'T REFUSE.
## [2] | CARPE DIEM.
##     | SEIZE THE DAY, BOYS.
## [3] | YOU'VE GOT TO ASK YOURSELF ONE QUESTION: "DO I FEEL LUCKY?"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Title Case}
\FunctionTok{str\_view}\NormalTok{(}\FunctionTok{str\_to\_title}\NormalTok{(ex\_lines))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] | I'm Gonna Make Him An Offer He Can't Refuse.
## [2] | Carpe Diem.
##     | Seize The Day, Boys.
## [3] | You've Got To Ask Yourself One Question: "Do I Feel Lucky?"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Sentence case}
\FunctionTok{str\_view}\NormalTok{(}\FunctionTok{str\_to\_sentence}\NormalTok{(ex\_lines))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] | I'm gonna make him an offer he can't refuse.
## [2] | Carpe diem.
##     | Seize the day, boys.
## [3] | You've got to ask yourself one question: "do i feel lucky?"
\end{verbatim}

\hypertarget{controlling-matching-behaviour-with-modifier-functions-in-stringr}{%
\section{Controlling matching behaviour with modifier functions in stringr}\label{controlling-matching-behaviour-with-modifier-functions-in-stringr}}

Usually ans specifically while working with English language text, we may require two type of modifier functions in detecting/extracting matches.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  One is \texttt{fixed()}, which compares literal bytes. But this has an extra argument \texttt{ignore\_case} which can be used to ignore/not ignore the cases while matching/extracting pattern from string vectors.
\item
  Second is \texttt{regex} which has several other arguments apart from \texttt{ignore\_case}.
\end{enumerate}

See these examples.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ex\_str }\OtherTok{\textless{}{-}} \StringTok{"This is an example string."}
\FunctionTok{str\_view}\NormalTok{(ex\_str, }\StringTok{"t"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] | This is an example s<t>ring.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str\_view}\NormalTok{(ex\_str, }\FunctionTok{fixed}\NormalTok{(}\StringTok{"."}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] | This is an example string<.>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str\_view}\NormalTok{(ex\_str, }\FunctionTok{regex}\NormalTok{(}\StringTok{"."}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] | <T><h><i><s>< ><i><s>< ><a><n>< ><e><x><a><m><p><l><e>< ><s><t><r><i><n><g><.>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str\_view}\NormalTok{(ex\_str, }\FunctionTok{regex}\NormalTok{(}\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{b.\{2\}}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{b"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] | This <is> <an> example string.
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  There is one more control function \texttt{boundary()} which matches boundary between strings. It has an argument \texttt{type} which accepts one of the values \texttt{c("character",\ "line\_break",\ "sentence",\ "word")}.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str\_view}\NormalTok{(ex\_str, }\FunctionTok{boundary}\NormalTok{(}\StringTok{"word"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] | <This> <is> <an> <example> <string>.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str\_view}\NormalTok{(ex\_lines, }\FunctionTok{boundary}\NormalTok{(}\StringTok{"sentence"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] | <I'm gonna make him an offer he can't refuse.>
## [2] | <Carpe diem.
##     | ><Seize the day, boys.>
## [3] | <You've got to ask yourself one question: "Do I feel lucky?">
\end{verbatim}

\hypertarget{extracting-text-from-strings}{%
\section{Extracting text from strings}\label{extracting-text-from-strings}}

In above parts, we learnt about the function \texttt{str\_subset()} which returns the strings where the matching text/pattern is found. But what about the cases where we want those specific matching text/patterns to be returned. For such cases, stringr has \texttt{str\_extract} and \texttt{str\_extract\_all()} in its powerhouse. It will be clear from the following example, wherein we will extract PAN numbers from the given text string(s).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ex\_text }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"My PAN number is TEMPZ9999Z."}\NormalTok{,}
             \StringTok{"He has mentioned TEMP9999Z as his PAN number, incorrectly."}\NormalTok{,}
             \StringTok{"Is your PAN ABCTY1234D?"}\NormalTok{)}
\CommentTok{\# Let\textquotesingle{}s define simple regex for PAN}
\NormalTok{pan }\OtherTok{\textless{}{-}} \StringTok{"[A{-}Z]\{5\}[0{-}9]\{4\}[A{-}Z]"}
\CommentTok{\# str\_subset will return strings which contain PAN numbers}
\FunctionTok{str\_subset}\NormalTok{(ex\_text, }\AttributeTok{pattern =} \FunctionTok{regex}\NormalTok{(pan))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "My PAN number is TEMPZ9999Z." "Is your PAN ABCTY1234D?"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# str\_extract will however, extract those.}
\FunctionTok{str\_extract}\NormalTok{(ex\_text, }\AttributeTok{pattern =} \FunctionTok{regex}\NormalTok{(pan))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "TEMPZ9999Z" NA           "ABCTY1234D"
\end{verbatim}

This function will return first of the match if found. Its variant \texttt{str\_extract\_all()} will return all the matches, as expected, in a list.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{text\_2 }\OtherTok{\textless{}{-}} \FunctionTok{str\_flatten}\NormalTok{(ex\_text, }\AttributeTok{collapse =} \StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\FunctionTok{str\_extract}\NormalTok{(text\_2, }\FunctionTok{regex}\NormalTok{(pan))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "TEMPZ9999Z"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str\_extract\_all}\NormalTok{(text\_2, }\FunctionTok{regex}\NormalTok{(pan))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [[1]]
## [1] "TEMPZ9999Z" "ABCTY1234D"
\end{verbatim}

This latter function has an additional argument to simplify the output in form of a matrix, if \texttt{TRUE}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str\_extract\_all}\NormalTok{(text\_2, }\FunctionTok{regex}\NormalTok{(pan), }\AttributeTok{simplify =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1]         [,2]        
## [1,] "TEMPZ9999Z" "ABCTY1234D"
\end{verbatim}

So, if we have to find out how many PAN numbers are stored in \texttt{text\_2} above.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str\_count}\NormalTok{(text\_2, }\FunctionTok{regex}\NormalTok{(pan))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2
\end{verbatim}

\hypertarget{splitting-strings}{%
\section{Splitting strings}\label{splitting-strings}}

In its kitty, stringr has another powerful function \texttt{str\_split()} which is used to split strings into meaningful fragments using a \texttt{pattern}. The output format, as expected would be a list.

Example-

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str\_split}\NormalTok{(ex\_text, }\FunctionTok{boundary}\NormalTok{(}\StringTok{"word"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [[1]]
## [1] "My"         "PAN"        "number"     "is"         "TEMPZ9999Z"
## 
## [[2]]
## [1] "He"          "has"         "mentioned"   "TEMP9999Z"   "as"         
## [6] "his"         "PAN"         "number"      "incorrectly"
## 
## [[3]]
## [1] "Is"         "your"       "PAN"        "ABCTY1234D"
\end{verbatim}

It has an argument \texttt{n} which is used to specify the maximum pieces to return. Default is \texttt{Inf}. Extra results will however be flattened.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Extract first two words from each string.}
\FunctionTok{str\_split}\NormalTok{(ex\_text, }\FunctionTok{boundary}\NormalTok{(}\StringTok{"word"}\NormalTok{), }\AttributeTok{n =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [[1]]
## [1] "My"                        "PAN number is TEMPZ9999Z."
## 
## [[2]]
## [1] "He"                                                     
## [2] "has mentioned TEMP9999Z as his PAN number, incorrectly."
## 
## [[3]]
## [1] "Is"                   "your PAN ABCTY1234D?"
\end{verbatim}

This function has three more variants. First is \texttt{str\_split\_fixed()} which splits each string in a character vector into a fixed number of pieces, returning a character matrix. Example -

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Here value of \textasciigrave{}n\textasciigrave{} is required}
\FunctionTok{str\_split\_fixed}\NormalTok{(ex\_text, }\FunctionTok{boundary}\NormalTok{(}\StringTok{"word"}\NormalTok{), }\AttributeTok{n =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2]   [,3]                                                 
## [1,] "My" "PAN"  "number is TEMPZ9999Z."                              
## [2,] "He" "has"  "mentioned TEMP9999Z as his PAN number, incorrectly."
## [3,] "Is" "your" "PAN ABCTY1234D?"
\end{verbatim}

Another variant is \texttt{str\_split\_1()} which takes a single string and splits it into pieces, returning a single character vector.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Note that vector with one element should be passed.}
\FunctionTok{str\_split\_1}\NormalTok{(ex\_text[}\DecValTok{1}\NormalTok{], }\FunctionTok{boundary}\NormalTok{(}\StringTok{"word"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "My"         "PAN"        "number"     "is"         "TEMPZ9999Z"
\end{verbatim}

Last one is \texttt{str\_split\_i()} which splits each string in a character vector into pieces and extracts the \texttt{i}th value, returning a character vector.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str\_split\_i}\NormalTok{(ex\_text, }\FunctionTok{boundary}\NormalTok{(}\StringTok{"word"}\NormalTok{), }\AttributeTok{i =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "My" "He" "Is"
\end{verbatim}

\hypertarget{replacing-values-with-str_replace-str_replace_all}{%
\section{\texorpdfstring{Replacing values with \texttt{str\_replace()}, \texttt{str\_replace\_all()}}{Replacing values with str\_replace(), str\_replace\_all()}}\label{replacing-values-with-str_replace-str_replace_all}}

So the matched text strings/values if required to be replaced with some other values, we can use \texttt{str\_replace()} and/or \texttt{str\_replace\_all()}.

As expected these functions require additional argument \texttt{replacement}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Example Task: mask all PAN numbers from \textasciigrave{}text\_2\textasciigrave{}}
\CommentTok{\# Let\textquotesingle{}s view the string}
\FunctionTok{str\_view}\NormalTok{(text\_2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] | My PAN number is TEMPZ9999Z.
##     | He has mentioned TEMP9999Z as his PAN number, incorrectly.
##     | Is your PAN ABCTY1234D?
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Replace first match only}
\FunctionTok{str\_replace}\NormalTok{(text\_2, }\FunctionTok{regex}\NormalTok{(pan), }\AttributeTok{replacement =} \StringTok{"XXXXX0000X"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{str\_view}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] | My PAN number is XXXXX0000X.
##     | He has mentioned TEMP9999Z as his PAN number, incorrectly.
##     | Is your PAN ABCTY1234D?
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Replace all matches}
\FunctionTok{str\_replace\_all}\NormalTok{(text\_2, }\FunctionTok{regex}\NormalTok{(pan), }\AttributeTok{replacement =} \StringTok{"XXXXX0000X"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{str\_view}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] | My PAN number is XXXXX0000X.
##     | He has mentioned TEMP9999Z as his PAN number, incorrectly.
##     | Is your PAN XXXXX0000X?
\end{verbatim}

For \texttt{replacement} of multiple matches, vectors of same length in both \texttt{pattern} and \texttt{replacement} can be provided. This may be understood from the following example.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a new string vector}
\NormalTok{fruits }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"one apple"}\NormalTok{,}
            \StringTok{"two bananas"}\NormalTok{,}
            \StringTok{"three pineapples"}\NormalTok{)}
\CommentTok{\# See what\textquotesingle{}s there in \textasciigrave{}fruits\textasciigrave{}}
\FunctionTok{str\_view}\NormalTok{(fruits)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] | one apple
## [2] | two bananas
## [3] | three pineapples
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Let\textquotesingle{}s replace each number word to numeral}
\FunctionTok{str\_replace\_all}\NormalTok{(}
\NormalTok{  fruits,}
  \AttributeTok{pattern =} \FunctionTok{c}\NormalTok{(}\StringTok{"one"}\NormalTok{, }\StringTok{"two"}\NormalTok{, }\StringTok{"three"}\NormalTok{),}
  \AttributeTok{replacement =} \FunctionTok{c}\NormalTok{(}\StringTok{"1"}\NormalTok{, }\StringTok{"2"}\NormalTok{, }\StringTok{"3"}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "1 apple"      "2 bananas"    "3 pineapples"
\end{verbatim}

Alternatively, a named vector \texttt{(c(pattern1\ =\ replacement1,\ ...))}, may be supplied to \texttt{pattern} argument, in order to perform multiple replacements in each element of string more effectively.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str\_replace\_all}\NormalTok{(}
\NormalTok{  fruits,}
  \AttributeTok{pattern =} \FunctionTok{c}\NormalTok{(}\AttributeTok{one =} \StringTok{"1"}\NormalTok{, }\AttributeTok{two =} \StringTok{"2"}\NormalTok{, }\AttributeTok{three =} \StringTok{"3"}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "1 apple"      "2 bananas"    "3 pineapples"
\end{verbatim}

Note: In a named vector, names need not be quoted.

Back-references: References of the form \texttt{⁠\textbackslash{}}1⁠\texttt{,}⁠\textbackslash2⁠, etc will be replaced with the contents of the respective matched group (created by \texttt{⁠(}..)

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# If any consonant is repeated, make it single}
\FunctionTok{str\_replace\_all}\NormalTok{(fruits, }
                \AttributeTok{pattern =} \FunctionTok{regex}\NormalTok{(}\StringTok{"([\^{}aeiou])}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{1"}\NormalTok{, }\AttributeTok{ignore\_case =} \ConstantTok{TRUE}\NormalTok{),}
                \AttributeTok{replacement =} \StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{1"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "one aple"        "two bananas"     "three pineaples"
\end{verbatim}

In \texttt{replacement} argument of these functions, we may also supply a function, which will be called once for each match (from right to left) and its return value will be used to replace the match.

Another example.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Change case of all PAN numbers which are in lower case.}
\NormalTok{text\_3 }\OtherTok{\textless{}{-}} \FunctionTok{str\_to\_lower}\NormalTok{(text\_2)}
\CommentTok{\# Let\textquotesingle{}s view the string}
\FunctionTok{str\_view}\NormalTok{(text\_3)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] | my pan number is tempz9999z.
##     | he has mentioned temp9999z as his pan number, incorrectly.
##     | is your pan abcty1234d?
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Change case of all lower case PAN numbers}
\FunctionTok{str\_replace\_all}\NormalTok{(text\_3, }
                \AttributeTok{pattern =} \FunctionTok{regex}\NormalTok{(pan, }\AttributeTok{ignore\_case =} \ConstantTok{TRUE}\NormalTok{), }
                \AttributeTok{replacement =}\NormalTok{ str\_to\_upper) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{str\_view}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] | my pan number is TEMPZ9999Z.
##     | he has mentioned temp9999z as his pan number, incorrectly.
##     | is your pan ABCTY1234D?
\end{verbatim}

\hypertarget{removing-textpattern-using-str_remove-and-str_remove_all}{%
\section{\texorpdfstring{Removing text/pattern using \texttt{str\_remove} and \texttt{str\_remove\_all}}{Removing text/pattern using str\_remove and str\_remove\_all}}\label{removing-textpattern-using-str_remove-and-str_remove_all}}

Removing text or pattern from the strings is similar to replacing matches with empty text \texttt{""}. See example where we are removing numbers(digits) from a valid PAN number, if any, in the given text.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str\_remove\_all}\NormalTok{(ex\_text,}
               \AttributeTok{pattern =} \FunctionTok{regex}\NormalTok{(}\StringTok{"(?\textless{}=[A{-}Z]\{5\})(}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d\{4\})(?=[A{-}Z])"}\NormalTok{, }\AttributeTok{ignore\_case =} \ConstantTok{TRUE}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{str\_view}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] | My PAN number is TEMPZZ.
## [2] | He has mentioned TEMP9999Z as his PAN number, incorrectly.
## [3] | Is your PAN ABCTYD?
\end{verbatim}

\hypertarget{unicode}{%
\section{Unicode}\label{unicode}}

Unicode in R, precedes with\texttt{\textbackslash{}U}. Some examples of \href{https://en.wikipedia.org/wiki/Emoticons_(Unicode_block)}{emoticons}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{writeLines}\NormalTok{(}\StringTok{"\textbackslash{}U1f600"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 😀
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{writeLines}\NormalTok{(}\StringTok{"\textbackslash{}U1f634"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 😴
\end{verbatim}

\hypertarget{formatting-numbers-with-format-and-formatc}{%
\section{\texorpdfstring{Formatting numbers with \texttt{format} and \texttt{formatC}}{Formatting numbers with format and formatC}}\label{formatting-numbers-with-format-and-formatc}}

Sometimes numbers may be required to format in special types like preceding with currency symbol, thousand separator or scientific format to fixed format (or vice versa). In such case \texttt{format} function from base R comes handy. The \texttt{scientific} argument to \texttt{format()} controls whether the numbers are displayed in \emph{fixed} (\texttt{scientific\ =\ FALSE}) or \emph{scientific} (\texttt{scientific\ =\ TRUE}) format. When the representation is \texttt{scientific}, the \texttt{digits} argument is the number of digits before the exponent. Whereas, when the representation is \texttt{fixed}, \texttt{digits} controls the significant digits used for the smallest (in magnitude) number.

Each other number will be formatted to match the number of decimal places in the smallest number. This means the number of decimal places we get in our output depends on all the values we are formatting.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Some example numbers}
\NormalTok{numbers }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FloatTok{0.00123}\NormalTok{, }\DecValTok{123}\NormalTok{, }\FloatTok{1.2356}\NormalTok{)}
\CommentTok{\# Scientific (default)}
\FunctionTok{format}\NormalTok{(numbers, }\AttributeTok{digits =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{writeLines}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 1e-03
## 1e+02
## 1e+00
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Fixed format}
\FunctionTok{format}\NormalTok{(numbers, }\AttributeTok{digits =} \DecValTok{1}\NormalTok{, }\AttributeTok{scientific =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{writeLines}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   0.001
## 123.000
##   1.236
\end{verbatim}

Explanation above: In above the smallest number is \texttt{0.00123} which is controlling the number of decimals in all other numbers. Significant digit in this number is \texttt{1} which require three decimal places.

We may also note in the above output that it is nicely aligned with decimal. To stop this behavior we may set \texttt{trim\ =\ TRUE} in above.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{format}\NormalTok{(numbers, }
       \AttributeTok{digits =} \DecValTok{1}\NormalTok{, }
       \AttributeTok{scientific =} \ConstantTok{FALSE}\NormalTok{,}
       \AttributeTok{trim =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{writeLines}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 0.001
## 123.000
## 1.236
\end{verbatim}

The function \texttt{formatC()} provides an alternative way to format numbers based on \texttt{C} style syntax.

Rather than a \texttt{scientific} argument, \texttt{formatC()} has a \texttt{format} argument that takes a code representing the required format. The most useful are:

\begin{itemize}
\tightlist
\item
  \texttt{"f"} for fixed format. In this case, \texttt{digits} is the number of digits after the decimal point. This is more predictable than \texttt{format()}, because the number of places after the decimal is fixed regardless of the values being formatted.
\item
  \texttt{"e"} for scientific. Here, \texttt{digits} argument behaves like it does in \texttt{format()}; it specifies the number of significant digits.
\item
  \texttt{"g"} for fixed unless scientific saves space.
\end{itemize}

Function \texttt{formatC()} also formats numbers individually, which means you always get the same output regardless of other numbers in the vector.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{formatC}\NormalTok{(numbers,}
        \AttributeTok{format =} \StringTok{"f"}\NormalTok{,}
        \AttributeTok{digits =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{writeLines}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 0.00
## 123.00
## 1.24
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{formatC}\NormalTok{(numbers,}
        \AttributeTok{format =} \StringTok{"g"}\NormalTok{,}
        \AttributeTok{digits =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{writeLines}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 0.0012
## 1.2e+02
## 1.2
\end{verbatim}

Lastly there is one more package \texttt{scales} which also does pretty job of formatting numbers.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{scales::percent()}: It forces decimal display of numbers (i.e.~don't use scientific notation)
\item
  \texttt{scales::comma()} : It inserts a comma every three digit.
\item
  \texttt{scales::dollar} : Used to format numbers with currency symbol.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(scales)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Attaching package: 'scales'
\end{verbatim}

\begin{verbatim}
## The following objects are masked from 'package:psych':
## 
##     alpha, rescale
\end{verbatim}

\begin{verbatim}
## The following object is masked from 'package:readr':
## 
##     col_factor
\end{verbatim}

\begin{verbatim}
## The following object is masked from 'package:purrr':
## 
##     discard
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# In per cent up to two digits after decimal}
\FunctionTok{percent}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\FloatTok{0.001}\NormalTok{, }\FloatTok{0.1234}\NormalTok{, }\FloatTok{0.002}\NormalTok{), }\AttributeTok{accuracy =} \FloatTok{0.01}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{writeLines}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 0.10%
## 12.34%
## 0.20%
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# With thousand separator}
\FunctionTok{comma}\NormalTok{(numbers}\SpecialCharTok{*}\DecValTok{1000}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{writeLines}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 1
## 123,000
## 1,236
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# With rupee symbol}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\FunctionTok{runif}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{1000}\NormalTok{, }\DecValTok{90000}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{dollar}\NormalTok{(}\AttributeTok{prefix =} \StringTok{"\textbackslash{}U20b9"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{writeLines}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## ₹26,594.40
## ₹71,159.16
## ₹37,398.95
\end{verbatim}

\hypertarget{sorting-strings}{%
\section{Sorting strings}\label{sorting-strings}}

To sort the strings, we have three powerful functions in the kitty of stringr.

\begin{itemize}
\tightlist
\item
  \texttt{str\_sort()} returns the sorted vector.
\item
  \texttt{str\_order()} returns an integer vector that returns the desired order when used for sub-setting, i.e.~\texttt{x{[}str\_order(x){]}} is the same as \texttt{str\_sort()}
\item
  \texttt{str\_rank()} returns the ranks of the values, i.e.~\texttt{arrange(df,\ str\_rank(x))} is the same as \texttt{str\_sort(df\$x)}
\end{itemize}

Besides doing sorting for us, these functions have an argument \texttt{numeric} which if set to \texttt{TRUE} will sort digits numerically, instead of as strings. The following example will clarify the purpose.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str\_view}\NormalTok{(fruits)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] | one apple
## [2] | two bananas
## [3] | three pineapples
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Let\textquotesingle{}s sort these alphabetically}
\FunctionTok{str\_sort}\NormalTok{(fruits)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "one apple"        "three pineapples" "two bananas"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Let\textquotesingle{}s find the alphabetic order}
\FunctionTok{str\_order}\NormalTok{(fruits)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1 3 2
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# Example{-}2}
\NormalTok{ex\_text }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"₹100"}\NormalTok{, }\StringTok{"₹200"}\NormalTok{, }\StringTok{"₹1000"}\NormalTok{, }\StringTok{"₹500"}\NormalTok{, }\StringTok{"₹5000"}\NormalTok{, }\StringTok{"₹10000"}\NormalTok{)}

\CommentTok{\# default sorting}
\FunctionTok{str\_sort}\NormalTok{(ex\_text)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "₹100"   "₹1000"  "₹10000" "₹200"   "₹500"   "₹5000"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Order}
\FunctionTok{str\_order}\NormalTok{(ex\_text)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1 3 6 2 4 5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Rank}
\FunctionTok{str\_rank}\NormalTok{(ex\_text)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1 4 2 5 6 3
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# sorting based on numbers}
\FunctionTok{str\_sort}\NormalTok{(ex\_text, }\AttributeTok{numeric =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "₹100"   "₹200"   "₹500"   "₹1000"  "₹5000"  "₹10000"
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{regex---a-quick-introduction}{%
\chapter{Regex - A quick introduction}\label{regex---a-quick-introduction}}

A \textbf{Regular Expression}, or \textbf{regex} for short, is a powerful tool, which helps us writing code for pattern matching in texts. Regex, is a pattern that describes a set of strings. It is a sequence of characters that define a search pattern. It is used to search for and manipulate text. Regex can be used in many programming languages, including R.

Regex patterns are made up of a combination of regular characters and special characters. Regular characters include letters, digits, and punctuation marks. Special characters have a specific meaning in regex and are used to represent patterns of characters.

Regex patterns can be used for a variety of purposes, including:

\begin{itemize}
\tightlist
\item
  Searching for specific strings in text
\item
  Extracting specific parts of a string
\item
  Replacing parts of a string with other text
\item
  Validating input from users
\end{itemize}

In R, we can use the \texttt{grep} and \texttt{gsub} functions to search for and manipulate text using regex.

\hypertarget{basic-regex---literal-characters}{%
\section{Basic Regex - Literal Characters}\label{basic-regex---literal-characters}}

Every \emph{literal character}, in itself is a \texttt{regex} that matches itself. Thus, \texttt{a} matches third character in text \texttt{Charles}. These literal characters are case sensitive.

Example-1

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ex\_text }\OtherTok{\textless{}{-}} \StringTok{"This is an example text"}
\CommentTok{\# Match literal \textasciigrave{}x\textasciigrave{}}
\FunctionTok{str\_view}\NormalTok{(ex\_text, }\StringTok{"x"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] | This is an e<x>ample te<x>t
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Match Upper case literal "X"}
\FunctionTok{str\_view}\NormalTok{(ex\_text, }\StringTok{"X"}\NormalTok{, }\AttributeTok{match =} \ConstantTok{NA}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] | This is an example text
\end{verbatim}

\hypertarget{case-sensitivity}{%
\subsection{Case sensitivity}\label{case-sensitivity}}

As the literals are case\_sensitive and we sometimes are not aware of exact case, to match case insensitive literals, we can make use of \texttt{stringr} function \texttt{regex} in this case, wherein an argument \texttt{ignore\_case} (note snake case) is there. Actually, behind the scenes, all regex expressions in stringr are wrapped in this function with argument defaults as \texttt{FALSE}. Thus, the code in above example is actually equivalent to the following-

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Match literal \textasciigrave{}x\textasciigrave{}}
\FunctionTok{str\_view}\NormalTok{(ex\_text, }\FunctionTok{regex}\NormalTok{(}\StringTok{"x"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] | This is an e<x>ample te<x>t
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Match Upper case literal "X"}
\FunctionTok{str\_view}\NormalTok{(ex\_text, }\FunctionTok{regex}\NormalTok{(}\StringTok{"X"}\NormalTok{), }\AttributeTok{match =} \ConstantTok{NA}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] | This is an example text
\end{verbatim}

Thus, to match case insensitive literals (or other regex expressions) we may make use of the argument \texttt{ignore\_case} like this-

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Match literal \textasciigrave{}x\textasciigrave{}}
\FunctionTok{str\_view}\NormalTok{(ex\_text, }\FunctionTok{regex}\NormalTok{(}\StringTok{"X"}\NormalTok{, }\AttributeTok{ignore\_case =} \ConstantTok{TRUE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] | This is an e<x>ample te<x>t
\end{verbatim}

\hypertarget{metacharacters}{%
\section{Metacharacters}\label{metacharacters}}

\hypertarget{character-sets}{%
\subsection{Character sets}\label{character-sets}}

It is always not feasible to put every literal characters. We may also match literal characters from a given set of options. To \textbf{match a group of characters} we have to put all these in square brackets. So, \texttt{{[}abc{]}} matches either of \texttt{a}, \texttt{b}, or \texttt{c}.

Example-

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ex\_vec }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Apple"}\NormalTok{, }\StringTok{"Orange"}\NormalTok{, }\StringTok{"Myrrh"}\NormalTok{)}
\CommentTok{\# matches a vowel}
\FunctionTok{str\_view}\NormalTok{(ex\_vec, }\StringTok{"[aeiou]"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] | Appl<e>
## [2] | Or<a>ng<e>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# matches a vowel irrespective of case}
\FunctionTok{str\_view}\NormalTok{(ex\_vec, }\FunctionTok{regex}\NormalTok{(}\StringTok{"[aeiou]"}\NormalTok{, }\AttributeTok{ignore\_case =} \ConstantTok{TRUE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] | <A>ppl<e>
## [2] | <O>r<a>ng<e>
\end{verbatim}

To \textbf{match a range of characters/numbers} we can separate these by hyphen in square brackets. So, \texttt{{[}a-n{]}} will match a character from range \texttt{{[}abcdefghijklmn{]}}.

Example-

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ex\_text }\OtherTok{\textless{}{-}} \StringTok{"The quick brown fox jumps over the lazy dog"}
\CommentTok{\# Match a, b or c in lower case}
\FunctionTok{str\_view}\NormalTok{(ex\_text, }\FunctionTok{regex}\NormalTok{(}\StringTok{"[a{-}c]"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] | The qui<c>k <b>rown fox jumps over the l<a>zy dog
\end{verbatim}

Example-2

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ex\_colors }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"grey"}\NormalTok{, }\StringTok{"black"}\NormalTok{, }\StringTok{"gray"}\NormalTok{)}
\FunctionTok{str\_view}\NormalTok{(ex\_colors, }\StringTok{"gr[ae]y"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] | <grey>
## [3] | <gray>
\end{verbatim}

We can also use \textbf{pre-built character classes} listed below.

\begin{itemize}
\tightlist
\item
  \texttt{{[}:punct:{]}} punctuation.
\item
  \texttt{{[}:alpha:{]}} letters.
\item
  \texttt{{[}:lower:{]}} lowercase letters.
\item
  \texttt{{[}:upper:{]}} uppercase letters.
\item
  \texttt{{[}:digit:{]}} digits.
\item
  \texttt{{[}:xdigit:{]}} hex digits.
\item
  \texttt{{[}:alnum:{]}} letters and numbers.
\item
  \texttt{{[}:cntrl:{]}} control characters.
\item
  \texttt{{[}:graph:{]}} letters, numbers, and punctuation.
\item
  \texttt{{[}:print:{]}} letters, numbers, punctuation, and white-space.
\item
  \texttt{{[}:space:{]}} space characters (basically equivalent to \texttt{\textbackslash{}\textbackslash{}s}).
\item
  \texttt{{[}:blank:{]}} space and tab.
\end{itemize}

Example-

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ex\_vec2 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"One apple"}\NormalTok{, }\StringTok{"2 Oranges"}\NormalTok{)}
\FunctionTok{str\_view}\NormalTok{(ex\_vec2, }\StringTok{"[:digit:]"}\NormalTok{, }\AttributeTok{match =} \ConstantTok{NA}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] | One apple
## [2] | <2> Oranges
\end{verbatim}

\hypertarget{non-printable-characters-meta-characters-short-hand-character-classes}{%
\subsection{Non-printable characters/ Meta characters (short-hand character classes)}\label{non-printable-characters-meta-characters-short-hand-character-classes}}

We can use special character sequences to put non-printable characters in our regular expression(s). E.g. \texttt{\textbackslash{}t} matches a tab character. \textbf{But since \texttt{\textbackslash{}} is an escape character in R, we need to escape it too.} So to match a tab character we have to put \texttt{\textbackslash{}\textbackslash{}t} in our regex sequence. Regex for that matches new line (line feed) is \texttt{\textbackslash{}\textbackslash{}n}. \texttt{Regex} for other meta characters is listed below-

\begin{itemize}
\tightlist
\item
  \texttt{\textbackslash{}\textbackslash{}s} matches a white-space character. Moreover, its complement \texttt{\textbackslash{}\textbackslash{}S} matches any character except a white-space.
\item
  \texttt{\textbackslash{}\textbackslash{}w} matches any alphanumeric character. Similarly, its complement is \texttt{\textbackslash{}\textbackslash{}W} which matches any character except alphanumeric characters.
\item
  \texttt{\textbackslash{}\textbackslash{}d} matches any digit. Similarly, its complement is \texttt{\textbackslash{}\textbackslash{}D} which matches any character except digits.
\item
  \texttt{\textbackslash{}\textbackslash{}b} matches any word boundary. Thus, \texttt{\textbackslash{}\textbackslash{}B} matches any character except a word boundary.
\item
  \texttt{.} matches any character. To match a literal dot \texttt{.} we have to escape that; and thus \texttt{\textbackslash{}\textbackslash{}.} matches a dot character.
\end{itemize}

See these examples-

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ex\_vec3 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"One apple"}\NormalTok{, }\StringTok{"2 oranges \& 3 bananas."}\NormalTok{)}
\CommentTok{\# match word character}
\FunctionTok{str\_view}\NormalTok{(ex\_vec3, }\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{w"}\NormalTok{, }\AttributeTok{match =} \ConstantTok{NA}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] | <O><n><e> <a><p><p><l><e>
## [2] | <2> <o><r><a><n><g><e><s> & <3> <b><a><n><a><n><a><s>.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# match any character followed by a dot character}
\FunctionTok{str\_view}\NormalTok{(ex\_vec3, }\StringTok{".}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{."}\NormalTok{, }\AttributeTok{match =} \ConstantTok{NA}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] | One apple
## [2] | 2 oranges & 3 banana<s.>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Note both character and dot will be matched}
\end{Highlighting}
\end{Shaded}

\hypertarget{quantifiers}{%
\section{Quantifiers}\label{quantifiers}}

What if we want to match more than one literal/character through \texttt{regex}? Let's say if we want to check whether the given string or string vector contain two consecutive vowels. One method may be to use character classes two times i.e.~using \texttt{{[}aeiou{]}{[}aeiou{]}}. But this method is against the principles of \textbf{DRY}\footnote{Dont repeat yourself} which is one of the common principle of programming. To solve these issues, we have quantifiers.

\begin{itemize}
\tightlist
\item
  \texttt{+} \textbf{1 or more} occurrences
\item
  \texttt{*} \textbf{0 or more}
\item
  \texttt{?} \textbf{0 or 1}
\item
  \texttt{\{\}} specified numbers

  \begin{itemize}
  \tightlist
  \item
    \texttt{\{n\}} exactly n
  \item
    \texttt{\{n,\}} n or more
  \item
    \texttt{\{n,m\}} between n and m
  \end{itemize}
\end{itemize}

Thus, we may match two consecutive vowels using \texttt{{[}aeiou{]}\{2\}}. See this example

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ex\_vec }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Apple"}\NormalTok{, }\StringTok{"Banana"}\NormalTok{, }\StringTok{"pineapple"}\NormalTok{)}
\FunctionTok{str\_view}\NormalTok{(ex\_vec, }\StringTok{"[aeiou]\{2\}"}\NormalTok{, }\AttributeTok{match =} \ConstantTok{NA}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] | Apple
## [2] | Banana
## [3] | pin<ea>pple
\end{verbatim}

\hypertarget{alternation}{%
\section{Alternation}\label{alternation}}

Alternation in regular expressions allows you to match one pattern or another, depending on which one appears first in the input string. The pipe symbol \texttt{\textbar{}} is used to separate the alternative patterns.

\hypertarget{basic-alternation}{%
\paragraph{Basic Alternation}\label{basic-alternation}}

Let's start with a basic example to illustrate how alternation works:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{string }\OtherTok{\textless{}{-}} \StringTok{"I have an apple and a banana"}
\NormalTok{pattern }\OtherTok{\textless{}{-}} \StringTok{"apple|banana"}

\FunctionTok{str\_extract}\NormalTok{(string, pattern)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "apple"
\end{verbatim}

\hypertarget{order-of-precedence}{%
\paragraph{Order of Precedence}\label{order-of-precedence}}

When using alternation, it's important to keep in mind the order of precedence rules. In general, the first pattern that matches the input string will be selected, and subsequent patterns will not be considered. Here's an example to illustrate this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{string }\OtherTok{\textless{}{-}} \StringTok{"I have a pineapple and an apple"}
\FunctionTok{str\_extract}\NormalTok{(string, }\AttributeTok{pattern =} \StringTok{"apple|pineapple"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "pineapple"
\end{verbatim}

In this example, we have a string \texttt{string} that contains the words ``apple'' and ``pineapple''. We want to extract the first occurrence of either ``apple'' or ``pineapple'' from this text using a regular expression pattern that utilizes alternation. The pattern \texttt{apple\textbar{}pineapple} means ``match `apple' OR `pineapple'\,''. However, since the input string contains ``pineapple'' before ``apple'', the \texttt{str\_extract()} function selects the first matching string ``pineapple''.

\hypertarget{grouping-alternatives}{%
\paragraph{Grouping Alternatives}\label{grouping-alternatives}}

We can also use parentheses to group alternative patterns together. This can be useful for specifying more complex patterns. Example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{string }\OtherTok{\textless{}{-}} \StringTok{"Apple and pineapples are good for health"}
\NormalTok{pattern }\OtherTok{\textless{}{-}} \StringTok{"(apple|banana|cherry) (and|or) (pineapple|kiwi|mango)"}

\FunctionTok{str\_view}\NormalTok{(string, }\FunctionTok{regex}\NormalTok{(pattern, }\AttributeTok{ignore\_case =} \ConstantTok{TRUE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] | <Apple and pineapple>s are good for health
\end{verbatim}

In above examples, we have used \texttt{stringr::regex()} to modify regex flag to ignore cases while matching.

\hypertarget{anchors}{%
\section{Anchors}\label{anchors}}

Anchors in regular expressions allow you to match patterns at specific positions within the input string. In R, you can use various anchors in your regular expressions to match the beginning, end, or specific positions within the input text.

\hypertarget{beginning-and-end-anchors}{%
\subsection{Beginning and End Anchors}\label{beginning-and-end-anchors}}

The beginning anchor \texttt{\^{}} and end anchor \texttt{\$} are used to match patterns at the beginning or end of the input string, respectively. Example

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{string }\OtherTok{\textless{}{-}} \StringTok{"The quick brown fox jumps over the lazy dog. The fox is brown."}
\NormalTok{pattern }\OtherTok{\textless{}{-}} \StringTok{"\^{}the"}
\FunctionTok{str\_view}\NormalTok{(string, }\FunctionTok{regex}\NormalTok{(pattern, }\AttributeTok{ignore\_case =} \ConstantTok{TRUE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] | <The> quick brown fox jumps over the lazy dog. The fox is brown.
\end{verbatim}

In the above example, we are matching word \texttt{the} which is at the beginning of a sentence only.

\hypertarget{word-boundary-anchors}{%
\subsection{Word Boundary Anchors}\label{word-boundary-anchors}}

The word boundary anchor \texttt{\textbackslash{}\textbackslash{}b} is used to match patterns at the beginning or end of a word within the input string. Example

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{string }\OtherTok{\textless{}{-}} \StringTok{\textquotesingle{}Apple and pineapple, both are good for health\textquotesingle{}}
\NormalTok{pattern }\OtherTok{\textless{}{-}} \StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{bapple}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{b\textquotesingle{}}
\FunctionTok{str\_view}\NormalTok{(string, }\FunctionTok{regex}\NormalTok{(pattern, }\AttributeTok{ignore\_case =} \ConstantTok{TRUE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] | <Apple> and pineapple, both are good for health
\end{verbatim}

In the above example, though \texttt{apple} string is contained in another word \texttt{pineapple} we are limiting our search for whole words only.

\hypertarget{capture-groups}{%
\section{Capture Groups}\label{capture-groups}}

A capture group is a way to group a part of a regular expression and capture it as a separate sub-string. This can be useful when you want to extract or replace a specific part of a string. In R, capture groups are denoted by parentheses \texttt{()}. Anything inside the parentheses is captured and can be referenced later in the regular expression or in the replacement string.

One use of capturing group is to refer back to it within a match with back reference: \texttt{\textbackslash{}1} refers to the match contained in the first parenthesis, \texttt{\textbackslash{}2} in the second parenthesis, and so on.

Example-1

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_fruits }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}apple\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}banana\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}coconut\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}berry\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}cucumber\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}date\textquotesingle{}}\NormalTok{)}
\CommentTok{\# search for repeated alphabet}
\NormalTok{pattern }\OtherTok{\textless{}{-}} \StringTok{\textquotesingle{}(.)}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{1\textquotesingle{}}
\FunctionTok{str\_view}\NormalTok{(my\_fruits, }\FunctionTok{regex}\NormalTok{(pattern), }\AttributeTok{match =} \ConstantTok{NA}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] | a<pp>le
## [2] | banana
## [3] | coconut
## [4] | be<rr>y
## [5] | cucumber
## [6] | date
\end{verbatim}

Example-2

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# search for repeated pair of alphabets}
\NormalTok{pattern }\OtherTok{\textless{}{-}} \StringTok{\textquotesingle{}(..)}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{1\textquotesingle{}}
\FunctionTok{str\_view}\NormalTok{(my\_fruits, }\FunctionTok{regex}\NormalTok{(pattern), }\AttributeTok{match =} \ConstantTok{NA}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] | apple
## [2] | b<anan>a
## [3] | <coco>nut
## [4] | berry
## [5] | <cucu>mber
## [6] | date
\end{verbatim}

Another way to use capturing group is, when we want to replace the pattern with something else. It is better to understand this with the following example-

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# We have names in last\_name, first\_name format}
\NormalTok{names }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}Hanks, Tom\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Affleck, Ben\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Damon, Matt\textquotesingle{}}\NormalTok{)}
\FunctionTok{str\_view}\NormalTok{(names)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] | Hanks, Tom
## [2] | Affleck, Ben
## [3] | Damon, Matt
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Using this regex, we can convert these to first\_name last\_name format}
\FunctionTok{str\_replace\_all}\NormalTok{(names, }\StringTok{\textquotesingle{}(}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{w+),}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{s+(}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{w+)\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{2 }\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{1\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Tom Hanks"   "Ben Affleck" "Matt Damon"
\end{verbatim}

\hypertarget{lookarounds}{%
\section{Lookarounds}\label{lookarounds}}

\textbf{Look-ahead} and \textbf{look-behinds} are zero-width assertions in regex. They are used to match a pattern only if it is followed or preceded by another pattern, respectively. The pattern in the look-ahead or look-behind is not included in the match.

\hypertarget{lookahead}{%
\subsection{Lookahead}\label{lookahead}}

A look-ahead is used to match a pattern only if it is followed by another pattern. \emph{Positive Lookaheads} are written as \texttt{(?=...)}, where \texttt{...} is the pattern that must follow the match.

For example, the regex pattern \texttt{hello(?=\ world)} matches ``hello'' only if it is followed by '' world''. It matches ``hello world'' but not ``hello there world'' or ``hello''.

Example

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{string }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"hello world"}\NormalTok{, }\StringTok{"hello there world"}\NormalTok{, }\StringTok{"hello"}\NormalTok{)}
\FunctionTok{str\_view}\NormalTok{(string, }\StringTok{"hello(?= world)"}\NormalTok{, }\AttributeTok{match =} \ConstantTok{NA}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] | <hello> world
## [2] | hello there world
## [3] | hello
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Note that "world" is not included in the match}
\end{Highlighting}
\end{Shaded}

\hypertarget{lookbehind}{%
\subsection{Lookbehind}\label{lookbehind}}

A look-behind is used to match a pattern only if it is preceded by another pattern. Look-behinds are written as \texttt{(?\textless{}=...)}, where \texttt{...} is the pattern that must precede the match.

For example, the regex pattern \texttt{(?\textless{}=hello\ )world} matches ``world'' only if it is preceded by ``hello''. It matches ``hello world'' but not ``world hello'' or ``hello there world''.

Example

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{string }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"hello world"}\NormalTok{, }\StringTok{"world hello"}\NormalTok{, }\StringTok{"hello there world"}\NormalTok{)}
\FunctionTok{str\_view}\NormalTok{(string, }\StringTok{"(?\textless{}=hello )world"}\NormalTok{, }\AttributeTok{match =} \ConstantTok{NA}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] | hello <world>
## [2] | world hello
## [3] | hello there world
\end{verbatim}

\hypertarget{negative-lookahead-and-lookbehinds}{%
\subsection{Negative Lookahead and Lookbehinds}\label{negative-lookahead-and-lookbehinds}}

Negative look-ahead and negative look-behinds are used to match a pattern only if it is not followed or preceded by another pattern, respectively. Negative look-ahead and look-behinds are written as \texttt{(?!...)} and \texttt{(?\textless{}!...)}, respectively.

For example, the regex pattern \texttt{hello(?!\ world)} matches ``hello'' only if it is not followed by '' world''. It matches ``hello there'' but not ``hello world'' or ``hello world there''.

Example-

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{string }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"hello there"}\NormalTok{, }\StringTok{"hello world"}\NormalTok{, }\StringTok{"hello world there"}\NormalTok{)}
\FunctionTok{str\_view}\NormalTok{(string, }\StringTok{"hello(?! world)"}\NormalTok{, }\AttributeTok{match =} \ConstantTok{NA}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] | <hello> there
## [2] | hello world
## [3] | hello world there
\end{verbatim}

And the regex pattern \texttt{(?\textless{}!hello\ )world} matches ``world'' only if it is not preceded by ``hello''. It matches ``world hello'' and ``hello there world'' but not ``hello world''.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{string }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"hello world"}\NormalTok{, }\StringTok{"world hello"}\NormalTok{, }\StringTok{"hello there world"}\NormalTok{)}
\FunctionTok{str\_view}\NormalTok{(string, }\StringTok{"(?\textless{}!hello )world"}\NormalTok{, }\AttributeTok{match =} \ConstantTok{NA}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] | hello world
## [2] | <world> hello
## [3] | hello there <world>
\end{verbatim}

\emph{While the difference between the look-ahead and look-behind may be subtle, yet these become clear when string/pattern replacement or extraction is required.}

Examples-

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{string }\OtherTok{\textless{}{-}} \StringTok{"I have 10 apples, 6 pineapples and 5 bananas"}

\CommentTok{\# look{-}behind to match "apples" preceded by a digit and a space}
\NormalTok{pattern1 }\OtherTok{\textless{}{-}} \StringTok{"(?\textless{}=}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{s)apples"}  

\CommentTok{\# look{-}ahead to match count of apples}
\NormalTok{pattern2 }\OtherTok{\textless{}{-}} \StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+(?=}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{sapple)"}  

\FunctionTok{str\_view}\NormalTok{(}\AttributeTok{string =}\NormalTok{ string, }\AttributeTok{pattern =}\NormalTok{ pattern1, }\AttributeTok{match =} \ConstantTok{NA}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] | I have 10 <apples>, 6 pineapples and 5 bananas
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str\_view}\NormalTok{(}\AttributeTok{string =}\NormalTok{ string, }\AttributeTok{pattern =}\NormalTok{ pattern2, }\AttributeTok{match =} \ConstantTok{NA}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] | I have <10> apples, 6 pineapples and 5 bananas
\end{verbatim}

\hypertarget{comments}{%
\section{Comments}\label{comments}}

\hypertarget{comments-within-regex}{%
\subsection{Comments within regex}\label{comments-within-regex}}

We can use the \# character to add comments within a regex pattern. Any text following the \texttt{\#} symbol on a line is ignored by the regex engine and treated as a comment. This can be useful for documenting your regex patterns or temporarily disabling parts of a pattern for testing or debugging. Example-

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str\_view}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"xyz"}\NormalTok{,}\StringTok{"abc"}\NormalTok{), }\StringTok{"x(?\#this is a comment)"}\NormalTok{, }\AttributeTok{match =} \ConstantTok{NA}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] | <x>yz
## [2] | abc
\end{verbatim}

\hypertarget{verbose-mode-multi-line-comments}{%
\subsection{Verbose Mode (multi-line comments)}\label{verbose-mode-multi-line-comments}}

In regular expressions, verbose mode is a feature that allows you to write more readable and maintainable regex patterns by adding comments and white-space without affecting their behavior. To enable verbose mode, we can use the \texttt{(?x)} or \texttt{(?verbose)} modifier at the beginning of your regex pattern.

Example - Using this regex we can extract words that contain a vowel at third place.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{string }\OtherTok{\textless{}{-}} \StringTok{"The quick brown fox jumps over the lazy dog"}
\NormalTok{pattern }\OtherTok{\textless{}{-}} \StringTok{"(?x)      \# Enable verbose mode}
\StringTok{            }\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{b       \# Match word boundary}
\StringTok{            }\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{w\{2\}    \# matches first two alphabets}
\StringTok{            [aeiou]   \# Match a vowel}
\StringTok{            }\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{w*      \# Match optional word characters}
\StringTok{            }\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{b       \# Match word boundary"}
\FunctionTok{str\_view}\NormalTok{(string, pattern, }\AttributeTok{match =} \ConstantTok{NA}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] | <The> <quick> <brown> fox jumps <over> <the> lazy dog
\end{verbatim}

\hypertarget{regex-in-human-readble-format-using-rebus}{%
\chapter{\texorpdfstring{Regex in human readble format using \texttt{rebus}}{Regex in human readble format using rebus}}\label{regex-in-human-readble-format-using-rebus}}

\emph{The content is under development}

\hypertarget{text-analytics-in-r}{%
\chapter{Text Analytics in R}\label{text-analytics-in-r}}

\emph{The content is under development/finalisation}

Text analytics is crucial in data analytics, as text data is becoming increasingly significant across various applications, including marketing analytics. Text is often replacing other forms of unstructured data because it is cost-effective and up-to-date. To fully leverage the potential of text data, we need to understand how to process, clean, summarize, and model it. In this chapter, we will use the R workhorse tools to efficiently begin working with text. We will gain skills in wrangling and visualizing text, so that we are able to perform sentiment analysis in next chapter. We will also discuss a little about running and interpreting topic models, thereby highlighting the indispensable role of text analytics in modern data analysis.

Text data processing faces unique challenges due to the complexity and variability of human language. Unlike structured data, text is unstructured and highly diverse, encompassing different languages, dialects, slang, and abbreviations. This variability complicates standardization and requires sophisticated preprocessing techniques to clean and prepare the data. Additionally, the context-dependent nature of language makes accurate interpretation difficult, as words and phrases can have varying meanings based on their usage.

\hypertarget{text-pre-processing}{%
\section*{Text pre-processing}\label{text-pre-processing}}
\addcontentsline{toc}{section}{Text pre-processing}

\hypertarget{tokenisation}{%
\section{Tokenisation}\label{tokenisation}}

\hypertarget{stop-words}{%
\section{Stop words}\label{stop-words}}

\hypertarget{stemming}{%
\section{Stemming}\label{stemming}}

\hypertarget{text-cleaning-vy-removing-punctuation-and-other-unwanted-characterstext}{%
\section{Text Cleaning vy removing punctuation and other unwanted characters/text}\label{text-cleaning-vy-removing-punctuation-and-other-unwanted-characterstext}}

\hypertarget{sentiment-analysis}{%
\chapter{Sentiment Analysis}\label{sentiment-analysis}}

\emph{The content is under development}

\hypertarget{visualising-text-analytics-through-wordcloud-etc.}{%
\chapter{Visualising Text analytics through Wordcloud, etc.}\label{visualising-text-analytics-through-wordcloud-etc.}}

\emph{The content is under development is following is not finalised.}

\hypertarget{frequency-plots}{%
\section{Frequency plots}\label{frequency-plots}}

\hypertarget{wordclouds}{%
\section{Wordclouds}\label{wordclouds}}

\hypertarget{step-1prepare-data-and-load-libraries}{%
\subsection{Step-1:Prepare data and load libraries}\label{step-1prepare-data-and-load-libraries}}

As an example we will create a word cloud with Budget Speech made by Finance Minister during her Budget speech\footnote{Data Source: \href{https://www.indiabudget.gov.in/}{Indian Budget Portal}} 2022-23. All of the budget speech is available in file called \texttt{budget.txt}.

Load Libraries

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(tidytext) }\CommentTok{\#install.packages("tidytext")}
\FunctionTok{library}\NormalTok{(wordcloud) }\CommentTok{\#install.packages("wordcloud")}
\FunctionTok{library}\NormalTok{(ggtext)}
\FunctionTok{library}\NormalTok{(ggalt)}
\FunctionTok{library}\NormalTok{(ggthemes)}
\FunctionTok{library}\NormalTok{(ggpubr)}
\end{Highlighting}
\end{Shaded}

Load data

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat }\OtherTok{\textless{}{-}} \FunctionTok{read.table}\NormalTok{(}\StringTok{\textquotesingle{}data/budget.txt\textquotesingle{}}\NormalTok{, }\AttributeTok{header =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{fill =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{step-2-reshape-the-.txt-data-frame-into-one-column}{%
\subsection{Step-2: Reshape the .txt data frame into one column}\label{step-2-reshape-the-.txt-data-frame-into-one-column}}

Above steps will create one row per line. Let's create a tidy data frame out of this data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tidy\_dat }\OtherTok{\textless{}{-}}\NormalTok{ dat }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_longer}\NormalTok{(}\FunctionTok{everything}\NormalTok{(), }\AttributeTok{values\_to =} \StringTok{\textquotesingle{}word\textquotesingle{}}\NormalTok{, }\AttributeTok{names\_to =} \ConstantTok{NULL}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{step-3-tokenize-the-datawords}{%
\subsection{Step-3: Tokenize the data/words}\label{step-3-tokenize-the-datawords}}

To tokenize the words we will use function \texttt{unnest\_tokens()} from \texttt{tidytext} library. As a further step we will have a count of each word, using \texttt{dplyr::count} which will create a column \texttt{n} against each word.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tokens }\OtherTok{\textless{}{-}}\NormalTok{ tidy\_dat }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{unnest\_tokens}\NormalTok{(word, word) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{count}\NormalTok{(word, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\hypertarget{step-4-clean-stop-words}{%
\subsection{Step-4: Clean stop words}\label{step-4-clean-stop-words}}

The library \texttt{tidytext} has a default database which can eliminate stop words from above data. Let's load this default stop words data.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(}\StringTok{"stop\_words"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We may then remove stop words using \texttt{dplyr::anti\_join}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tokens\_clean }\OtherTok{\textless{}{-}}\NormalTok{ tokens }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{anti\_join}\NormalTok{(stop\_words, }\AttributeTok{by=}\StringTok{\textquotesingle{}word\textquotesingle{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \CommentTok{\# remove numbers}
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{str\_detect}\NormalTok{(word, }\StringTok{"\^{}[0{-}9]"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

We may remove additional stop words those specific to this data/input. To have an idea of these stop words, we may at firt, skip this step altogether and proceed to generate word cloud in next step directly. After having a first look, we can identify and then remove these additional stop words seen in first round(s).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{uni\_sw }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{word =} \FunctionTok{c}\NormalTok{(}\StringTok{"cent"}\NormalTok{, }\StringTok{"pm"}\NormalTok{, }\StringTok{"crore"}\NormalTok{, }
                              \StringTok{"lakh"}\NormalTok{, }\StringTok{"set"}\NormalTok{,}
                              \StringTok{"level"}\NormalTok{, }\StringTok{"sir"}\NormalTok{))}

\NormalTok{tokens\_clean }\OtherTok{\textless{}{-}}\NormalTok{ tokens\_clean }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{anti\_join}\NormalTok{(uni\_sw, }\AttributeTok{by =} \StringTok{"word"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{step-5-plotgenerate-word-cloud}{%
\subsection{Step-5: Plot/generate word cloud}\label{step-5-plotgenerate-word-cloud}}

Output/Word cloud of following code can be seen in figure \ref{fig:wordcloud}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pal }\OtherTok{\textless{}{-}}\NormalTok{ RColorBrewer}\SpecialCharTok{::}\FunctionTok{brewer.pal}\NormalTok{(}\DecValTok{8}\NormalTok{,}\StringTok{"Dark2"}\NormalTok{)}

\CommentTok{\# plot the 40 most common words}
\NormalTok{tokens\_clean }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{with}\NormalTok{(}\FunctionTok{wordcloud}\NormalTok{(word, }
\NormalTok{                 n, }
                 \AttributeTok{random.order =} \ConstantTok{FALSE}\NormalTok{, }
                 \AttributeTok{max.words =} \DecValTok{40}\NormalTok{, }
                 \AttributeTok{colors=}\NormalTok{pal,}
                 \AttributeTok{scale=}\FunctionTok{c}\NormalTok{(}\FloatTok{2.5}\NormalTok{, .}\DecValTok{5}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{DauR_files/figure-latex/wordcloud-1} 

}

\caption{Word Cloud of FM's Budget Speech 2022}\label{fig:wordcloud}
\end{figure}

\hypertarget{finding-string-similarity}{%
\chapter{Finding string similarity}\label{finding-string-similarity}}

Comparison of two (or more) numeric fields is an easy job in the sense that we can use multiple statistical methods available to measure comparison between these. On the other hand, comparing strings in any way, shape or form is not a trivial task. Despite this complexity, comparing text strings is a common and fundamental task in many text-processing algorithms. Basic objective of all string similarity algorithms are to quantify the similarity between two text strings in terms of string metrics.

The fuzzy matching problems are to input two strings and return a score quantifying the likelihood that they are expressions of the same entity. So (\texttt{Geeta} and \texttt{Gita}) should get a high score but not (\texttt{Apple} and \texttt{Microsoft}). Over several decades, various algorithms for fuzzy string matching have emerged. They have varying strengths and weaknesses. These fall into two broad categories: \texttt{lexical\ matching} and \texttt{phonetic\ matching}.

\hypertarget{lexical-matching}{%
\section{Lexical matching}\label{lexical-matching}}

\emph{Lexical matching algorithms} match two strings based on some model of errors. Typically they are meant to match strings that differ due to spelling or typing errors. Consider \texttt{Atharv} and \texttt{ahtarv}. A lexical matching algorithm would pick up that \texttt{ht} is a transposition of \texttt{th}. Such transposition errors are common. Given this, and that the rest of the two strings match exactly and are long enough, we should score this match as high.

Normally, algorithms to find lexical matching, can be classified into `edit distance based' or `token based'.

\hypertarget{levenshtein-algorithm}{%
\subsection{Levenshtein algorithm}\label{levenshtein-algorithm}}

It is named after \emph{Vladimir Levenshtein}, who considered this distance in 1965. The \texttt{Levenshtein\ distance} between two words is the minimum number of single-character edits (i.e.~insertions, deletions or substitutions) required to change one word into the other. Levenshtein distance may also be referred to as \emph{edit distance}, although it may also denote a larger family of distance metrics. It is closely related to pairwise string alignments.

For the two words \texttt{helo} and \texttt{hello}, it is obvious that there is a missing character \texttt{"l"}. Thus to transform the word \texttt{helo} to \texttt{hello} all we need to do is insert that character. The distance, in this case, is \texttt{1} because there is only one edit needed.

\hypertarget{hamming-distance}{%
\subsection{Hamming distance}\label{hamming-distance}}

This distance is computed by overlaying one string over another and finding the places where the strings vary. Note, classical implementation was meant to handle strings of same length. Some implementations may bypass this by adding a padding at prefix or suffix. Nevertheless, the logic is to find the total number of places one string is different from the other.

\hypertarget{jaro-winkler}{%
\subsection{Jaro-Winkler}\label{jaro-winkler}}

This algorithms gives high scores to two strings if,

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  they contain same characters, but within a certain distance from one another, and
\item
  the order of the matching characters is same.
\end{enumerate}

To be exact, the distance of finding similar character is 1 less than half of length of longest string. So if longest strings has length of 5, a character at the start of the string 1 must be found before or on ((5/2)--1) \textasciitilde{} 2nd position in the string 2 to be considered valid match. Because of this, the algorithm is directional and gives high score if matching is from the beginning of the strings.

\hypertarget{q-gram}{%
\subsection{Q-Gram}\label{q-gram}}

\emph{Q-Grams} is based on the difference between occurences of \texttt{Q} consecutive characters in two strings. To illustrate take a case of Q=3 (this special case is also called trigrams). For \texttt{atharv} and its possible typo \texttt{ahtarv} the trigrams will be

\begin{itemize}
\tightlist
\item
  For atharv \texttt{\{ath\ tha\ har\ arv\}}
\item
  for ahtarv \texttt{\{aht\ hta\ tar\ arv\}}
\end{itemize}

We can see that a total of \texttt{7} unique trigrams have been formed and out of these only \texttt{1} is similar. Thus, 3-gram similarility would be \texttt{1/7=14\%}. We can see that this algorithm is not very effective for transpositions.

\hypertarget{phonetic-matching}{%
\section{Phonetic matching}\label{phonetic-matching}}

\emph{Phonetic matching algorithms} match strings based on how similar they sound. Consider \texttt{Geeta} and \texttt{Gita.} They sound similar enough that one person might spell as \texttt{Geetha} or \texttt{Geeta}, another as \texttt{Gita.} As in this case, one is not necessarily a misspelling of the other. just sounds similar.

\hypertarget{soundex}{%
\subsection{Soundex}\label{soundex}}

Created by \emph{Robert Russel} and \emph{Margaret King Odell} in 1918, this algorithm intended to match names and surnames based on the basic rules of English pronunciation, hence, similar names get the same value.

\hypertarget{metaphone}{%
\subsection{Metaphone}\label{metaphone}}

Developed by \emph{Lawrence Philips} in 1990, the Metaphone is also more accurate compared with the \texttt{Soundex} method as it takes into consideration the groups of letters. The disadvantage shows up when we apply it to reconcile the strings that are not in English, as it is based on the rules of English pronunciation.

\hypertarget{double-metaphone}{%
\subsection{Double Metaphone}\label{double-metaphone}}

Following \texttt{Metaphone}, \emph{Philips} also designed the \emph{Double Metaphone}. As its name suggests, it returns two codes, so you have more chances to match the items, however, at the same time, it means a higher probability of an error. According to the algorithm, there are three matching levels:

\begin{itemize}
\tightlist
\item
  \texttt{primary\ key\ to\ the\ primary\ key\ =\ strongest\ match},
\item
  \texttt{secondary\ key\ to\ the\ primary\ key\ =\ normal\ match},
\item
  \texttt{secondary\ key\ against\ the\ secondary\ key\ =\ weakest\ match}.
\end{itemize}

\hypertarget{metaphone-3}{%
\subsection{Metaphone 3}\label{metaphone-3}}

\emph{Philips} further refined the double metaphone algorithm to produce better results. The algorithm (Metaphone 3) is however, proprietary and is not open-source.

\hypertarget{examples}{%
\section{Examples}\label{examples}}

In R, we can use \texttt{stringdist} package to calculate many of the above mentioned distances. The function is vectorised. The synatx is

\begin{verbatim}
stringdist(
  a,
  b,
  method = c("osa", "lv", "dl", "hamming", "lcs", "qgram", "cosine", "jaccard", "jw",
    "soundex"),
  useBytes = FALSE,
  weight = c(d = 1, i = 1, s = 1, t = 1),
  q = 1,
  p = 0,
  bt = 0,
  nthread = getOption("sd_num_thread")
)
\end{verbatim}

where -

\begin{itemize}
\tightlist
\item
  \texttt{a} and \texttt{b} are two strings/vectors for which similarity/distance is to be measured.
\item
  \texttt{method} to be used. Default is

  \begin{itemize}
  \tightlist
  \item
    \texttt{osa} for \emph{Optimal String Alignment}. Other methods are-
  \item
    \texttt{lv} for \emph{Levenstein distance},
  \item
    \texttt{dl} for \emph{Damerau-Levenshtein}
  \item
    \texttt{hamming} for \emph{Hamming distance}
  \item
    \texttt{lcs} for \emph{Longest Common Substring}
  \item
    \texttt{qgram} for Q-Grams
  \item
    \texttt{cosine} for cosine
  \item
    \texttt{jaccard} for Jaccard's algorithm
  \item
    \texttt{jw} for Jaro-Winkler
  \item
    \texttt{soundex} for Soundex
  \end{itemize}
\item
  Other arguments are needed on the basis of algorithm chosen.
\end{itemize}

To calculate `metaphone' index we can use \texttt{phonics} package and for `Double Metaphone' we can use \texttt{PGRdup} package in R.

Example - Suppose we have a set of two names.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nameset1 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}Geeta\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Susheel\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Ram\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Dr. Suchitra\textquotesingle{}}\NormalTok{)}
\NormalTok{nameset2 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}Gita\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Sushil\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Rama\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Suchitra\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Note most of these distances/similarity indices are cases sensitive, and therefore we have to use these methods with a bit cleaning first. We can convert cases of all strings to lower-case to eliminate these (if) unwanted errors.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(stringdist)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Attaching package: 'stringdist'
\end{verbatim}

\begin{verbatim}
## The following object is masked from 'package:tidyr':
## 
##     extract
\end{verbatim}

\begin{verbatim}
## The following object is masked from 'package:magrittr':
## 
##     extract
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{suppressPackageStartupMessages}\NormalTok{(}\FunctionTok{library}\NormalTok{(dplyr))}

\FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{nameset1 =} \FunctionTok{tolower}\NormalTok{(nameset1),}
  \AttributeTok{nameset2 =} \FunctionTok{tolower}\NormalTok{(nameset2)}
\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{lv\_dist =} \FunctionTok{stringdist}\NormalTok{(nameset1, nameset2, }\AttributeTok{method =} \StringTok{\textquotesingle{}lv\textquotesingle{}}\NormalTok{),}
         \AttributeTok{jw\_dist =} \FunctionTok{stringdist}\NormalTok{(nameset1, nameset2, }\AttributeTok{method =} \StringTok{\textquotesingle{}jw\textquotesingle{}}\NormalTok{),}
         \AttributeTok{qgram\_3 =} \FunctionTok{stringdist}\NormalTok{(nameset1, nameset2, }\AttributeTok{method =} \StringTok{\textquotesingle{}qgram\textquotesingle{}}\NormalTok{, }\AttributeTok{q=}\DecValTok{3}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       nameset1 nameset2 lv_dist    jw_dist qgram_3
## 1        geeta     gita       2 0.21666667       5
## 2      susheel   sushil       2 0.15079365       5
## 3          ram     rama       1 0.08333333       1
## 4 dr. suchitra suchitra       4 0.25694444       4
\end{verbatim}

Creating Metaphone and Double Metaphone

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(phonics)}

\FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{nameset1 =} \FunctionTok{tolower}\NormalTok{(nameset1),}
  \AttributeTok{nameset2 =} \FunctionTok{tolower}\NormalTok{(nameset2)}
\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{metaphone\_1 =} \FunctionTok{metaphone}\NormalTok{(nameset1),}
         \AttributeTok{metaphone\_2 =} \FunctionTok{metaphone}\NormalTok{(nameset2))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: There was 1 warning in `mutate()`.
## i In argument: `metaphone_1 = metaphone(nameset1)`.
## Caused by warning in `metaphone()`:
## ! unknown characters found, results may not be consistent
\end{verbatim}

\begin{verbatim}
##       nameset1 nameset2 metaphone_1 metaphone_2
## 1        geeta     gita          JT          JT
## 2      susheel   sushil         SXL         SXL
## 3          ram     rama          RM          RM
## 4 dr. suchitra suchitra        <NA>        SXTR
\end{verbatim}

Note that we cannot calculate \texttt{metaphone} for special characters even for spaces.

\emph{Double metaphone} is not vectorised. So we have to use \texttt{apply} family of functions here.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{suppressPackageStartupMessages}\NormalTok{(}\FunctionTok{library}\NormalTok{(PGRdup))}
\FunctionTok{library}\NormalTok{(purrr)}

\FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{nameset1 =} \FunctionTok{tolower}\NormalTok{(nameset1),}
  \AttributeTok{nameset2 =} \FunctionTok{tolower}\NormalTok{(nameset2)}
\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{DMP\_1\_1 =} \FunctionTok{map\_chr}\NormalTok{(nameset1, }\SpecialCharTok{\textasciitilde{}}\FunctionTok{DoubleMetaphone}\NormalTok{(.x)[[}\DecValTok{1}\NormalTok{]]),}
         \AttributeTok{DMP\_1\_2 =} \FunctionTok{map\_chr}\NormalTok{(nameset1, }\SpecialCharTok{\textasciitilde{}}\FunctionTok{DoubleMetaphone}\NormalTok{(.x)[[}\DecValTok{2}\NormalTok{]]),}
         \AttributeTok{DMP\_2\_1 =} \FunctionTok{map\_chr}\NormalTok{(nameset2, }\SpecialCharTok{\textasciitilde{}}\FunctionTok{DoubleMetaphone}\NormalTok{(.x)[[}\DecValTok{1}\NormalTok{]]),}
         \AttributeTok{DMP\_2\_2 =} \FunctionTok{map\_chr}\NormalTok{(nameset2, }\SpecialCharTok{\textasciitilde{}}\FunctionTok{DoubleMetaphone}\NormalTok{(.x)[[}\DecValTok{2}\NormalTok{]]))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       nameset1 nameset2 DMP_1_1 DMP_1_2 DMP_2_1 DMP_2_2
## 1        geeta     gita      JT      KT      JT      KT
## 2      susheel   sushil     SXL     SXL     SXL     SXL
## 3          ram     rama      RM      RM      RM      RM
## 4 dr. suchitra suchitra    TRSX    TRSK    SXTR    SKTR
\end{verbatim}

\hypertarget{part-viii-geo-computation-in-r}{%
\chapter*{Part-VIII: Geo computation in R}\label{part-viii-geo-computation-in-r}}
\addcontentsline{toc}{chapter}{Part-VIII: Geo computation in R}

\hypertarget{maps-in-r}{%
\chapter{Maps in R}\label{maps-in-r}}

\emph{The content is under development}

\hypertarget{geo-coding-in-r}{%
\chapter{Geo-Coding in R}\label{geo-coding-in-r}}

\emph{The content is under development}

\hypertarget{reverse-geo-coding}{%
\chapter{Reverse geo-coding}\label{reverse-geo-coding}}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

\textbf{Geo-coding} is the process of converting a human-readable address into geographic coordinates, such as latitude and longitude. Geocoding is the process of determining the location of an address on a map. So, plotting the places on geographical map layers require extraction of latitudes and longitudes from master databases.

\textbf{Reverse geo-coding}, on the other hand, is the process of converting geographic coordinates (latitude and longitude) into a human-readable address, such as a street address, city, state/province, and country. In other words, it's the process of determining a location's address based on its geographic coordinates.

Both geo-coding and reverse geo-coding are important in geospatial data analysis, as they allow us to link spatial information with non-spatial information, such as demographic data, land use data, or other forms of spatial data. This information can then be used to gain insights into patterns and relationships in the data, and to make informed decisions based on location information.

Reverse geo-coding can be used in forensic data analysis to help identify the location of transactions or events. For example, it can be used to determine the location of an ATM or point of sale device where a fraudulent transaction occurred. It can also be used to analyze patterns of activity in a particular geographic area, such as the frequency and timing of certain types of transactions. As another example, we may think of an application data, where mobile or hand held devices are used to capture data at the time and place of intended service delivery. Auditors may use the geo-spatial data captured by these devices to cross check/verify the actual points of service delivery.

\hypertarget{widely-used-databases}{%
\section{Widely used databases}\label{widely-used-databases}}

\textbf{OpenStreetMap} (OSM) is one of the most popular and widely used master databases for geo-coding and reverse geo-coding. OSM is a free and open-source map of the world, created and maintained by a community of volunteers. It provides a rich and detailed set of spatial data, including street names, addresses, and other points of interest.

Other popular master databases for geo-coding and reverse geo-coding include \textbf{Google Maps}. Google Maps provides a rich set of geo-coding and reverse geo-coding APIs that are widely used in web and mobile applications. Google Maps provides a comprehensive set of spatial data, including street names, addresses, and other points of interest.

\hypertarget{example-in-r}{%
\section{Example in R}\label{example-in-r}}

\hypertarget{prerequisites}{%
\subsection{Prerequisites}\label{prerequisites}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidygeocoder)}
\end{Highlighting}
\end{Shaded}

\hypertarget{function-to-be-used}{%
\subsection{Function to be used}\label{function-to-be-used}}

We will use \texttt{reverse\_geo()} function from this library. Syntax is

\begin{verbatim}
reverse_geo(
  lat,
  long,
  method = "osm",
  address = "address",
  full_results = FALSE
  ...
)
\end{verbatim}

Here -

\begin{itemize}
\tightlist
\item
  \texttt{lat} and \texttt{long} are, as the names suggest, latitude and longitudes for the place, of which address is to be extracted.
\item
  \texttt{method} argument provides for geo service to be used. Default is \texttt{osm}. Other services available are - \texttt{arcgis}, \texttt{geocodio}, \texttt{google}, etc.
\item
  \texttt{address} provides the column name to be used.
\item
  \texttt{full\_results} by default is \texttt{FALSE}. But if set to \texttt{TRUE} returns all available data from the geocoding service.
\item
  for other arguments, please check help of the function.
\end{itemize}

\hypertarget{practical-data-set}{%
\subsection{Practical data-set}\label{practical-data-set}}

\textbf{Remember one thing, the function is not vectorised.} So we will have to use \texttt{apply} family or \texttt{purrr::map} family of functions to get reverse geo-coding information.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Coordinates of tajmahal}
\FunctionTok{reverse\_geo}\NormalTok{(}\AttributeTok{lat =} \FloatTok{27.1751}\NormalTok{, }
            \AttributeTok{long =} \FloatTok{78.421}\NormalTok{, }
            \AttributeTok{full\_results =} \ConstantTok{TRUE}\NormalTok{, }
            \AttributeTok{method =} \StringTok{\textquotesingle{}osm\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Passing 1 coordinate to the Nominatim single coordinate geocoder
\end{verbatim}

\begin{verbatim}
## Query completed in: 1 seconds
\end{verbatim}

\begin{verbatim}
## # A tibble: 1 x 25
##     lat  long address     place_id licence osm_type osm_id osm_lat osm_lon class
##   <dbl> <dbl> <chr>          <int> <chr>   <chr>     <int> <chr>   <chr>   <chr>
## 1  27.2  78.4 Sailai, Fi~   2.45e8 Data ©~ way      4.56e8 27.175~ 78.421~ high~
## # i 15 more variables: type <chr>, place_rank <int>, importance <dbl>,
## #   addresstype <chr>, name <chr>, suburb <chr>, town <chr>, county <chr>,
## #   state_district <chr>, state <chr>, `ISO3166-2-lvl4` <chr>, postcode <chr>,
## #   country <chr>, country_code <chr>, boundingbox <list>
\end{verbatim}

\hypertarget{using-it-in-a-dataset}{%
\subsection{Using it in a dataset}\label{using-it-in-a-dataset}}

Let's build a sample dataset of say 4 values/places of interest in India

\begin{tabular}{r|r}
\hline
lat & long\\
\hline
27.1751 & 78.0421\\
\hline
24.8318 & 79.9199\\
\hline
27.1795 & 78.0211\\
\hline
28.5245 & 77.1855\\
\hline
\end{tabular}

Now extracting information for these using \texttt{purrr::map} family

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(dplyr, }\AttributeTok{warn.conflicts =} \ConstantTok{FALSE}\NormalTok{)}
\FunctionTok{library}\NormalTok{(purrr, }\AttributeTok{warn.conflicts =} \ConstantTok{FALSE}\NormalTok{)}

\NormalTok{results }\OtherTok{\textless{}{-}} \FunctionTok{pmap\_dfr}\NormalTok{(sample,}
     \SpecialCharTok{\textasciitilde{}} \FunctionTok{reverse\_geo}\NormalTok{(..}\DecValTok{1}\NormalTok{, }\CommentTok{\# first column in sample}
\NormalTok{                   ..}\DecValTok{2}\NormalTok{, }\CommentTok{\# second column in sample}
                   \AttributeTok{full\_results =} \ConstantTok{TRUE}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(lat, long, country, state, city, village, address, postcode)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Passing 1 coordinate to the Nominatim single coordinate geocoder
\end{verbatim}

\begin{verbatim}
## Query completed in: 1 seconds
\end{verbatim}

\begin{verbatim}
## Passing 1 coordinate to the Nominatim single coordinate geocoder
\end{verbatim}

\begin{verbatim}
## Query completed in: 1 seconds
\end{verbatim}

\begin{verbatim}
## Passing 1 coordinate to the Nominatim single coordinate geocoder
\end{verbatim}

\begin{verbatim}
## Query completed in: 1 seconds
\end{verbatim}

\begin{verbatim}
## Passing 1 coordinate to the Nominatim single coordinate geocoder
\end{verbatim}

\begin{verbatim}
## Query completed in: 1 seconds
\end{verbatim}

\hypertarget{view-above-results}{%
\subsection{View above results}\label{view-above-results}}

\begin{tabular}{r|r|l|l|l|l|l|l}
\hline
lat & long & country & state & city & village & address & postcode\\
\hline
27.1751 & 78.0421 & India & Uttar Pradesh & Agra & NA & Taj Mahal, Taj East Gate Road, Taj Ganj, Agra, Uttar Pradesh, 282004, India & 282004\\
\hline
24.8318 & 79.9199 & India & Madhya Pradesh & NA & Jatkra & Khajuraho, Jatkra, Rajnagar Tahsil, Chhatarpur District, Madhya Pradesh, 471006, India & 471006\\
\hline
27.1795 & 78.0211 & India & Uttar Pradesh & Agra & NA & Agra Fort, SH62, Agra, Uttar Pradesh, 282004, India & 282004\\
\hline
28.5245 & 77.1855 & India & Delhi & NA & NA & Qutub Minar Complex, Kalka Das Marg, Mehrauli, Mehrauli Tehsil, South Delhi, Delhi, 110030, India & 110030\\
\hline
\end{tabular}

\hypertarget{alternate-syntax-for-those-who-do-not-want-to-use-purrrpmap_dfr}{%
\subsection{\texorpdfstring{Alternate syntax (for those who do not want to use \texttt{purrr::pmap\_dfr})}{Alternate syntax (for those who do not want to use purrr::pmap\_dfr)}}\label{alternate-syntax-for-those-who-do-not-want-to-use-purrrpmap_dfr}}

\begin{verbatim}
sapply(seq(nrow(sample)),
       function(x) reverse_geo(sample$lat[x], sample$long[x], full_results = TRUE)) %>% 
  map_dfr(rbind)
\end{verbatim}

\hypertarget{part-ix-identifying-anamolous-observations-for-audit}{%
\chapter*{Part-IX: Identifying anamolous observations for audit}\label{part-ix-identifying-anamolous-observations-for-audit}}
\addcontentsline{toc}{chapter}{Part-IX: Identifying anamolous observations for audit}

\hypertarget{benford-testsanalysis}{%
\chapter{Benford Tests/Analysis}\label{benford-testsanalysis}}

\hypertarget{introduction-and-historical-context}{%
\section{Introduction and Historical context}\label{introduction-and-historical-context}}

Benford's Law stands out as an analysis method for both visualizing and evaluating numerical data, especially when focused on detecting fraud. It's really handy for catching potential trickery, especially in spotting fraud. This rule tells us how often different digits (like 1, 2, 3, etc.) should show up as the first number in lots of real-world data. This law describes the frequency distribution of the first digit, from left hand side, in many real-life data sets which counter-intuitively is not uniform, and is shown in Figure \ref{fig:benlaw}. Significant differences from the anticipated occurrence rates could signal that the data is questionable and might have been altered. For instance, eligibility for government assistance often hinges on meeting specific criteria, like having an income below a certain level. As a result, data might be manipulated to meet these criteria. This kind of manipulation is precisely what Benford's Law can detect since fabricated numbers won't align with the expected frequency pattern outlined by the law.

The Law is named after physicist \textbf{Frank Benford}, who worked on the theory in 1938 and as a result a paper titled \textbf{The Law of Anomalous Numbers} was published.\citep{frank_ben}. However, its discovery is associated more than five decades earlier when astronomer \textbf{Simon Newcomb} observed that initial pages of log tables booklet were more worn out than later pages and published a two page article titled \textbf{Note on the Frequency of Use of the Different Digits in Natural Numbers} in 1881 \citep{newcomb}.

More researchers continued to work on Benford's law and its extensions. However, it took several decades to find a truly practical application. It was in last decade of twentieth Century, when Dr.~Mark J. Nigrini, an accounting Professor, used the law for fraud detection/anaytics and came up with a practical fraud application. He reviewed multiple data sources like sales figures, insurance claim costs, and expense reimbursement claims and did studies on detecting overstatements and understatement of financial figures. His research confirmed the law's proven usefulness to fraud examiners and auditors in accounting engagements.

His theory is that - \emph{If somebody tries to falsify, say, their tax return then invariably they will have to invent some data. When trying to do this, the tendency is for people to use too many numbers starting with digits in the mid range, 5,6,7 and thus not enough numbers starting with 1.}

\begin{figure}

{\centering \includegraphics[width=0.32\linewidth,height=0.32\textheight]{images/frank_benford} \includegraphics[width=0.32\linewidth,height=0.32\textheight]{images/Simon} \includegraphics[width=0.32\linewidth,height=0.32\textheight]{images/nigrini} 

}

\caption{(L to R) Frank Benford, Simon Newcomb, and Mark Nigrini (Source: Wiki)}\label{fig:benpics}
\end{figure}

\hypertarget{benfords-law-properties-and-extensions}{%
\section{Benford's Law, properties and extensions}\label{benfords-law-properties-and-extensions}}

\hypertarget{law-of-first-digit}{%
\subsection{Law of first digit}\label{law-of-first-digit}}

When considering the likelihood of any digit being in the first position (from the left), our initial assumption might be a simple \emph{one out of nine} scenario, following a uniform distribution. However, this notion was challenged by Canadian-American astronomer \textbf{Simon Newcomb} in 1881, who noticed unusual wear patterns in logarithmic tables. While casually flipping through a logarithmic tables booklet, he discerned a curious pattern--- the initial pages exhibited more wear and tear than their later counterparts.

Subsequently, \textbf{Frank Benford} conducted a comprehensive analysis of 20 diverse datasets encompassing river sizes, chemical compound weights, population data, and more. His findings revealed a successive diminishment in probability from digit 1 to 9. In essence, the probability of digit 1 occurring in the initial position is the highest, while that of digit 9 is the lowest.

Mathematically, \emph{Benford's Law} or \emph{Law of first digits} states that the probability of any digit in first place should follow the equation \eqref{eq:ben1}.

\begin{equation} 
P(d_i) = \log_{10}\left(1 + \frac{1}{d}\right) 
\label{eq:ben1}
\end{equation}

\begin{itemize}
\tightlist
\item
  Where \(d_i\) ranges from \(1\) to \(9\).
\end{itemize}

The probabilities when plotted will generate plot as depicted in Figure \ref{fig:benlaw}.

\begin{figure}

{\centering \includegraphics[height=0.32\textheight]{DauR_files/figure-latex/benlaw-1} 

}

\caption{Diminishing Probabilities of First Digits - Benford Law}\label{fig:benlaw}
\end{figure}

To test the proposed law, Benford analysed 20 different data-sets and he observed that nearly all follow the distribution mentioned in equation \eqref{eq:ben1}.

Let us also try to see whether the law holds by anlaysing six different datasets, which are included in R's package called \texttt{benford.analysis}. Though we will discuss about the package in detail later in section \ref{pracben}. The six datasets are mentioned in Table \ref{tab:sixben}.

\begin{table}
\centering
\caption{\label{tab:sixben}List of six datasets for testing Benford Analysis}
\centering
\begin{tabu} to \linewidth {>{\centering}X>{\centering}X>{\centering}X}
\hline
Item & Title & Column\\
\hline
census.2000\_2010 & Population data - US - 2000 and 2010 & pop.2000\\
\cline{1-3}
census.2009 & Population data of Towns and Cities of the US - 2009 & pop.2009\\
\cline{1-3}
corporate.payment & Corporate payments of a West Coast utility company - 2010 & Amount\\
\cline{1-3}
lakes.perimeter & Perimeter of lakes arround the world & perimeter.km\\
\cline{1-3}
sino.forest & Financial Statemens of Sino Forest Corporation's 2010 Report & value\\
\cline{1-3}
taxable.incomes.1978 & Taxable Income 1978 & taxIncomes\\
\hline
\end{tabu}
\end{table}

The results of Benford's law of first digit on these six datasets are calculated and have been mentioned in Table \ref{tab:tab1}. It can be seen that actual frequencies of first digits, in these six datasets follow Benford's Law. We can even plot the actual frequencies to inspect results visually. Actual Frequencies in these six datasets are plotted in \ref{fig:benplots} and it may be seen that these follow Benford's Law largely.

\begin{table}
\centering
\caption{\label{tab:tab1}Results of First order tests on six datasets}
\centering
\begin{tabu} to \linewidth {>{\raggedleft}X>{\raggedleft}X>{\raggedleft}X>{\raggedleft}X>{\raggedleft}X>{\raggedleft}X>{\raggedleft}X>{\raggedleft}X}
\toprule
digits & Benford & Census 2000\_2010 & Census 2009 & Corporate Payment & Lakes Perimeter & Sino Forest & Taxable Incomes 1978\\
\midrule
1 & 0.3010300 & 0.3092126 & 0.2941207 & 0.3175548 & 0.1508888 & 0.2992228 & 0.3278721\\
2 & 0.1760913 & 0.1797896 & 0.1814547 & 0.1611007 & 0.0687752 & 0.1606218 & 0.2140886\\
3 & 0.1249387 & 0.1271916 & 0.1200472 & 0.1101452 & 0.2170936 & 0.1256477 & 0.1235673\\
4 & 0.0969100 & 0.0975454 & 0.0946743 & 0.0828655 & 0.1818372 & 0.0906736 & 0.0895397\\
5 & 0.0791812 & 0.0656678 & 0.0799118 & 0.1016301 & 0.1309577 & 0.0829016 & 0.0722473\\
\addlinespace
6 & 0.0669468 & 0.0656678 & 0.0702240 & 0.0602811 & 0.0930143 & 0.0699482 & 0.0521491\\
7 & 0.0579919 & 0.0541919 & 0.0597673 & 0.0498209 & 0.0682885 & 0.0518135 & 0.0411117\\
8 & 0.0511525 & 0.0548295 & 0.0534625 & 0.0503666 & 0.0502118 & 0.0699482 & 0.0393606\\
9 & 0.0457575 & 0.0459037 & 0.0463376 & 0.0662351 & 0.0389329 & 0.0492228 & 0.0400637\\
\bottomrule
\end{tabu}
\end{table}

\begin{figure}

{\centering \includegraphics{DauR_files/figure-latex/benplots-1} 

}

\caption{Distribution of first digit frequencies in six datasets}\label{fig:benplots}
\end{figure}

\hypertarget{scale-invariance}{%
\subsection{Scale Invariance}\label{scale-invariance}}

Later in 1961, \textbf{Roger Pinkham} showed that law is invariant to scaling \citep{pinkham}. By \emph{Scale Invariance}, he actually showed that the law is invariant to measurements units. In other words, the law still holds if we convert units from one unit to another. For example, if price or amount figures are measured either in USD or in INR, length is measured either in KMs or Miles, the digit frequencies still follow the Benford's Law.

Let us check this on one of the six datasets mentioned above, namely \texttt{census.2009}. This data-set contains the figures of population of towns and cities of the United States, as of July of 2009. We can see that first digit frequencies follow Benford's Law/Pinkham's Corollary in Figure \ref{fig:census}. Left plot shows frequencies on original data whereas right plot shows these on randonly scaled data.

\begin{figure}

{\centering \includegraphics[width=0.45\linewidth]{DauR_files/figure-latex/census-1} \includegraphics[width=0.45\linewidth]{DauR_files/figure-latex/census-2} 

}

\caption{First Digit Analysis on US Census 2009 data (Left) and Scaled Data (Right)}\label{fig:census}
\end{figure}

Figure \ref{fig:census} (Left) shows that the law holds for the data. Let us also test the Pinkham's corollary on the aforesaid data. For this let's multiply all the figures of population by a random positive number. Through figure \ref{fig:census} (Right) it is clear that law still holds after scaling.

\hypertarget{first-two-digits}{%
\subsection{First two digits}\label{first-two-digits}}

Nigrini's contributions gained widespread recognition among scholars and practitioners, highlighting the applicability of Benford's Law as a valuable forensic accounting and auditing tool across various datasets, particularly in the financial domain. \textbf{Theodore P. Hill} further \citep{t_hill} extended the scope of the law, demonstrating its validity beyond just the first digit to encompass other digits as well. Hill's work expanded the utility of Benford's Law, affirming its effectiveness in detecting irregularities and patterns not only in leading digits but throughout numerical sequences.

The formula for second significant digit can be written down in equation \eqref{eq:ben2}.

\begin{equation} 
P(d_i) = \sum_{k = 1}^{9}\log_{10}\left(1 + \frac{1}{10k + d_i}\right)\;;\; d = 0,1,..9
\label{eq:ben2}
\end{equation}

\begin{itemize}
\tightlist
\item
  where \(k\) represents first digit,
\item
  \(d_i\) represents second digit.
\end{itemize}

The probabilities have been calculated, as depicted in Table \ref{tab:tab3}. Each cell depicts the probability of occurrence of any two digit, in left side, by first digit in rows and second digit in columns. We may also verify that, row totals thereby depicting probability of occurrence of first digit corresponds Benford's Law of First Digit. For example, the probability of having first two digits as 10 will be highest at 4.14\%.

\begin{table}
\centering
\caption{\label{tab:tab3}First and Second Digit distributions}
\centering
\begin{tabu} to \linewidth {>{\raggedleft}X>{\raggedleft}X>{\raggedleft}X>{\raggedleft}X>{\raggedleft}X>{\raggedleft}X>{\raggedleft}X>{\raggedleft}X>{\raggedleft}X>{\raggedleft}X>{\raggedleft}X>{\raggedleft}X}
\toprule
\multicolumn{1}{c}{ } & \multicolumn{10}{c}{Second Significant Digit} & \multicolumn{1}{c}{ } \\
\cmidrule(l{3pt}r{3pt}){2-11}
First Digit & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & First Digit Freq\\
\midrule
1 & 4.14\% & 3.78\% & 3.48\% & 3.22\% & 3.00\% & 2.80\% & 2.63\% & 2.48\% & 2.35\% & 2.23\% & 30.10\%\\
\cmidrule{1-12}
2 & 2.12\% & 2.02\% & 1.93\% & 1.85\% & 1.77\% & 1.70\% & 1.64\% & 1.58\% & 1.52\% & 1.47\% & 17.61\%\\
\cmidrule{1-12}
3 & 1.42\% & 1.38\% & 1.34\% & 1.30\% & 1.26\% & 1.22\% & 1.19\% & 1.16\% & 1.13\% & 1.10\% & 12.49\%\\
\cmidrule{1-12}
4 & 1.07\% & 1.05\% & 1.02\% & 1.00\% & 0.98\% & 0.95\% & 0.93\% & 0.91\% & 0.90\% & 0.88\% & 9.69\%\\
\cmidrule{1-12}
5 & 0.86\% & 0.84\% & 0.83\% & 0.81\% & 0.80\% & 0.78\% & 0.77\% & 0.76\% & 0.74\% & 0.73\% & 7.92\%\\
\cmidrule{1-12}
6 & 0.72\% & 0.71\% & 0.69\% & 0.68\% & 0.67\% & 0.66\% & 0.65\% & 0.64\% & 0.63\% & 0.62\% & 6.69\%\\
\cmidrule{1-12}
7 & 0.62\% & 0.61\% & 0.60\% & 0.59\% & 0.58\% & 0.58\% & 0.57\% & 0.56\% & 0.55\% & 0.55\% & 5.80\%\\
\cmidrule{1-12}
8 & 0.54\% & 0.53\% & 0.53\% & 0.52\% & 0.51\% & 0.51\% & 0.50\% & 0.50\% & 0.49\% & 0.49\% & 5.12\%\\
\cmidrule{1-12}
9 & 0.48\% & 0.47\% & 0.47\% & 0.46\% & 0.46\% & 0.45\% & 0.45\% & 0.45\% & 0.44\% & 0.44\% & 4.58\%\\
\cmidrule{1-12}
Second Digit Freq & 11.97\% & 11.39\% & 10.88\% & 10.43\% & 10.03\% & 9.67\% & 9.34\% & 9.04\% & 8.76\% & 8.50\% & 100.00\%\\
\bottomrule
\end{tabu}
\end{table}

The law of second digit combined with original Benford's Law of first digit thus, gives us Law of first two digits. We can verify it in the example on \texttt{census.2009} data. The resultant plot as depicted in figure \ref{fig:ben2} shows us that the law of first two digits also holds.

\begin{figure}

{\centering \includegraphics{DauR_files/figure-latex/ben2-1} 

}

\caption{Law holds for first two digits as well}\label{fig:ben2}
\end{figure}

\hypertarget{second-order-test}{%
\subsection{Second order test}\label{second-order-test}}

\textbf{Nigrini} and \textbf{Miller}, in 2009 \citep{article2009}, introduced another advanced test based on Benford's Law. The test states that:

\begin{quote}
Let \(x_1\), \ldots, \(x_N\) be a data set comprising \(N\) observations, and let \(y_1\), \ldots, \(y_N\) be the observations \(x_i\)'s in ascending order. Then, for many natural data sets, and for large \(N\), the digits of the differences between adjacent observations \(y_{i+1} – y_i\) is close to Benford's Law. Large deviations from Benford's Law indicate an anomaly that should be investigated.
\end{quote}

So, the steps may be listed as

\begin{itemize}
\tightlist
\item
  Sort data from smallest to largest
\item
  calculate \(N-1\) differences of \(N\) consecutive observations
\item
  Apply Benford's law on these calculated new data.
\end{itemize}

Nigrini showed that these digits are expected to closely follow the frequencies of Benford law. Using four different datasets he showed that this test can detect (i) anomalies occurring in data, (ii) whether the data has been rounded and (iii) use of fake data OR `statistically generated data' in place of actual (transactional) data.

\hypertarget{summation-test}{%
\subsection{Summation Test}\label{summation-test}}

The \textbf{summation test}, another second order test, looks for excessively large numbers in a dataset. It identifies numbers that are large compared to the norm for that data. The test was also proposed by \textbf{Nigrini} \citep{nigrinifraud} and it is based on the fact that the sums of all numbers in a Benford distribution with first-two digits (10, 11, 12, \ldots99) should be the same. Therefore, for each of the 90 first-two digits groups sum proportions should be equal, i.e.~1/90 or 0.011. The spikes, if any indicate that there are some large single numbers or set of numbers.

In the next section, we will see how to implement all these tests through R.

\hypertarget{limitations-of-benford-tests}{%
\subsection{Limitations of Benford Tests}\label{limitations-of-benford-tests}}

Benford's Law may not hold in the following circumstances-

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  When the data-set is comprised of assigned numbers. Like cheque numbers, invoices numbers, telephone numbers, pincodes, etc.
\item
  Numbers that may be influenced viz.~ATM withdrawals, etc.
\item
  Where amounts have either lower bound, or upper bounds or both. E.g. passengers onboard airplane, hourly wage rate, etc.
\item
  Count of transactions less than 500.
\end{enumerate}

Before carrying out analyics let us also see the evaluation metrics which will help us to evaluate the goodness of fit of data to Benford's law. Three statistics are commonly used.

\hypertarget{goodness-of-fit-metrics}{%
\section{Goodness of fit metrics}\label{goodness-of-fit-metrics}}

In table \ref{tab:tab1} we saw that digit frequencies largely followed Benford's Law in six different datasets. However, as to evaluate how close is the actual distribution with theoretical distribution, we need to evaluate the fit on some metrics. Here we will use three different metrics as follows.

\hypertarget{chis}{%
\subsection{Chi-square statistic}\label{chis}}

In first of these test, we will use Chi Square Statistic. This statistic is used to test the statistical significance to the whole distribution in observed frequency of first digit and first two digits against their expected frequency under Benford's Law (BL). The \textbf{Null hypothesis states that digits follow Benford's Law.} Mathematical formula is,

\begin{equation} 
\chi^2 = \sum_{i=1}^{9} \frac{(O_i - E_i)^2}{E_i}
\label{eq:ben3}
\end{equation}

where -

\begin{itemize}
\tightlist
\item
  \(O_i\) is the observed frequency of the i-th digit.
\item
  \(E_i\) is the expected frequency of the i-th digit predicted by Benford's Law.
\end{itemize}

This calculated chi-square statistic is compared to a critical value. The critical value for Chi-Square Test, comes from a chi-square distribution available easily in any Statistical textbook\footnote{\url{https://www.itl.nist.gov/div898/handbook/eda/section3/eda3674.htm}}. However, for first digit test and first two digits test, the critical values are reproduced in Table \ref{tab:tab6}.

\begin{table}

\caption{\label{tab:tab6}Critical values for Chi-Square Test}
\centering
\begin{tabular}[t]{lrr}
\toprule
\multicolumn{1}{c}{ } & \multicolumn{1}{c}{First Digit Test} & \multicolumn{1}{c}{Two Digit Test} \\
\cmidrule(l{3pt}r{3pt}){2-2} \cmidrule(l{3pt}r{3pt}){3-3}
Degrees of Freedom & 8 & 89\\
\midrule
10\% & 13.362 & 106.469\\
5\% & 15.507 & 112.022\\
2.5\% & 17.535 & 116.989\\
1\% & 20.090 & 122.942\\
0.1\% & 26.125 & 135.978\\
\bottomrule
\end{tabular}
\end{table}

To check goodness of fit, we have to compare calculated \(χ^2\) statistic with these critical values. If the observed value is above these critical values we may conclude that our initial hypothesis that data follows BL, should be rejected. Or simply that data does not conforms Benford law/Distribution.

For example, in \texttt{census.2009} data the chi-square statistic calculates to \texttt{17.524} which is less than 2.5\% critical value 17.535. Thus, we can say with 5\% confidence that \texttt{census.2009} data follows BL (first digit law).

\hypertarget{z-score}{%
\subsection{Z-score}\label{z-score}}

Z-statistic checks whether the individual distribution significantly differs from Benford's Law distribution. Mathematically, Z-Statistics considers the absolute magnitude of the difference from actual to the expected, size of the data and expected proportion.

\begin{equation} 
Z = \frac{(\lvert p - p_0\rvert) - (\frac{1}{2n})}{\sqrt{\frac{p_0(1-p_0)}{n}}}
\label{eq:ben4}
\end{equation} 

where -

\begin{itemize}
\tightlist
\item
  \(p\) is the observed frequency of the leading digits in the dataset.
\item
  \(p_0\) is the expected frequency under Benford's Law.
\item
  \(n\) is the number of records
\end{itemize}

In equation \eqref{eq:ben4}, the last term in the numerator \(\frac{1}{2N}\) is a continuity correction term and is used only when it is smaller than the first term in the numerator. Mark Nigrini has proposed that if the values of Z-statistic exceed the critical value 1.96, the null hypothesis \(H_{0A}\) is rejected at 5\% of significance level. Also note that \textbf{Null hypothesis is same, which states that digits follow Benford's Law.}

If the significant levels are 1\% or 10\%, the corresponding critical values are 2.57 and 1.64 respectively.

\hypertarget{mean-absolute-deviation}{%
\subsection{Mean absolute deviation}\label{mean-absolute-deviation}}

Another Statistic, Mean Absolute Deviation also sometimes referred to as \textbf{M.A.D.}, measures absolute deviations of observed frequencies from theoritical ones. The mathematical formula is written in equation \eqref{eq:ben5}.

\begin{equation} 
MAD = \frac{1}{9} \sum_{i=1}^{9} |O_i - E_i|
\label{eq:ben5}
\end{equation}

As there are no objective critical scores for the absolute deviations, the critical values prescribed by Mark J Nigrini are -

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2603}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2466}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2466}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2466}}@{}}
\caption{\label{tab:tab7} Critical Scores for MAD test}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
First Digits
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
First-Two Digits
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
First Digits
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
First-Two Digits
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0.000 to 0.006 & Close conformity & 0.000 to 0.012 & Close conformity \\
0.006 to 0.012 & Acceptable conformity & 0.012 to 0.018 & Acceptable conformity \\
0.012 to 0.015 & Marginally acceptable conformity & 0.018 to 0.022 & Marginally acceptable conformity \\
above 0.015 & Nonconformity & above 0.022 & Nonconformity \\
\end{longtable}

\hypertarget{other-descriptive-statistics}{%
\subsection{Other descriptive Statistics}\label{other-descriptive-statistics}}

If the data follows Benford's Law, the numbers should be close to those shown in table following, as suggested by Mark Nigrini.

\begin{longtable}[]{@{}cc@{}}
\caption{\label{tab:benford} Ideal Statistics for data that follows Benford's Law}\tabularnewline
\toprule\noalign{}
Statistic & Value \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
Statistic & Value \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Mean & 0.5 \\
Variance & 1/12 (0.08333\ldots) \\
Ex. Kurtosis & -1.2 \\
Skewness & 0 \\
\end{longtable}

\hypertarget{important}{%
\section{Important}\label{important}}

Benford's Law analysis serves as a powerful tool in uncovering potential irregularities in datasets, but it's crucial to note that deviations from this statistical phenomenon don't always signify fraudulent activities. While it highlights notable discrepancies between expected and observed frequencies of digits in naturally occurring datasets, these variations might stem from various legitimate factors such as data entry errors, fluctuations in processes, or different sources of data. Understanding that Benford's Law offers a signal rather than a definitive confirmation of fraud allows for a more nuanced interpretation, encouraging further investigation to discern the true nature behind these deviations.

Conversely, just because a dataset adheres to Benford's Law, it doesn't guarantee the absence of fraud. While conformity to this statistical principle generally suggests consistency within the data, sophisticated fraudsters might deliberately manipulate information to mimic expected distributions, masking their illicit activities. Therefore, while adherence to Benford's Law might lessen suspicion, it doesn't serve as an absolute assurance against fraudulent behavior.

Benford's Law acting as a warning signal indicates potential irregularities in the numbers. It's vital to dive deeper and investigate why these figures seem odd. Further scrutiny helps differentiate between a minor data hiccup and a potentially significant issue. This additional examination might mean cross-checking other data, validating records, or engaging with those connected to the information. This thorough approach is crucial for unraveling the story behind these uncommon figures.

\hypertarget{pracben}{%
\section{Practical approach in R}\label{pracben}}

As already stated we will use package \texttt{benford.analysis} for carrying out analytics on Benford's Law, in R. Let us load it.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(benford.analysis)}
\end{Highlighting}
\end{Shaded}

This package provides tools that make it easier to validate data using Benford's Law. This package has been developed by \textbf{Carlos Cinelli}. As the package author himself states that the main purpose of the package is to identify suspicious data that need further verification, it should always be kept in mind that these analytics only provide us red-flagged transactions that should be validated further.

Apart from useful functions in the package, this also loads some default datasets specially those which were used by Frank Benford while proposing his law. Let us load the census 2009 data containing the population of towns and cities of the United States, as of July of 2009.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(}\StringTok{"census.2009"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Let us view the top 6 rows of the data.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(census}\FloatTok{.2009}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     state             town pop.2009
## 1 Alabama   Abbeville city     2930
## 2 Alabama  Adamsville city     4782
## 3 Alabama     Addison town      709
## 4 Alabama       Akron town      433
## 5 Alabama   Alabaster city    29861
## 6 Alabama Albertville city    20115
\end{verbatim}

In fact, this contains 19509 records.

\textbf{Problem Statement:} Let us test Benford's law on 2009 population data. Let us see whether the data conforms Benford's law.

The main function \texttt{benford()} takes a vector of values to be tested as input, and creates an output of special class \texttt{benford} The syntax is

\begin{verbatim}
benford(data, number.of.digits=2)
\end{verbatim}

where-

\begin{itemize}
\tightlist
\item
  \texttt{data} is numeric vector on which analysis has to be performed.
\item
  \texttt{number.of.digits} is number of digits on which analysis has to be performed. Default value is \texttt{2}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{census\_first\_digit }\OtherTok{\textless{}{-}} \FunctionTok{benford}\NormalTok{(census}\FloatTok{.2009}\SpecialCharTok{$}\NormalTok{pop}\FloatTok{.2009}\NormalTok{, }\AttributeTok{number.of.digits =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Above syntax will create \texttt{census\_first\_digit} object which store various useful information for Benford Analytics. We may view its summary -

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(census\_first\_digit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                   Length Class      Mode     
## info               4     -none-     list     
## data               4     data.table list     
## s.o.data           2     data.table list     
## bfd               13     data.table list     
## mantissa           2     data.table list     
## MAD                1     -none-     numeric  
## MAD.conformity     1     -none-     character
## distortion.factor  1     -none-     numeric  
## stats              2     -none-     list
\end{verbatim}

Let us also print the object to see what all is stored therein.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(census\_first\_digit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Benford object:
##  
## Data: census.2009$pop.2009 
## Number of observations used = 19509 
## Number of obs. for second order = 7950 
## First digits analysed = 1
## 
## Mantissa: 
## 
##    Statistic  Value
##         Mean  0.503
##          Var  0.084
##  Ex.Kurtosis -1.207
##     Skewness -0.013
## 
## 
## The 5 largest deviations: 
## 
##   digits absolute.diff
## 1      1        134.79
## 2      2        104.64
## 3      3         95.43
## 4      6         63.94
## 5      8         45.07
## 
## Stats:
## 
##  Pearson's Chi-squared test
## 
## data:  census.2009$pop.2009
## X-squared = 17.524, df = 8, p-value = 0.0251
## 
## 
##  Mantissa Arc Test
## 
## data:  census.2009$pop.2009
## L2 = 4.198e-05, df = 2, p-value = 0.4409
## 
## Mean Absolute Deviation (MAD): 0.003119261
## MAD Conformity - Nigrini (2012): Close conformity
## Distortion Factor: 0.7404623
## 
## Remember: Real data will never conform perfectly to Benford's Law. You should not focus on p-values!
\end{verbatim}

Results of Chi-Square distribution test, MAD etc. are printed apart from top deviations. The MAD value of \texttt{0.003} shows \texttt{close\ conformity} with Benford's law. Chi Square statistic at 17.524 is slightly greater than 5\% critical value of 15.507. In second example we will see that results of \texttt{print} command on benford object can be further customised, using its other arguments.

Let us also visualise the plots. We will use \texttt{plot} command to generate the plots.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(census\_first\_digit)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{DauR_files/figure-latex/cenbenplot-1} 

}

\caption{Benford Analysis Results of Census 2009 Data}\label{fig:cenbenplot}
\end{figure}

We can see that by default five charts are printed.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Digits distribution
\item
  Second Order Test digit distribution
\item
  Summation test - digit distribution
\item
  Chi-Square differences
\item
  Summation differences
\end{enumerate}

\emph{Similarly, in second example we will see how to customise plot outputs.}

We can see that first digits in census 2009 data, follows Benford's Law closely.

\hypertarget{other-useful-functions-in-package}{%
\subsection{Other Useful functions in package}\label{other-useful-functions-in-package}}

You may be wondering whether we have to depend upon print function every time to get analytics insights out the object created. In fact there are several other functions in this package which are very useful while carrying out risk analysis through Benford's Law.

\begin{itemize}
\tightlist
\item
  \texttt{chisq}: Gets the Chi-squared test of a Benford object. Takes a benford object as input.
\item
  \texttt{duplicatesTable} Shows the duplicates of the data. Similarly, takes a benford object as input.
\item
  \texttt{extract.digits} Extracts the leading digits from the data. Takes data as input. This is useful, while carrying out analysis manually.
\item
  \texttt{getBfd} Gets the the statistics of the first Digits of a benford object. E.g.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{getBfd}\NormalTok{(census\_first\_digit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    digits  data.dist data.second.order.dist benford.dist
##     <int>      <num>                  <num>        <num>
## 1:      1 0.29412066             0.55811321   0.30103000
## 2:      2 0.18145471             0.15471698   0.17609126
## 3:      3 0.12004716             0.08968553   0.12493874
## 4:      4 0.09467425             0.05761006   0.09691001
## 5:      5 0.07991184             0.04364780   0.07918125
## 6:      6 0.07022400             0.03308176   0.06694679
## 7:      7 0.05976729             0.02553459   0.05799195
## 8:      8 0.05346250             0.01987421   0.05115252
## 9:      9 0.04633759             0.01773585   0.04575749
##    data.second.order.dist.freq data.dist.freq benford.dist.freq
##                          <num>          <num>             <num>
## 1:                        4437           5738         5872.7942
## 2:                        1230           3540         3435.3644
## 3:                         713           2342         2437.4298
## 4:                         458           1847         1890.6174
## 5:                         347           1559         1544.7469
## 6:                         263           1370         1306.0649
## 7:                         203           1166         1131.3649
## 8:                         158           1043          997.9346
## 9:                         141            904          892.6829
##    benford.so.dist.freq data.summation abs.excess.summation difference
##                   <num>          <num>                <num>      <num>
## 1:            2393.1885       51237849             29880783 -134.79419
## 2:            1399.9255       33272136             11915070  104.63563
## 3:             993.2630       22810354              1453288  -95.42981
## 4:             770.4346       15763499              5593567  -43.61744
## 5:             629.4909       15799838              5557228   14.25307
## 6:             532.2270       14527377              6829689   63.93508
## 7:             461.0360       11371006              9986060   34.63511
## 8:             406.6626       18814056              2543010   45.06544
## 9:             363.7720        8617475             12739591   11.31712
##    squared.diff absolute.diff
##           <num>         <num>
## 1:    3.0938378     134.79419
## 2:    3.1870315     104.63563
## 3:    3.7362508      95.42981
## 4:    1.0062752      43.61744
## 5:    0.1315102      14.25307
## 6:    3.1297790      63.93508
## 7:    1.0603039      34.63511
## 8:    2.0350972      45.06544
## 9:    0.1434744      11.31712
\end{verbatim}

\begin{itemize}
\tightlist
\item
  \texttt{getSuspects} Gets the `suspicious' observations according to Benford's Law. Takes both data as well as benford object, as inputs. Example in second case study.
\item
  \texttt{MAD} Gets the MAD of a Benford object.
\item
  \texttt{suspectsTable} Shows the first digits ordered by the mains discrepancies from Benford's Law. Notice the difference from \texttt{getSuspects}
\end{itemize}

\hypertarget{example-2-corporate-payments-data}{%
\subsection{Example-2: Corporate payments data}\label{example-2-corporate-payments-data}}

\textbf{Problem Statement-2:} Let us analyse red-flags, on dataset of the 2010's payments data (189470 records) of a division of a West Coast utility company. This data, \texttt{corporate.payments} is also available with the package. This time we will use first two digits in our analysis.

\textbf{Step-1:} Load the dataset and view its top rows. Let's also see its summary.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(}\StringTok{"corporate.payment"}\NormalTok{)}
\FunctionTok{head}\NormalTok{(corporate.payment)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   VendorNum       Date  InvNum Amount
## 1      2001 2010-01-02 0496J10  36.08
## 2      2001 2010-01-02 1726J10  77.80
## 3      2001 2010-01-02 2104J10  34.97
## 4      2001 2010-01-02 2445J10  59.00
## 5      2001 2010-01-02 3281J10  59.56
## 6      2001 2010-01-02 3822J10  50.38
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(corporate.payment)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   VendorNum              Date               InvNum              Amount        
##  Length:189470      Min.   :2010-01-02   Length:189470      Min.   :  -71388  
##  Class :character   1st Qu.:2010-02-28   Class :character   1st Qu.:      50  
##  Mode  :character   Median :2010-06-04   Mode  :character   Median :     200  
##                     Mean   :2010-06-16                      Mean   :    2588  
##                     3rd Qu.:2010-09-30                      3rd Qu.:     835  
##                     Max.   :2010-12-31                      Max.   :26763476
\end{verbatim}

We can see it has 189470 records having

\begin{verbatim}
+   Vendor Numbers
+   Date of Transaction
+   Invoice Number
+   Amount of invoice/transaction
\end{verbatim}

\textbf{Step-2:} Create benford object

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{corp\_bfd }\OtherTok{\textless{}{-}} \FunctionTok{benford}\NormalTok{(corporate.payment}\SpecialCharTok{$}\NormalTok{Amount, }\AttributeTok{number.of.digits =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\textbf{Step-3:} Let us first visually inspect the results. This time we will use another argument of \texttt{plot} function in \texttt{benford.analysis} library which is \texttt{except}. Actually this can create seven different plots and by default it creates five plots as stated earlier. Thus, by writing \texttt{except\ =\ "none"} we can include all seven plots if we want. Otherwise we will have to mention exclusions from \texttt{c("digits",\ "second\ order",\ "summation",\ "mantissa",\ "chi\ squared",\ "abs\ diff",\ "ex\ summation")}. There is one more argument namely \texttt{multiple} which is TRUE by default and plots multiple charts in same window.

So let us build (i) Digit distribution and (ii) Second order digit distribution plots.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(}
\NormalTok{  corp\_bfd,}
  \AttributeTok{except =} \FunctionTok{c}\NormalTok{(}
    \StringTok{"summation"}\NormalTok{,}
    \StringTok{"mantissa"}\NormalTok{,}
    \StringTok{"chi squared"}\NormalTok{,}
    \StringTok{"abs diff"}\NormalTok{,}
    \StringTok{"ex summation"}\NormalTok{,}
    \StringTok{"chisq diff"}\NormalTok{,}
    \StringTok{"legend"}
\NormalTok{  ),}
  \AttributeTok{multiple =} \ConstantTok{TRUE}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.47\linewidth,height=0.3\textheight]{DauR_files/figure-latex/corpben-1} \includegraphics[width=0.47\linewidth,height=0.3\textheight]{DauR_files/figure-latex/corpben-2} 

}

\caption{Benford Analysis results on Corporate payments Data}\label{fig:corpben}
\end{figure}

We can see that largely the data follows Benford's Law except an abnormal peak at 50.

\textbf{Step-4:} Let us now see what is inside of this object. Function \texttt{print} in \texttt{benford.analysis} package has another argument \texttt{how.many} which simply tells us to print how many of the absolute differences.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(corp\_bfd, }\AttributeTok{how.many =} \DecValTok{7}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Benford object:
##  
## Data: corporate.payment$Amount 
## Number of observations used = 185083 
## Number of obs. for second order = 65504 
## First digits analysed = 2
## 
## Mantissa: 
## 
##    Statistic  Value
##         Mean  0.496
##          Var  0.092
##  Ex.Kurtosis -1.257
##     Skewness -0.002
## 
## 
## The 7 largest deviations: 
## 
##   digits absolute.diff
## 1     50       5938.25
## 2     11       3331.98
## 3     10       2811.92
## 4     14       1043.68
## 5     98        889.95
## 6     90        736.81
## 7     92        709.01
## 
## Stats:
## 
##  Pearson's Chi-squared test
## 
## data:  corporate.payment$Amount
## X-squared = 32094, df = 89, p-value < 2.2e-16
## 
## 
##  Mantissa Arc Test
## 
## data:  corporate.payment$Amount
## L2 = 0.0039958, df = 2, p-value < 2.2e-16
## 
## Mean Absolute Deviation (MAD): 0.002336614
## MAD Conformity - Nigrini (2012): Nonconformity
## Distortion Factor: -1.065467
## 
## Remember: Real data will never conform perfectly to Benford's Law. You should not focus on p-values!
\end{verbatim}

We can see that digit 50 has indeed the largest abolute difference. One of the reasons for availability of invoices in this digit group may be due to some tax capping or some other reason, which an auditor may need to investigate further.

Using \texttt{suspectsTable()} we can also get similar information.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{suspectsTable}\NormalTok{(corp\_bfd) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{head}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    digits absolute.diff
##     <int>         <num>
## 1:     50     5938.2544
## 2:     11     3331.9798
## 3:     10     2811.9177
## 4:     14     1043.6833
## 5:     98      889.9470
## 6:     90      736.8084
## 7:     92      709.0129
\end{verbatim}

\textbf{Step-5:} Let us also get the Chi Square and other metrics

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{chisq}\NormalTok{(corp\_bfd)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Pearson's Chi-squared test
## 
## data:  corporate.payment$Amount
## X-squared = 32094, df = 89, p-value < 2.2e-16
\end{verbatim}

Going strictly by numbers and p-value, which we should not depend upon in Benford Analytics, we see that Null hypothesis (Ref: section \ref{chis}) has been rejected. In other words, chi-square statistic tells us that data does not follow Benford Law.

To get Mean Absolute Deviation

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{MAD}\NormalTok{(corp\_bfd)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.002336614
\end{verbatim}

Whether the value conforms to values suggested by Mark Nigrini, we can do

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{corp\_bfd}\SpecialCharTok{$}\NormalTok{MAD.conformity}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Nonconformity"
\end{verbatim}

\textbf{Step-6:} Let us generate duplicate values avilable if any, in the data. For sake of brevity here, we will print top-5 results.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{duplicatesTable}\NormalTok{(corp\_bfd) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{head}\NormalTok{(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     number duplicates
##      <num>      <int>
## 1:   50.00       6022
## 2: 1153.35       2264
## 3: 1083.45       1185
## 4:  150.00       1056
## 5:  988.35       1018
\end{verbatim}

Examining output above, we can see that there are 6022 invoices having all amount of USD50 each. Probably this could be the reason for failing of null hypothesis in the data.

\textbf{Step-7:} We can extract all distribution data using \texttt{getBFD} function.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{getBfd}\NormalTok{(corp\_bfd) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{head}\NormalTok{(}\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     digits  data.dist data.second.order.dist benford.dist
##      <int>      <num>                  <num>        <num>
##  1:     10 0.05658542            0.374786273   0.04139269
##  2:     11 0.05579119            0.015922692   0.03778856
##  3:     12 0.03236926            0.014609795   0.03476211
##  4:     13 0.03116440            0.013266365   0.03218468
##  5:     14 0.02432422            0.011113825   0.02996322
##  6:     15 0.03038637            0.011510747   0.02802872
##  7:     16 0.02385416            0.010365779   0.02632894
##  8:     17 0.02179563            0.009129213   0.02482358
##  9:     18 0.02085011            0.009358207   0.02348110
## 10:     19 0.02043408            0.008106375   0.02227639
##     data.second.order.dist.freq data.dist.freq benford.dist.freq
##                           <num>          <num>             <num>
##  1:                       24550          10473          7661.082
##  2:                        1043          10326          6994.020
##  3:                         957           5991          6433.875
##  4:                         869           5768          5956.838
##  5:                         728           4502          5545.683
##  6:                         754           5624          5187.640
##  7:                         679           4415          4873.039
##  8:                         598           4034          4594.423
##  9:                         613           3859          4345.952
## 10:                         531           3782          4122.982
##     benford.so.dist.freq data.summation abs.excess.summation difference
##                    <num>          <num>                <num>      <num>
##  1:             2711.386       28701407             23224143  2811.9177
##  2:             2475.302       22324748             16847484  3331.9798
##  3:             2277.057       16258127             10780863  -442.8749
##  4:             2108.225       15520165             10042901  -188.8378
##  5:             1962.711       27393259             21915996 -1043.6833
##  6:             1835.994       49191988             43714724   436.3597
##  7:             1724.651       12523174              7045911  -458.0390
##  8:             1626.044       11994778              6517515  -560.4233
##  9:             1538.106        7545939              2068675  -486.9517
## 10:             1459.193        6987397              1510133  -340.9820
##     squared.diff absolute.diff
##            <num>         <num>
##  1:  1032.084049     2811.9177
##  2:  1587.368773     3331.9798
##  3:    30.485235      442.8749
##  4:     5.986347      188.8378
##  5:   196.418497     1043.6833
##  6:    36.704517      436.3597
##  7:    43.053153      458.0390
##  8:    68.359901      560.4233
##  9:    54.561565      486.9517
## 10:    28.200147      340.9820
\end{verbatim}

\textbf{Step-8:} To get suspected/high risk records, we may make use of \texttt{getSuspects} function. As already stated it requires both benford object and data as inputs.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# We are printing 10 records only}
\FunctionTok{getSuspects}\NormalTok{(corp\_bfd, corporate.payment) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{head}\NormalTok{(}\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     VendorNum       Date      InvNum  Amount
##        <char>     <Date>      <char>   <num>
##  1:      2001 2010-01-02     3822J10   50.38
##  2:      2001 2010-01-07    100107-2 1166.29
##  3:      2001 2010-01-08 11210084007 1171.45
##  4:      2001 2010-01-08     1585J10   50.42
##  5:      2001 2010-01-08     4733J10  113.34
##  6:      2001 2010-01-08     6263J10  117.22
##  7:      2001 2010-01-08     6673J10   50.80
##  8:      2001 2010-01-08     9181J10  114.78
##  9:      2001 2010-01-09     1510J10   50.49
## 10:      2001 2010-01-09     1532J10   50.45
\end{verbatim}

Moreover, by using \texttt{slice\_max} function from \texttt{dplyr} we can also get \texttt{n} high-valued `suspects'.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{getSuspects}\NormalTok{(corp\_bfd, corporate.payment) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice\_max}\NormalTok{(}\AttributeTok{order\_by =}\NormalTok{ Amount, }\AttributeTok{n =} \DecValTok{10}\NormalTok{, }\AttributeTok{with\_ties =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     VendorNum       Date                InvNum    Amount
##        <char>     <Date>                <char>     <num>
##  1:      2817 2010-10-27                10-10A 1156428.2
##  2:     17141 2010-04-05                040510 1135003.6
##  3:      2817 2010-11-30            1033500002 1112304.3
##  4:     16721 2010-09-16 SEE ATTACHED BALSHEET 1100000.0
##  5:      6118 2010-12-17             103511001  509093.7
##  6:      2817 2010-05-28                 40821  506971.5
##  7:     17284 2010-03-24                032400  504580.6
##  8:      6118 2010-08-26             102381001  504334.6
##  9:     17284 2010-03-10                 31000  502132.2
## 10:      2088 2010-03-24            1008300003  500000.0
\end{verbatim}

\hypertarget{conclusion}{%
\subsubsection*{Conclusion}\label{conclusion}}
\addcontentsline{toc}{subsubsection}{Conclusion}

Though by statistics (goodness of fit metrics), the data did not conform to BL, yet we observed that there were abnormally high records starting with digits \texttt{50}. The reasons can be further investigated. By charts we also observed that, otherwise the data conform to BL. We also extracted suspected records for further investigation on other parameters/tests/verification. To sum up, we can say that, Benford Analysis can be a good starting point for fraud/forensic analytics while auditing. Before closing, let us also delve in one other example.

\hypertarget{example-3-lakes-perimeter}{%
\subsection{Example-3: Lakes Perimeter}\label{example-3-lakes-perimeter}}

Let us apply this on \texttt{lakes.perimeter}\footnote{A dataset of the perimeter of the lakes arround the water from the global lakes and wetlands database (GLWD) \url{http://www.worldwildlife.org/pages/global-lakes-and-wetlands-database}} data which is available with the package.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# load sample data}
\FunctionTok{data}\NormalTok{(lakes.perimeter) }
\CommentTok{\# Number of rows}
\FunctionTok{nrow}\NormalTok{(lakes.perimeter)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 248607
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# View top rows}
\FunctionTok{head}\NormalTok{(lakes.perimeter)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   perimeter.km
## 1          1.0
## 2          1.0
## 3          1.1
## 4          1.1
## 5          1.1
## 6          1.1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Generate Benford Object}
\NormalTok{lake\_ben }\OtherTok{\textless{}{-}} \FunctionTok{benford}\NormalTok{(lakes.perimeter}\SpecialCharTok{$}\NormalTok{perimeter.km, }\AttributeTok{number.of.digits =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Let us see the plots, metrics and top outliers

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(lake\_ben)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{DauR_files/figure-latex/unnamed-chunk-500-1} 

}

\caption{Benford Analysis - lake Perimeter Data}\label{fig:unnamed-chunk-500}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Chisq test}
\FunctionTok{chisq}\NormalTok{(lake\_ben)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Pearson's Chi-squared test
## 
## data:  lakes.perimeter$perimeter.km
## X-squared = 88111, df = 89, p-value < 2.2e-16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# MAD}
\FunctionTok{MAD}\NormalTok{(lake\_ben)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.006012766
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Whether it conforms?}
\NormalTok{lake\_ben}\SpecialCharTok{$}\NormalTok{MAD.conformity}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Nonconformity"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Get top{-}10 suspects}
\FunctionTok{getSuspects}\NormalTok{(lake\_ben, lakes.perimeter) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{head}\NormalTok{(}\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     perimeter.km
##            <num>
##  1:          1.5
##  2:          1.5
##  3:          1.5
##  4:          1.5
##  5:          1.5
##  6:          1.5
##  7:          1.5
##  8:          1.5
##  9:          1.5
## 10:          1.5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Get top{-}10 suspects on Squared Differences}
\FunctionTok{getSuspects}\NormalTok{(lake\_ben, lakes.perimeter, }
            \AttributeTok{by =} \StringTok{"squared.diff"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{head}\NormalTok{(}\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     perimeter.km
##            <num>
##  1:          3.6
##  2:          3.6
##  3:          3.6
##  4:          3.6
##  5:          3.6
##  6:          3.6
##  7:          3.6
##  8:          3.6
##  9:          3.6
## 10:          3.6
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Get top{-}10 suspects on Absolute Excess Summation}
\FunctionTok{getSuspects}\NormalTok{(lake\_ben, lakes.perimeter, }
            \AttributeTok{by =} \StringTok{"abs.excess.summation"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{head}\NormalTok{(}\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     perimeter.km
##            <num>
##  1:          1.0
##  2:          1.0
##  3:          1.3
##  4:          1.3
##  5:          1.3
##  6:          1.3
##  7:          1.3
##  8:          1.3
##  9:          1.3
## 10:          1.3
\end{verbatim}

\hypertarget{conclusion-1}{%
\subsubsection*{Conclusion}\label{conclusion-1}}
\addcontentsline{toc}{subsubsection}{Conclusion}

We observed that data does not conform Benford's law which is evident from plot as well as MAD value. Chi-Squared Value of \texttt{88111} also exceeds critical value very significantly. Nigrini and Miller gave some plausible explanations in their Research paper \citep{article2007} for this non-conformity. One of the possible reasons, they propose, was that \emph{perimeter is not a correct measurement for the size of a lake.}

\hypertarget{conclusion-2}{%
\section{Conclusion}\label{conclusion-2}}

As we conclude this chapter on Benford Analytics, it's clear that this statistical phenomenon holds remarkable potential across diverse fields. The inherent simplicity of Benford's Law belies its complexity and applicability. Its ability to unveil anomalies, authenticate data integrity, and aid in forensic investigations underscores its significance in modern data analysis. As we delve deeper into its intricacies and practical applications, we unravel a tool that not only scrutinizes numbers but also illuminates new avenues for precision, authenticity, and trust in our data-driven world.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Further Reading-

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  ISACA JOURNAL ARCHIVES - \href{https://www.isaca.org/resources/isaca-journal/past-issues/2011/understanding-and-applying-benfords-law}{Understanding and Applying Benford's Law - 1 May 2011}
\item
  Newcomb, Simon. ``Note on the Frequency of Use of the Different Digits in Natural Numbers.'' American Journal of Mathematics, vol.~4, no. 1, 1881, pp.~39--40. JSTOR, \url{https://doi.org/10.2307/2369148}. Accessed 15 Jun.~2022.
\item
  Durtschi, Cindy \& Hillison, William \& Pacini, Carl. (2004). The Effective Use of Benford's Law to Assist in Detecting Fraud in Accounting Data. J. Forensic Account.
\end{enumerate}

\hypertarget{anomaly-detection}{%
\chapter{Anomaly Detection}\label{anomaly-detection}}

Anomalies play a significant role in the field of audit. As auditors start examining data whether it be financial statements, or transactions, or other relevant data, the detection of anomalies can provide valuable insights and help identify potential issues or irregularities. Anomalies often indicate the presence of fraudulent activities. Unusual or unexpected patterns in financial transactions or account balances may suggest potential fraud or misappropriation of assets. Auditors actively search for anomalies that may indicate fraudulent behavior, such as fictitious transactions, unauthorized access, or unusual changes in financial data.

Anomalies in the audit context serve as indicators of potential issues, including fraud, material misstatements, control weaknesses, compliance violations, and process inefficiencies. Detecting and investigating anomalies is crucial for auditors to provide accurate and reliable financial information, enhance internal controls, and support informed decision-making by stakeholders.

\hypertarget{definition-and-types-of-anomalies}{%
\section{Definition and types of anomalies}\label{definition-and-types-of-anomalies}}

Anomalies are patterns or data points that deviate significantly from the expected or normal behavior within a dataset. These are also known as outliers, novelties, or deviations and can provide valuable insights into unusual events or behavior in various domains.

Types of anomalies:

\begin{itemize}
\item
  \textbf{Point Anomalies:} Individual data points that are considered anomalous when compared to the rest of the dataset. For example, a temperature sensor reading that is significantly different from the expected range.
\item
  \textbf{Contextual Anomalies:} Data points that are considered anomalous within a specific context or subset of the dataset. For instance, a sudden increase in obsolete website traffic.
\item
  \textbf{Collective Anomalies:} Groups of data points that exhibit anomalous behavior when analyzed together but might appear normal when considered individually. An example is a sudden drop in sales for multiple related products.
\end{itemize}

\textbf{Anomaly detection, in R}

\hypertarget{anomaly-detection---by-inspection}{%
\section{Anomaly detection - by inspection}\label{anomaly-detection---by-inspection}}

Several times, anomalies or outliers are detectable by observation. The \texttt{summary()} function prints the \emph{maximum}, \emph{minimum}, \emph{upper} and \emph{lower quartiles}, and the \emph{mean} and \emph{median}, and can give a sense for how far an extreme point lies from the rest of the data. E.g. Let's visualise the mean annual temperatures in degrees Fahrenheit in New Haven, Connecticut, from 1912 to 1971, through a boxplot (Refer Figure \ref{fig:an1}. This data is available in base R, as \texttt{nhtemp}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(nhtemp)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   47.90   50.58   51.20   51.16   51.90   54.60
\end{verbatim}

The easiest way to get a sense for how unusual a particular value is is by using a graphical summary like a boxplot. In R, this is created using the \texttt{boxplot} function. The \texttt{boxplot} function takes a column of values as an input argument, here illustrated with the temperature data, and produces a box and whiskers representation of the distribution of the values. The extreme values are represented as distinct points, making them easier to spot. We can also make use of ggplot2. Examples from both base R and ggplot2 are shown in Figure \ref{fig:an1}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{boxplot}\NormalTok{(nhtemp, }\AttributeTok{main =} \StringTok{"Box plot of nhtemp data"}\NormalTok{)}
\FunctionTok{ggplot}\NormalTok{(iris, }\FunctionTok{aes}\NormalTok{(Species, Petal.Length, }\AttributeTok{color =}\NormalTok{ Species )) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{(}\AttributeTok{outlier.colour =} \StringTok{"red"}\NormalTok{, }\AttributeTok{outlier.size =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Petal Lengths in Iris Data"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.45\linewidth]{DauR_files/figure-latex/an1-1} \includegraphics[width=0.45\linewidth]{DauR_files/figure-latex/an1-2} 

}

\caption{Outliers through Boxplot}\label{fig:an1}
\end{figure}

It's important to note that a \textbf{point anomaly} is not necessarily always extreme. A point anomaly can also arise as an unusual combination of values across several attributes.

A \textbf{collective anomaly}, on the other hand, is a collection of similar data instances that can be considered anomalous together when compared to the rest of the data. For example, a consecutive 5 day period of high temperatures are shown by the red points in the plot. These daily temperatures are unusual because they occur together, and are likely caused by the same underlying weather event. Refer Figure \ref{fig:an2}.

\begin{figure}

{\centering \includegraphics[height=0.27\textheight]{DauR_files/figure-latex/an2-1} 

}

\caption{Collective Anomalies}\label{fig:an2}
\end{figure}

\hypertarget{grubbs-test}{%
\section{Grubb's Test}\label{grubbs-test}}

We saw that visual assessment of outliers works well when the majority of data points are grouped together and rest of them lie separate. In statistics there are several tests to detect anomalies. \textbf{Grubb's Test} is one of these. Grubb's test assesses whether the point that lies farthest from the mean in a dataset could be an outlier. This point will either be the largest or smallest value in the data. This test is however based on assumption that data points are normally distributed. So before proceeding for this test, we must be sure that there is a plausible explanation for this assumption. \emph{(We can check normality of data points by plotting a histogram. For other methods please refer to chapter on linear regression.)}

Let's run this test on \texttt{nhtemp} data example, which we have already seen in \ref{fig:an1} (Left). So let's check its normality assumption.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(nhtemp, }\AttributeTok{breaks =} \DecValTok{20}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[height=0.27\textheight]{DauR_files/figure-latex/an3-1} 

}

\caption{Histogram for Grubb's test}\label{fig:an3}
\end{figure}

In figure \ref{fig:an3} we can see that our assumption is nearly satisfied. Let's run the test.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(outliers)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Attaching package: 'outliers'
\end{verbatim}

\begin{verbatim}
## The following object is masked from 'package:psych':
## 
##     outlier
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{outliers}\SpecialCharTok{::}\FunctionTok{grubbs.test}\NormalTok{(nhtemp)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Grubbs test for one outlier
## 
## data:  nhtemp
## G = 2.71806, U = 0.87266, p-value = 0.1539
## alternative hypothesis: highest value 54.6 is an outlier
\end{verbatim}

As p value is \texttt{0.15} we do not have strong evidence to reject null hypothesis that extreme maximum value is an outlier.

\hypertarget{seasonal-hybrid-esd-algorithm}{%
\section{Seasonal Hybrid ESD Algorithm}\label{seasonal-hybrid-esd-algorithm}}

As we have seen that above test may not be appropriate for anomaly detection (normality assumption as well as detection of extreme values only), particularly detecting anomalies from a time series that may have seasonal variations. We may install \texttt{AnomalyDetection} package development version from github using \texttt{devtools::install\_github("twitter/AnomalyDetection")}. Example: in \texttt{JohnsonJohnson} data having quarterly sales data, we can use the following syntax.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#devtools::install\_github("twitter/AnomalyDetection")}
\FunctionTok{library}\NormalTok{(AnomalyDetection)}
\FunctionTok{AnomalyDetectionVec}\NormalTok{(}\FunctionTok{as.vector}\NormalTok{(JohnsonJohnson), }
                    \AttributeTok{period =} \DecValTok{4}\NormalTok{, }\CommentTok{\# Sesaonality}
                    \AttributeTok{plot =} \ConstantTok{TRUE}\NormalTok{, }\CommentTok{\# set FALSE when plot not needed}
                    \AttributeTok{direction =} \StringTok{\textquotesingle{}both\textquotesingle{}} \CommentTok{\# set \textquotesingle{}pos\textquotesingle{} for higher values}
                                       \CommentTok{\# or \textquotesingle{}neg\textquotesingle{} for lower values}
\NormalTok{                    )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $anoms
##   index anoms
## 1    74 12.06
## 2    75 12.15
## 3    77 14.04
## 4    78 12.96
## 5    79 14.85
## 6    81 16.20
## 7    82 14.67
## 8    83 16.02
## 
## $plot
\end{verbatim}

\begin{figure}

{\centering \includegraphics[height=0.28\textheight]{DauR_files/figure-latex/an4-1} 

}

\caption{Seasonal Hybrid ESD Algorithm}\label{fig:an4}
\end{figure}

In Figure \ref{fig:an4}, we can see that in output \texttt{\$anoms} containing anomalies are denoted in blue dots.

\hypertarget{k-nearest-neighbour-distance-score}{%
\section{k-Nearest Neighbour Distance Score}\label{k-nearest-neighbour-distance-score}}

One of greatest limitation of above two methods was that, these were applicable on univariate data series, whereas, in real world, data analysis will rarely be univariate. The \texttt{knn} technique works on multivariate data, but for visulaisation purposes, we will first visulaise the results on bivariate data only.

\textbf{K-Nearest Neighbour} or \textbf{KNN} is a distance-based classifier, meaning that it implicitly assumes that the smaller the distance between two points, the more similar they are. For bivariate data, we can understand the algorithm by plotting the data-points in a two dimensional scatter plot. Now as the distance between any two points are usually calculated using \emph{Euclidean Distance Metric}, we should ensure that the data is normalised/scaled before proceeding for distance calculation.

\textbf{Problem Statement-1:} Let us try to identify outliers in \texttt{virginica} Species' flower measurements. So let's prepare the data. We will make use of \texttt{scale} function available in base R, to normalise the data. Remember that \texttt{scale} returns a matrix, so we may need to convert it into data.frame while using \texttt{ggplot2}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Sample data}
\NormalTok{df }\OtherTok{\textless{}{-}}\NormalTok{ iris[}\DecValTok{101}\SpecialCharTok{:}\DecValTok{150}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{]}
\CommentTok{\#  Scale the data}
\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{scale}\NormalTok{(df)}
\end{Highlighting}
\end{Shaded}

Let us visualise the data (in first two dimensions only). However, as our data is scaled already, we can try to visualise all the dimensions using a boxplot. See both figures in \ref{fig:an5}. The points/ouliers have been numbered (on the basis of row numbers) for interpretaion purposes.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{as.data.frame}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{row =} \FunctionTok{row\_number}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(Sepal.Length, Sepal.Width)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{color =} \StringTok{"seagreen"}\NormalTok{) }\SpecialCharTok{+}
\NormalTok{  ggrepel}\SpecialCharTok{::}\FunctionTok{geom\_text\_repel}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{label =}\NormalTok{ row), }\AttributeTok{arrow =}\NormalTok{ grid}\SpecialCharTok{::}\FunctionTok{arrow}\NormalTok{()) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{()}

\CommentTok{\# Helper Function}
\NormalTok{find\_outlier }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x) \{}
  \FunctionTok{return}\NormalTok{(x }\SpecialCharTok{\textless{}} \FunctionTok{quantile}\NormalTok{(x, .}\DecValTok{25}\NormalTok{) }\SpecialCharTok{{-}} \FloatTok{1.5}\SpecialCharTok{*}\FunctionTok{IQR}\NormalTok{(x) }\SpecialCharTok{|}\NormalTok{ x }\SpecialCharTok{\textgreater{}} \FunctionTok{quantile}\NormalTok{(x, .}\DecValTok{75}\NormalTok{) }\SpecialCharTok{+} \FloatTok{1.5}\SpecialCharTok{*}\FunctionTok{IQR}\NormalTok{(x))}
\NormalTok{\}}

\NormalTok{df }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{as.data.frame}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{row =} \FunctionTok{row\_number}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_longer}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{row, }\AttributeTok{names\_to =} \StringTok{"Dimension"}\NormalTok{, }\AttributeTok{values\_to =} \StringTok{"Values"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(Dimension) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{outlier =} \FunctionTok{ifelse}\NormalTok{(}\FunctionTok{find\_outlier}\NormalTok{(Values), row, }\ConstantTok{NA}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(Dimension, Values)) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{(}\AttributeTok{outlier.colour =} \StringTok{"red"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_text}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{label=}\NormalTok{outlier), }\AttributeTok{na.rm=}\ConstantTok{TRUE}\NormalTok{, }\AttributeTok{hjust=}\SpecialCharTok{{-}}\NormalTok{.}\DecValTok{5}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.45\linewidth]{DauR_files/figure-latex/an5-1} \includegraphics[width=0.45\linewidth]{DauR_files/figure-latex/an5-2} 

}

\caption{Sepal Length Vs. Widths in Virginica}\label{fig:an5}
\end{figure}

Now, let's proceed to identify outliers in R. We will make use of \texttt{get.knn} function from \texttt{FNN} package. However, \texttt{k} parameter is required beforehand.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load the library}
\FunctionTok{library}\NormalTok{(FNN)}
\CommentTok{\# get kNN object, using k = 5}
\NormalTok{viginica\_knn }\OtherTok{\textless{}{-}}\NormalTok{ FNN}\SpecialCharTok{::}\FunctionTok{get.knn}\NormalTok{(df[, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{], }\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The \texttt{knn} object created above will have two sub-objects (both matrices having columns equal to chosen \texttt{k}), one having nearest neighbors' indices and another having distances from those. Let's view their top 6 rows.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(viginica\_knn}\SpecialCharTok{$}\NormalTok{nn.index)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2] [,3] [,4] [,5]
## [1,]   37   16   49   11   45
## [2,]    2   15   22   35   14
## [3,]   30   40   42    8   13
## [4,]   34   27   29   33   28
## [5,]    5   17   46   38   41
## [6,]   36    8   30   23   31
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(viginica\_knn}\SpecialCharTok{$}\NormalTok{nn.dist)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           [,1]      [,2]      [,3]      [,4]      [,5]
## [1,] 0.3100808 0.3476803 0.3476803 0.4416741 0.6290499
## [2,] 0.0000000 0.3100808 0.4416741 0.5645648 0.6397904
## [3,] 0.1572625 0.4416741 0.4416741 0.4416741 0.4717874
## [4,] 0.3100808 0.3476803 0.3476803 0.3476803 0.4416741
## [5,] 0.0000000 0.0000000 0.3145250 0.3476803 0.4416741
## [6,] 0.1572625 0.5645648 0.6290499 0.6397904 0.6953605
\end{verbatim}

Using \texttt{rowMeans} we can calculate mean distance for each data point. The bigger this score is, the chances of that record being an outlier are relatively higher. Let's also store that mean distance in a variable/column say \texttt{score} in main dataset, and visualise the results by setting the point-size with this mean distance (actually its square root). In Figure \ref{fig:an8}, we may notice that points lying far away are bigger becuase their chances of being outliers is high.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{score }\OtherTok{\textless{}{-}} \FunctionTok{rowMeans}\NormalTok{(viginica\_knn}\SpecialCharTok{$}\NormalTok{nn.dist)}
\NormalTok{df }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{as.data.frame}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{row =} \FunctionTok{row\_number}\NormalTok{(),}
         \AttributeTok{score =}\NormalTok{ score) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(Sepal.Length, Sepal.Width)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{size =}\NormalTok{ score),}\AttributeTok{color =} \StringTok{"seagreen"}\NormalTok{) }\SpecialCharTok{+}
\NormalTok{  ggrepel}\SpecialCharTok{::}\FunctionTok{geom\_text\_repel}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{label =}\NormalTok{ row), }\AttributeTok{arrow =}\NormalTok{ grid}\SpecialCharTok{::}\FunctionTok{arrow}\NormalTok{()) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"KNN method of outlier"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[height=0.28\textheight]{DauR_files/figure-latex/an8-1} 

}

\caption{k-Nearest Neighbour Distance}\label{fig:an8}
\end{figure}

Now, we can run the algorithm to find the outlier on the basis of all variables in the dataset.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{virginica\_knn }\OtherTok{\textless{}{-}}\NormalTok{ FNN}\SpecialCharTok{::}\FunctionTok{get.knn}\NormalTok{(df, }\DecValTok{5}\NormalTok{)}
\NormalTok{score }\OtherTok{\textless{}{-}} \FunctionTok{rowMeans}\NormalTok{(virginica\_knn}\SpecialCharTok{$}\NormalTok{nn.dist)}
\CommentTok{\# Which point is farthest{-}Outlier}
\FunctionTok{which.max}\NormalTok{(score)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 32
\end{verbatim}

\hypertarget{local-outlier-factor}{%
\section{Local Outlier Factor}\label{local-outlier-factor}}

As against kNN which uses distances of k neighbors, this algorithm uses density of each data point vis-a-vis density of its nearest neighbors. kNN distance seems to be good at detecting points that are really far from their neighbors, sometimes called global anomalies, but sometimes fail to capture all of the points that might be considered anomalous like local anomalies. Local Anomalies may lie near to a cluster still they won't be like their neighbors. To understand it better, see the plot in Figure \ref{fig:an9} consisting of dummy data.

\begin{figure}

{\centering \includegraphics[height=0.28\textheight]{DauR_files/figure-latex/an9-1} 

}

\caption{Local Outlier factor}\label{fig:an9}
\end{figure}

The red points may be global outliers, being far from its immediate neigbors, yet the blue points may be local anomalies as these are not like their immediate neighbors and may be local anomalies.

As stated, LOF segregates the data points based on the ratio of density of that point with that of densities of its neighbors. A score \texttt{\textgreater{}\ 1} thus indicates that the data point may be an anomaly. Let's see that on same problem statement.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(dbscan)}
\NormalTok{lof\_score }\OtherTok{\textless{}{-}} \FunctionTok{lof}\NormalTok{(df, }\DecValTok{5}\NormalTok{)}
\NormalTok{df }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{as.data.frame}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{row =} \FunctionTok{row\_number}\NormalTok{(),}
         \AttributeTok{score =}\NormalTok{ lof\_score) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(Sepal.Length, Sepal.Width)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{size =}\NormalTok{ score),}\AttributeTok{color =} \StringTok{"seagreen"}\NormalTok{) }\SpecialCharTok{+}
\NormalTok{  ggrepel}\SpecialCharTok{::}\FunctionTok{geom\_text\_repel}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{label =}\NormalTok{ row), }\AttributeTok{arrow =}\NormalTok{ grid}\SpecialCharTok{::}\FunctionTok{arrow}\NormalTok{()) }\SpecialCharTok{+}
  \FunctionTok{scale\_size\_continuous}\NormalTok{(}\AttributeTok{guide =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}  
  \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"LOF method of outlier"}\NormalTok{)}

\NormalTok{df\_prcomp }\OtherTok{\textless{}{-}} \FunctionTok{prcomp}\NormalTok{(df, }\AttributeTok{scale. =} \ConstantTok{TRUE}\NormalTok{)}

\NormalTok{df }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{as.data.frame}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{row =} \FunctionTok{row\_number}\NormalTok{(),}
         \AttributeTok{score =}\NormalTok{ lof\_score,}
         \AttributeTok{PC1 =}\NormalTok{ df\_prcomp}\SpecialCharTok{$}\NormalTok{x[,}\DecValTok{1}\NormalTok{],}
         \AttributeTok{PC2 =}\NormalTok{ df\_prcomp}\SpecialCharTok{$}\NormalTok{x[,}\DecValTok{2}\NormalTok{]) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(PC1, PC2)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{size =}\NormalTok{ score),}\AttributeTok{color =} \StringTok{"seagreen"}\NormalTok{) }\SpecialCharTok{+}
\NormalTok{  ggrepel}\SpecialCharTok{::}\FunctionTok{geom\_text\_repel}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{label =}\NormalTok{ row), }\AttributeTok{arrow =}\NormalTok{ grid}\SpecialCharTok{::}\FunctionTok{arrow}\NormalTok{()) }\SpecialCharTok{+}
  \FunctionTok{scale\_size\_continuous}\NormalTok{(}\AttributeTok{guide =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}  
  \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"LOF method of outlier}\SpecialCharTok{\textbackslash{}n}\StringTok{Visualised on principal components"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.48\linewidth]{DauR_files/figure-latex/an10-1} \includegraphics[width=0.48\linewidth]{DauR_files/figure-latex/an10-2} 

}

\caption{LOF}\label{fig:an10}
\end{figure}

Clearly, in Figure \ref{fig:an10} (left), where we have plotted the data in 2 dimensions only despite that we have attempted to find the LOF on the basis of all four dimensions. We can see presence of local anomalies, within the clustered data points. E.g. points nos. 31, 23, etc. were earlier given more weight instead of point nos.15, 10, etc. which are now given more weight. However, if we want to visaulise it on first two principal components, we can do that (Refer Figure \ref{fig:an10} (Right)). We can also verify this.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Biggest Anomaly {-} kNN}
\FunctionTok{which.max}\NormalTok{(score)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 32
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Biggest Anomaly {-} LOF}
\FunctionTok{which.max}\NormalTok{(lof\_score)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 7
\end{verbatim}

Histograms of both \texttt{knn} and \texttt{LOF} scores can also be drawn, as in Figure \ref{fig:an12}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(score, }\AttributeTok{breaks =} \DecValTok{40}\NormalTok{, }
     \AttributeTok{main =} \StringTok{"Histogram of KNN scores in IRIS{-}Virginica data"}\NormalTok{)}
\FunctionTok{hist}\NormalTok{(lof\_score, }\AttributeTok{breaks =} \DecValTok{40}\NormalTok{, }
     \AttributeTok{main =} \StringTok{"Histogram of LOF scores in IRIS{-}Virginica data"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.48\linewidth]{DauR_files/figure-latex/an12-1} \includegraphics[width=0.48\linewidth]{DauR_files/figure-latex/an12-2} 

}

\caption{Histogram - KNN(Left) and LOF (Right)}\label{fig:an12}
\end{figure}

\hypertarget{isolation-treesforest}{%
\section{Isolation Trees/Forest}\label{isolation-treesforest}}

Isolation Forest is an unsupervised tree based model, which actually works on path length instead of distance or density measures. Tree based models use decision tree(s) to determine the value/class of an observation given its value. In other words, it determines or try to identify a path from the root node to a leaf node based on the value of the observation in question. Forest (or Random Forest) are actually collection of smaller trees and thus using ensemble learning methods to make decision, instead of a single complex tree.

Let us work on the same problem statement above. Firstly, we will build a single decision tree. WE will use a development version package \texttt{isofor} which can be downloaded using \texttt{remotes::install\_github("Zelazny7/isofor")}. To build forest/tree we will use function \texttt{iForest}. Its argument \texttt{nt} will determine the number of trees to be built in ensemble. Another important argument is \texttt{phi} which determines the number of samples to draw without replacement to construct each tree. So let's use \texttt{nt\ =\ 1} and \texttt{phi\ =\ 100}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# remotes::install\_github("Zelazny7/isofor")}
\FunctionTok{library}\NormalTok{(isofor)}
\CommentTok{\# Generate a single tree}
\CommentTok{\# Specify number of samples explicitly}
\NormalTok{viginica\_1 }\OtherTok{\textless{}{-}} \FunctionTok{iForest}\NormalTok{(df, }\AttributeTok{nt =} \DecValTok{1}\NormalTok{, }\AttributeTok{phi =} \DecValTok{100}\NormalTok{)}

\CommentTok{\# Generate isolation score}
\NormalTok{iso\_score\_1 }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(viginica\_1, df)}

\CommentTok{\# View fisrt 10 scores}
\NormalTok{iso\_score\_1[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 0.3372822 0.3437950 0.3694749 0.4425250 0.3694749 0.4517359 0.8198250
##  [8] 0.6085593 0.4989121 0.6085593
\end{verbatim}

\emph{Score interpretations}: The closer the score is to 1, the more likely the point is an anomaly. However, if their scores are below 0.5, they are probably just normal points within the trend.

Let's just visualise the scores on Principal Component plot again.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{as.data.frame}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{row =} \FunctionTok{row\_number}\NormalTok{(),}
         \AttributeTok{score =}\NormalTok{ iso\_score\_1,}
         \AttributeTok{PC1 =}\NormalTok{ df\_prcomp}\SpecialCharTok{$}\NormalTok{x[,}\DecValTok{1}\NormalTok{],}
         \AttributeTok{PC2 =}\NormalTok{ df\_prcomp}\SpecialCharTok{$}\NormalTok{x[,}\DecValTok{2}\NormalTok{]) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(PC1, PC2)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{size =}\NormalTok{ score),}\AttributeTok{color =} \StringTok{"seagreen"}\NormalTok{) }\SpecialCharTok{+}
\NormalTok{  ggrepel}\SpecialCharTok{::}\FunctionTok{geom\_text\_repel}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{label =}\NormalTok{ row), }\AttributeTok{arrow =}\NormalTok{ grid}\SpecialCharTok{::}\FunctionTok{arrow}\NormalTok{()) }\SpecialCharTok{+}
  \FunctionTok{scale\_size\_continuous}\NormalTok{(}\AttributeTok{guide =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}  
  \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Isolation Tree based outlier}\SpecialCharTok{\textbackslash{}n}\StringTok{Visualised on principal components"}\NormalTok{)}

\NormalTok{df }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{as.data.frame}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{row =} \FunctionTok{row\_number}\NormalTok{(),}
         \AttributeTok{score =}\NormalTok{ iso\_score\_1,}
         \AttributeTok{PC1 =}\NormalTok{ df\_prcomp}\SpecialCharTok{$}\NormalTok{x[,}\DecValTok{1}\NormalTok{],}
         \AttributeTok{PC2 =}\NormalTok{ df\_prcomp}\SpecialCharTok{$}\NormalTok{x[,}\DecValTok{2}\NormalTok{],}
         \AttributeTok{is\_outlier =} \FunctionTok{factor}\NormalTok{(}\FunctionTok{ifelse}\NormalTok{(}\FunctionTok{ntile}\NormalTok{(iso\_score\_1, }\DecValTok{10}\NormalTok{) }\SpecialCharTok{\textgreater{}=} \DecValTok{10}\NormalTok{, }\StringTok{"O"}\NormalTok{, }\StringTok{"N"}\NormalTok{))) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(PC1, PC2)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{size =}\NormalTok{ score, }\AttributeTok{color =}\NormalTok{ is\_outlier)) }\SpecialCharTok{+}
\NormalTok{  ggrepel}\SpecialCharTok{::}\FunctionTok{geom\_text\_repel}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{label =}\NormalTok{ row), }\AttributeTok{arrow =}\NormalTok{ grid}\SpecialCharTok{::}\FunctionTok{arrow}\NormalTok{()) }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_manual}\NormalTok{(}\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\AttributeTok{O =} \StringTok{"red"}\NormalTok{, }\AttributeTok{N =} \StringTok{"black"}\NormalTok{), }\AttributeTok{guide =} \StringTok{"none"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_size\_continuous}\NormalTok{(}\AttributeTok{guide =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}  
  \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Isolation Tree based outlier}\SpecialCharTok{\textbackslash{}n}\StringTok{Top{-}10\%"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.48\linewidth]{DauR_files/figure-latex/an24-1} \includegraphics[width=0.48\linewidth]{DauR_files/figure-latex/an24-2} 

}

\caption{Isolation Tree Method}\label{fig:an24}
\end{figure}

Let's also try a forest (ensemble) approach. Refer Figure \ref{fig:an14}, we can see that scores are modified using ensemble methods. However, when we gradually increase number of trees, these scores will stabilise.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iso\_100 }\OtherTok{\textless{}{-}} \FunctionTok{iForest}\NormalTok{(df, }\AttributeTok{nt =} \DecValTok{100}\NormalTok{, }\AttributeTok{phi =} \DecValTok{100}\NormalTok{)}

\CommentTok{\# Generate scores}

\NormalTok{iso\_score\_100 }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(iso\_100, df)}

\CommentTok{\# View Results}
\NormalTok{df }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{as.data.frame}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{row =} \FunctionTok{row\_number}\NormalTok{(),}
         \AttributeTok{score =}\NormalTok{ iso\_score\_100,}
         \AttributeTok{PC1 =}\NormalTok{ df\_prcomp}\SpecialCharTok{$}\NormalTok{x[,}\DecValTok{1}\NormalTok{],}
         \AttributeTok{PC2 =}\NormalTok{ df\_prcomp}\SpecialCharTok{$}\NormalTok{x[,}\DecValTok{2}\NormalTok{],}
         \AttributeTok{is\_outlier =} \FunctionTok{factor}\NormalTok{(}\FunctionTok{ifelse}\NormalTok{(}\FunctionTok{ntile}\NormalTok{(iso\_score\_100, }\DecValTok{10}\NormalTok{) }\SpecialCharTok{\textgreater{}=} \DecValTok{10}\NormalTok{, }\StringTok{"O"}\NormalTok{, }\StringTok{"N"}\NormalTok{))) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(PC1, PC2)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{size =}\NormalTok{ score, }\AttributeTok{color =}\NormalTok{ is\_outlier)) }\SpecialCharTok{+}
\NormalTok{  ggrepel}\SpecialCharTok{::}\FunctionTok{geom\_text\_repel}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{label =}\NormalTok{ row), }\AttributeTok{arrow =}\NormalTok{ grid}\SpecialCharTok{::}\FunctionTok{arrow}\NormalTok{()) }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_manual}\NormalTok{(}\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\AttributeTok{O =} \StringTok{"red"}\NormalTok{, }\AttributeTok{N =} \StringTok{"black"}\NormalTok{), }\AttributeTok{guide =} \StringTok{"none"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_size\_continuous}\NormalTok{(}\AttributeTok{guide =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}  
  \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Isolation Tree based outlier}\SpecialCharTok{\textbackslash{}n}\StringTok{Number of Trees = 100"}\NormalTok{)}

\FunctionTok{plot}\NormalTok{(iso\_score\_1, iso\_score\_100, }\AttributeTok{xlim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{), }
     \AttributeTok{main =} \StringTok{"Comparision of Tree Vs. Forest Method"}\NormalTok{)}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{a =} \DecValTok{0}\NormalTok{, }\AttributeTok{b =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.48\linewidth]{DauR_files/figure-latex/an14-1} \includegraphics[width=0.48\linewidth]{DauR_files/figure-latex/an14-2} 

}

\caption{100 Isolation Trees (Left) Comparison of 1 vs. 100 trees (Right)}\label{fig:an14}
\end{figure}

\textbf{Contour plots:} We can also visualise the results of scores of anomaly detection, using contour plots. See the plot in Figure \ref{fig:an16}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create PRCOMP data}
\NormalTok{df\_grid }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{PC1 =}\NormalTok{ df\_prcomp}\SpecialCharTok{$}\NormalTok{x[,}\DecValTok{1}\NormalTok{],}
  \AttributeTok{PC2 =}\NormalTok{ df\_prcomp}\SpecialCharTok{$}\NormalTok{x[,}\DecValTok{2}\NormalTok{]}
\NormalTok{)}

\CommentTok{\# Create Sequences}
\NormalTok{pc1\_seq }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\FunctionTok{min}\NormalTok{(df\_prcomp}\SpecialCharTok{$}\NormalTok{x[,}\DecValTok{1}\NormalTok{]), }\FunctionTok{max}\NormalTok{(df\_prcomp}\SpecialCharTok{$}\NormalTok{x[,}\DecValTok{1}\NormalTok{]), }\AttributeTok{length.out =} \DecValTok{25}\NormalTok{)}
\NormalTok{pc2\_seq }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\FunctionTok{min}\NormalTok{(df\_prcomp}\SpecialCharTok{$}\NormalTok{x[,}\DecValTok{2}\NormalTok{]), }\FunctionTok{max}\NormalTok{(df\_prcomp}\SpecialCharTok{$}\NormalTok{x[,}\DecValTok{2}\NormalTok{]), }\AttributeTok{length.out =} \DecValTok{25}\NormalTok{)}
\CommentTok{\# Create Grid}
\NormalTok{my\_grid }\OtherTok{\textless{}{-}} \FunctionTok{expand.grid}\NormalTok{(}\AttributeTok{PC1 =}\NormalTok{ pc1\_seq, }\AttributeTok{PC2 =}\NormalTok{ pc2\_seq)}

\CommentTok{\# Create model for Pr comp}
\NormalTok{iso\_model }\OtherTok{\textless{}{-}} \FunctionTok{iForest}\NormalTok{(df\_grid, }\AttributeTok{nt =} \DecValTok{100}\NormalTok{, }\AttributeTok{phi =} \DecValTok{100}\NormalTok{)}
\CommentTok{\# append scores}
\NormalTok{my\_grid}\SpecialCharTok{$}\NormalTok{scores }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(iso\_model, my\_grid)}

\CommentTok{\# Draw Plot}
\FunctionTok{library}\NormalTok{(lattice)}
\FunctionTok{contourplot}\NormalTok{(scores }\SpecialCharTok{\textasciitilde{}}\NormalTok{ PC1 }\SpecialCharTok{+}\NormalTok{ PC2, }\AttributeTok{data =}\NormalTok{ my\_grid, }\AttributeTok{region =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{DauR_files/figure-latex/an16-1} 

}

\caption{Contour Plot}\label{fig:an16}
\end{figure}

\textbf{Including categorical variables} One benefit of using forest tree method of anomaly detection, is that we can include categorical values also. Only condition is that these should be of \texttt{factor} type.

\textbf{Problem Statement-2:} Let's now try to find out anomalies on full \texttt{iris} data. We can check column types before proceeding.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sapply}\NormalTok{(iris, class)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Sepal.Length  Sepal.Width Petal.Length  Petal.Width      Species 
##    "numeric"    "numeric"    "numeric"    "numeric"     "factor"
\end{verbatim}

Our condition is met. So we can proceed directly to build a decision tree/Forest. For sake of simplicity, let's build a simple tree (one). Refer Figure \ref{fig:an18}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# New Iforest Model}
\NormalTok{iso\_model\_new }\OtherTok{\textless{}{-}} \FunctionTok{iForest}\NormalTok{(iris, }\AttributeTok{nt =} \DecValTok{1}\NormalTok{, }\AttributeTok{phi =} \DecValTok{100}\NormalTok{)}
\NormalTok{new\_scores }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(iso\_model\_new, iris)}
\FunctionTok{head}\NormalTok{(new\_scores)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.3687105 0.3687105 0.3687105 0.3687105 0.3687105 0.6607827
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Full PRCOMP for Visual}
\NormalTok{iris\_pr }\OtherTok{\textless{}{-}} \FunctionTok{prcomp}\NormalTok{(iris[, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{])}

\FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{PC1 =}\NormalTok{ iris\_pr}\SpecialCharTok{$}\NormalTok{x[,}\DecValTok{1}\NormalTok{],}
  \AttributeTok{PC2 =}\NormalTok{ iris\_pr}\SpecialCharTok{$}\NormalTok{x[,}\DecValTok{2}\NormalTok{]}
\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{score =}\NormalTok{ new\_scores,}
         \AttributeTok{Species =}\NormalTok{ iris}\SpecialCharTok{$}\NormalTok{Species) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(PC1, PC2, }\AttributeTok{color =}\NormalTok{ Species)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{size =}\NormalTok{ score)) }\SpecialCharTok{+}
  \FunctionTok{guides}\NormalTok{(}\AttributeTok{size =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Decision Tree}\SpecialCharTok{\textbackslash{}n}\StringTok{Visualised on Principal Components"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.98\linewidth]{DauR_files/figure-latex/an18-1} 

}

\caption{Including categorical variable}\label{fig:an18}
\end{figure}

\hypertarget{including-categorical-variables-in-lof}{%
\section{Including categorical variables in LOF}\label{including-categorical-variables-in-lof}}

We can also include categorical variable in \texttt{Local\ outlier\ factor} using \texttt{gower\ distance} calculation method. Gower method lets us calculate distance between categorical observations, most importantly when the categories are not ordered. To calculate it we can use function \texttt{daisy} from library \texttt{cluster} in R. Let's see LOF score calculation on the above example. The plot generated can be seen in Figure \ref{fig:an20}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(cluster)}
\NormalTok{iris\_dist }\OtherTok{\textless{}{-}} \FunctionTok{daisy}\NormalTok{(iris, }\AttributeTok{metric =} \StringTok{"gower"}\NormalTok{)}
\NormalTok{iris\_lof }\OtherTok{\textless{}{-}} \FunctionTok{lof}\NormalTok{(iris\_dist, }\AttributeTok{minPts =} \DecValTok{5}\NormalTok{)}

\FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{PC1 =}\NormalTok{ iris\_pr}\SpecialCharTok{$}\NormalTok{x[,}\DecValTok{1}\NormalTok{],}
  \AttributeTok{PC2 =}\NormalTok{ iris\_pr}\SpecialCharTok{$}\NormalTok{x[,}\DecValTok{2}\NormalTok{]}
\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{score =}\NormalTok{ iris\_lof,}
         \AttributeTok{Species =}\NormalTok{ iris}\SpecialCharTok{$}\NormalTok{Species) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(PC1, PC2, }\AttributeTok{color =}\NormalTok{ Species)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{size =}\NormalTok{ score)) }\SpecialCharTok{+}
  \FunctionTok{guides}\NormalTok{(}\AttributeTok{size =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"LOF Scores}\SpecialCharTok{\textbackslash{}n}\StringTok{Visualised on Principal Components"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{DauR_files/figure-latex/an20-1} 

}

\caption{Including categorical variable}\label{fig:an20}
\end{figure}

\hypertarget{time-series-anomalies}{%
\section{Time Series Anomalies}\label{time-series-anomalies}}

In time series data, an anomaly/outlier can be termed as a data point which is not following the common collective trend or seasonal or cyclic pattern of the entire data and is significantly distinct from rest of the data.

To calculate/detect anomalies, in R, we make use of package \texttt{timetk}. The package works on \texttt{tibble} instead of \texttt{time\ series} data, so we may to prep our data/time series accordingly.

\textbf{Problem Statement} Let's find out anomalies, if any on Sunspots data available in base R\footnote{Monthly numbers of sunspots, as from the World Data Center, aka SIDC. This is the version of the data that will occasionally be updated when new counts become available.}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{start\_date }\OtherTok{\textless{}{-}} \FunctionTok{as.Date}\NormalTok{(}\StringTok{"1749{-}01{-}01"}\NormalTok{)}
\NormalTok{end\_date }\OtherTok{\textless{}{-}} \FunctionTok{as.Date}\NormalTok{(}\StringTok{"2013{-}09{-}01"}\NormalTok{)}
\NormalTok{date\_sequence }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(start\_date, end\_date, }\AttributeTok{by =} \StringTok{"months"}\NormalTok{)}

\NormalTok{sunspot\_df }\OtherTok{\textless{}{-}}\NormalTok{ sunspot.month }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{as.data.frame}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{set\_names}\NormalTok{(}\StringTok{"value"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{date =}\NormalTok{ date\_sequence)}

\FunctionTok{library}\NormalTok{(timetk)}
\NormalTok{sunspot\_df }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{plot\_anomaly\_diagnostics}\NormalTok{(date, value,}
                           \AttributeTok{.interactive =} \ConstantTok{FALSE}\NormalTok{) }

\NormalTok{sunspot\_df }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{tk\_anomaly\_diagnostics}\NormalTok{(date, value) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(anomaly }\SpecialCharTok{==} \StringTok{\textquotesingle{}Yes\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 37 x 11
##    date       observed season trend remainder seasadj remainder_l1 remainder_l2
##    <date>        <dbl>  <dbl> <dbl>     <dbl>   <dbl>        <dbl>        <dbl>
##  1 1749-11-01     159. -1.07   76.3      83.4    160.        -64.8         66.7
##  2 1769-09-01     149. -0.524  78.8      70.5    149.        -64.8         66.7
##  3 1769-10-01     158.  1.17   79.3      77.7    157.        -64.8         66.7
##  4 1769-11-01     148. -1.07   79.8      69.4    149.        -64.8         66.7
##  5 1771-05-01     153.  0.963  77.5      74.3    152.        -64.8         66.7
##  6 1777-11-01     146  -1.07   75.7      71.4    147.        -64.8         66.7
##  7 1777-12-01     157. -0.495  77.8      80.0    158.        -64.8         66.7
##  8 1778-01-01     177. -2.18   79.9      99.6    179.        -64.8         66.7
##  9 1778-05-01     239.  0.963  87.3     151.     238.        -64.8         66.7
## 10 1778-06-01     172.  0.368  89.2      82.1    171.        -64.8         66.7
## # i 27 more rows
## # i 3 more variables: anomaly <chr>, recomposed_l1 <dbl>, recomposed_l2 <dbl>
\end{verbatim}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{DauR_files/figure-latex/an17-1} 

}

\caption{Time Series Anomalies}\label{fig:an17}
\end{figure}

We can anomalies highlighted in red, in Figure \ref{fig:an17} and anomalies filtered out in code above. We may also implement Seasonal Hybrid ESD algorithm already discussed above.

\hypertarget{finding-anamolous-outliers}{%
\chapter{Finding Anamolous Outliers}\label{finding-anamolous-outliers}}

\emph{The content is under development}

\hypertarget{duplicate-detection}{%
\chapter{Duplicate Detection}\label{duplicate-detection}}

\emph{The content is under development}

\hypertarget{detecting-gaps-in-sequences}{%
\chapter{Detecting gaps in sequences}\label{detecting-gaps-in-sequences}}

\emph{The content is under development and finalisation}
Audit analytics often requires us to check for gaps in sequences of numbers. Gaps in Sequentially numbered objects such as purchase orders, invoice numbers, cheque numbers, etc should be accounted for. Thus, auditors may require to exercise this audit check as a part of audit analytics.

\hypertarget{when-sequence-numbers-are-available-as-numeric-column}{%
\section{\texorpdfstring{When sequence numbers are available as \texttt{numeric} column}{When sequence numbers are available as numeric column}}\label{when-sequence-numbers-are-available-as-numeric-column}}

We will have starting and ending numbers in such sequence. Let us allocate these in two variables.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{start\_num }\OtherTok{\textless{}{-}} \DecValTok{500301} \CommentTok{\# say}
\NormalTok{end\_num }\OtherTok{\textless{}{-}} \DecValTok{503500} \CommentTok{\# say}
\end{Highlighting}
\end{Shaded}

It means we have 3200 terms (say cheques) issued in the series. Further suppose, the cheque numbers issued are stored in some column say \texttt{cheque\_no} in a given data frame, have a total count say 3177. To simulate

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{cheque\_no }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(start\_num}\SpecialCharTok{:}\NormalTok{end\_num, }\DecValTok{3177}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

To find out the gaps we may simply use function \texttt{setdiff} on these two.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{setdiff}\NormalTok{(start\_num}\SpecialCharTok{:}\NormalTok{end\_num, cheque\_no)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 500398 500445 500457 500716 500747 500862 501017 501018 501109 501333
## [11] 501459 501609 501823 501908 502160 502191 502609 502937 502974 503002
## [21] 503284 503385 503422
\end{verbatim}

\hypertarget{when-sequence-numbers-are-available-as-character-column}{%
\section{\texorpdfstring{When sequence numbers are available as \texttt{character()} column}{When sequence numbers are available as character() column}}\label{when-sequence-numbers-are-available-as-character-column}}

We may easily replicate above procedure for gap detection, even if the sequence column is of character type. E.g. If the cheque numbers have a prefix say, `A', then the cheque numbers may look like-

\begin{verbatim}
##  [1] "A502763" "A502811" "A502527" "A500826" "A500495" "A503286" "A502142"
##  [8] "A501442" "A501553" "A501568"
\end{verbatim}

In these case, we may first substitute prefix with nothing and then proceed as above.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{modified\_cheques }\OtherTok{\textless{}{-}} \FunctionTok{sub}\NormalTok{(}\StringTok{"A"}\NormalTok{, }\StringTok{""}\NormalTok{, cheque\_no) }\SpecialCharTok{|\textgreater{}} \FunctionTok{as.integer}\NormalTok{()}
\NormalTok{missing\_cheques }\OtherTok{\textless{}{-}} \FunctionTok{setdiff}\NormalTok{(start\_num}\SpecialCharTok{:}\NormalTok{end\_num, modified\_cheques)}
\NormalTok{missing\_cheques}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 500398 500445 500457 500716 500747 500862 501017 501018 501109 501333
## [11] 501459 501609 501823 501908 502160 502191 502609 502937 502974 503002
## [21] 503284 503385 503422
\end{verbatim}

\hypertarget{data-envelopment-analysis}{%
\chapter{Data Envelopment Analysis}\label{data-envelopment-analysis}}

Data Envelopment Analysis (DEA) is a method for measuring the efficiency of a set of decision-making units (DMUs) that was developed in the late 1970s by Abraham Charnes, William Cooper, and Edwardo Rhodes. It was originally developed to evaluate the performance of U.S. banks, but has since been applied to a wide range of industries and sectors.

It provides a way to measure the relative efficiency of different DMUs, even when they produce different combinations of outputs and use different combinations of inputs. This can be useful for identifying best practices and areas for improvement, and for making decisions about which DMUs to invest in or prioritize.

\hypertarget{how-it-works}{%
\section{How it works?}\label{how-it-works}}

At a high level, DEA works by comparing the input-output combinations of different decision-making units (DMUs) to determine which DMUs are operating efficiently and which are not. Specifically, DEA tries to identify the ``efficient frontier'' of DMUs that are using their inputs to produce the optimum amount of outputs possible.

To do this, DEA uses a mathematical model that takes into account the inputs and outputs of each DMU, as well as any weights or constraints that may apply. The model then calculates a ``score'' for each DMU based on how well it performs relative to the other DMUs in the group. DMUs that score higher are considered more efficient than those that score lower.

\hypertarget{use-cases}{%
\section{Use-cases}\label{use-cases}}

DEA has a wide range of applications across different industries and sectors. Here are a few examples:

\begin{itemize}
\item
  \textbf{Firm Performance Evaluation}: DEA can be used to evaluate the performance of firms in a given industry, by comparing their input-output combinations to identify which firms are operating more efficiently than others. This can help investors and policymakers make decisions about which firms to invest in or support.
\item
  \textbf{Education Quality Assessment}: DEA can be used to assess the quality of schools, by comparing their input-output combinations to identify which schools are producing better outcomes (e.g., higher test scores) with fewer resources. This can help policymakers allocate education funding more effectively.
\item
  \textbf{Healthcare Efficiency Analysis}: DEA can be used to analyze the efficiency of hospitals and healthcare providers, by comparing their input-output combinations to identify which providers are delivering better outcomes (e.g., lower mortality rates) with lower costs. This can help healthcare systems optimize resource allocation and improve patient outcomes.
\item
  \textbf{Regional Development Analysis}: DEA can be used to compare the efficiency of different regions or countries, by comparing their input-output combinations to identify which regions/countries are making the best use of their resources. This can help policymakers identify areas for improvement and develop targeted interventions to promote economic growth.
\end{itemize}

\hypertarget{possible-uses-in-audit}{%
\section{Possible uses in Audit}\label{possible-uses-in-audit}}

Data Envelopment Analysis (DEA) can be used in audit to evaluate the efficiency of an organization or process. Auditors can use DEA to identify areas of inefficiency within an organization by comparing the inputs and outputs of different departments or processes. For example, an auditor could use DEA to compare the efficiency of different production lines within a manufacturing plant or the efficiency of different branches within a bank.

By using DEA, auditors can identify areas where an organization is not using its resources effectively and recommend changes to improve efficiency. This can help organizations reduce costs, improve productivity, and increase profits. DEA can also be used to evaluate the efficiency of different audit methods and procedures. For example, an auditor could use DEA to compare the efficiency of different sampling methods to determine which method is most effective at detecting errors or fraud. Undoubtedly, DEA is a powerful tool that can help auditors identify areas of inefficiency and recommend changes to improve organizational performance.

\hypertarget{a-practical-example-in-r}{%
\section{A practical example in R}\label{a-practical-example-in-r}}

We will now see a complete practical example of doing DEA in R.

\hypertarget{pre-requisites-1}{%
\subsection{Pre-requisites}\label{pre-requisites-1}}

We will use library \texttt{deaR} to measuring DEA efficiency.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(deaR)}
\end{Highlighting}
\end{Shaded}

\hypertarget{sample-data-set}{%
\subsection{Sample data-set}\label{sample-data-set}}

Let us assume, we have a data/KPIs of six stores located across India.

\begin{tabular}{l|r|r|r|r}
\hline
Store & x1 & x2 & y1 & y2\\
\hline
Delhi & 51 & 38 & 169 & 119\\
\hline
Gurgaon & 60 & 45 & 243 & 167\\
\hline
Bengalore & 43 & 33 & 173 & 158\\
\hline
Chennai & 53 & 43 & 216 & 138\\
\hline
Mumbai & 43 & 38 & 155 & 161\\
\hline
Nagpur & 44 & 35 & 169 & 157\\
\hline
\end{tabular}

In the above sample, we have only two inputs, \texttt{x1} and \texttt{x2} and two outputs, \texttt{y1} and \texttt{y2}. Here -

\begin{itemize}
\tightlist
\item
  \texttt{x1} and \texttt{x2} represents machine hours and employee + management hours per week;
\item
  \texttt{y1} and \texttt{y2} represents quantity of products \texttt{A} and \texttt{B} produced per week, at the locations given.
\end{itemize}

\hypertarget{performing-dea-in-r}{%
\subsection{Performing DEA in R}\label{performing-dea-in-r}}

Now our objective is to first find out the effective DMUs, i.e.~which DMUs are using their inputs effectively. Here we will also assume constant returns to scale i.e.~output will be doubled if inputs are doubled.

DEA in \texttt{deaR} is a two step process.

\begin{itemize}
\tightlist
\item
  \textbf{Step-1}: We will make data in the form as required for the package; using \texttt{make\_deadata()} function. \texttt{inputs}, \texttt{outputs} and \texttt{dmus} require column references in the data provided.
\item
  \textbf{Step-2}: Performing actual analysis, using \texttt{model\_basic} which takes previously created data (step-1) and two other essential arguments,

  \begin{itemize}
  \tightlist
  \item
    \texttt{orientation}: which takes a string value; \texttt{"io"} (input oriented), \texttt{"oo"} (output oriented), or \texttt{"dir"} (directional)
  \item
    \texttt{rts}: again takes a string; specifying type of returns to scale, equal to \texttt{"crs"} (constant), \texttt{"vrs"} (variable), \texttt{"nirs"} (non-increasing), \texttt{"ndrs"} (non-decreasing) or \texttt{"grs"} (generalized)
  \item
    Other arguments may be passed on as per need. See \texttt{?model\_basic} for more help.
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{store\_dea }\OtherTok{\textless{}{-}} \FunctionTok{make\_deadata}\NormalTok{(store, }\AttributeTok{inputs =} \DecValTok{2}\SpecialCharTok{:}\DecValTok{3}\NormalTok{, }\AttributeTok{outputs =} \DecValTok{4}\SpecialCharTok{:}\DecValTok{5}\NormalTok{, }\AttributeTok{dmus =} \DecValTok{1}\NormalTok{)}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{model\_basic}\NormalTok{(store\_dea, }\AttributeTok{orientation =} \StringTok{\textquotesingle{}io\textquotesingle{}}\NormalTok{, }\AttributeTok{rts =} \StringTok{\textquotesingle{}crs\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We have stored the DEA model in \texttt{model} variable just created. Now we can use the following functions to get \texttt{efficiencies}, \texttt{targets} and \texttt{lambdas} respectively.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{efficiencies}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     Delhi   Gurgaon Bengalore   Chennai    Mumbai    Nagpur 
## 0.8254374 1.0000000 1.0000000 1.0000000 1.0000000 0.9685549
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{targets}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $target_input
##                 x1       x2
## Delhi     41.74913 31.36662
## Gurgaon   60.00000 45.00000
## Bengalore 43.00000 33.00000
## Chennai   53.00000 43.00000
## Mumbai    43.00000 38.00000
## Nagpur    42.61641 33.38805
## 
## $target_output
##            y1  y2
## Delhi     169 119
## Gurgaon   243 167
## Bengalore 173 158
## Chennai   216 138
## Mumbai    155 161
## Nagpur    169 157
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{lambdas}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           Delhi Gurgaon Bengalore Chennai  Mumbai Nagpur
## Delhi         0 0.64348   0.07303       0 0.00000      0
## Gurgaon       0 1.00000   0.00000       0 0.00000      0
## Bengalore     0 0.00000   1.00000       0 0.00000      0
## Chennai       0 0.00000   0.00000       1 0.00000      0
## Mumbai        0 0.00000   0.00000       0 1.00000      0
## Nagpur        0 0.00000   0.85459       0 0.13649      0
\end{verbatim}

\hypertarget{interpreting-above-results}{%
\subsection{Interpreting above results}\label{interpreting-above-results}}

\begin{itemize}
\tightlist
\item
  As evident from the named function \texttt{efficiencies} returned the efficiencies of all DMUs. We can see that all DMUs except \texttt{Delhi} and \texttt{Nagpur} are working efficiently.
\item
  \texttt{targets} give us the optimum values of inputs and/or outputs for the given values. We can see that we have to reduce \texttt{x1} and \texttt{x2} to given values at \texttt{Delhi} and \texttt{Nagpur}.
\item
  \texttt{lambdas} give the lambdas of the DEA solution.
\item
  Further, the \texttt{plot()} function will output the following plots which are easy to inerpret.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Press [enter] to continue
\end{verbatim}

\begin{verbatim}
## Press [enter] to continue
\end{verbatim}

\begin{figure}

{\centering \includegraphics[width=0.3\linewidth]{DauR_files/figure-latex/deaplots-1} \includegraphics[width=0.3\linewidth]{DauR_files/figure-latex/deaplots-2} \includegraphics[width=0.3\linewidth]{DauR_files/figure-latex/deaplots-3} 

}

\caption{Data Envelopment Analysis Plots}\label{fig:deaplots}
\end{figure}

The example was simple enough. However, we can easily perform DEA on any number of inputs and outputs, using R.

\hypertarget{appendix-appendix}{%
\appendix}


\hypertarget{various-datasets-in-base-r-used-in-this-book}{%
\chapter{Various Datasets, in base R, used in this book}\label{various-datasets-in-base-r-used-in-this-book}}

\begin{itemize}
\item
  \texttt{Nile}:
  Measurements of the annual flow of the river Nile at Aswan (formerly
  `Assuan'), 1871-1970, in 10\^{}8 m\^{}3, ``with apparent changepoint near
  1898'' (Cobb(1978), Table 1, p.249).
\item
  \texttt{lynx}:
  Annual numbers of lynx trappings for 1821-1934 in Canada. Taken from
  Brockwell \& Davis (1991), this appears to be the series considered by
  Campbell \& Walker (1977).
\item
  \texttt{mtcars}:
  The data was extracted from the 1974 \emph{Motor Trend} US magazine, and
  comprises fuel consumption and 10 aspects of automobile design and
  performance for 32 automobiles (1973-74 models).
\item
  \texttt{sunspot.month}:
  Monthly numbers of sunspots, as from the World Data Center, aka SIDC.
  This is the version of the data that will occasionally be updated when
  new counts become available.
\item
  \texttt{nhtemp}:
  The mean annual temperature in degrees Fahrenheit in New Haven,
  Connecticut, from 1912 to 1971.
\item
  \texttt{airquality}:
  Daily air quality measurements in New York, May to September 1973.
\item
  \texttt{JohnsonJohnson}:
  Quarterly earnings (dollars) per Johnson \& Johnson share 1960-80.
\item
  \texttt{iris}:
  This famous (Fisher's or Anderson's) iris data set gives the
  measurements in centimeters of the variables sepal length and width and
  petal length and width, respectively, for 50 flowers from each of 3
\item
  \texttt{attitude}:
  From a survey of the clerical employees of a large financial
  organization, the data are aggregated from the questionnaires of the
  approximately 35 employees for each of 30 (randomly selected)
  departments. The numbers give the percent proportion of favourable
  responses to seven questions in each department.
\item
  \texttt{longley}:
  A macroeconomic data set which provides a well-known example for a
  highly collinear regression.
\item
  \texttt{USArrests}:
  This data set contains statistics, in arrests per 100,000 residents for
  assault, murder, and rape in each of the 50 US states in 1973. Also
  given is the percent of the population living in urban areas.
\end{itemize}

  \bibliography{book.bib,packages.bib}

\end{document}
