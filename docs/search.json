[{"path":"index.html","id":"welcome-to-r-for-audit-analytics","chapter":"Welcome to R for Audit Analytics","heading":"Welcome to R for Audit Analytics","text":"\nR Audit Analytics Anil Goyal licensed CC -NC 4.0The author works Government India. opinions expressed book personal author construed Government India, author official capacity. data-sets examples used book either sample data-sets available R allied packages available online (sources) open data-sets created author demonstrate case discussion. None data-sets examples used book pertain entity ever handled author official capacity breach official secrecy manner.website book R Audit Analytics hosted absolutely free use . work Anil Goyal. Suggestions, errors, etc. may communicated author email anilyayavar@gmail.com github version book.","code":""},{"path":"preface.html","id":"preface","chapter":"Preface","heading":"Preface","text":"“R Swiss Army knife data science. ’s versatile powerful tool can handle nearly data-related task, data cleaning preparation statistical modeling machine learning. ’s open-source, ’s vast ecosystem packages tools available extend capabilities even .” - Norman Matloff, professor computer science University California, Davis.Welcome book R language, designed specifically auditors interested learning data analytics using open source resources. today’s digital age, data abundant ubiquitous, importance overstated. result, auditors must equipped necessary skills leverage data draw insights . book result love R passion data analytics. book, tried present R programming concepts case studies useful audit analysis well forensic audit fraud investigation.book written based notes R learning language . included sufficient figures examples make concepts easy understand readers. figures created except , due credit given.R open-source programming language gaining popularity recent years due versatility flexibility data analysis. journey R began COVID-19 lock-, introduced R data analysis one colleagues. Since , R become essential tool toolbox data analysis.book, tried make concepts R programming data analytics accessible possible auditors may little zero knowledge programming. first part book covers R programming concepts absolutely necessary work R. Second part onward covers data wrangling/transformation techniques well different techniques useful forensic audit fraud investigation.hope book useful resource auditors want learn data analytics using open source resources like R. invite readers share suggestions comments book help improve .Happy reading happy learning!","code":""},{"path":"preface.html","id":"acknowledgments","chapter":"Preface","heading":"Acknowledgments","text":"Writing book journey filled learning, challenges, invaluable support around . First foremost, extend deepest gratitude wife, Reena, whose unwavering encouragement belief abilities persuaded embark endeavour. steadfast support cornerstone motivation throughout process.also profoundly grateful esteemed colleagues, Chandersheel Niti, whose expertise, insights, feedback greatly enriched content book. willingness share knowledge, provide suggestions, meticulously point errors mistakes instrumental refining quality accuracy material presented herein.Furthermore, extend thanks contributed project various capacities, whether discussions, reviews, moral support. contributions invaluable undoubtedly played significant role shaping book.Lastly, like express appreciation readers engage book. sincere hope knowledge imparted within pages proves valuable contributes advancement auditing practices utilizing R language.Thank , depths heart, part journey.","code":""},{"path":"preface.html","id":"r-not-just-a-letter","chapter":"Preface","heading":"R, not just a letter","text":"R programming language extended version S programming language. John Chambers, creator S programming language 1976 Bell laboratories. 1988, official version S language came existence name S-PLUS. R language almost unchanged version S-PLUS.1991, R created Ross Ihaka Robert Gentleman Department Statistics University Auckland. Ross’s Robert’s experience developing R documented 1996 paper Journal Computational Graphical Statistics.1 1997 R Core Group formed, containing people associated S S-PLUS. Currently, core group controls source code R solely able check changes main R source tree. Finally, 2000 R version 1.0.0 released public. \nFigure 0.1: Brief History R\n","code":""},{"path":"preface.html","id":"why-r","chapter":"Preface","heading":"Why R?","text":"R programming language open-source programming language statistical computation. supports n number statistical analysis techniques, machine learning models, graphical visualization data analysis. serves purpose free software environment statistical computation graphics. R easy understand implement. packages available create effective R program, data models, graphical charts. research analytics purposes, popular language among statisticians data scientists.\nFigure 0.2: R\nalways admire Hadley Wickham’s contributions development R programming concepts, made data analysis accessible efficient. tidyverse, collection R packages developed Wickham, transformed way data analysts data scientists work data. tidyverse promotes consistent coherent way working data, making easier write code easier read, understand, maintain. Wickham’s contributions R become foundation many data analysis tools programming languages, including Python Julia. hope book inspire readers explore vast potential R contributions made Wickham data analytics.One advantages using free/open source tools like R can easily customized extended suit specific needs user. Additionally, free/open source tools often updated frequently licensed tools, ensuring users access latest features bug fixes. Using licensed data analytics tools like Caseware IDEA, Tableau, Power BI others can expensive, licensing fees can significant burden smaller organizations individuals. using open source tools like R, users can significantly reduce costs still access powerful data analytics capabilities.Another strength R extensive library packages, includes many tools statistical analysis data visualization.","code":""},{"path":"about-author.html","id":"about-author","chapter":"About author","heading":"About author","text":"Anil Goyal data analytics enthusiast working Indian Audit Accounts Department since 1998. Anil passion learning applying programming languages/tools R Tableau solve data-related challenges.Anil self-taught expert R, involved variety data analytics projects audits.Anil holds post-graduate degree Mathematics University Rajasthan, Jaipur, received 1998. book first book. continues expand skill set knowledge field. Anil also loves solve problems raised various users StackOverflow.com mainly related R language.working data, Anil enjoys pursuing personal interests, include photographying birds, nature; reading watching movies, etc..","code":""},{"path":"gearing-up.html","id":"gearing-up","chapter":"Gearing up","heading":"Gearing up","text":"","code":""},{"path":"gearing-up.html","id":"download-and-installation","chapter":"Gearing up","heading":"0.1 Download and installation","text":"R programming language local computer can downloaded web portal Comprehensive R Archive Network, short mostly referred CRAN, network ftp web servers around world store identical, --date, versions code documentation R. portal address https://cran.r-project.org/ -\nFigure 0.3: CRAN Portal\nDownload specific (per operating system) file port install following instructions. R programming interface looks like-\nFigure 0.4: R Workspace\n","code":""},{"path":"gearing-up.html","id":"writing-your-first-code","chapter":"Gearing up","heading":"0.2 Writing your first code","text":"Writing code R pretty easy. Just type command front >, shown figure 0.4 prompt press Enter(Return) key. R display results next line.\nFigure 0.5: Left - Writing first Code R; Right - Indenting code necessary recommended\n","code":""},{"path":"gearing-up.html","id":"things-to-remember","chapter":"Gearing up","heading":"0.3 Things to remember","text":"R case sensitive. remembered writing/storing/calling functions objects. Anil, ANIL, anil different objects R.White spaces different pieces codes don’t matter. See figure-0.5 . 3+4 3 + 4 evaluate . However, better readability always better use spaces.Parenthesis () generally used change natural order precedence. Moreover, also used passing arguments functions, discussed detail chapter-3 onward.Multi-line code(s) aren’t required indented R. R, indents meaning. However, following best practices write code understandable readers, proper indentation suggested. See figure-0.5 (right) .incomplete code written first line code (useful single line sufficient write complete code), R automatically prompt displaying + beginning line, instead >. See figure-0.5 (right) .Indices R always start 1 (0). discussed detail chapter-2.Code start hash symbol # execute. Even line # appears line, code place get executed. See following example. Comments may used codes either purposes -\nCode Readability\nExplanation code\nInclusion metadata, references, etc.\nPrevent execution certain line code\nCode ReadabilityExplanation codeInclusion metadata, references, etc.Prevent execution certain line codeTip: clear workspace, just click ctrl + l.Normally R code files extension .R R files may extensions, project files .Rproj, markdown files .Rmd, many .programming/code writing may done R. may noticed code executed edited. code written (Tip: get previous executed command just use scroll key keyboard). Thus, order use many smart features, write code R scripts .e. .R files, using popular IDE R R Studio.Rstudio IDE popular among using R, many people distinguish R IDE. Even Stack Overflow popular forum seek online help explicitly asks users tag ‘R studio’ general R code problems2.","code":"\n# 1 + 3 (this won't be executed)\n1 + 3 # +5## [1] 4"},{"path":"gearing-up.html","id":"r-studio-ide","chapter":"Gearing up","heading":"0.4 R studio IDE","text":"RStudio free open source IDE (Integrated Development Environment) R, available Windows, Mac OS LINUX. can downloaded portal https://posit.co/download/rstudio-desktop/. data analytics needs, require Rstudio desktop version, available free download installation.includes console, syntax-highlighting editor supports direct code execution, variety robust tools plotting, viewing history, debugging managing work-space. downloading installing local machine, work-space/UI similar shown following figure, opened.\nFigure 0.6: R Studio interface\nfour panelsTop-left:\nScripts Files: script files working , opened displayed . open new script, just need click new script button  just file menu.; using keyboard shortcut ctrl + shift + n\nScripts Files: script files working , opened displayed . open new script, just need click new script button  just file menu.; using keyboard shortcut ctrl + shift + nBottom-left:\nR console: R commands can written see output. Even commands run script show output panel.\nR console: R commands can written see output. Even commands run script show output panel.\n+ Terminal: can access system shell.\n- Top-right:\n+ Environment: see objects saved current environment. panel also used import data current environment.\n+ History view history commands run, current session\n+ Connections: Used connect/import external database/data\n- Bottom-right:\n+ Files tree folders, see file structure current working directory\n+ Plots graph window, output R command plot/graph, generated .\n+ Packages, download load external packages using mouse click\n+ Help, window get help desired functions. Even help sought r command displayed window.\n+ Viewer: can used view local web content.Readers may note execute code .R file slightly different execute console pressing Enter/Return key just executes gives us result next line. run script Scripts Files pane (Top-left) can either following -Select code press ctrl/command + Enter/Return keys.cursor anywhere code even anywhere line(s) code/code-block, can press ctrl/command + Enter/Return keys.alternatively, can make use Run button  given top-right side Files Scripts pane.get quick overview (later-references) readers may refer Rstudio cheatsheet available Posit Cheatsheets page, wherein many cheatsheets also available.","code":""},{"path":"gearing-up.html","id":"packages-and-libraries-and-conflicts","chapter":"Gearing up","heading":"0.5 Packages and libraries and conflicts","text":"already stated, one strength R numerous user-written packages (libraries) available Comprehensive R Archive Network .e. CRAN. Package installation perhaps easiest jobs R.command fairly simple -downloads given package name (given quotes case-sensitive), compiles load specified/default directory. however, load memory/R current session. libraries/packages downloaded computer/system need loaded every new session R, using command- Quotes , optional package name still case sensitive. install load tidyverse need run first command (download package local computer) second command (load current R session) every new session.Rstudio pane Packages may also used, shown following image (taken cheatsheet).Also notice output library command . Besides loading successfully, nine packages discuss section 0.7, given message conflicts.conflicts? Actually function exactly name resides multiple package conflict arises, R default prefers conflicted functions loaded last. , package stats part base R also consists function filter overridden package dplyr loaded part tidyverse. Thus, loading dplyr, function filter masked stats.case want use filter masked stats may eithercall using double colon operator (Refer section 0.5.1), .e. using stats::filter(); ormake use another package conflicted part tidyverse follows-Usage package conflicted advised bit caution, loading package causes restrict usage conflicted function altogether .e. without giving explicit preference.","code":"install.packages(\"library_name\")library(library_name)install.packages('tidyverse')\nlibrary(tidyverse)## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n## ✔ dplyr     1.1.4     ✔ readr     2.1.5\n## ✔ forcats   1.0.0     ✔ stringr   1.5.1\n## ✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n## ✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n## ✔ purrr     1.0.2     \n## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n## ✖ dplyr::filter() masks stats::filter()\n## ✖ dplyr::lag()    masks stats::lag()\n## ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errorslibrary(conflicted)\nconflict_prefer(\"filter\", \"stats\")"},{"path":"gearing-up.html","id":"doublec","chapter":"Gearing up","heading":"0.5.1 Double Colon operator ::","text":"R, can use double colon operator .e. :: access functions defined part internal functions package uses. may used least two cases-call function say filter package dplyr may use dplyr::filter() without actually loading .cases conflicts discussed preceding section, e.g. stats::filter().","code":""},{"path":"gearing-up.html","id":"help","chapter":"Gearing up","heading":"0.6 Getting Help within R","text":"R installed, comprehensive built-help system. can use following commands-Alternatively, features Help menu help pane, can also used.","code":"help.start()   # general help\nhelp(foo)      # help about function `foo`\n?foo           # same as above\napropos(\"foo\") # show all functions containing word `foo`\nexample(foo)   # show an example of function `foo`"},{"path":"gearing-up.html","id":"TIDYVE","chapter":"Gearing up","heading":"0.7 tidyverse","text":"tidyverse package packages work harmony share common data representations ‘API’ design. package designed make easy install load multiple ‘tidyverse’ packages single step.Though tidyverse collection 20+ packages (fact 80+ packages installed including depended packages) installed install.packages(\"tidyverse\") command, yet library(tidyverse) load nine . Others (like readxl) loaded explicitly.ggplot2 system decoratively creating graphics, based Grammar Graphics.dplyr provides grammar data manipulation, providing consistent set verbs solve common data manipulation challenges.tidyr provides set functions useful data transformation.readr used read write rectangular/tabular data formats.purrr functional programming (FP) toolkit working functions vectors.tibble provides functionalities related displaying data frames.stringr provides set functions designed work strings. built top another package stringi.forcats provides suite useful tools solve common problems factors.lubridate makes easier things R date-times.latest version Tidyverse, loading lubridate also loads default.\nFigure 0.7: tidyverse\nseveral tidyverse packages working -hmsreadxlglue","code":""},{"path":"part-i-basic-r-programming-concepts.html","id":"part-i-basic-r-programming-concepts","chapter":"Part-I: Basic R Programming Concepts","heading":"Part-I: Basic R Programming Concepts","text":"","code":""},{"path":"r-programming-language.html","id":"r-programming-language","chapter":"1 R Programming Language","heading":"1 R Programming Language","text":"","code":""},{"path":"r-programming-language.html","id":"calculator","chapter":"1 R Programming Language","heading":"1.1 Use R as a calculator","text":"start learning R, just start entering equations directly command prompt > press enter. , 3+4 give result 7. Common mathematical operators listed table 1.1.Table 1.1:  Common Mathematical Operators RStrings Characters enclosed single ' double\" quotes (strings section 1.3.4). examples calculations can performed R -Note R follows common mathematical order precedence evalauting expressions. may changed using simple parenthesis .e. (). Also note brackets/braces .e. curly braces {} [] assigned different meaning, change nested order operations () may used.","code":"\n4 + 3 ^ 2## [1] 13\n8 * (9 + 4)## [1] 104"},{"path":"r-programming-language.html","id":"object-assignment","chapter":"1 R Programming Language","heading":"1.2 Object Assignment","text":"R object-oriented language.3 means objects created stored R environment can used later.object? object can something simple number (value) can assigned variable. Think like ; Suppose greet user /name prefixing hello /name. Now user’s name may saved work environment later use. Thus, user name saved variable can retrieved later , calling variable name instead asking user name . object can also data-set complex model output function. Thus, object created R can hold multiple values.important thing objects objects created R, using assignment operator <-. Use equals sign = set something object recommended thought work properly cases. now stick assignment operator, interpret left side object name storing object information specified right side. -> right hand side assignment used, needless say things mentioned interchange.Case sensitive nature: Names variables even objects R case sensitive, thus user, USER useR; different variables.","code":"\n# user name\nuser_name <- 'Anil Goyal'\n\n# when the above variable is called\nuser_name## [1] \"Anil Goyal\""},{"path":"r-programming-language.html","id":"atomic-data-types-in-r","chapter":"1 R Programming Language","heading":"1.3 Atomic data types in R","text":"seen objects R can created store values/data. Even objects can contain objects well. question arises, atomic/basic data type R. atomic mean object split . Thus, atomic objects created R can thought variables holding one single value. E.g. user’s name, user’s age, etc. Now atomic objects created R can six types-logical (Boolean .e. TRUE FALSE etc.)integer (non-decimal numeric values like 0, 1, etc.)double ( floating decimal type .e. numeric values decimal .e. 1.0 5.25, etc.)character (string data type alphanumeric value)complex (numbers real imaginary parts e.g. 1+1i)raw (discussed )\nFigure 1.1: Data types R\nLet us discuss .Note: use pre-built function typeof() check type given value/variable. However, functions discussed later-.","code":""},{"path":"r-programming-language.html","id":"logical","chapter":"1 R Programming Language","heading":"1.3.1 Logical","text":"R logical values stored either TRUE FALSE (caps)NA: one special type logical value .e. NA (short Available). used missing data.Remember missing data empty string. difference two explained section 1.3.4.","code":"\nTRUE## [1] TRUE\ntypeof(TRUE)## [1] \"logical\"\nmy_val <- TRUE\ntypeof(my_val)## [1] \"logical\""},{"path":"r-programming-language.html","id":"integer","chapter":"1 R Programming Language","heading":"1.3.2 Integer","text":"Numeric values can either integer (.e. without floating point decimal) floating decimal value (called double r). Now integers R differentiated suffix L. E.g.","code":"\nmy_val1 <- 2L\ntypeof(my_val1)## [1] \"integer\"\ntypeof(2)## [1] \"double\""},{"path":"r-programming-language.html","id":"double","chapter":"1 R Programming Language","heading":"1.3.3 Double","text":"Numeric values decimals stored objects type double. kept mind storing integer value directly variable, suffix L must used otherwise object stored double type shown example.double type, exponential formats hexadecimal formats store numerals may also used.Note: Suffix L may also used numerals hexadecimal (e.g. 0xcafeL) exponential formats (e.g. 1.23e4L), coerce numerals integer format.Thus, integer double data types may understood R sub-types numeric data. three types special numerals (specifically doubles) Inf, -Inf NaN. first two infinity (positive negative) last one denotes indefinite number (NaN short Number).","code":"\nmy_val2 <- 2.5\nmy_val3 <- 1.23e4\nmy_val4 <- 0xcafe # hexadecimal format (prefixed by 0x)\n\ntypeof(my_val2)## [1] \"double\"\ntypeof(my_val3)## [1] \"double\"\ntypeof(my_val4)## [1] \"double\"\ntypeof(0xcafeL)## [1] \"integer\"\n1/0## [1] Inf\n-45/0## [1] -Inf\n0/0## [1] NaN"},{"path":"r-programming-language.html","id":"string","chapter":"1 R Programming Language","heading":"1.3.4 Character","text":"Strings stored R character type. Strings either surrounded single quotes '' double quotes \"\"4.[Notes:\\\\](Notes:){.uri} 1. Though NA basically type logical yet used store missing values data type also shown subsequent chapter(s). 2. Special characters escaped \\; Type ?Quotes console check documentation full details. 3. simple use \\ escape character may use \" ' within quotes. Check Example-3 .Example-1: Usage double single quote interchangeably.Example-2: Usage escape character.Example-3: Usage escape character store single/double quotes string .Note: absence indices noticed code output, learn cat function .","code":"\nmy_val5 <- 'Anil Goyal'\nmy_val6 <- \"Anil Goyal\"\nmy_val7 <- \"\" # empty string\nmy_missing_val <- NA # missing value\n\ntypeof(my_val5)## [1] \"character\"\ntypeof(my_val6)## [1] \"character\"\ntypeof(my_val7)## [1] \"character\"\ntypeof(my_missing_val)## [1] \"logical\"\nmy_val8 <- \"R's book\"\nmy_val8## [1] \"R's book\"\ncat(\"This is first line.\\nThis is new line\")## This is first line.\n## This is new line\ncat(\"\\' is single quote and \\\" is double quote\")## ' is single quote and \" is double quote"},{"path":"r-programming-language.html","id":"null","chapter":"1 R Programming Language","heading":"1.3.5 NULL","text":"NULL (note: caps) specific data type used create empty vector. Even NULL can used vector .","code":"\ntypeof(NULL)## [1] \"NULL\"\nvec <- 1:5\nvec## [1] 1 2 3 4 5\nvec <- NULL\nvec## NULL"},{"path":"r-programming-language.html","id":"complex","chapter":"1 R Programming Language","heading":"1.3.6 Complex","text":"Complex numbers made real imaginary parts. used data analysis tasks, discussed detail .","code":"\nmy_complex_no <- 1+1i\ntypeof(my_complex_no)## [1] \"complex\""},{"path":"r-programming-language.html","id":"data-structuresobject-types-in-r","chapter":"1 R Programming Language","heading":"1.4 Data structures/Object Types in R","text":"Objects R can either homogeneous heterogeneous.\nFigure 1.2: Objects/Data structures R, can either homogeneous (left) heterogeneous (right)\n","code":""},{"path":"r-programming-language.html","id":"homogeneous-objects","chapter":"1 R Programming Language","heading":"Homogeneous objects","text":"","code":""},{"path":"r-programming-language.html","id":"vectors","chapter":"1 R Programming Language","heading":"1.4.1 Vectors","text":"vector? vector simply collection values/data type.\nFigure 1.3: Vectors homegeneous data structures R\n","code":""},{"path":"r-programming-language.html","id":"simple-vectors-unnamed-vectors","chapter":"1 R Programming Language","heading":"1.4.1.1 Simple vectors (Unnamed vectors)","text":"Though, Vector atomic data type used R, yet can hold multiple values (type) simultaneously. fact vector collection multiple values type. vector atomic can hold multiple values? may noticed [1] printed start line output whenever variable called/printed. [1] actually index element. Thus, R instead scalar(s) atomic type, vector(s) containing one element. Whenever vector called values stored displayed index start new line .Even processing multiple values simultaneously, stored vector, produce desired output, one powerful strengths R. three variables shown figure , vectors.\nFigure 1.4: Examples Vectors\ncreate vector? Vectors R created using either -c() function shortest commonly used function r. elements concatenated (hence shortcut c function) using comma , ; ORvector() produces vector given length mode.Function c() can also used join two vectors.\nFigure 1.5: Vector Concatenation\n","code":"\nmy_vector <- c(1, 2, 3)\nmy_vector## [1] 1 2 3\nmy_vector2 <- vector(mode = 'integer', length = 15)\nmy_vector2##  [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\nvec1 <- c(1, 2)\nvec2 <- c(11, 12)\nvec3 <- c(vec1, vec2)\nvec3## [1]  1  2 11 12"},{"path":"r-programming-language.html","id":"useful-functions-to-create-new-vectors","chapter":"1 R Programming Language","heading":"Useful Functions to create new vectors","text":"useful functions create new vectors R, discuss using vectors subsequent chapters.","code":""},{"path":"r-programming-language.html","id":"generate-integer-sequences-with-colon-operator","chapter":"1 R Programming Language","heading":"Generate integer sequences with Colon Operator :","text":"function generates sequence number preceding : next specified number, arithmetical difference 1 -1 case may . Notice output vector type integer.Note: One common mistakes colon operator assuming operator precedence. R, colon operator calculation precedence mathematical operator. Think outputs may get -","code":"\n1:25##  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n25:30## [1] 25 26 27 28 29 30\n10:1##  [1] 10  9  8  7  6  5  4  3  2  1\ntypeof(2:250)## [1] \"integer\"n <- 5\n1:n+1\n1:n*2"},{"path":"r-programming-language.html","id":"generate-specific-sequences-with-function-seq","chapter":"1 R Programming Language","heading":"Generate specific sequences with function seq","text":"function generates sequence given number another number, similar :, gives us control output desired. can provide difference specifically (double type also) argument. Otherwise length.argument provided calculates difference automatically.","code":"\nseq(1, 5, by = 0.3)##  [1] 1.0 1.3 1.6 1.9 2.2 2.5 2.8 3.1 3.4 3.7 4.0 4.3 4.6 4.9\nseq(1, 2, length.out = 11)##  [1] 1.0 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2.0"},{"path":"r-programming-language.html","id":"repeat-a-patternvector-with-function-rep","chapter":"1 R Programming Language","heading":"Repeat a pattern/vector with function rep","text":"name suggests rep short repeat thus repeat given element, given number times.","code":"\nrep('repeat this', 5)## [1] \"repeat this\" \"repeat this\" \"repeat this\" \"repeat this\" \"repeat this\"\n# We can even repeat already created vectors\nvec <- c(1, 10)\nrep(vec, 5)##  [1]  1 10  1 10  1 10  1 10  1 10\nrep(vec, each = 5) # notice the difference in results##  [1]  1  1  1  1  1 10 10 10 10 10"},{"path":"r-programming-language.html","id":"generate-english-alphabet-with-letters-letters","chapter":"1 R Programming Language","heading":"Generate english alphabet with LETTERS / letters","text":"two inbuilt vectors R 26 alphabets upper lower cases respectively.","code":"\nLETTERS##  [1] \"A\" \"B\" \"C\" \"D\" \"E\" \"F\" \"G\" \"H\" \"I\" \"J\" \"K\" \"L\" \"M\" \"N\" \"O\" \"P\" \"Q\" \"R\" \"S\"\n## [20] \"T\" \"U\" \"V\" \"W\" \"X\" \"Y\" \"Z\"\nletters##  [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\" \"k\" \"l\" \"m\" \"n\" \"o\" \"p\" \"q\" \"r\" \"s\"\n## [20] \"t\" \"u\" \"v\" \"w\" \"x\" \"y\" \"z\""},{"path":"r-programming-language.html","id":"generate-gregorian-calendar-month-names-with-month.name-month.abb","chapter":"1 R Programming Language","heading":"Generate gregorian calendar month names with month.name / month.abb","text":"","code":"\nmonth.name##  [1] \"January\"   \"February\"  \"March\"     \"April\"     \"May\"       \"June\"     \n##  [7] \"July\"      \"August\"    \"September\" \"October\"   \"November\"  \"December\"\nmonth.abb##  [1] \"Jan\" \"Feb\" \"Mar\" \"Apr\" \"May\" \"Jun\" \"Jul\" \"Aug\" \"Sep\" \"Oct\" \"Nov\" \"Dec\""},{"path":"r-programming-language.html","id":"named-vectors","chapter":"1 R Programming Language","heading":"1.4.1.2 Named Vectors","text":"Vectors R, can named also, .e. element name. E.g.\nFigure 1.6: Vector elements can names\nNote assigning names element, names enclosed quotes similar variable assignment. Also notice time R printed numeric indices/index first element (new line). ways assign names existing vector. can use names() function, displays names elements vector ( time quotes displayed vector).Using function can assign names existing vector. SeeNames may also assigned using setNames() creating vector simultaneously.Function unname() may used remove names. Even names can removed assigning NULL names vector. Also remember unname modify vector place. change assigned unnamed vector vector . Check ,","code":"\nages <- c(A = 10, B = 20, C = 15)\nages##  A  B  C \n## 10 20 15\nnames(ages)## [1] \"A\" \"B\" \"C\"\nvec1## [1] 1 2\nnames(vec1) <- c('first_element', 'second_element')\nvec1##  first_element second_element \n##              1              2\nnew_vec <- setNames(1:26, LETTERS)\nnew_vec##  A  B  C  D  E  F  G  H  I  J  K  L  M  N  O  P  Q  R  S  T  U  V  W  X  Y  Z \n##  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\nunname(new_vec)##  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n## [26] 26\nnew_vec##  A  B  C  D  E  F  G  H  I  J  K  L  M  N  O  P  Q  R  S  T  U  V  W  X  Y  Z \n##  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\nnew_vec <- unname(new_vec)\nnew_vec##  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n## [26] 26"},{"path":"r-programming-language.html","id":"type-coercion","chapter":"1 R Programming Language","heading":"Type coercion","text":"occasions different classes R objects get mixed together. Sometimes happens accident can also happen purpose. Let us deal .prior let us learn check type vector. course can check type vector using function typeof() want check whether vector specific type. .*() functions check , functions return either TRUE FALSE..logical().integer().double().character().complex()","code":"\nis.integer(1:10)## [1] TRUE\nis.logical(LETTERS)## [1] FALSE"},{"path":"r-programming-language.html","id":"implicit-coercion","chapter":"1 R Programming Language","heading":"Implicit Coercion","text":"already stated, vector atomic data object R. Even elements vector (multiple elements) vectors . also discussed vectors homogeneous types. happens try mix elements different types vector.fact try mix elements different types vector, resultant vector coerced type feasible. Since numeral say 56 can easily converted complex number (56+0i) character (\"56\"), alphabet say , converted numeral, atomic data types normally follow order precedence, tabulated table 1.2.Table 1.2:  Order Precedence Atomic Data TypesFor e.g. following diagram, notice individual elements first vector. types elements therein, character type highest rank thus resultant vector silently coerced character vector. Similarly, second third vectors coerced double (second element) integer (first element) respectively.\nFigure 1.7: Implicit Coercion Vectors\nalso important note implicit coercion without warning silently performed. implicit coercion also carried two () vectors different data types concatenated together.Example- vec existing vector type integer. try add extra element say character type, vec type coerced character.R also implicitly coerces vectors appropriate type try perform calculations vectors types. Example","code":"\nvec <- 1:5\ntypeof(vec)## [1] \"integer\"\nvec <- append(vec, 'ABCD')\ntypeof(vec)## [1] \"character\"\n(TRUE == FALSE) + 1## [1] 1\ntypeof(TRUE + 1:100)## [1] \"integer\"\ntypeof(FALSE + 56)## [1] \"double\""},{"path":"r-programming-language.html","id":"explicit-coercion","chapter":"1 R Programming Language","heading":"Explicit Coercion","text":"can explicitly coerce using .*() function, like .logical(), .integer(), .double(), .character(). Failed coercion strings generates warning missing value:","code":"\nas.double(c(TRUE, FALSE))## [1] 1 0\nas.integer(c(1, 'one', 1L))## Warning: NAs introduced by coercion## [1]  1 NA  1"},{"path":"r-programming-language.html","id":"coercion-precedence","chapter":"1 R Programming Language","heading":"1.4.1.3 Coercion precedence","text":"Sometimes, inside R coercion happen time. one precede ? Actually, implicit coercion precede explicit coercion always. Consider example. However, without seeing result try guess output.Explanation: vector c('TRUE', 1) coerces c('TRUE', '1') due implicit coercion first thereafter explicit coercion forces second element .logical('1') NA. Though .logical(1) resulted TRUE .logical(\"1\") result NA.","code":"\nas.logical(c('TRUE', 1))## [1] TRUE   NA"},{"path":"r-programming-language.html","id":"checking-dimensions","chapter":"1 R Programming Language","heading":"Checking dimensions","text":"Now vector can n number vectors (recall element vector ) times may need check many elements given vector contains. Using function length(), can check number elements.","code":"\nlength(1:100)## [1] 100\nlength(LETTERS)## [1] 26\nlength('LENGTH') # If you thought its output should have been 6, check again.## [1] 1"},{"path":"r-programming-language.html","id":"matrix-matrices","chapter":"1 R Programming Language","heading":"1.4.2 Matrix (Matrices)","text":"Matrix (plural matrices) two dimensional arrangement (similar matrix linear algebra hence name) elements type vectors. E.g.\\[\\begin{array}{ccc}\nx_{11} & x_{12} & x_{13}\\\\\nx_{21} & x_{22} & x_{23}\n\\end{array}\\]Thus, matrices vectors attribute named dimension.dimension attribute integer vector length 2 (number rows, number columns).","code":""},{"path":"r-programming-language.html","id":"create-a-new-matrix","chapter":"1 R Programming Language","heading":"Create a new matrix","text":"new matrix can created using function matrix() vector given converted matrix either number rows nrow number columns ncol may given.Another useful argument byrow default FALSE. explicitly changed, get\nFigure 1.8: Arrangement Matrix, byrow argument used\nMatrix can type. rules explicit implicit coercion (explained vectors) also apply .","code":"\nmatrix(1:12, nrow = 3)##      [,1] [,2] [,3] [,4]\n## [1,]    1    4    7   10\n## [2,]    2    5    8   11\n## [3,]    3    6    9   12\nmatrix(1:12, ncol=3)##      [,1] [,2] [,3]\n## [1,]    1    5    9\n## [2,]    2    6   10\n## [3,]    3    7   11\n## [4,]    4    8   12\nmatrix(1:12, ncol=3, byrow = TRUE)##      [,1] [,2] [,3]\n## [1,]    1    2    3\n## [2,]    4    5    6\n## [3,]    7    8    9\n## [4,]   10   11   12\nmatrix(LETTERS, nrow = 2)##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13]\n## [1,] \"A\"  \"C\"  \"E\"  \"G\"  \"I\"  \"K\"  \"M\"  \"O\"  \"Q\"  \"S\"   \"U\"   \"W\"   \"Y\"  \n## [2,] \"B\"  \"D\"  \"F\"  \"H\"  \"J\"  \"L\"  \"N\"  \"P\"  \"R\"  \"T\"   \"V\"   \"X\"   \"Z\"\nmatrix(c(LETTERS, 1:4), nrow=5)##      [,1] [,2] [,3] [,4] [,5] [,6]\n## [1,] \"A\"  \"F\"  \"K\"  \"P\"  \"U\"  \"Z\" \n## [2,] \"B\"  \"G\"  \"L\"  \"Q\"  \"V\"  \"1\" \n## [3,] \"C\"  \"H\"  \"M\"  \"R\"  \"W\"  \"2\" \n## [4,] \"D\"  \"I\"  \"N\"  \"S\"  \"X\"  \"3\" \n## [5,] \"E\"  \"J\"  \"O\"  \"T\"  \"Y\"  \"4\""},{"path":"r-programming-language.html","id":"names-in-matrices","chapter":"1 R Programming Language","heading":"Names in matrices","text":"Similar vectors, rows columns matrices may names. Check ?matrix() complete documentation.","code":""},{"path":"r-programming-language.html","id":"dimension","chapter":"1 R Programming Language","heading":"Dimension","text":"check dimension matrix can use dim() (short dimension) (similar length case vectors) return vector two numbers (rows first, followed columns).gives us another method create matrix vector. See","code":"\nmy_mat <- matrix(c(LETTERS, 1:4), nrow=5)\ndim(my_mat)## [1] 5 6\nmy_mat2 <- 1:10\ndim(my_mat2) <- c(2,5)\nmy_mat2##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    3    5    7    9\n## [2,]    2    4    6    8   10"},{"path":"r-programming-language.html","id":"have-a-check-on-replication","chapter":"1 R Programming Language","heading":"Have a check on replication","text":"happens product given dimensions less greater given vector converted. replicates advised check properly resultant vector may desired. Check cases, notice R gives result silently warning.","code":"\nmatrix(1:10, nrow=5, ncol=5)## Warning in matrix(1:10, nrow = 5, ncol = 5): data length differs from size of\n## matrix: [10 != 5 x 5]##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    6    1    6    1\n## [2,]    2    7    2    7    2\n## [3,]    3    8    3    8    3\n## [4,]    4    9    4    9    4\n## [5,]    5   10    5   10    5\nmatrix(1:1000, nrow=2, ncol=3)## Warning in matrix(1:1000, nrow = 2, ncol = 3): data length [1000] is not a\n## sub-multiple or multiple of the number of columns [3]##      [,1] [,2] [,3]\n## [1,]    1    3    5\n## [2,]    2    4    6"},{"path":"r-programming-language.html","id":"combining-matrices","chapter":"1 R Programming Language","heading":"Combining matrices","text":"Using cbind() rbind() can combine two matrices column-wise row-wise respectively.\nFigure 1.9: Binding Two matrices together\nSee two examples.Example-2","code":"\nmat1 <- matrix(1:4, nrow = 2)\nmat2 <- matrix(5:8, nrow = 2)\ncbind(mat1, mat2)##      [,1] [,2] [,3] [,4]\n## [1,]    1    3    5    7\n## [2,]    2    4    6    8\nrbind(mat1, mat2)##      [,1] [,2]\n## [1,]    1    3\n## [2,]    2    4\n## [3,]    5    7\n## [4,]    6    8"},{"path":"r-programming-language.html","id":"arrays","chapter":"1 R Programming Language","heading":"1.4.3 Arrays","text":"Till now seen elements one dimension represented vectors two dimension matrices. question arises , many dimensions can . Actually can n number dimensions r, object type array, ’ll become increasingly difficult comprehend thus discussed . Check however understanding,Try creating 4 5 dimensional arrays console see results.properties vectors, matrices discussed next chapter sub-setting indexing learn retrieve specific elements vector/matrices/etc. till now created objects elements type. want different types elements/data retaining types, together single variable? Answer next section, discuss hetergeneous objects.","code":"\narray(1:24, dim = c(3,2,4)) # a three dimensional array## , , 1\n## \n##      [,1] [,2]\n## [1,]    1    4\n## [2,]    2    5\n## [3,]    3    6\n## \n## , , 2\n## \n##      [,1] [,2]\n## [1,]    7   10\n## [2,]    8   11\n## [3,]    9   12\n## \n## , , 3\n## \n##      [,1] [,2]\n## [1,]   13   16\n## [2,]   14   17\n## [3,]   15   18\n## \n## , , 4\n## \n##      [,1] [,2]\n## [1,]   19   22\n## [2,]   20   23\n## [3,]   21   24"},{"path":"r-programming-language.html","id":"heterogeneous-objects","chapter":"1 R Programming Language","heading":"Heterogeneous objects","text":"","code":""},{"path":"r-programming-language.html","id":"lists","chapter":"1 R Programming Language","heading":"1.4.4 Lists","text":"lists used want combine elements different types together. Function used create list list(). Check thisPictorially list can depicted \nFigure 1.10: list R heterogeneous object\nInterestingly list can contain vectors, matrices, arrays individual elements. See\nFigure 1.11: list R, can contain vector, matrices, array even lists\nSimilar vectors elements can named also.\nFigure 1.12: Similar vector elements, elements list can named also\nORMore interestingly, lists can even contain another lists.Number items first level can checked using length vectors. Checking number items second level onward covered subsequent chapter(s).","code":"\nlist(1, 2, 3, 'My string', TRUE)## [[1]]\n## [1] 1\n## \n## [[2]]\n## [1] 2\n## \n## [[3]]\n## [1] 3\n## \n## [[4]]\n## [1] \"My string\"\n## \n## [[5]]\n## [1] TRUE\nlist(1:3, LETTERS, TRUE, my_mat2)## [[1]]\n## [1] 1 2 3\n## \n## [[2]]\n##  [1] \"A\" \"B\" \"C\" \"D\" \"E\" \"F\" \"G\" \"H\" \"I\" \"J\" \"K\" \"L\" \"M\" \"N\" \"O\" \"P\" \"Q\" \"R\" \"S\"\n## [20] \"T\" \"U\" \"V\" \"W\" \"X\" \"Y\" \"Z\"\n## \n## [[3]]\n## [1] TRUE\n## \n## [[4]]\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    3    5    7    9\n## [2,]    2    4    6    8   10\nlist(first_item = 1:5, second_item = my_mat2)## $first_item\n## [1] 1 2 3 4 5\n## \n## $second_item\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    3    5    7    9\n## [2,]    2    4    6    8   10\nmy_list <- list(first=c(A=1, B=2, C=3),second=my_mat2)\nmy_list## $first\n## A B C \n## 1 2 3 \n## \n## $second\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    3    5    7    9\n## [2,]    2    4    6    8   10\nmy_list2 <- list(my_list, new_item = LETTERS)\nmy_list2## [[1]]\n## [[1]]$first\n## A B C \n## 1 2 3 \n## \n## [[1]]$second\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    3    5    7    9\n## [2,]    2    4    6    8   10\n## \n## \n## $new_item\n##  [1] \"A\" \"B\" \"C\" \"D\" \"E\" \"F\" \"G\" \"H\" \"I\" \"J\" \"K\" \"L\" \"M\" \"N\" \"O\" \"P\" \"Q\" \"R\" \"S\"\n## [20] \"T\" \"U\" \"V\" \"W\" \"X\" \"Y\" \"Z\"\nlength(my_list)## [1] 2\nlength(my_list2) # If you thought its output should have been 3, think again.## [1] 2"},{"path":"r-programming-language.html","id":"data-frame","chapter":"1 R Programming Language","heading":"1.4.5 Data Frame","text":"Data frames used store tabular data (rectangular) R. important type object R.\nFigure 1.13: example data frame\nData frames represented special type list every element list length. element list can thought column length element list number rows.\nFigure 1.14: data frame R, just special kind list\nUnlike matrices, data frames can store different classes objects column. (Remember matrices must every element class).create data frame scratch use function data.frame(). SeeNote R, , allocated row names numbers row .course times data frames ready us analyse thus learn import/read external data r, subsequent chapters. check dimensions data frame use dim matrix.Thus, object types R, can depicted adjoining figure.\nFigure 1.15: important Data structures, R\n","code":"\nmy_df <- data.frame(emp_name = c('Thomas', 'Andrew', 'Jonathan', 'Bob', 'Charles'),\n                    department = c('HR', 'Accounts', 'Accounts', 'Execution', 'Tech'),\n                    age = c(40, 43, 39, 42, 25),\n                    salary = c(20000, 22000, 21000, 25000, NA),\n                    whether_permanent = c(TRUE, TRUE, FALSE, NA, NA))\nmy_df##   emp_name department age salary whether_permanent\n## 1   Thomas         HR  40  20000              TRUE\n## 2   Andrew   Accounts  43  22000              TRUE\n## 3 Jonathan   Accounts  39  21000             FALSE\n## 4      Bob  Execution  42  25000                NA\n## 5  Charles       Tech  25     NA                NA\ndim(my_df)## [1] 5 5"},{"path":"r-programming-language.html","id":"other-data-types","chapter":"1 R Programming Language","heading":"1.5 Other Data types","text":"course, data types R three particularly useful factor, date date-time. types actually built base atomic types, integer, double double respectively ’s discussed separately. types built S3 objects R, users may also define data types object oriented programming. OOP concept core programming concepts therefore scope .However, understand S3 objects better, understand atomic objects (sake simplicity consider vectors) can attributes.Example One attributes vector names, unnamed vector empty (NULL). Attributes object can viewed/called function attributes().Using attr() may assign new attribute R object/variable.can see, example, new attribute added vector. clear now apart names, attributes may also assigned vector.","code":"\n# Let us create a vector\nvec <- 1:26\n# Convert this to a named vector using function setNames()\n# This function takes first argument as vector\n# Second argument should be a character vector of equal length.\nvec <- setNames(vec, LETTERS)\n# let's check what are the attributes of `vec`\nattributes(vec)## $names\n##  [1] \"A\" \"B\" \"C\" \"D\" \"E\" \"F\" \"G\" \"H\" \"I\" \"J\" \"K\" \"L\" \"M\" \"N\" \"O\" \"P\" \"Q\" \"R\" \"S\"\n## [20] \"T\" \"U\" \"V\" \"W\" \"X\" \"Y\" \"Z\"\n# Let's also assign a new attribute say `x` having value \"New Attribute\" to `vec`\nattr(vec, \"x\") <- \"New Attribute\"\n# Now let's check its attributes again\nattributes(vec)## $names\n##  [1] \"A\" \"B\" \"C\" \"D\" \"E\" \"F\" \"G\" \"H\" \"I\" \"J\" \"K\" \"L\" \"M\" \"N\" \"O\" \"P\" \"Q\" \"R\" \"S\"\n## [20] \"T\" \"U\" \"V\" \"W\" \"X\" \"Y\" \"Z\"\n## \n## $x\n## [1] \"New Attribute\""},{"path":"r-programming-language.html","id":"fctrss","chapter":"1 R Programming Language","heading":"1.5.1 Factors","text":"factor vector can contain predefined values. used store categorical data. Factors built top integer vector two attributes: class, ‘factor’, makes behave differently regular integer vectors, levels, defines set allowed values. create factors use function factor.typeof factor returning integer, check type? may use class .factor case.Now factor can ordered also. may use argument ordered = TRUE along another argument levels.Another argument labels can also used display labels, may different levels.Attribute levels can used function retrieve/modify .Remember factors look like (often behave like) character vectors, built top integers. Try think output .factor(c(my_factor, \"UG\")) running console.learn data types detail chapter 31.","code":"\nfac <- factor(c('a', 'b', 'c', 'a'))\nfac## [1] a b c a\n## Levels: a b c\ntypeof(fac) # notice its output## [1] \"integer\"\nattributes(fac)## $levels\n## [1] \"a\" \"b\" \"c\"\n## \n## $class\n## [1] \"factor\"\nclass(fac)## [1] \"factor\"\nis.factor(fac)## [1] TRUE\nmy_degrees <- c(\"PG\", \"PG\", \"Doctorate\", \"UG\", \"PG\")\nmy_factor <- factor(my_degrees, levels = c('UG', 'PG', 'Doctorate'), ordered = TRUE)\nmy_factor # notice output here## [1] PG        PG        Doctorate UG        PG       \n## Levels: UG < PG < Doctorate\nis.ordered(my_factor)## [1] TRUE\nmy_factor <- factor(my_degrees, levels = c('UG', 'PG', 'Doctorate'), \n                    labels = c(\"Under-Graduate\", \"Post Graduate\", \"Ph.D\"),\n                    ordered = TRUE)\nmy_factor # notice output here## [1] Post Graduate  Post Graduate  Ph.D           Under-Graduate Post Graduate \n## Levels: Under-Graduate < Post Graduate < Ph.D\nis.factor(c(my_factor, \"UG\"))## [1] FALSE\nlevels(my_factor)## [1] \"Under-Graduate\" \"Post Graduate\"  \"Ph.D\"\nlevels(my_factor) <- c(\"Grad\", \"Masters\", \"Doctorate\")\nmy_factor## [1] Masters   Masters   Doctorate Grad      Masters  \n## Levels: Grad < Masters < Doctorate"},{"path":"r-programming-language.html","id":"date","chapter":"1 R Programming Language","heading":"1.5.2 Date","text":"Date vectors built top double vectors. class “Date” attributes. common way create date vectors R, converting character string date using .Date() (see case carefully),check arguments .Date running ?.Date() console. check whether given variable type Date r, function like .Date base r, may use inherits() case.","code":"\nmy_date <- as.Date(\"1970-01-31\")\nmy_date## [1] \"1970-01-31\"\nattributes(my_date)## $class\n## [1] \"Date\"\ninherits(my_date, 'Date')## [1] TRUE"},{"path":"r-programming-language.html","id":"date-time-posixct","chapter":"1 R Programming Language","heading":"1.5.3 Date-time (POSIXct)","text":"Times represented POSIXct POSIXlt class.POSIXct just large integer hood. use useful class want store times something like data frame.POSIXlt list underneath stores bunch useful information like day week, day year, month, day month.","code":"\nmy_time <- Sys.time()\nmy_time## [1] \"2024-08-29 07:58:54 IST\"\nclass(my_time)## [1] \"POSIXct\" \"POSIXt\"\nmy_time2 <- as.POSIXlt(my_time)\nclass(my_time2)## [1] \"POSIXlt\" \"POSIXt\"\nnames(unclass(my_time2))##  [1] \"sec\"    \"min\"    \"hour\"   \"mday\"   \"mon\"    \"year\"   \"wday\"   \"yday\"  \n##  [9] \"isdst\"  \"zone\"   \"gmtoff\""},{"path":"r-programming-language.html","id":"duration-difftime","chapter":"1 R Programming Language","heading":"1.5.4 Duration (difftime)","text":"Duration, represent amount time pairs dates date-times, stored difftimes. Difftimes built top doubles, units attribute determines integer interpreted.top, data types discussed detail chapter 24.","code":"\ntwo_days <- as.difftime(2, units = 'days')\ntwo_days## Time difference of 2 days"},{"path":"subset.html","id":"subset","chapter":"2 Subsetting R objects or accesing specific elements","heading":"2 Subsetting R objects or accesing specific elements","text":"multiple methods sub-setting R objects (vectors, matrices, data frames, lists, etc.) uses benefits. discuss one . Three operators [, [[ & $ used.","code":""},{"path":"subset.html","id":"subsetting-vectors","chapter":"2 Subsetting R objects or accesing specific elements","heading":"2.1 Subsetting vectors","text":"Let us first start sub-setting vectors, learned, atomic object R. subset vectors use [.\nuse following x vector, 6 elements (names) starting alphabets F.","code":"\nx <- c('Andrew', 'Bob', 'Chris', 'Danny', 'Edmund', 'Freddie')"},{"path":"subset.html","id":"subsetting-through-a-vector-of-positive-integers","chapter":"2 Subsetting R objects or accesing specific elements","heading":"2.1.1 Subsetting through a vector of positive integers","text":"Sub-setting positive integers give us elements given position (indices). See thisNote: Check happens integer vector repeated integers.","code":"\n# fourth element\nx[4]## [1] \"Danny\"\n# third to fifth element\nx[3:5]## [1] \"Chris\"  \"Danny\"  \"Edmund\"\n# first and fifth element\nx[c(1,3)]## [1] \"Andrew\" \"Chris\""},{"path":"subset.html","id":"subsetting-through-a-vector-of-negative-integers","chapter":"2 Subsetting R objects or accesing specific elements","heading":"2.1.2 Subsetting through a vector of negative integers","text":"Sub-setting negative integers give us elements except given indices. SeeNote: Try mixing sub-setting vector positive negative integers console check happens.","code":"\n# all elements except that at fourth\nx[-4]## [1] \"Andrew\"  \"Bob\"     \"Chris\"   \"Edmund\"  \"Freddie\"\n# all elements except third to fifth\nx[-(3:5)]## [1] \"Andrew\"  \"Bob\"     \"Freddie\"\n# all elements except first and fifth\nx[-c(1,5)]## [1] \"Bob\"     \"Chris\"   \"Danny\"   \"Freddie\""},{"path":"subset.html","id":"subsetting-through-a-logical-vector","chapter":"2 Subsetting R objects or accesing specific elements","heading":"2.1.3 Subsetting through a logical vector","text":"can also subset given vector another vector logical values .e. TRUE FALSE. can understand output/result elements places TRUE .Recycling important concept sub-setting though logical vector. recycles given logical vector length vector subset. Thus, x[TRUE] give us original x .Note: Try subset vector logical vector missing values .e. NA along TRUE /FALSE console check happens.Sub-setting logical vector important used sub-setting method see subsequent chapter/sections filter vector basis conditions.","code":"\n# First, third and fifth element only\nx[c(TRUE, FALSE, TRUE, TRUE, FALSE, FALSE)]## [1] \"Andrew\" \"Chris\"  \"Danny\"\nx[TRUE]## [1] \"Andrew\"  \"Bob\"     \"Chris\"   \"Danny\"   \"Edmund\"  \"Freddie\"\nx[c(TRUE, FALSE)] # will give elements at odd indices## [1] \"Andrew\" \"Chris\"  \"Edmund\""},{"path":"subset.html","id":"subsetting-through-a-character-vector","chapter":"2 Subsetting R objects or accesing specific elements","heading":"2.1.4 Subsetting through a character vector","text":"method used given vector named. can pass desired names inside [] get/filter desired elements. See example.Note used quotes method sub-setting. can use method names saved another variable. See thisNote: Similar positive integer indexing get repeated values character vector repeated names.two methods indexing used frequently important know debugging code sometimes subset vector may NULL zero","code":"\n# let us create a named vector `y`\ny <- setNames(1:6, LETTERS[1:6])\n# display `y`\ny## A B C D E F \n## 1 2 3 4 5 6\n# subset elements named `A` and `C`\ny[c('A', 'C')]## A C \n## 1 3\nvar <- c('A', 'C', 'E')\n# subset those elements from `y` which are named as per `var`\ny[var] # notice that since `var` is a variable, we have not used quotes.## A C E \n## 1 3 5\ny[c(\"A\", \"A\", \"C\", \"A\")]## A A C A \n## 1 1 3 1"},{"path":"subset.html","id":"subsetting-through-nothing","chapter":"2 Subsetting R objects or accesing specific elements","heading":"2.1.5 Subsetting through nothing","text":"Indexing nothing .e. simply [] give us original vector.","code":"\nx[]## [1] \"Andrew\"  \"Bob\"     \"Chris\"   \"Danny\"   \"Edmund\"  \"Freddie\""},{"path":"subset.html","id":"subsetting-through-zero","chapter":"2 Subsetting R objects or accesing specific elements","heading":"2.1.6 Subsetting through Zero","text":"Sub-setting NULL 0 give us zero length vector.important interesting note subsetting NULL NULL zero length vector instead.","code":"\nx[NULL]## character(0)\ny[0]## named integer(0)\nis.null(x[NULL])## [1] FALSE"},{"path":"subset.html","id":"subsetting-matrices-and-arrays","chapter":"2 Subsetting R objects or accesing specific elements","heading":"2.2 Subsetting Matrices and arrays","text":"can subset higher dimensional structures (Matrix - 2 dimensional arrays - dimension greater 2) using () multiple vectors, (ii) single vector (iii) matrix.Let us first create 5x5 matrix say mat elements named \\(A_{mn}\\) m denote row number n denote column number.","code":"##      [,1]  [,2]  [,3]  [,4]  [,5] \n## [1,] \"A11\" \"A12\" \"A13\" \"A14\" \"A15\"\n## [2,] \"A21\" \"A22\" \"A23\" \"A24\" \"A25\"\n## [3,] \"A31\" \"A32\" \"A33\" \"A34\" \"A35\"\n## [4,] \"A41\" \"A42\" \"A43\" \"A44\" \"A45\"\n## [5,] \"A51\" \"A52\" \"A53\" \"A54\" \"A55\""},{"path":"subset.html","id":"indexing-through-multiple-vectors","chapter":"2 Subsetting R objects or accesing specific elements","heading":"2.2.1 Indexing through Multiple vectors","text":"extension sub-setting methods explained vector. objects higher dimensionality provide one vector dimension. Blank values, may understood (ref - sub-setting nothing explained ) nothing return dimension complete.idea can extended named matrix also.example must noticed indexing objects higher dimensionality may return objects lower dimensionality. E.g. sub-setting matrix may return vector. can control dimensionality reduction argument drop=FALSE default TRUE may thus introduce bugs code.","code":"\n# first and second row with third and fifth column\nmat[1:2, c(3,5)]##      [,1]  [,2] \n## [1,] \"A13\" \"A15\"\n## [2,] \"A23\" \"A25\"\n# third to fifth column, all rows\nmat[,3:5]##      [,1]  [,2]  [,3] \n## [1,] \"A13\" \"A14\" \"A15\"\n## [2,] \"A23\" \"A24\" \"A25\"\n## [3,] \"A33\" \"A34\" \"A35\"\n## [4,] \"A43\" \"A44\" \"A45\"\n## [5,] \"A53\" \"A54\" \"A55\"\n# all columns except third\nmat[, -3]##      [,1]  [,2]  [,3]  [,4] \n## [1,] \"A11\" \"A12\" \"A14\" \"A15\"\n## [2,] \"A21\" \"A22\" \"A24\" \"A25\"\n## [3,] \"A31\" \"A32\" \"A34\" \"A35\"\n## [4,] \"A41\" \"A42\" \"A44\" \"A45\"\n## [5,] \"A51\" \"A52\" \"A54\" \"A55\"\n# Odd rows, all columns\nmat[c(TRUE, FALSE),]##      [,1]  [,2]  [,3]  [,4]  [,5] \n## [1,] \"A11\" \"A12\" \"A13\" \"A14\" \"A15\"\n## [2,] \"A31\" \"A32\" \"A33\" \"A34\" \"A35\"\n## [3,] \"A51\" \"A52\" \"A53\" \"A54\" \"A55\"\n# First create a named matrix\nrownames(mat) <- paste0(\"Row\", 1:5)\ncolnames(mat) <- paste0(\"Col\", 1:5)\nmat##      Col1  Col2  Col3  Col4  Col5 \n## Row1 \"A11\" \"A12\" \"A13\" \"A14\" \"A15\"\n## Row2 \"A21\" \"A22\" \"A23\" \"A24\" \"A25\"\n## Row3 \"A31\" \"A32\" \"A33\" \"A34\" \"A35\"\n## Row4 \"A41\" \"A42\" \"A43\" \"A44\" \"A45\"\n## Row5 \"A51\" \"A52\" \"A53\" \"A54\" \"A55\"\n# filter desired rows/columns\nmat[c(\"Row1\"), c(\"Col2\", \"Col3\")]##  Col2  Col3 \n## \"A12\" \"A13\"\nmat[c(\"Row1\"), c(\"Col2\", \"Col3\"), drop=FALSE]##      Col2  Col3 \n## Row1 \"A12\" \"A13\"\n#check this\ndim(mat[c(\"Row1\"), c(\"Col2\", \"Col3\"), drop=FALSE])## [1] 1 2\n#versus this\ndim(mat[c(\"Row1\"), c(\"Col2\", \"Col3\")])## NULL"},{"path":"subset.html","id":"subsetting-through-one-vector","chapter":"2 Subsetting R objects or accesing specific elements","heading":"2.2.2 Subsetting through one vector","text":"now clear objects higher dimensionality like matrices, array actually vectors core r, displayed acting like objects one dimension. sub-setting single vector objects coerce behavior objects vectors give output exactly shown previous section.","code":"\nmat[c(1, 10, 15, 25)]## [1] \"A11\" \"A52\" \"A53\" \"A55\"\n# OR\nmat[c(TRUE, FALSE)]##  [1] \"A11\" \"A31\" \"A51\" \"A22\" \"A42\" \"A13\" \"A33\" \"A53\" \"A24\" \"A44\" \"A15\" \"A35\"\n## [13] \"A55\""},{"path":"subset.html","id":"subsetting-through-a-matrix","chapter":"2 Subsetting R objects or accesing specific elements","heading":"2.2.3 Subsetting through a matrix","text":"can also subset objects higher dimensionality integer matrix (number columns equal dimensions). words, subset matrix (2D) help matrix need 2 column matrix first column indicate row number second column indicate column number. See","code":"\nselection_matrix <- matrix(c(1,1, # Element at Row 1 Col 1\n                             2,2, # Element at Row 2 Col 2\n                             3,3), # Element at Row 3 Col 3\n                           ncol = 2, \n                           byrow = TRUE)\nmat[selection_matrix]## [1] \"A11\" \"A22\" \"A33\""},{"path":"subset.html","id":"subsetting-lists","chapter":"2 Subsetting R objects or accesing specific elements","heading":"2.3 Subsetting lists","text":"List sub-setting can done using either [], [[]] $. understand difference , let us consider one one. done earlier let us consider list 4 elements - one vector, one matrix, one list one data frame. now let us consider list unnamed.","code":"\nmy_list <- list(\n  11:20,                                                       # first element\n  outer(1:4, 1:4, FUN = function(x, y) paste0('B', x, y)),     # second element\n  list(LETTERS[1:8], TRUE),                                    # third element  \n  data.frame(col1 = letters[1:4], col2 = 5:8)                  # fourth element\n)\n# display the list\nmy_list## [[1]]\n##  [1] 11 12 13 14 15 16 17 18 19 20\n## \n## [[2]]\n##      [,1]  [,2]  [,3]  [,4] \n## [1,] \"B11\" \"B12\" \"B13\" \"B14\"\n## [2,] \"B21\" \"B22\" \"B23\" \"B24\"\n## [3,] \"B31\" \"B32\" \"B33\" \"B34\"\n## [4,] \"B41\" \"B42\" \"B43\" \"B44\"\n## \n## [[3]]\n## [[3]][[1]]\n## [1] \"A\" \"B\" \"C\" \"D\" \"E\" \"F\" \"G\" \"H\"\n## \n## [[3]][[2]]\n## [1] TRUE\n## \n## \n## [[4]]\n##   col1 col2\n## 1    a    5\n## 2    b    6\n## 3    c    7\n## 4    d    8"},{"path":"subset.html","id":"subsetting-lists-with","chapter":"2 Subsetting R objects or accesing specific elements","heading":"2.3.1 Subsetting lists with []","text":"Sub-setting lists [] always result list containing desired element(s).can apply ideas vector sub-setting explained earlier list sub-setting. output also list containing one items.","code":"\nmy_list[2]## [[1]]\n##      [,1]  [,2]  [,3]  [,4] \n## [1,] \"B11\" \"B12\" \"B13\" \"B14\"\n## [2,] \"B21\" \"B22\" \"B23\" \"B24\"\n## [3,] \"B31\" \"B32\" \"B33\" \"B34\"\n## [4,] \"B41\" \"B42\" \"B43\" \"B44\"\nclass(my_list[1])## [1] \"list\""},{"path":"subset.html","id":"subsetting-lists-with-1","chapter":"2 Subsetting R objects or accesing specific elements","heading":"2.3.2 Subsetting lists with [[]]","text":"Sub-setting list [[]] return specific item (per index given) output type specific item.Notice difference outputs created my_list[2] my_list[[2]] 2 code blocks.Needless say, one index/subset lists using multiple indices. Check my_list[[1:2]] console results may think.","code":"\nmy_list[[2]]##      [,1]  [,2]  [,3]  [,4] \n## [1,] \"B11\" \"B12\" \"B13\" \"B14\"\n## [2,] \"B21\" \"B22\" \"B23\" \"B24\"\n## [3,] \"B31\" \"B32\" \"B33\" \"B34\"\n## [4,] \"B41\" \"B42\" \"B43\" \"B44\"\nclass(my_list[[4]])## [1] \"data.frame\""},{"path":"subset.html","id":"chaining-or-multiple-subsetting","chapter":"2 Subsetting R objects or accesing specific elements","heading":"2.3.3 Chaining or multiple subsetting","text":"can subset/index vector/variable R using chaining .e. combining one methods discussed .","code":"\n# third element of second element\nmy_list[[2]][3] # recall that by default matrix is by column## [1] \"B31\"\n# or\nmy_list[[2]][1:3,2:4]##      [,1]  [,2]  [,3] \n## [1,] \"B12\" \"B13\" \"B14\"\n## [2,] \"B22\" \"B23\" \"B24\"\n## [3,] \"B32\" \"B33\" \"B34\""},{"path":"subset.html","id":"subsetting-with","chapter":"2 Subsetting R objects or accesing specific elements","heading":"2.3.4 Subsetting with $","text":"$ shorthand operator: x$y roughly equivalent x[[\"y\"]]. check let us assign list names.Notice rules dimensionality reduction also applies $.Another difference [[ sub-setting versus $ sub-setting partial matching (left right ), possible $ [[. See","code":"\nnames(my_list) <- c(\"first\", \"second\", \"third\", \"fourth\")\n# Now see\nmy_list$first##  [1] 11 12 13 14 15 16 17 18 19 20\nmy_list$fourth$col2## [1] 5 6 7 8\nmy_list$fir##  [1] 11 12 13 14 15 16 17 18 19 20\nmy_list[['fir']]## NULL"},{"path":"subset.html","id":"data-frames","chapter":"2 Subsetting R objects or accesing specific elements","heading":"2.4 Data frames","text":"already explained data frames basically lists element equal length, rules sub-setting lists apply data frames. One addition data frames can also subset using rules matrix sub-setting.RememberIf sub-setting data frames single vector, data frame behave like lists, default. get output data.frame use drop= FALSE argument.however, sub-setting data frame two vectors, behave like matrices.Examples","code":"\nmtcars # it is a default data frame in r##                      mpg cyl  disp  hp drat    wt  qsec vs am gear carb\n## Mazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\n## Datsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\n## Hornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\n## Hornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\n## Valiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\n## Duster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\n## Merc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\n## Merc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\n## Merc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\n## Merc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\n## Merc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\n## Merc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\n## Merc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\n## Cadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\n## Lincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\n## Chrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\n## Fiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\n## Honda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\n## Toyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\n## Toyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\n## Dodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\n## AMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\n## Camaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\n## Pontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\n## Fiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\n## Porsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\n## Lotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\n## Ford Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\n## Ferrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\n## Maserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\n## Volvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n# list type sub-setting\nmtcars[[2]]  # second column ##  [1] 6 6 4 6 8 6 8 4 4 6 6 8 8 8 8 8 8 4 4 4 4 8 8 8 8 4 4 4 8 6 8 4\n# matrix type\nmtcars[1:4, 2:3] # first four rows with second & third columns##                cyl disp\n## Mazda RX4        6  160\n## Mazda RX4 Wag    6  160\n## Datsun 710       4  108\n## Hornet 4 Drive   6  258\n#Default\nmtcars[1:5, 2]## [1] 6 6 4 6 8\n# using drop argument\nmtcars[1:5, 2, drop = FALSE]##                   cyl\n## Mazda RX4           6\n## Mazda RX4 Wag       6\n## Datsun 710          4\n## Hornet 4 Drive      6\n## Hornet Sportabout   8"},{"path":"subset.html","id":"subsetting-and-assignment","chapter":"2 Subsetting R objects or accesing specific elements","heading":"2.5 Subsetting and assignment","text":"sub-setting seen can used assignment well.","code":"\nmy_list$first <- mtcars[1:4, 2:3]\nmy_list## $first\n##                cyl disp\n## Mazda RX4        6  160\n## Mazda RX4 Wag    6  160\n## Datsun 710       4  108\n## Hornet 4 Drive   6  258\n## \n## $second\n##      [,1]  [,2]  [,3]  [,4] \n## [1,] \"B11\" \"B12\" \"B13\" \"B14\"\n## [2,] \"B21\" \"B22\" \"B23\" \"B24\"\n## [3,] \"B31\" \"B32\" \"B33\" \"B34\"\n## [4,] \"B41\" \"B42\" \"B43\" \"B44\"\n## \n## $third\n## $third[[1]]\n## [1] \"A\" \"B\" \"C\" \"D\" \"E\" \"F\" \"G\" \"H\"\n## \n## $third[[2]]\n## [1] TRUE\n## \n## \n## $fourth\n##   col1 col2\n## 1    a    5\n## 2    b    6\n## 3    c    7\n## 4    d    8"},{"path":"func.html","id":"func","chapter":"3 Functions and operations in R","heading":"3 Functions and operations in R","text":"function? Mathematically, function \\(f\\) relationship map input \\(x\\) specific output, denoted \\(f(x)\\). two conditions .e. every input output, input passed function multiple times, produce output time. \\(x=y\\) \\(f(x)=f(y)\\).\nFigure 3.1: Author’s illustration function\nexample squaring considered numbers function. denote \\(f(x)=x^2\\). , square-root positive numbers also function.Now may one input, let us assume three inputs x, y z function’s job add three times x, two times z one time y together. write function \\(f(x,y,z) = 3x+y+2z\\). programming language pre-defined functions. inputs usually termed arguments. Normally values arguments passed users, many times ’s default value arguments. value argument supplier user/coder explicitly, function uses default value silently produces result.R’s engine calculates output per definition function gives us output. output assigned variable R displays/prints anything function performed output displayed usually, exception many times function carried silently nothing returned.chapter learn pre-defined functions shall used data analysis operations. can also define custom functions learn chapter 3.1.example, sum() predefined function available R, produces sum one vectors passed function arguments.check arguments available pre-defined function, can use another function args() take function name argument returns available arguments function.see argument (anmed argument0 na.rm default value FALSE. Actually, argument silently takes default value produces results. TRUE required value argument need explicitly mentioned.get definition existing function, may just type name without parenthesis console, definition returned output.get help existing function, refer section 0.6.","code":"\nsum(1:10, 15:45)## [1] 985\nargs(sum)## function (..., na.rm = FALSE) \n## NULL\nsum(1:10, NA)## [1] NA\nsum(1:10, NA, na.rm = TRUE)## [1] 55\nsum## function (..., na.rm = FALSE)  .Primitive(\"sum\")"},{"path":"func.html","id":"cust","chapter":"3 Functions and operations in R","heading":"3.1 Custom Functions","text":"One R’s greatest strengths user’s ability add functions. fact, many functions R functions existing functions. structure function looks like :Note: Objects function local function. object returned can data type, scalar list.Let’s take look example. create function take 3 numbers, give output adding thrice first, second twice third.arguments provided named, take arguments order defined.However, can provide named arguments order. See thisPartial matching names also allowed. ExampleWe can also provide default values argument. default values however, overridden specific values given. See example.may functions require argument. See example","code":"myfunctionname <- function(arg1, arg2, ... ){\n  statements\n  return(object)\n}\nmy_fun1 <- function(first,second,third){\n  first*3+second+third*2\n}\n# let's check whether it is working as desired\nmy_fun1(3,1,10)## [1] 30\nmy_fun1(second=3, first=1, third=10)## [1] 26\nmy_fun1(sec=3,fir=1,thi=10)## [1] 26\n# let's create a new function which adds twice the second argument to first argument, which in turn by default is 10\nmy_fun2 <- function(first=10, second){\n  first+second*2\n}\nmy_fun2(second = 10)## [1] 30\nmy_fun2(1, 10)## [1] 21\nmy_fun3 <- function(){\n  print('Hi')\n}\nmy_fun3()## [1] \"Hi\""},{"path":"func.html","id":"special-argument-ellipsis-...","chapter":"3 Functions and operations in R","heading":"Special argument ellipsis ...","text":"searching help function r, may came across something like sum(..., na.rm = FALSE). three dots ... referred ellipsis. Basically means function designed take number named unnamed arguments.Thus means can provide number arguments place .... Now point noted values agruments occurring ... must named. See example-Now can even use three dots custom functions. Just unpack writing actual statement function. See simple example-","code":"\nsum(1:100, NA, TRUE)## [1] NA\nsum(1:100, NA, na.rm = TRUE)## [1] 5050\nmy_ellipsis_func <- function(...){\n  l <- list(...) # unpack ellipsis\n  length(l) # return length of l\n}\nmy_ellipsis_func(1:10, 11:20, 'a string') # we are passing three arguments## [1] 3"},{"path":"func.html","id":"environment-issues","chapter":"3 Functions and operations in R","heading":"Environment issues","text":"argument values saved/updated global environment. See exampleEven create another variable inside function, variable available outside function’s environment.however, want create variable (update existing variable) inside function intentionally, may use forced assignment denoted <<-. See exampleAs already stated, can create object type using custom function.","code":"\nx <- 10\nmy_fun4 <- function(x){\n  x*2\n}\nmy_fun4(2)## [1] 4\nx## [1] 10\ny <- 5\nmy_fun5 <- function(){\n  y <- 1\n  return(y)\n}\nmy_fun5()## [1] 1\ny## [1] 5\ny <- 5\nmy_fun5 <- function(){\n  y <<- 1\n  return(y)\n}\nmy_fun5()## [1] 1\ny## [1] 1\nmy_list_fun <- function(x){\n  list(sum=sum(x),\n       mean = mean(x),\n       sd = sd(x))\n}\nmy_list_fun(1:10)## $sum\n## [1] 55\n## \n## $mean\n## [1] 5.5\n## \n## $sd\n## [1] 3.02765"},{"path":"existing-and-useful-functions-in-base-r.html","id":"existing-and-useful-functions-in-base-r","chapter":"4 Existing and useful functions in base R","heading":"4 Existing and useful functions in base R","text":"R lot inbuilt/existing functions useful therefore good know . Let us discuss existing functions useful data analytics allied jobs.Firstly, let’s learn logical operators useful check various conditions. doesn’t know operators , may simply think operators special kind functions exactly two arguments.","code":""},{"path":"existing-and-useful-functions-in-base-r.html","id":"conditions-and-logical-operatorsoperands","chapter":"4 Existing and useful functions in base R","heading":"4.1 Conditions and logical operators/operands","text":"Table 4.1:  Conditions logical operators/operands","code":""},{"path":"existing-and-useful-functions-in-base-r.html","id":"vectorisation-of-operations-and-functions","chapter":"4 Existing and useful functions in base R","heading":"Vectorisation of operations and functions","text":"mentioned operators vectorised. Except || && return vector length comparing. Check","code":"\nLETTERS[1:4] == letters[1:4]## [1] FALSE FALSE FALSE FALSE\n10:1 >= 1:10##  [1]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE\n# TRUE will act as 1 and FALSE as 0\nx <- c(TRUE, FALSE, FALSE, TRUE)\ny <- c(1, 0, 1, 10)\nx == y## [1]  TRUE  TRUE FALSE FALSE\n# Examples of element wise operations\nx & y## [1]  TRUE FALSE FALSE  TRUE\nx | y## [1]  TRUE FALSE  TRUE  TRUE\n# character strings may be checked for alphabetic order\n'ABCD' >= 'AACD'## [1] TRUE"},{"path":"existing-and-useful-functions-in-base-r.html","id":"recycling","chapter":"4 Existing and useful functions in base R","heading":"4.2 Recycling","text":"Recycling rules apply two vectors equal length. See examples.operator %% behaves slightly different . searches element LHS RHS gives result logical vector equal length LHS vector. See examples carefully.","code":"\n# Notice that results are displayed silently\nLETTERS[1:4] == 'A'## [1]  TRUE FALSE FALSE FALSE\n#Notice that results are displayed with a warning\nLETTERS[1:5] == LETTERS[1:3]## Warning in LETTERS[1:5] == LETTERS[1:3]: longer object length is not a multiple\n## of shorter object length## [1]  TRUE  TRUE  TRUE FALSE FALSE\n'A' %in% LETTERS## [1] TRUE\nLETTERS %in% LETTERS[1:4]##  [1]  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n## [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n## [25] FALSE FALSE"},{"path":"existing-and-useful-functions-in-base-r.html","id":"handling-missing-values-na-in-these-operations","chapter":"4 Existing and useful functions in base R","heading":"4.3 Handling Missing values NA in these operations","text":"checking condition TRUE FALSE missing values NA /NaN handled carefully bug may introduced. See examples-Thus, condition evaluated vector, can NA output along TRUE FALSE. See exampleThese missing values however behaves slightly different logical operators & |. See examples.","code":"\nFALSE != NA## [1] NA\nTRUE != NA## [1] NA\nx <- c(1, 5, 15, NA, 2, 3)\nx <= 5## [1]  TRUE  TRUE FALSE    NA  TRUE  TRUE\nTRUE | NA## [1] TRUE\nFALSE & NA## [1] FALSE"},{"path":"existing-and-useful-functions-in-base-r.html","id":"use-of-above-logical-operators-for-subsetting","chapter":"4 Existing and useful functions in base R","heading":"4.4 Use of above logical operators for subsetting","text":"Since logical operations vectors gives logical vector output, can used sub-setting well. See examples.","code":"\nmy_ages <- c(40, 45, 31, 51, 25, 27, 59, 45)\n# filter ages greater than or equal to 30\nmy_ages[my_ages >= 30]## [1] 40 45 31 51 59 45\nmy_names <- c(\"Andrew\", \"Bob\", \"Carl\", \"Daven\", \"Earl\")\n# filter names which start with alphabet either A, B or C\nmy_names[my_names <= \"D\"]## [1] \"Andrew\" \"Bob\"    \"Carl\""},{"path":"existing-and-useful-functions-in-base-r.html","id":"conditions-with-ifelse","chapter":"4 Existing and useful functions in base R","heading":"4.5 Conditions with ifelse","text":"Syntax ifelse(test, yes, ) used return value (shape test) filled elements selected either yes depending whether elements test TRUE FALSE. See example","code":"\nx <- c(1:5, NA, 16:20)\nifelse(x>5, 'Greater than 5', 'Upto 5')##  [1] \"Upto 5\"         \"Upto 5\"         \"Upto 5\"         \"Upto 5\"        \n##  [5] \"Upto 5\"         NA               \"Greater than 5\" \"Greater than 5\"\n##  [9] \"Greater than 5\" \"Greater than 5\" \"Greater than 5\""},{"path":"existing-and-useful-functions-in-base-r.html","id":"functions-all-and-any","chapter":"4 Existing and useful functions in base R","heading":"4.6 Functions all() and any()","text":"shortcut functions tell us whether elements given object TRUE. See exampleAll mentioned operators (along listed section 1.1) vectorised. Check examples.Recycling also applies mathematical operators. See examples notice R gives results silently warning.-mentioned operators/functions may also used matrices, arrays larger dimension, since already seen matrices/arrays actually vectors core.","code":"\nx <- 11:20\nall(x > 5)## [1] TRUE\nany(x > 20)## [1] FALSE\nx <- 1:5\ny <- 6:10\n\nx + y## [1]  7  9 11 13 15\nx - y## [1] -5 -5 -5 -5 -5\nx * y## [1]  6 14 24 36 50\nx / y## [1] 0.1666667 0.2857143 0.3750000 0.4444444 0.5000000\nx ^ y## [1]       1     128    6561  262144 9765625\n# Caution: here RHS is not a vector\ny %% 3## [1] 0 1 2 0 1\ny %/% 3## [1] 2 2 2 3 3\n10:15 + 4## [1] 14 15 16 17 18 19\n100:110 - 50##  [1] 50 51 52 53 54 55 56 57 58 59 60\n# when length of one vector is multiple of length of smaller vector\nx <- c(5, 2, 7, 9)\ny <- c(7, 8)\nx + y## [1] 12 10 14 17\n# when length of one vector is not multiple of length of smaller vector\nx + c(1, 2, 3)## Warning in x + c(1, 2, 3): longer object length is not a multiple of shorter\n## object length## [1]  6  4 10 10\nmat1 <- matrix()"},{"path":"existing-and-useful-functions-in-base-r.html","id":"common-arithmetical-functions","chapter":"4 Existing and useful functions in base R","heading":"4.7 Common arithmetical Functions","text":"Table 4.2:  Common Arithmetical FunctionsSee examples.","code":"\nsum(1:100, 1:10)## [1] 5105\nMat1 <- matrix(1:10, nrow = 2)\nMat2 <- matrix(1:4, nrow = 2)\n\nprod(Mat1, Mat2)## [1] 87091200\nsqrt(Mat2)##          [,1]     [,2]\n## [1,] 1.000000 1.732051\n## [2,] 1.414214 2.000000\nlog10(Mat1)##         [,1]      [,2]      [,3]     [,4]      [,5]\n## [1,] 0.00000 0.4771213 0.6989700 0.845098 0.9542425\n## [2,] 0.30103 0.6020600 0.7781513 0.903090 1.0000000\nfactorial(10:1)##  [1] 3628800  362880   40320    5040     720     120      24       6       2\n## [10]       1"},{"path":"existing-and-useful-functions-in-base-r.html","id":"missing-values","chapter":"4 Existing and useful functions in base R","heading":"4.7.1 Missing values","text":"vector calculating sum etc., missing values, use argument na.rm = TRUE functions (Check documentation functions individually ). See examples -","code":"\nx <- c(1:50, NA)\nsum(x)## [1] NA\nsum(x, na.rm = TRUE)## [1] 1275\nmean(x, na.rm = TRUE)## [1] 25.5"},{"path":"existing-and-useful-functions-in-base-r.html","id":"some-statistical-functions","chapter":"4 Existing and useful functions in base R","heading":"4.8 Some Statistical functions","text":"Table 4.3:  commonly used Statistical FunctionsExamples-","code":"\nmedian(1:100)## [1] 50.5\nrange(1:100, 45, 789)## [1]   1 789\nquantile(1:100)##     0%    25%    50%    75%   100% \n##   1.00  25.75  50.50  75.25 100.00\nquantile(0:100, probs = 1:10 / 10)##  10%  20%  30%  40%  50%  60%  70%  80%  90% 100% \n##   10   20   30   40   50   60   70   80   90  100"},{"path":"existing-and-useful-functions-in-base-r.html","id":"prob","chapter":"4 Existing and useful functions in base R","heading":"4.9 Functions related to sampling and probability distributions","text":"","code":""},{"path":"existing-and-useful-functions-in-base-r.html","id":"set-the-random-seed-with-set.seed","chapter":"4 Existing and useful functions in base R","heading":"4.9.1 Set the random seed with set.seed()","text":"way specify random seed integer vector, containing random number generator (RNG) state random number generation R. given output makes code reproducible use.","code":""},{"path":"existing-and-useful-functions-in-base-r.html","id":"generate-random-numbers-with-rnorm-runif-rpois-etc.","chapter":"4 Existing and useful functions in base R","heading":"4.9.2 Generate random numbers with rnorm() / runif() / rpois() etc.","text":"Used generate random numbers normal, uniform poisson distributions respectively. course numerous functions calculate random numbers calculate probability, density probability distributions (binomial, t), beyond scope book. E.g.","code":"\nrnorm(n=10) #default mean is 0 and SD is 1##  [1]  1.43970538  0.29334271  0.08667814 -1.80015849  0.41030548 -0.88027141\n##  [7]  1.01739508 -0.21150205 -0.31840665  0.66373749\nrnorm(n=10) # notice these will produce different results each time.##  [1] -0.2505991 -0.7407645  0.4356199 -1.3957920 -1.6417699 -0.4235112\n##  [7]  0.1605122 -2.2504796 -0.7415453 -0.9622846\n# If however seed is fixed as above, these will be reproducible.\nset.seed(123)\nrunif(10) # default min and max are 0 and 1 respectively##  [1] 0.2875775 0.7883051 0.4089769 0.8830174 0.9404673 0.0455565 0.5281055\n##  [8] 0.8924190 0.5514350 0.4566147\nset.seed(123)\nrunif(10)##  [1] 0.2875775 0.7883051 0.4089769 0.8830174 0.9404673 0.0455565 0.5281055\n##  [8] 0.8924190 0.5514350 0.4566147"},{"path":"existing-and-useful-functions-in-base-r.html","id":"random-sample-with-sample","chapter":"4 Existing and useful functions in base R","heading":"4.9.3 Random Sample with sample()","text":"Used take sample specified size elements x using either without replacement. E.g.sampling proportionate given probabilities can provided prob argument.","code":"\nset.seed(123)\nsample(LETTERS, 5, replace = FALSE)## [1] \"O\" \"S\" \"N\" \"C\" \"J\"\nset.seed(111)\nsample(LETTERS, 15, replace = TRUE)##  [1] \"N\" \"T\" \"S\" \"O\" \"Y\" \"E\" \"C\" \"H\" \"Z\" \"Q\" \"M\" \"J\" \"D\" \"O\" \"H\"\nset.seed(12)\nsample(LETTERS, 5, replace = FALSE, prob = 1:26)## [1] \"Z\" \"K\" \"F\" \"V\" \"X\""},{"path":"existing-and-useful-functions-in-base-r.html","id":"other-mathematical-functions","chapter":"4 Existing and useful functions in base R","heading":"4.10 Other Mathematical functions","text":"","code":""},{"path":"existing-and-useful-functions-in-base-r.html","id":"progressive-calculations-with-cumsum-cumprod","chapter":"4 Existing and useful functions in base R","heading":"4.10.1 Progressive calculations with cumsum() /cumprod()","text":"Used calculate running total product. Output vector length equal input vector.similar functions like cummax() (cumulative maximum) cummin() may also useful.","code":"\ncumsum(1:10)##  [1]  1  3  6 10 15 21 28 36 45 55\ncumprod(-5:5)##  [1]   -5   20  -60  120 -120    0    0    0    0    0    0\nset.seed(1)\nx <- sample(1:100, 10)\ncummin(x)##  [1] 68 39  1  1  1  1  1  1  1  1\ncummax(x)##  [1] 68 68 68 68 87 87 87 87 87 87"},{"path":"existing-and-useful-functions-in-base-r.html","id":"progressive-difference-diff","chapter":"4 Existing and useful functions in base R","heading":"4.10.2 Progressive difference diff()","text":"Used calculate running difference (difference two consecutive elements) given numeric vector. Output shorter one element. E.g.","code":"\nset.seed(123)\nx <- rnorm(10)\nx##  [1] -0.56047565 -0.23017749  1.55870831  0.07050839  0.12928774  1.71506499\n##  [7]  0.46091621 -1.26506123 -0.68685285 -0.44566197\ndiff(x)## [1]  0.33029816  1.78888580 -1.48819992  0.05877934  1.58577725 -1.25414878\n## [7] -1.72597744  0.57820838  0.24119088\nlength(diff(x))## [1] 9"},{"path":"existing-and-useful-functions-in-base-r.html","id":"string-manipulation-functions","chapter":"4 Existing and useful functions in base R","heading":"4.11 String Manipulation functions","text":"","code":""},{"path":"existing-and-useful-functions-in-base-r.html","id":"concatenate-strings-with-paste-and-paste0","chapter":"4 Existing and useful functions in base R","heading":"4.11.1 Concatenate strings with paste() and paste0()","text":"R’s inbuilt function paste() concatenates element one vectors given argument. Argument sep used provide separator , default space .e. \" \". sep argument available paste0 thus concatenates elements without separator.Note: paste paste0 returns vector length equal length larger vector. Thus requirement concatenate element given vector(s), use another argument collapse. See example.","code":"\npaste(LETTERS, letters)##  [1] \"A a\" \"B b\" \"C c\" \"D d\" \"E e\" \"F f\" \"G g\" \"H h\" \"I i\" \"J j\" \"K k\" \"L l\"\n## [13] \"M m\" \"N n\" \"O o\" \"P p\" \"Q q\" \"R r\" \"S s\" \"T t\" \"U u\" \"V v\" \"W w\" \"X x\"\n## [25] \"Y y\" \"Z z\"\npaste0(letters, '_', 1:26) # check replication here##  [1] \"a_1\"  \"b_2\"  \"c_3\"  \"d_4\"  \"e_5\"  \"f_6\"  \"g_7\"  \"h_8\"  \"i_9\"  \"j_10\"\n## [11] \"k_11\" \"l_12\" \"m_13\" \"n_14\" \"o_15\" \"p_16\" \"q_17\" \"r_18\" \"s_19\" \"t_20\"\n## [21] \"u_21\" \"v_22\" \"w_23\" \"x_24\" \"y_25\" \"z_26\"\npaste0(letters, 1:26, collapse = '+')## [1] \"a1+b2+c3+d4+e5+f6+g7+h8+i9+j10+k11+l12+m13+n14+o15+p16+q17+r18+s19+t20+u21+v22+w23+x24+y25+z26\""},{"path":"existing-and-useful-functions-in-base-r.html","id":"functions-startswith-endswith","chapter":"4 Existing and useful functions in base R","heading":"4.11.2 Functions startsWith() / endsWith()","text":"check whether given string vector say x start end string (entries ) prefix suffix can use startsWith(x, prefix) endsWith(x, suffix) respectively. E.g.Note functions return logical vectors length x.","code":"\nx <- c('apples', 'oranges', 'apples and oranges', 'oranges and apples', 'apricots')\nstartsWith(x, 'apples')## [1]  TRUE FALSE  TRUE FALSE FALSE\nstartsWith(x, 'ap')## [1]  TRUE FALSE  TRUE FALSE  TRUE\nendsWith(x, 'oranges')## [1] FALSE  TRUE  TRUE FALSE FALSE"},{"path":"existing-and-useful-functions-in-base-r.html","id":"check-number-of-characters-in-string-vector-using-nchar","chapter":"4 Existing and useful functions in base R","heading":"4.11.3 Check number of characters in string vector using nchar()","text":"count number characters element string vector, say x, can use nchar(x) return vector integer types. E.g.","code":"\nnchar(x)## [1]  6  7 18 18  8\ny <- c('', ' ', '   ', NA)\nnchar(y)## [1]  0  1  3 NA"},{"path":"existing-and-useful-functions-in-base-r.html","id":"change-case-using-toupper-tolower","chapter":"4 Existing and useful functions in base R","heading":"4.11.4 Change case using toupper() / tolower()","text":"Changes case given vector UPPER lower case respectively.\nExample-","code":"\nx <- c('Andrew', 'Bob')\ntolower(x)## [1] \"andrew\" \"bob\"\ntoupper(x)## [1] \"ANDREW\" \"BOB\""},{"path":"existing-and-useful-functions-in-base-r.html","id":"extract-a-portion-of-string-using-substr","chapter":"4 Existing and useful functions in base R","heading":"Extract a portion of string using substr()","text":"extract characters given vector say x given start position stop position (integers) use substr(x, start, stop). E.g.","code":"\nsubstr(x, 2, 8)## [1] \"ndrew\" \"ob\""},{"path":"existing-and-useful-functions-in-base-r.html","id":"split-a-character-vector-using-strsplit","chapter":"4 Existing and useful functions in base R","heading":"4.11.5 Split a character vector using strsplit()","text":"split elements character vector x sub-strings according matches sub-string split within . E.g.Notice output list type.","code":"\nstrsplit(x, split = ' ')## [[1]]\n## [1] \"Andrew\"\n## \n## [[2]]\n## [1] \"Bob\""},{"path":"existing-and-useful-functions-in-base-r.html","id":"replace-portions-of-string-vectors-sub-gsub","chapter":"4 Existing and useful functions in base R","heading":"4.11.6 Replace portions of string vectors sub() / gsub()","text":"two functions used perform replacement first matches respectively. E.g.","code":"\n#Replace only first match\nsub(pattern = 'B', replacement = '12', x, ignore.case = TRUE)## [1] \"Andrew\" \"12ob\"\n# Replace all matches\ngsub(pattern = 'B', replacement = '12', x, ignore.case = TRUE)## [1] \"Andrew\" \"12o12\""},{"path":"existing-and-useful-functions-in-base-r.html","id":"match-patterns-using-grep-grepl-regexpr-gregexpr","chapter":"4 Existing and useful functions in base R","heading":"4.11.7 Match patterns using grep() / grepl() / regexpr() / gregexpr()","text":"functions used match string passed argument pattern string vector. four however, differ output/results. E.g.Note regexpr() outputs character position first instance pattern match within elements given vector.\ngregexpr() regexpr() finds instances pattern. Output list format. E.g.","code":"\ngrep(pattern = 'an', x) # will give indices.  ## integer(0)\n#                         Output will be integer vector and length may be shorter than that of `x`\ngrepl(pattern = 'an', x) # will give a logical vector of same length as `x`## [1] FALSE FALSE\nregexpr(pattern = 'an', x) # output will have multiple attributes## [1] -1 -1\n## attr(,\"match.length\")\n## [1] -1 -1\n## attr(,\"index.type\")\n## [1] \"chars\"\n## attr(,\"useBytes\")\n## [1] TRUE\ngregexpr(pattern = 'an', x)## [[1]]\n## [1] -1\n## attr(,\"match.length\")\n## [1] -1\n## attr(,\"index.type\")\n## [1] \"chars\"\n## attr(,\"useBytes\")\n## [1] TRUE\n## \n## [[2]]\n## [1] -1\n## attr(,\"match.length\")\n## [1] -1\n## attr(,\"index.type\")\n## [1] \"chars\"\n## attr(,\"useBytes\")\n## [1] TRUE"},{"path":"existing-and-useful-functions-in-base-r.html","id":"other-functions","chapter":"4 Existing and useful functions in base R","heading":"4.12 Other functions","text":"","code":""},{"path":"existing-and-useful-functions-in-base-r.html","id":"transpose-a-matrix-using-t","chapter":"4 Existing and useful functions in base R","heading":"4.12.1 Transpose a matrix using t()","text":"Used return transpose given matrix. E.g.","code":"\nmat <- outer(1:5, 1:5, FUN = \\(x, y) paste0('A', x, y))\nmat##      [,1]  [,2]  [,3]  [,4]  [,5] \n## [1,] \"A11\" \"A12\" \"A13\" \"A14\" \"A15\"\n## [2,] \"A21\" \"A22\" \"A23\" \"A24\" \"A25\"\n## [3,] \"A31\" \"A32\" \"A33\" \"A34\" \"A35\"\n## [4,] \"A41\" \"A42\" \"A43\" \"A44\" \"A45\"\n## [5,] \"A51\" \"A52\" \"A53\" \"A54\" \"A55\"\nt(mat)##      [,1]  [,2]  [,3]  [,4]  [,5] \n## [1,] \"A11\" \"A21\" \"A31\" \"A41\" \"A51\"\n## [2,] \"A12\" \"A22\" \"A32\" \"A42\" \"A52\"\n## [3,] \"A13\" \"A23\" \"A33\" \"A43\" \"A53\"\n## [4,] \"A14\" \"A24\" \"A34\" \"A44\" \"A54\"\n## [5,] \"A15\" \"A25\" \"A35\" \"A45\" \"A55\""},{"path":"existing-and-useful-functions-in-base-r.html","id":"generate-a-frequency-table-using-table","chapter":"4 Existing and useful functions in base R","heading":"4.12.2 Generate a frequency table using table()","text":"Returns frequency/contingency table counts combination factor levels. E.g.one argument passed-","code":"\nset.seed(123)\nx <- sample(LETTERS[1:5], 100, replace = TRUE)\ntable(x)## x\n##  A  B  C  D  E \n## 21 20 23 17 19\nset.seed(1234)\ndf <- data.frame(State_code = x,\n                 Code2 = sample(LETTERS[11:15], 100, replace = TRUE))\nmy_table <- table(df$State_code, df$Code2)\nmy_table##    \n##     K L M N O\n##   A 5 5 4 4 3\n##   B 4 3 6 2 5\n##   C 6 3 3 6 5\n##   D 2 2 4 6 3\n##   E 2 6 4 4 3"},{"path":"existing-and-useful-functions-in-base-r.html","id":"generate-proportion-of-frequencies-using-prop.table","chapter":"4 Existing and useful functions in base R","heading":"4.12.3 Generate proportion of frequencies using prop.table()","text":"function takes table object input calculate proportion frequencies.","code":"\nprop.table(my_table)##    \n##        K    L    M    N    O\n##   A 0.05 0.05 0.04 0.04 0.03\n##   B 0.04 0.03 0.06 0.02 0.05\n##   C 0.06 0.03 0.03 0.06 0.05\n##   D 0.02 0.02 0.04 0.06 0.03\n##   E 0.02 0.06 0.04 0.04 0.03"},{"path":"existing-and-useful-functions-in-base-r.html","id":"column-wise-or-row-wise-sums-using-colsums-rowsums","chapter":"4 Existing and useful functions in base R","heading":"4.12.4 Column-wise or Row-wise sums using colSums() / rowSums()","text":"Used sum rows/columns matrix/data.frame. E.g.Note Similar colSums()/ rowSums() also colMeans() rowMeans().","code":"\n# Row sums\nrowSums(my_table)##  A  B  C  D  E \n## 21 20 23 17 19\n# Col sums\ncolSums(my_table)##  K  L  M  N  O \n## 19 19 21 22 19\nrowMeans(my_table)##   A   B   C   D   E \n## 4.2 4.0 4.6 3.4 3.8"},{"path":"existing-and-useful-functions-in-base-r.html","id":"extract-unique-values-using-unique","chapter":"4 Existing and useful functions in base R","heading":"4.12.5 Extract unique values using unique()","text":"Used extract unique values/elements given vector. E.g.","code":"\nunique(x) # note the output## [1] \"C\" \"B\" \"E\" \"D\" \"A\""},{"path":"existing-and-useful-functions-in-base-r.html","id":"check-if-two-vectors-are-identical-using-identical","chapter":"4 Existing and useful functions in base R","heading":"4.12.6 Check if two vectors are identical using identical()","text":"Used check whether two given vectors/objects identical.","code":"\nidentical(unique(x), LETTERS)## [1] FALSE"},{"path":"existing-and-useful-functions-in-base-r.html","id":"retreive-duplicate-items-in-a-vector-using-duplicated","chapter":"4 Existing and useful functions in base R","heading":"4.12.7 Retreive duplicate items in a vector using duplicated()","text":"Used check elements already appeared vector thus duplicate.","code":"\nset.seed(123)\nx <- sample(LETTERS[1:5], 8, replace = TRUE)\nx## [1] \"C\" \"C\" \"B\" \"B\" \"C\" \"E\" \"D\" \"A\"\nduplicated(x)## [1] FALSE  TRUE FALSE  TRUE  TRUE FALSE FALSE FALSE"},{"path":"existing-and-useful-functions-in-base-r.html","id":"generate-sequences-using-other-objects-with-seq_len-seq_along","chapter":"4 Existing and useful functions in base R","heading":"4.12.8 Generate sequences using other objects with seq_len() / seq_along()","text":"Used generate sequence given integer length starting 1, length equal given vector, respectively. E.g.","code":"\nseq_len(5)## [1] 1 2 3 4 5\nx <- c('Andrew', 'Bob')\nseq_along(x)## [1] 1 2"},{"path":"existing-and-useful-functions-in-base-r.html","id":"cutt","chapter":"4 Existing and useful functions in base R","heading":"4.12.9 Divide a vector into categories (factor) using cut()","text":"function divides range x intervals codes values x according interval fall. leftmost interval corresponds level one, next leftmost level two . output vector type factor.Example-1:Example-2:Note: output factor ordered.","code":"\nx <- c(1,2,3,4,5,2,3,4,5,6,7)\ncut(x, 3)##  [1] (0.994,3] (0.994,3] (0.994,3] (3,5]     (3,5]     (0.994,3] (0.994,3]\n##  [8] (3,5]     (3,5]     (5,7.01]  (5,7.01] \n## Levels: (0.994,3] (3,5] (5,7.01]\ncut(x, 3, dig.lab = 1, ordered_result = TRUE)##  [1] (1,3] (1,3] (1,3] (3,5] (3,5] (1,3] (1,3] (3,5] (3,5] (5,7] (5,7]\n## Levels: (1,3] < (3,5] < (5,7]"},{"path":"existing-and-useful-functions-in-base-r.html","id":"scale-the-columns-of-a-matrix-using-scale","chapter":"4 Existing and useful functions in base R","heading":"4.12.10 Scale the columns of a matrix using scale()","text":"Used scale columns numeric matrix.Note: output always matrix type two attributes. See example","code":"\nx <- matrix(1:10, ncol = 2)\nx##      [,1] [,2]\n## [1,]    1    6\n## [2,]    2    7\n## [3,]    3    8\n## [4,]    4    9\n## [5,]    5   10\nscale(x)##            [,1]       [,2]\n## [1,] -1.2649111 -1.2649111\n## [2,] -0.6324555 -0.6324555\n## [3,]  0.0000000  0.0000000\n## [4,]  0.6324555  0.6324555\n## [5,]  1.2649111  1.2649111\n## attr(,\"scaled:center\")\n## [1] 3 8\n## attr(,\"scaled:scale\")\n## [1] 1.581139 1.581139\nscale(1:5)##            [,1]\n## [1,] -1.2649111\n## [2,] -0.6324555\n## [3,]  0.0000000\n## [4,]  0.6324555\n## [5,]  1.2649111\n## attr(,\"scaled:center\")\n## [1] 3\n## attr(,\"scaled:scale\")\n## [1] 1.581139"},{"path":"existing-and-useful-functions-in-base-r.html","id":"cat","chapter":"4 Existing and useful functions in base R","heading":"4.12.11 Output the results using cat()","text":"Outputs objects, concatenating representations. cat performs much less conversion print.Note: indices now printed. cat may print objects also. Example-2:cat useful print special characters. Example-3:","code":"\ncat('ABCD')## ABCD\ncat(month.name)## January February March April May June July August September October November December\ncat('Budget Allocation is \\u20b91.5 crore')## Budget Allocation is ₹1.5 crore"},{"path":"existing-and-useful-functions-in-base-r.html","id":"sort-a-vector-using-sort","chapter":"4 Existing and useful functions in base R","heading":"4.12.12 Sort a vector using sort()","text":"Used sort given vector. Example-1:Argumemt decreasing = TRUE used sort vector descending order instead default ascending order.\nExample-2:","code":"\nvec <- c(5, 8, 4, 1, 6)\nsort(vec)## [1] 1 4 5 6 8\nsort(vec, decreasing =  TRUE)## [1] 8 6 5 4 1"},{"path":"existing-and-useful-functions-in-base-r.html","id":"arrange-the-elements-of-a-vector-using-order","chapter":"4 Existing and useful functions in base R","heading":"4.12.13 Arrange the elements of a vector using order()","text":"contrast sort() explained , order() returns indices given vector ascending order. ExampleThus, sort(vec) essentially perform operations vec[order(vec)]. may check-","code":"\norder(vec)## [1] 4 3 1 5 2\nidentical(vec[order(vec)], sort(vec))## [1] TRUE"},{"path":"existing-and-useful-functions-in-base-r.html","id":"check-structure-using-str","chapter":"4 Existing and useful functions in base R","heading":"4.12.14 Check structure using str()","text":"short str confused strings instead short structure. Thus, str returns structure given object. ExampleExtremely useful need inspect data frames.","code":"\nstr(vec)##  num [1:5] 5 8 4 1 6\nstr(iris)## 'data.frame':    150 obs. of  5 variables:\n##  $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n##  $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n##  $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n##  $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n##  $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ..."},{"path":"existing-and-useful-functions-in-base-r.html","id":"generate-a-summary-using-summary","chapter":"4 Existing and useful functions in base R","heading":"Generate a summary using summary()","text":"addition str explained , summary() also useful getting result summaries given objects. Example-1: given object vectorWe observe numeric vector passed, produces quantile summary. Example-2: input object data frame.","code":"\nsummary(vec)##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##     1.0     4.0     5.0     4.8     6.0     8.0\nsummary(iris)##   Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n##  Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n##  1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n##  Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n##  Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n##  3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n##  Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n##        Species  \n##  setosa    :50  \n##  versicolor:50  \n##  virginica :50  \n##                 \n##                 \n## "},{"path":"pipes-in-r.html","id":"pipes-in-r","chapter":"5 Pipes in R","heading":"5 Pipes in R","text":"Now like introduce concept pipes R. two types pipes used-|> native pipe R. introduced R version 4.1%>% pipe introduced magrittr package5, now part tidyverse use extensively data analysis tasks.\nFigure 5.1: Magrittr, R package, named surrealist painter Rene’ Magritte. painting self captioned, ‘pipe.’\nActually %>% predecessor native R’s pipe |>. pipes powerful tools clearly expressing sequence operations transform object, without need actually creating object step. Let us understand concept following example. Suppose, three functions say FIRST , SECOND THIRD object OBJ sequence. order operations either like-creating intermediate objects, instead actually need intermediate objects.actually require OBJ1 OBJ2. cases either compromise readability code .e. inside create unwanted objects. Pipes actually mitigate issues simultaneously. pipes can write operations either -diagrammatic representation given figure 5.2.\nFigure 5.2: diagrammtic illustation Pipe concept base R tidyverse\nNow two questions may arise -multiple arguments passed operations?difference two pipes? yes, better pros cons ?answer questions, discuss pipes separately.\nFigure 5.3: Using pipes R\n","code":"THIRD(SECOND(FIRST(OBJ)))OBJ1 <- FIRST(OBJ)\nOBJ2 <- SECOND(OBJ1)\nOBJ3 <- THIRD(OBJ2)OBJ1 |> FIRST() |> SECOND() |> THIRD()\nOBJ1 %>% FIRST() %>% SECOND() %>% THIRD()"},{"path":"pipes-in-r.html","id":"magrittrdplyr-pipe","chapter":"5 Pipes in R","heading":"5.1 Magrittr/Dplyr pipe %>%","text":"Pipes usually pass result previous operation silently first argument next/right expression. data %>% filter(col == '') means filter(data, col==''). may cases result previous (LHS) expression required passed second argument RHS expression. simple example may function lm, data argument second argument. cases can make use special placeholder . result LHS specifically. words aforesaid filter example can written placeholder data %>% filter(. , col == ''). Now using placeholder can use result LHS wherever want. See exampleThus x %>% f(y) equivalent f(x, y) x %>% f(y, .) equivalent f(y, x).","code":"\niris %>% lm(Sepal.Length ~ Sepal.Width, data = .)## \n## Call:\n## lm(formula = Sepal.Length ~ Sepal.Width, data = .)\n## \n## Coefficients:\n## (Intercept)  Sepal.Width  \n##      6.5262      -0.2234"},{"path":"pipes-in-r.html","id":"base-r-pipe-version-4.2.0","chapter":"5 Pipes in R","heading":"5.2 Base R pipe |> (Version 4.2.0 +)","text":"R version 4.2.0 introduced concept placeholder _ similar dplyr/magrittr, differences.argument _ used, must named. f(y, z = x) can written x |> f(y, z= _).requirement named argument dplyr pipe. essentially, iris %>% lm(Sepal.Length ~ Sepal.Width, .) also work. base R iris |> lm(Sepal.Length ~ Sepal.Width, _) work throw error. Thus, cases argument placeholder named, use anonymous function. See -Type ?`|>` console see help page details.","code":"\niris |> lm(Sepal.Length ~ Sepal.Width, data = _) |> summary()## \n## Call:\n## lm(formula = Sepal.Length ~ Sepal.Width, data = iris)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1.5561 -0.6333 -0.1120  0.5579  2.2226 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)   6.5262     0.4789   13.63   <2e-16 ***\n## Sepal.Width  -0.2234     0.1551   -1.44    0.152    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.8251 on 148 degrees of freedom\n## Multiple R-squared:  0.01382,    Adjusted R-squared:  0.007159 \n## F-statistic: 2.074 on 1 and 148 DF,  p-value: 0.1519# placeholder without named argument\niris |> lm(Sepal.Length ~ Sepal.Width, _)\n# Correct way to use unnamed argument\niris |> {\\(.x) lm(Sepal.Length ~ Sepal.Width, .x)}() |> summary()## \n## Call:\n## lm(formula = Sepal.Length ~ Sepal.Width, data = .x)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1.5561 -0.6333 -0.1120  0.5579  2.2226 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)   6.5262     0.4789   13.63   <2e-16 ***\n## Sepal.Width  -0.2234     0.1551   -1.44    0.152    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.8251 on 148 degrees of freedom\n## Multiple R-squared:  0.01382,    Adjusted R-squared:  0.007159 \n## F-statistic: 2.074 on 1 and 148 DF,  p-value: 0.1519"},{"path":"control-statements.html","id":"control-statements","chapter":"6 Control statements","heading":"6 Control statements","text":"previous chapter learnt many useful pre-built functions R. chapter learn create customized functions suited needs.Though core concepts programming language, yet reading chapter advised better understanding better application using r data analytics.","code":""},{"path":"control-statements.html","id":"control-flowloops","chapter":"6 Control statements","heading":"6.1 Control flow/Loops","text":"","code":""},{"path":"control-statements.html","id":"if-else","chapter":"6 Control statements","heading":"if else","text":"basic form(s) else statement R, -, test true, do_this_if_true performed optionally test true else_do_this performed. See example-Note /else evaluated single TRUE FALSE .e. control flow vectorised case ifelse() function vectorised.","code":"if (test) do_this_if_true\nif (test) do_this_if_true else else_do_this\nx <- 50\nif(x < 10){\n  'Smaller than 10'\n} else {\n  '10 or more'\n}## [1] \"10 or more\""},{"path":"control-statements.html","id":"for-loop","chapter":"6 Control statements","heading":"for loop","text":"loops r used iterate given items. , basic structure loops -Thus, item vector, perform_some_action called ; updating value item time. can understood following simple example-Conventionally used example iterate given vector 1:3, however symbol may also used.use name existing variable item iterate given object, loop assigns item current environment, overwriting existing variable name. See example -\nFigure 6.1: Diagrammatic representation Loop\nidea can also used iterate object number times want. See two examples.Example-1Example-2There 2 ways terminate loop early-next exits current iteration onlybreak breaks entire loop.See examples.Example-1Example-2","code":"for(item in vector) perform_some_action\n\n# OR\n\nfor(item in vector) {\n  perform_some_action\n}\nfor(i in 1:3){\n  print(i)\n}## [1] 1\n## [1] 2\n## [1] 3\nfor(item in 1:3){\n  print(item)\n}## [1] 1\n## [1] 2\n## [1] 3\nx <- 500\nfor(x in 1:3){\n  # do nothing\n}\nx## [1] 3\nmy_names <- c('Andrew', 'Bob', 'Charles', 'Dylan', 'Edward')\n# If we want first 4 elements\nfor(i in 1:4){\n  print(my_names[i])\n}## [1] \"Andrew\"\n## [1] \"Bob\"\n## [1] \"Charles\"\n## [1] \"Dylan\"\n# if we want all elements\nfor(i in seq_along(my_names)){\n  print(my_names[i])\n}## [1] \"Andrew\"\n## [1] \"Bob\"\n## [1] \"Charles\"\n## [1] \"Dylan\"\n## [1] \"Edward\"\nfor(i in 1:5){\n  if (i == 4){\n    next\n  }\n  print(i)\n}## [1] 1\n## [1] 2\n## [1] 3\n## [1] 5\nfor(i in 1:5){\n  if (i == 4){\n    break\n  }\n  print(i)\n}## [1] 1\n## [1] 2\n## [1] 3"},{"path":"control-statements.html","id":"while-loop","chapter":"6 Control statements","heading":"while loop","text":"seen loop used iterate set known values least known number times. however, want perform iterative action unknown number times, may use loop iterates till given condition TRUE. Another option repeat loop can used iterate number times till encounters break.basic syntax loop -See examples-Example-1We may check value executing loopExample-2:Note: make sure statements inside brackets modify condition sooner later given condition longer TRUE otherwise loop never end go forever.\nFigure 6.2: Author’s illustration Loop\nLooping R can inefficient time consuming ’re processing rows columns large data-sets. Even one greatest feature R parallel processing vector objects. Thus, whenever possible, ’s better use R’s built-numerical character functions conjunction apply family functions.(discuss detail chapter related functional programming)","code":"while (condition) action\ni <- 1\nwhile(i <=4){\n  print(LETTERS[i])\n  i <- i+1\n}## [1] \"A\"\n## [1] \"B\"\n## [1] \"C\"\n## [1] \"D\"\ni## [1] 5\ni <- 4\nwhile(i >=0){\n  print(LETTERS[i])\n  i <- i-1\n}## [1] \"D\"\n## [1] \"C\"\n## [1] \"B\"\n## [1] \"A\"\n## character(0)"},{"path":"functional-programming.html","id":"functional-programming","chapter":"7 Functional Programming","heading":"7 Functional Programming","text":"","code":""},{"path":"functional-programming.html","id":"what-is-functional-programming","chapter":"7 Functional Programming","heading":"7.1 What is functional programming?","text":"Conceptually functional programming philosophy based lambda calculus. Lambda calculus framework developed Alonzo Church6 study computations functions.Functional programming programming paradigm try bind everything pure mathematical functions style. declarative type programming style. main focus solve contrast imperative style main focus solve. elaborated definition readers may see wikipedia link. Simply putting functional programming like something repeatedly declarative style. , functions primary method carry tasks. actions just implementations functions using.Functional programming use high order functions. high order function actually function accepts function argument, returns function; short, function operates upon function. already seen one example may without noticing , args() function take function argument turn return arguments.Let us learn bit .","code":""},{"path":"functional-programming.html","id":"usage-of-functional-programming-in-r","chapter":"7 Functional Programming","heading":"7.1.1 Usage of functional programming in R","text":"Strictly speaking R functional programming language. already seen one greatest strengths R parallel operations vectors. fact need functional programming concurrency parallelism required. Till now seen functions work atomic objects (vectors, matrices, arrays, etc.), working functions recursive objects .e. lists? Check example (console)-course, can solve problem using loops. SeeConsider another list, want calculate mean element list.course, may use loop , R operations can done easily apply group functions, one famous used features R.","code":"list1 <- list(50000, 5000, 56)\nsum(list1)\nlist1 <- list(50000, 5000, 56)\n# for loop strategy\nx <- c()\nfor(i in seq_along(list1)){\n  x[i] <- list1[[i]]\n}\nsum(x)## [1] 55056\nlist2 <- list(\n  1:10,\n  11:20,\n  21:30\n)"},{"path":"functional-programming.html","id":"apply-family-of-functions","chapter":"7 Functional Programming","heading":"7.2 apply family of functions","text":"First functions apply() works matrices/data frames.","code":""},{"path":"functional-programming.html","id":"function-apply","chapter":"7 Functional Programming","heading":"7.2.1 Function apply()","text":"basic syntax apply iswherem matrixMARGIN dimension. want apply function row use 1 else column-wise use 2FUN desired function want applyf_args optional set arguments, needed supplied fun.illustrative construction apply function can seen 7.1.\nFigure 7.1: Illustration function apply\nCheck exampleNote: rowMeans(mat) example given similar results, sake simplicity provided simplest example.note may also write customised function argument. See another example, take sum squares row. may define custom function purpose apply .need writing custom function hand may eliminated function defined used . may write anonymous function directly apply syntax -R version 4.1 onwards R devised shorthand style defining inline custom functions, can write backslash .e. \\ instead writing function. written expression -","code":"apply(m, MARGIN, FUN, f_args)\n(mat <- matrix(1:10, nrow = 5))##      [,1] [,2]\n## [1,]    1    6\n## [2,]    2    7\n## [3,]    3    8\n## [4,]    4    9\n## [5,]    5   10\napply(mat, 1, mean)## [1] 3.5 4.5 5.5 6.5 7.5\nmy_fun <- function(x){\n  sum(x^2)\n}\napply(mat, 1, my_fun)## [1]  37  53  73  97 125\napply(mat, 1, FUN = function(x) sum(x^2))## [1]  37  53  73  97 125\napply(mat, 1, FUN = \\(x) sum(x^2))\n## [1]  37  53  73  97 125"},{"path":"functional-programming.html","id":"apply-need-not-necessarily-output-vectors-only","chapter":"7 Functional Programming","heading":"apply() need not necessarily output vectors only","text":"FUN applied rows/columns matrix outputs vector length 1, output matrix format. thing note matrix displayed columnwise always irrespective fact whether MARGIN 1 2. easy example shown using function like sqrt, apply(matrix, MARGIN, sqrt) work like sqrt(matrix) . let’s take different example. Suppose want calculate column-wise cumulative sum given matrix.output eaxctly desired. , requirement take row-wise cumulative sum?may now noticed output actually transpose expecting. Actually output iteration apply function displayed one column always. Let us check understanding one example taking function may give output dependent input vector length.Thus may conclude -FUN outputs scalar, output apply vector length equal \nnumber rows input matrix given MARGIN selected 1,\nnumber columns input matrix given MARGIN selected 2.\nnumber rows input matrix given MARGIN selected 1,number columns input matrix given MARGIN selected 2.FUN outputs vector(length >1) output apply matrix number columns equal -\nnumber rows input matrix given MARGIN selected 1,\nnumber columns input matrix given MARGIN selected 2.\nnumber rows input matrix given MARGIN selected 1,number columns input matrix given MARGIN selected 2.tabulated table 7.1.Table 7.1:  Relation input output data structure applyWe may thus careful getting output apply function may lead introduction bug code.","code":"\napply(mat, 2, cumsum)##      [,1] [,2]\n## [1,]    1    6\n## [2,]    3   13\n## [3,]    6   21\n## [4,]   10   30\n## [5,]   15   40\napply(mat, 1, cumsum)##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    2    3    4    5\n## [2,]    7    9   11   13   15\nset.seed(1)\napply(mat, 1, sample, 4, TRUE)##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    7    8    4   10\n## [2,]    6    2    8    4   10\n## [3,]    1    2    3    4   10\n## [4,]    1    2    3    9    5"},{"path":"functional-programming.html","id":"apply-on-data-frames","chapter":"7 Functional Programming","heading":"apply() on data frames","text":"Now know data frames despite special type lists also behave like matrices, may use apply data frames . See example.","code":"\n(my_df <- as.data.frame(mat))##   V1 V2\n## 1  1  6\n## 2  2  7\n## 3  3  8\n## 4  4  9\n## 5  5 10\napply(my_df, 2, sum)## V1 V2 \n## 15 40"},{"path":"functional-programming.html","id":"function-lapply","chapter":"7 Functional Programming","heading":"7.2.2 Function lapply()","text":"Another cousin apply lapply can thought apply lists. name suggests applied lists instead matrices. Now since data frame also list lapply can applied . basic syntax lapply() -wherel listFUN desired function want applyf_args optional set arguments, needed supplied fun.may noted MRAGIN argument available . See examples.Now may noticed two things -output list type.Unlike apply MARGIN passed/available , applies FUN every element list. consider data.frame list column separate element list. FUN applied rows data.frame.Thus lapply() -loops list, iterating element listthen applies function FUN elementand returns list.Example-2: Let’s try find type column given data frame.Similar apply can define FUN inline (anonymously) also. Example-3:Example-4:Note even lapply applied vector, returns list .","code":"lapply(l, FUN, f_args)\nlapply(my_df, sum)## $V1\n## [1] 15\n## \n## $V2\n## [1] 40\nlapply(iris, typeof)## $Sepal.Length\n## [1] \"double\"\n## \n## $Sepal.Width\n## [1] \"double\"\n## \n## $Petal.Length\n## [1] \"double\"\n## \n## $Petal.Width\n## [1] \"double\"\n## \n## $Species\n## [1] \"integer\"\nlapply(my_df, \\(a) a^2) ## $V1\n## [1]  1  4  9 16 25\n## \n## $V2\n## [1]  36  49  64  81 100\nset.seed(1)\nlapply(1:4, runif, min=0, max=10)## [[1]]\n## [1] 2.655087\n## \n## [[2]]\n## [1] 3.721239 5.728534\n## \n## [[3]]\n## [1] 9.082078 2.016819 8.983897\n## \n## [[4]]\n## [1] 9.4467527 6.6079779 6.2911404 0.6178627"},{"path":"functional-programming.html","id":"function-sapply","chapter":"7 Functional Programming","heading":"7.2.3 Function sapply()","text":"much difference lapply() sapply(), sapply actually simplified lapply. simplifies argument much possible.Example:","code":"\nsapply(my_df, sum)## V1 V2 \n## 15 40"},{"path":"functional-programming.html","id":"other-loop-functions","chapter":"7 Functional Programming","heading":"7.3 Other loop functions","text":"","code":""},{"path":"functional-programming.html","id":"function-replicate","chapter":"7 Functional Programming","heading":"7.3.1 Function replicate()","text":"Function replicate()(replicate) used repeated evaluation expression. Syntax iswhere -n integer denoting number replicationsexpr expression evaluate repeatedlysimplify takes either ‘character’ ‘logical’ value indicate whether results simplified.Example:","code":"replicate(n, expr, simplify = \"array\")\nset.seed(123)\n# Default value of simplify will simplify the results as much possible\nreplicate(5, runif(3))##           [,1]      [,2]      [,3]      [,4]      [,5]\n## [1,] 0.2875775 0.8830174 0.5281055 0.4566147 0.6775706\n## [2,] 0.7883051 0.9404673 0.8924190 0.9568333 0.5726334\n## [3,] 0.4089769 0.0455565 0.5514350 0.4533342 0.1029247\n# Notice the difference with simplify=FALSE\nreplicate(3, runif(5), simplify = FALSE)## [[1]]\n## [1] 0.89982497 0.24608773 0.04205953 0.32792072 0.95450365\n## \n## [[2]]\n## [1] 0.8895393 0.6928034 0.6405068 0.9942698 0.6557058\n## \n## [[3]]\n## [1] 0.7085305 0.5440660 0.5941420 0.2891597 0.1471136"},{"path":"functional-programming.html","id":"function-split","chapter":"7 Functional Programming","heading":"7.3.2 Function split()","text":"split()(split() function) function takes object (vector ) splits groups determined given factor. basic syntax -wherex input object - vector list data.framef factor list factors. factor provided, coerced factor.drop argument indicates whether empty factors dropped.Example: (divide given list alternate elements)-Example-2: Find sum every odd even number 1:100-Example-3: Find mean mpg column splitting mtcars data cyl","code":"split(x, f, drop=FALSE, ...)\nsplit(LETTERS, rep(1:2, 13))## $`1`\n##  [1] \"A\" \"C\" \"E\" \"G\" \"I\" \"K\" \"M\" \"O\" \"Q\" \"S\" \"U\" \"W\" \"Y\"\n## \n## $`2`\n##  [1] \"B\" \"D\" \"F\" \"H\" \"J\" \"L\" \"N\" \"P\" \"R\" \"T\" \"V\" \"X\" \"Z\"\nsplit(1:100, (1:100) %% 2) |> lapply(sum)## $`0`\n## [1] 2550\n## \n## $`1`\n## [1] 2500\nsplit(mtcars$mpg, mtcars$cyl) |> sapply(mean)##        4        6        8 \n## 26.66364 19.74286 15.10000"},{"path":"functional-programming.html","id":"tapply","chapter":"7 Functional Programming","heading":"7.3.3 tapply()","text":"tapply() function(tapply() function) can thought combination split sapply vectors, exactly used example. actually applies function subsets given vector. basic syntax --X vectorINDEX factor list factorsFUN function applied... arguments, , FUN passedsimplify TRUE simplifies result.See exampleNeedless say simplify FALSE results simplified. See example-","code":"tapply(X, INDEX, FUN = NULL, ..., default = NA, simplify = TRUE)\ntapply(mtcars$mpg, mtcars$cyl, mean)##        4        6        8 \n## 26.66364 19.74286 15.10000\n# month-wise mean of temperatures from `airquality` data\ntapply(airquality$Temp, airquality$Month, mean, simplify = FALSE)## $`5`\n## [1] 65.54839\n## \n## $`6`\n## [1] 79.1\n## \n## $`7`\n## [1] 83.90323\n## \n## $`8`\n## [1] 83.96774\n## \n## $`9`\n## [1] 76.9"},{"path":"functional-programming.html","id":"by-function","chapter":"7 Functional Programming","heading":"7.3.4 by() function","text":"(() function) works something like tapply difference input object data.frame. See example","code":"\n# Split the data by `cyl` column and subset first six rows only\nby(mtcars, mtcars$cyl, head)## mtcars$cyl: 4\n##                 mpg cyl  disp hp drat    wt  qsec vs am gear carb\n## Datsun 710     22.8   4 108.0 93 3.85 2.320 18.61  1  1    4    1\n## Merc 240D      24.4   4 146.7 62 3.69 3.190 20.00  1  0    4    2\n## Merc 230       22.8   4 140.8 95 3.92 3.150 22.90  1  0    4    2\n## Fiat 128       32.4   4  78.7 66 4.08 2.200 19.47  1  1    4    1\n## Honda Civic    30.4   4  75.7 52 4.93 1.615 18.52  1  1    4    2\n## Toyota Corolla 33.9   4  71.1 65 4.22 1.835 19.90  1  1    4    1\n## ------------------------------------------------------------ \n## mtcars$cyl: 6\n##                 mpg cyl  disp  hp drat    wt  qsec vs am gear carb\n## Mazda RX4      21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag  21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\n## Hornet 4 Drive 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\n## Valiant        18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\n## Merc 280       19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\n## Merc 280C      17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\n## ------------------------------------------------------------ \n## mtcars$cyl: 8\n##                     mpg cyl  disp  hp drat   wt  qsec vs am gear carb\n## Hornet Sportabout  18.7   8 360.0 175 3.15 3.44 17.02  0  0    3    2\n## Duster 360         14.3   8 360.0 245 3.21 3.57 15.84  0  0    3    4\n## Merc 450SE         16.4   8 275.8 180 3.07 4.07 17.40  0  0    3    3\n## Merc 450SL         17.3   8 275.8 180 3.07 3.73 17.60  0  0    3    3\n## Merc 450SLC        15.2   8 275.8 180 3.07 3.78 18.00  0  0    3    3\n## Cadillac Fleetwood 10.4   8 472.0 205 2.93 5.25 17.98  0  0    3    4"},{"path":"functional-programming.html","id":"specifying-the-output-type-with-vapply","chapter":"7 Functional Programming","heading":"7.3.5 Specifying the output type with vapply()","text":"Function vapply()(vapply() function) works exactly like sapply() described , difference type return value (output) specifically provided FUN.VALUE argument. syntax -argument FUN.VALUE provide format type output. See example.FUN.VALUE = double(1) specifically provided output double type length 1. case find range column-try function dataset mixed type columns like iris dataset, vapply throw error.","code":"vapply(X, FUN, FUN.VALUE, ..., USE.NAMES = TRUE)\nvapply(mtcars, max, FUN.VALUE = double(1))##     mpg     cyl    disp      hp    drat      wt    qsec      vs      am    gear \n##  33.900   8.000 472.000 335.000   4.930   5.424  22.900   1.000   1.000   5.000 \n##    carb \n##   8.000\nvapply(mtcars, range, FUN.VALUE = double(2))##       mpg cyl  disp  hp drat    wt qsec vs am gear carb\n## [1,] 10.4   4  71.1  52 2.76 1.513 14.5  0  0    3    1\n## [2,] 33.9   8 472.0 335 4.93 5.424 22.9  1  1    5    8\nvapply(iris, range, FUN.VALUE = double())## Error in vapply(iris, range, FUN.VALUE = double()): values must be length 0,\n##  but FUN(X[[1]]) result is length 2"},{"path":"functional-programming.html","id":"purrr","chapter":"7 Functional Programming","heading":"7.4 Functional Programming in purrr","text":"Package purrr7, part core tidyverse, enhances R’s functional programming (FP) toolkit providing complete consistent set tools working functions vectors.","code":"\nlibrary(purrr)"},{"path":"functional-programming.html","id":"iterate-over-single-listvector-with-map_-family-of-functions","chapter":"7 Functional Programming","heading":"7.4.1 Iterate over single list/vector with map_*() family of functions","text":"package many families functions; primary family map family functions. map_*() works nearly similar vapply can control type output. syntax style functions nearly , accept one object (list vector) .x argument, one function (alternatively formula) .f argument; outputs object specified type.ExampleNote output type list. output can simplified atomic vector can use either functions depending upon output type vector.map_lgl logical formatmap_int integer formatmap_dbl double formatmap_chr character formatmap always return list.\nSee examples.Example-1:","code":"\nmap(mtcars, .f = sum)## $mpg\n## [1] 642.9\n## \n## $cyl\n## [1] 198\n## \n## $disp\n## [1] 7383.1\n## \n## $hp\n## [1] 4694\n## \n## $drat\n## [1] 115.09\n## \n## $wt\n## [1] 102.952\n## \n## $qsec\n## [1] 571.16\n## \n## $vs\n## [1] 14\n## \n## $am\n## [1] 13\n## \n## $gear\n## [1] 118\n## \n## $carb\n## [1] 90\nmap_dbl(mtcars, max)##     mpg     cyl    disp      hp    drat      wt    qsec      vs      am    gear \n##  33.900   8.000 472.000 335.000   4.930   5.424  22.900   1.000   1.000   5.000 \n##    carb \n##   8.000"},{"path":"functional-programming.html","id":"iterate-over-two-or-more-listsvectors-using-map2_-pmap_-family","chapter":"7 Functional Programming","heading":"7.4.2 Iterate over two or more lists/vectors using map2_*()/ pmap_*() family","text":"far seen map_*() family functions used iterate elements list. Even extra lists/vectors provided extra arguments, used , iteration, can seen first illustration figure8 7.2.\nFigure 7.2: Working map vs map2 family functions Source Advanced R Hadley Wickham\norder iterate two vectors/lists, however, need map2_*() family functions (Refer second illustration figure 7.2).See following exampleSimilarly, iterate multiple lists use pmap_*(), difference vectors/list collectively passed pmap list. can better understood illustration used Hadley Wickham book. reference see figure 7.3.\nFigure 7.3: Working pmap family functions Source Advanced R Hadley Wickham\n","code":"\nx <- list(1, 2, 3)\ny <- list(11, 12, 13)\nmap2(x, y, `*`)## [[1]]\n## [1] 11\n## \n## [[2]]\n## [1] 24\n## \n## [[3]]\n## [1] 39"},{"path":"file.html","id":"file","chapter":"8 File handling operations in R","heading":"8 File handling operations in R","text":"chapter 9 already learned reading writing data /files. section, learn functions useful reading writing data, - changing directory, creating file, renaming file, check existence file, listing files working directory, copying files creating directories.","code":""},{"path":"file.html","id":"handling-files","chapter":"8 File handling operations in R","heading":"8.1 Handling files","text":"","code":""},{"path":"file.html","id":"creating-a-file-within-r-using-file.create","chapter":"8 File handling operations in R","heading":"8.1.1 Creating a file within R, using file.create()","text":"Using file.create() function, can create new file console. file already exists truncates. function returns TRUE logical value file created otherwise, returns FALSE.Example - following command create blank text file current working directory.","code":"\nfile.create(\"my_new_text_file.txt\")## [1] TRUE"},{"path":"file.html","id":"checking-whether-a-file-exists-using-file.exists","chapter":"8 File handling operations in R","heading":"8.1.2 Checking whether a file exists, using file.exists()","text":"Similar , can check whether file given name exists, using function file.exists(). Example-","code":"\nfile.exists(\"my_new_text_file.txt\")## [1] TRUE"},{"path":"file.html","id":"renaming-file-with-file.rename","chapter":"8 File handling operations in R","heading":"8.1.3 Renaming file with file.rename()","text":"file name can changed within R console using, function file.rename(). Basic syntax file.rename(= \"old_name\", = \"new_name\"). function return TRUE FALSE depending upon successful execution. See example","code":"\nfile.rename(from = \"my_new_text_file.txt\", to = \"my_renamed_file.csv\")## [1] TRUE\n# Check whether old file exists\nfile.exists(\"my_new_text_file.txt\")## [1] FALSE"},{"path":"file.html","id":"copying-file-with-file.copy-function","chapter":"8 File handling operations in R","heading":"8.1.4 Copying file with file.copy() function","text":"Using file.copy(= \"old_path\", = \"new_path\") syntax files can copied one directory another.","code":""},{"path":"file.html","id":"deleting-file-with-file.remove","chapter":"8 File handling operations in R","heading":"8.1.5 Deleting file with file.remove()","text":"syntax function, removes file given name, also simple. Example-Check whether file really deleted.","code":"\nfile.remove(\"my_renamed_file.csv\")## [1] TRUE\nfile.exists(\"my_renamed_file.csv\")## [1] FALSE"},{"path":"file.html","id":"handling-directories","chapter":"8 File handling operations in R","heading":"8.2 Handling directories","text":"","code":""},{"path":"file.html","id":"getset-path-of-current-working-directory-using-getwd-setwd","chapter":"8 File handling operations in R","heading":"8.2.1 Get/Set path of current working directory using getwd()/ setwd()","text":"can check/get path current working directory (wd short) character vector, using getwd() function.Similarly, using setwd(\"given\\\\path\\\\\") can change current working directory.Two things noted - Either path given using forward slash / backslash \\ used need escaped, using extra \\ \\ escape character R.","code":"\ngetwd()## [1] \"D:/OneDrive - A c GeM CAG of INDIA/new book\""},{"path":"file.html","id":"create-new-directory-using-dir.create-and-other-operations","chapter":"8 File handling operations in R","heading":"8.2.2 Create new directory using dir.create() and other operations","text":"new directory can created using function dir.create(). Example- command create new directory named ‘new_dir’ current working directory. TRUE returned, directory given name created.can check whether directory named ‘new_dir’ exists current working directory, using function dir.exists() function. Function return either TRUE FALSE.can also check files exists current working directory/directory using list.files() function.given directory can removed using unlink() function specifically setting argument recursive TRUE. Example","code":"\ndir.create(\"new_dir\")\ndir.exists(\"new_dir\")## [1] TRUE\nany(list.files() == 'new_dir')## [1] TRUE\nunlink(\"new_dir\", recursive = TRUE)\ndir.exists(\"new_dir\")## [1] FALSE"},{"path":"file.html","id":"an-important-function-for-opening-a-dialog-box-for-selecting-files-and-folder","chapter":"8 File handling operations in R","heading":"8.3 An important function for opening a dialog box for selecting files and folder","text":"may use either choose.dir() file.choose(), let user select directory file /choice respectively.Try console","code":"list.files(choose.dir())\nfile.copy(from = file.choose(), to = \"new_name\")"},{"path":"file.html","id":"other-useful-functions-for-listingremoving-variables","chapter":"8 File handling operations in R","heading":"8.4 Other useful functions for listing/removing variables","text":"can list variables available current environment using function ls(). Another function rm() remove given variables. command like rm(lm()) remove available variables environment(Use caution erase saved variables/data).","code":""},{"path":"file.html","id":"using-save-to-save-objectscollection-of-objects","chapter":"8 File handling operations in R","heading":"8.5 Using save() to save objects/collection of objects","text":"can save objects using function save() saves objects disk later usage. saved objects can retrieved using load() function. See example-","code":"\nh <- hist(Nile)\nsave(h, file=\"nile_hist\")\nrm(h)\nany(ls() == 'h')## [1] FALSE\nload(\"nile_hist\")\nany(ls() == 'h')## [1] TRUE"},{"path":"file.html","id":"working-with-projects-in-rstudio","chapter":"8 File handling operations in R","heading":"8.6 Working with Projects in RStudio","text":"working project, often requirement keep scripts, data, results, charts, figures, etc. single place. R studio thus, concept working projects, associates specific directory specific project creates specific file extension .Rproj, can reopen complete scripts/ data/ etc. associated project.open new project Rstudio, click file menu New Project. Check screenshot Figure 8.1.\nFigure 8.1: Creating Projects R Studio\nCreating projects notice file extension .Rproj created R Studio selected directory/location project.resume working project directory, either double click file, open project using file menu .e. file \\(->\\) Open Project.R Studio, can create project -new directory (first option Figure 8.1)Existing Directory (second option Figure 8.1)cloning Version Control (like Git, etc.) shown third option Figure 8.1.Working projects, one data analytics projects, many benefits apart streamlining workflow, leading improved productivity focus analysis tasks, working directly using r scripts RStudio-Can get rid hassles setting/finding working directory starting session RStudio.paths now relative project directory. saves us lot hassles finding path directory scripts data well outputs (data, plots, etc.) saved.code related outputs now better reproducibility.project can R environment, preventing conflicts packages dependencies.Working projects also ensures consistent setup across different machines different team members.also allows project-specific settings configurations, enhancing user experience.","code":""},{"path":"read.html","id":"read","chapter":"9 Getting data in and out of R","heading":"9 Getting data in and out of R","text":"Till now, created datasets used sample datasets available R. practical usage, hardly case get data imported R . import load data sets R, analytics tasks. Even completing analytics, summarised data, reports, charts, etc. need exported. chapter intended tasks.First section deals functions related reading external data .e. importing objects R. followed another section dealing writing data external files .e. exporting data R.","code":""},{"path":"read.html","id":"importing-external-data-in-r---base-r-methods","chapter":"9 Getting data in and out of R","heading":"9.1 Importing external data in R - Base R methods","text":"Base R many functions can fulfill nearly jobs import external data R. However, packages customised certain tasks easier way thus, also learn two packages tidyverse also.important functions base R, used frequently import external data R environment. Let us discuss one one.","code":""},{"path":"read.html","id":"reading-tables-through-read.table-andor-read.csv","chapter":"9 Getting data in and out of R","heading":"Reading tables through read.table() and/or read.csv()","text":"Basically two functions commonly used functions R, get tabular data flat files. two functions namely read.table() read.csv() used respectively read tabular data flat files simple text format (.txt) comma separated values (.csv) formats respectively. three cousins functions.read.csv2() read csv files ; used delimiter , used decimals instead.read.delim() read delimited files tab character used delimiter . decimal.read.delim2(), similarly read delimited files tab character used delimiter , decimal.important arguments functions -file name file along complete path string9.header logical value indicating first line file read header .sep string indicating separator value separate columns.colClasses character vector indicating type columns read explicitly types/formats .skip integer, indicating many rows (beginning) skipped.Readers may check results ?read.table() get complete list arguments functions.Example: Let us try download data related World Happiness Report 201=21 available data.world portal.can see dataset named wh2021 20 columns 149 rows now available environment.Tip: Use \"clipboard\" file argument pasting copied data R. E.g. Copy rows cells excel spreadsheet run command read.delim(\"clipboard\", header = FALSE).","code":"wh2021 <- read.csv(\"https://query.data.world/s/qbsbmxlfj54sl4mq3y6uxsr3pkhhmo\")\n# Check dimensions\ndim(wh2021)\n# Check column names\ncolnames(wh2021)"},{"path":"read.html","id":"read-data-into-a-vector-through-scan-or-readline","chapter":"9 Getting data in and out of R","heading":"Read data into a vector through scan() or readline()","text":"afore-mentioned functions read.table() et al used reading data tables, may also require reading data vector simple list. two functions scan() readline() used purposes.basic syntax scan() --file name file, linkwhat format/data type read. Default type doublenmax mux number data values read, default -1 means .many usefule arguments, please check results ?scan console.Example -function sometimes useful read data keyboard vector. Just use blank string \"\" file name. See exampleFunction readline() hand similar job, prompt. See example","code":"scan(file = \"\",\n     what = double(),\n     nmax = -1,\n     ...\n)scan(\"http://mattmahoney.net/iq/digits.txt\", nmax = 10)"},{"path":"read.html","id":"reading-text-files-through-readlines","chapter":"9 Getting data in and out of R","heading":"Reading text files through readLines()","text":"Function readLines() used read text lines file (connection). see action, prepare text file (say \"txt.txt\") try reading using readLines(\"txt.txt\").","code":""},{"path":"read.html","id":"exporting-data-out-of-r---base-r-methods","chapter":"9 Getting data in and out of R","heading":"9.2 Exporting data out of R - Base R methods","text":"Since nature data anaytic jobs carried R, followed reading external data followed wrangling, transformation, modelling, etc., R. Exporting files used much reading external data. Still, times, wrangled data tables need exported R. different use cases, following functions almost complete export requirements.Let’s learn .","code":""},{"path":"read.html","id":"writing-tabular-data-through-write.table-andor-write.csv","chapter":"9 Getting data in and out of R","heading":"Writing tabular data through write.table() and/or write.csv()","text":"Exporting data frames, whether cleaning, wrangling, transformation, etc., can exported using functions. Latter used write data frames csv formats specifically. syntax --x data frame object exportedfile used give file name (along path)sep separator... - many arguments used cutomised export needs. See ?write.table() full details.E.g. - following command export iris data frame iris.csv file current working directory.","code":"write.table(x, file = \"\", sep = \" \", ...)\nwrite.csv(x, file = \"\", ...)\nwrite.csv2(x, file = \"\", ...)\nwrite.csv(iris, 'iris.csv')"},{"path":"read.html","id":"writing-character-data-line-by-line-to-a-file-through-writelines","chapter":"9 Getting data in and out of R","heading":"Writing character data line by line to a file through writeLines()","text":"Similar readLines, function writeLines() write text data file given file name. Type following code console check new file name my_new_file.txt created given contents current working directory.","code":"\nwriteLines(\"Andrew 25\n           Bob 45\n           Charles 56\", \"my_new_file.txt\")"},{"path":"read.html","id":"using-dput-to-get-a-code-representation-of-r-object","chapter":"9 Getting data in and out of R","heading":"Using dput() to get a code representation of R object","text":"function output code representation given R object. function particularly useful, searching help online need give sample data reproduce problem. E.g. Stack Overflow asking solution specific problem, reproducible data needs furnished. Please also refer section 9.3.3 .Example-1:reproducing someone else’s dput-","code":"\nmy_data <- data.frame(Name = c(\"Andrew\", \"Bob\", \"Charles\"),\n                      Age = c(25, 45, 56))\ndput(my_data)## structure(list(Name = c(\"Andrew\", \"Bob\", \"Charles\"), Age = c(25, \n## 45, 56)), class = \"data.frame\", row.names = c(NA, -3L))\nnow_mydata <- structure(list(Name = c(\"Andrew\", \"Bob\", \"Charles\"), Age = c(25, \n45, 56)), class = \"data.frame\", row.names = c(NA, -3L))\n\nnow_mydata##      Name Age\n## 1  Andrew  25\n## 2     Bob  45\n## 3 Charles  56"},{"path":"read.html","id":"using-external-packages-for-readingwriting-data","chapter":"9 Getting data in and out of R","heading":"9.3 Using external packages for reading/writing data","text":"","code":""},{"path":"read.html","id":"package-readr","chapter":"9 Getting data in and out of R","heading":"9.3.1 Package readr","text":"readr package10 part core tidyverse loaded directly load library(tidyverse). provides range analogous functions reading functions base R.question may asked ’s difference two sets functions. Firstly, readr alternatives much faster base R counterparts. Secondly, provides informative problem report parsing leads unexpected results., check results examples-Note column types, parsing data frame, now printed. column types guessed readr actually. column types actually required parsed, argument col_types may used. can also use spec() function retrieve data types guessed readr later-, can modified used col_types argument. See example","code":"\nread_csv(readr_example(\"chickens.csv\"))## Rows: 5 Columns: 4\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (3): chicken, sex, motto\n## dbl (1): eggs_laid\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.## # A tibble: 5 × 4\n##   chicken                 sex     eggs_laid motto                               \n##   <chr>                   <chr>       <dbl> <chr>                               \n## 1 Foghorn Leghorn         rooster         0 That's a joke, ah say, that's a jok…\n## 2 Chicken Little          hen             3 The sky is falling!                 \n## 3 Ginger                  hen            12 Listen. We'll either die free chick…\n## 4 Camilla the Chicken     hen             7 Bawk, buck, ba-gawk.                \n## 5 Ernie The Giant Chicken rooster         0 Put Captain Solo in the cargo hold.\nwrite.csv(iris, \"iris.csv\") #write a dummy data\nspec(read_csv(\"iris.csv\"))## New names:\n## Rows: 150 Columns: 6\n## ── Column specification\n## ──────────────────────────────────────────────────────── Delimiter: \",\" chr\n## (1): Species dbl (5): ...1, Sepal.Length, Sepal.Width, Petal.Length,\n## Petal.Width\n## ℹ Use `spec()` to retrieve the full column specification for this data. ℹ\n## Specify the column types or set `show_col_types = FALSE` to quiet this message.\n## • `` -> `...1`## cols(\n##   ...1 = col_double(),\n##   Sepal.Length = col_double(),\n##   Sepal.Width = col_double(),\n##   Petal.Length = col_double(),\n##   Petal.Width = col_double(),\n##   Species = col_character()\n## )\nread_csv(\"iris.csv\",\n         col_select = 2:6,\n         col_types = cols(\n           Sepal.Length = col_double(),\n           Sepal.Width = col_double(),\n           Petal.Length = col_double(),\n           Petal.Width = col_double(),\n           Species = col_factor(levels = c('setosa', 'versicolor', 'virginica'))\n         )\n) %>% head(2)## New names:\n## • `` -> `...1`## # A tibble: 2 × 5\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n##          <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n## 1          5.1         3.5          1.4         0.2 setosa \n## 2          4.9         3            1.4         0.2 setosa"},{"path":"read.html","id":"package-readxl","chapter":"9 Getting data in and out of R","heading":"9.3.2 Package readxl","text":"package also part tidyverse one loaded specifically using library(readxl). name suggests, functions useful read/write data /excel files. Excel files (extension .xls .xlsx) slightly different way may contain several sheets data . functions read_excel(), read_xlsx, read_xls designed reading sheets excel files. syntax isTo get names sheets excel file can use excel_sheets() function library.","code":"read_excel(path, sheet = NULL, range = NULL)\nread_xlsx(path, sheet = NULL, range = NULL)\nread_xls(path, sheet = NULL, range = NULL)excel_sheets(path)"},{"path":"read.html","id":"reprex","chapter":"9 Getting data in and out of R","heading":"9.3.3 Package reprex","text":"part tidyverse, loaded specifically calling library(reprex). name reprex actually short reproducible example. useful particularly stuck problem seek online help forum Stack Overflow, R Studio Community, etc.example consoleThereafter run reprex(), small window Viewer tab opened like . Moreover, code copied clipboard.reading please refer page.11","code":"library(reprex)\n\nx <- dput(head(iris))\nx"},{"path":"data-cleaning-in-r.html","id":"data-cleaning-in-r","chapter":"10 Data Cleaning in R","heading":"10 Data Cleaning in R","text":"Data cleansing one important steps data analysis. Multiple packages available r clean data sets. One packages janitor using chapter along packages.Let’s load ","code":"\nlibrary(janitor)## \n## Attaching package: 'janitor'## The following objects are masked from 'package:stats':\n## \n##     chisq.test, fisher.test"},{"path":"data-cleaning-in-r.html","id":"cleaning-column-names.","chapter":"10 Data Cleaning in R","heading":"10.1 Cleaning Column names.","text":"know names objects R follow certain conventions like may certain special characters names. space used quoted pair backticks `. generally read data files excel, can ‘dirty’ names, clean proceeding. clean_names() come handy. E.g.Using clean_names() also pipe friendly, can clean names one step. (Results snake case)-Parses letter cases separators consistent format.Default snake_case, cases like camelCase availableHandles special characters spaces, including transliterating characters like œ oe.Appends numbers duplicated namesConverts “%” “percent” “#” “number” retain meaningSpacing (lack thereof) around numbers preserved","code":"\n# Create a data.frame with dirty names\ntest_df <- as.data.frame(matrix(ncol = 6))\n\nnames(test_df) <- c(\"firstName\", \"ábc@!*\", \"% successful (2009)\",\n                    \"REPEAT VALUE\", \"REPEAT VALUE\", \"\")\n# View this data\ntest_df##   firstName ábc@!* % successful (2009) REPEAT VALUE REPEAT VALUE   \n## 1        NA     NA                  NA           NA           NA NA\ntest_df %>% \n  clean_names()##   first_name abc percent_successful_2009 repeat_value repeat_value_2  x\n## 1         NA  NA                      NA           NA             NA NA"},{"path":"data-cleaning-in-r.html","id":"handling-duplicate-records","chapter":"10 Data Cleaning in R","heading":"10.2 Handling duplicate records","text":"janitor package, ready use function get_dupes(). allows us find “similar” observations data set based certain characteristics. Syntax pretty simple, function pipe friendly . Suppose find duplicate mtcars dataset combination wt cyl.can see returns duplicate records additional column dupe_count duplicates can analysed separately.","code":"\nmtcars %>% \n  get_dupes(wt, cyl)##     wt cyl dupe_count  mpg  disp  hp drat  qsec vs am gear carb\n## 1 3.44   6          2 19.2 167.6 123 3.92 18.30  1  0    4    4\n## 2 3.44   6          2 17.8 167.6 123 3.92 18.90  1  0    4    4\n## 3 3.57   8          2 14.3 360.0 245 3.21 15.84  0  0    3    4\n## 4 3.57   8          2 15.0 301.0 335 3.54 14.60  0  1    5    8"},{"path":"data-cleaning-in-r.html","id":"remove-constant-redundant-columns","chapter":"10 Data Cleaning in R","heading":"10.3 Remove Constant (Redundant) columns","text":"Dropping columns data.frame contain single constant value throughout easy janitor::remove_constant().","code":""},{"path":"data-cleaning-in-r.html","id":"remove-empty-rows-andor-columns","chapter":"10 Data Cleaning in R","heading":"10.4 Remove empty rows and/or columns","text":"importing messy data excel files, may get empty rows /columns. Sorting issue, easy using janitor::remove_empty().","code":""},{"path":"data-cleaning-in-r.html","id":"fix-excel-dates-stored-as-serial-numbers","chapter":"10 Data Cleaning in R","heading":"10.5 Fix excel dates stored as serial numbers","text":"loading excel files R, may sometimes noticed 41590 instead date format. Sorting issue easy janitor function excel_numeric_to_date() . Example","code":"\njanitor::excel_numeric_to_date(41590)## [1] \"2013-11-12\""},{"path":"data-cleaning-in-r.html","id":"convert-a-mix-of-date-and-datetime-formats-to-date","chapter":"10 Data Cleaning in R","heading":"10.6 Convert a mix of date and datetime formats to date","text":"Similar , can also sort , column mix different date formats, using janitor::convert_to_date() janitor::convert_to_datetime(). See Examples-Note example, created heterogeneous vector, implicit coercion rules R converted forms character .real world examples, data entered multiple machines/data points simultaneously, may column mix date formats. case, may use parse_date_time() function lubridate package. allow different formats use order agument function. Example","code":"\nunsorted_dates <- c('2018-05-31', '41590', 41590)\njanitor::convert_to_date(unsorted_dates)## [1] \"2018-05-31\" \"2013-11-12\" \"2013-11-12\"\nmixed_dates <- c(\"13-11-1991\", \"13-Sep-22\", \n                 \"20 August 2000\", \"15 August 87\", \n                 \"03/31/23\", \"12-31-2022\")\n\nlubridate::parse_date_time(mixed_dates,\n                           orders = c(\"d m y\", \"d B Y\", \"m/d/y\", \"d B y\"),\n                           locale = \"eng\")## [1] \"1991-11-13 UTC\" \"2022-09-13 UTC\" \"2000-08-20 UTC\" \"1987-08-15 UTC\"\n## [5] \"2023-03-31 UTC\" \"2022-12-31 UTC\""},{"path":"merging-large-number-of-similar-datasets-into-one.html","id":"merging-large-number-of-similar-datasets-into-one","chapter":"11 Merging large number of similar datasets into one","heading":"11 Merging large number of similar datasets into one","text":"Data preparation performing analytics important task may require time actual analytics rarely data ideal format. Importing csv flat files rather easy job. However, considering large popularity MS Excel, times data saved excel files.Sometimes, one single data frame/table divided multiple sheets one excel file whereas sometimes tables divided multiple files. discussing cases, can reduce data preparation time effectively writing code import data environment.","code":""},{"path":"merging-large-number-of-similar-datasets-into-one.html","id":"case-1-merging-multiple-excel-sheets-into-one-data-frame","chapter":"11 Merging large number of similar datasets into one","heading":"11.1 Case-1: Merging multiple excel sheets into one data frame","text":"example, let’s say multiple States’ data saved different sheet one excel file say Daur.xlsx. See preview fig 11.1.\nFigure 11.1: Preview example excel file\nuse library readxl read excel files. library bundled tidyverse part core tidyverse, loaded explicitly, though explicit download required tidyverse installed system.following steps used-Step-1: Read pathStep-2: Collect Names sheetsStep-3: Set names elements vector onto itselfStep-4: Read combine tables one. use purrr::map_dfr .can see data sheets merged table one extra column created using sheet name. name column provided .id argument. new column required, simply don’t use argument.","code":"\nlibrary(tidyverse)\nlibrary(readxl)\n# Step-1\npath <- \"data/daur1.xlsx\"\n# Step-2\nstates_data <- excel_sheets(path) %>% \n# step-3\n    set_names(., .) %>% \n# step-4\n    map_dfr(read_excel, path=path, .id = 'State_name')\n# print file\nstates_data## # A tibble: 18 × 4\n##    State_name     Year    Metric_1 Metric_2\n##    <chr>          <chr>      <dbl>    <dbl>\n##  1 Andhra Pradesh 2015-16      119      121\n##  2 Andhra Pradesh 2016-17      114      134\n##  3 Andhra Pradesh 2017-18      115      122\n##  4 Andhra Pradesh 2018-19      129      137\n##  5 Andhra Pradesh 2019-20      149      129\n##  6 Andhra Pradesh 2020-21      104      124\n##  7 Assam          2015-16      104      145\n##  8 Assam          2016-17      114      144\n##  9 Assam          2017-18      116      116\n## 10 Assam          2018-19      129      130\n## 11 Assam          2019-20      144      134\n## 12 Assam          2020-21      124      116\n## 13 Bihar          2015-16      109      148\n## 14 Bihar          2016-17      134      106\n## 15 Bihar          2017-18      131      133\n## 16 Bihar          2018-19      131      100\n## 17 Bihar          2019-20      131      127\n## 18 Bihar          2020-21      128      103"},{"path":"merging-large-number-of-similar-datasets-into-one.html","id":"case-2-merging-multiple-files-into-one-data-frame","chapter":"11 Merging large number of similar datasets into one","heading":"11.2 Case-2: Merging multiple files into one data frame","text":"Often source data split multiple files, compile one single data frame proceeding case, may collect files one directory follow steps-Step-1: Store file names using list.files()Step-2: may read files one list using either lapply purrr::map.Step-3: data structures files , can directly use purrr::map_dfr read files give us data frame. however, structure data files , may convert columns character type merging files. can thereafter proceed merging data using either purrr::map_dfr lapply combination .call.","code":""},{"path":"merging-large-number-of-similar-datasets-into-one.html","id":"case-3-split-and-save-one-data-frame-into-multiple-excelcsv-files-simultaneously.","chapter":"11 Merging large number of similar datasets into one","heading":"11.3 Case-3: Split and save one data frame into multiple excel/csv files simultaneously.","text":"example use states_data created case-1. can use following algorithmStep-1: Create vector file names using paste0\nStep-2: Split data frame list separate dataframe state\nStep-3: Write separate file using purrr::walk2()complete algorithm isWe can check 3 new files state_names filenames created data folder/directory desired.","code":"\n# step-1 : create a vector of file names (output)\nfile_names <- paste0(\"data/\", unique(states_data$State_name), \".csv\")\n\nstates_data %>% \n  group_split(State_name) %>% \n  purrr::walk2(file_names, write.csv)"},{"path":"merging-large-number-of-similar-datasets-into-one.html","id":"case-4-splitting-one-data-into-muliple-files-having-multiple-sheets","chapter":"11 Merging large number of similar datasets into one","heading":"11.4 Case-4: Splitting one data into muliple files having multiple sheets","text":"Sometimes, may require split file multiple files, simultaeously require split file multiple excel sheets. E.g. data States districts split State-wise files separate sheet district.can achieved using writexl library. case, may write custom function can job easily.function book_and_sheets designed code helps us write data say df separate files based column x files divided sheets based column y. thing remembered pass, x y arguments character strings; df variable.Example - splitting mtcars files based cyl sheets based gearWe can check three excel files created directory data/.","code":"\nlibrary(tidyverse)\nlibrary(writexl)\n\nbook_and_sheets <- function(df, x, y){\n  df_by_x <- df %>% \n    split(.[[x]])\n  \n  save_to_excel <- function(a, b){\n    a %>% \n      split(.[[y]]) %>% \n      writexl::write_xlsx(\n        path = paste0(\"data/data_by_\", b,\"_\",x, \".xlsx\")\n      )\n    \n  }\n  \n  imap(df_by_x, save_to_excel)\n}\nbook_and_sheets(mtcars, 'cyl', 'gear')## $`4`\n## [1] \"D:\\\\OneDrive - A c GeM CAG of INDIA\\\\new book\\\\Data\\\\data_by_4_cyl.xlsx\"\n## \n## $`6`\n## [1] \"D:\\\\OneDrive - A c GeM CAG of INDIA\\\\new book\\\\Data\\\\data_by_6_cyl.xlsx\"\n## \n## $`8`\n## [1] \"D:\\\\OneDrive - A c GeM CAG of INDIA\\\\new book\\\\Data\\\\data_by_8_cyl.xlsx\""},{"path":"part-ii-exploratory-data-analysis.html","id":"part-ii-exploratory-data-analysis","chapter":"Part-II: Exploratory Data Analysis","heading":"Part-II: Exploratory Data Analysis","text":"","code":""},{"path":"visualisations-in-base-r.html","id":"visualisations-in-base-r","chapter":"12 Visualisations in Base R","heading":"12 Visualisations in Base R","text":"common practice visualize data soon start analysing . small data can easily visualised data format, common opening MS Excel format, becomes hurdle look data bigger. Undoubtedly, visualization essential tool data analysis enables us see patterns relationships may apparent simple numerical summary. example, histogram can show distribution variable, can help identify outliers skewness may hidden summary statistics. Similarly, scatter plot can show relationship two variables, can help identify correlation causation may apparent summary statistics.Visualization also plays crucial role communicating data analysis results others. well-designed graph chart can convey information effectively table numbers lengthy report. can also help identify errors anomalies data facilitate better decision-making.R, base graphics system provides powerful set functions creating various types graphs plots. chapter, cover commonly used functions creating visualizations base R, including plot(), boxplot(), barplot(), hist(), pie(), dotchart() kernel density plots.Note: next chapter cover visulaisation ggplot2 library extensive, developed theortical framework known ‘grammar graphics’. grammar, created Leland Wilkinson, implemented variety data visualisation software like Tableau12, Plotly, etc.","code":""},{"path":"visualisations-in-base-r.html","id":"using-plot","chapter":"12 Visualisations in Base R","heading":"12.1 Using plot()","text":"plot() function R versatile function can create wide variety visualizations. primary purpose create scatterplots, can also create line plots, bar plots, box plots, many types plots. “plot” function can customized many ways, allowing users create high-quality visualizations tailored specific needs.basic syntax “plot” function follows:x y arguments specify variables plotted x-axis y-axis, respectively. ... argument can include wide range optional arguments customize plot. commonly used optional arguments include:xlab: Specifies label x-axis.ylab: Specifies label y-axis.main: Specifies main title plot.xlim: Specifies limits x-axis.ylim: Specifies limits y-axis.type: Specifies type plot created (e.g., \"p\" points, \"l\" lines,\"b\" points lines, etc.).col: Specifies color plot elements (e.g., points, lines, etc.).pch: Specifies symbol used points plot.cex: Specifies size plot elements (e.g., points, lines, etc.).Note: x y refer vectors instead column names, see next chapter ggplot2 use column names, instead individual vectors. ’ll understand difference following example.Example scatterplot: (create scatterplot “wt” x-axis “mpg” y-axis, axis labels main title.)Example-2: Lineplot (create line plot “pressure”13 y-axis “time” x-axis, axis labels main title.)Example-3: Bar-Plot (bar plot number cars “cyl” .e. number cylinders)\nNote: example used table() function get frequency table mtcars$cyl.Example-4: Boxplot ()Example-5: density plot\nNote used density() function within plot() create density plot.Let us learn plotting functions base R.","code":"plot(x, y, ...)\nplot(mtcars$wt, \n     mtcars$mpg, \n     xlab = \"Car Weight (1000 lbs)\", \n     ylab = \"Miles per Gallon\", \n     main = \"Scatterplot of MPG vs Car Weight\")\nplot(pressure, \n     type = \"l\", \n     xlab = \"Time\", \n     ylab = \"Pressure\", \n     main = \"Line Plot of Pressure vs Time\")\nbarplot(table(mtcars$cyl), \n        xlab = \"Number of Cylinders\", \n        ylab = \"Frequency\", \n        main = \"Barplot of Number of Cars by Cylinders\")\n# Create a boxplot of \"Sepal.Length\" by \"Species\" in the iris dataset\nplot(x = iris$Species, \n     y = iris$Sepal.Length, \n     main = \"Boxplot of Sepal Length by Species\",\n     xlab = \"Species\", \n     ylab = \"Sepal Length\", \n     col = \"darkgray\")\n# Create a density plot of \"Sepal.Length\" in the iris dataset\nplot(density(iris$Sepal.Length), main = \"Density Plot of Sepal Length in the iris dataset\",\n     xlab = \"Sepal Length\", ylab = \"Density\", col = \"blue\")"},{"path":"visualisations-in-base-r.html","id":"bar-plots-using-barplot","chapter":"12 Visualisations in Base R","heading":"12.2 Bar plots using barplot()","text":"name suggests barplot() function used create bar charts R. basic syntax barplot(height, ...) height vector providing heights bar. arguments ellipsis ... can checked using ?barplot().examples use function:Example: Basic Bar ChartExample- Bar chart summarised dataExample- Grouped Bar ChartExample: Stacked Bar ChartIn fact, can also create barplots using formula directly. See example","code":"\n# Create a basic bar chart of the \"mpg\" dataset\n\nbarplot(mtcars$mpg, names.arg = rownames(mtcars), main = \"Miles Per gallon - mtcars\", xlab = \"Car Model\", ylab = \"MPG\")\n# Create a summarised bar chart of the \"PlantGrowth\" dataset\ndata(PlantGrowth)\nbarplot(height = t(tapply(PlantGrowth$weight, \n                          list(PlantGrowth$group), \n                          mean)), \n        main = \"Mean Weight of Plants by Group\", \n        ylab = \"Group\", \n        xlab = \"Weight\", \n        col = c(\"red\", \"green\", \"blue\"), \n        beside = TRUE, \n        horiz = TRUE)\n# Create a grouped bar chart of the \"ChickWeight\" dataset\ndata(ChickWeight)\nbarplot(height = t(tapply(ChickWeight$weight, \n                          list(ChickWeight$Diet, ChickWeight$Time), \n                          mean)), \n        main = \"Mean Weight of Chicks by Diet and Time\", \n        xlab = \"Diet and Time\", ylab = \"Weight\", \n        col = c(\"red\", \"green\", \"blue\", \"yellow\"), \n        beside = TRUE,\n        legend.text = c(\"Diet 1\", \"Diet 2\", \"Diet 3\", \"Diet 4\"))\n# Create a stacked bar chart of the \"VADeaths\" dataset\ndata(VADeaths)\nbarplot(as.matrix(VADeaths), \n        main = \"Death Rates by Age Group and Gender\", \n        xlab = \"Age Group\", \n        ylab = \"Death Rate\", \n        col = c(\"red\", \"green\", \"blue\", \"yellow\", \"purple\"), \n        legend.text = c(\"Females\", \"Males\"), \n        beside = FALSE)\nbarplot(GNP ~ Year, data = longley)"},{"path":"visualisations-in-base-r.html","id":"histograms-using-hist","chapter":"12 Visualisations in Base R","heading":"12.3 Histograms using hist()","text":"hist() function R used create histogram, graphical representation distribution numeric variable. basic syntax hist() follows:-x: data plotted, numeric vector matrix.breaks: number bins use histogram. default, R uses Sturges formula determine number bins, can also specify different number bins vector breakpoints.freq: logical value indicating whether plot frequency density data. TRUE, y-axis represents number observations bin. FALSE, y-axis represents density data.main: character string specifying title histogram.xlab: character string specifying label x-axis.ylab: character string specifying label y-axis....: Additional arguments passed plot() function, col, border, xlim.Example -Another example using two layersIn example used lines() function add another layer existing plot.","code":"hist(x, breaks = \"Sturges\", freq = TRUE, main = NULL,\n     xlab = NULL, ylab = \"Frequency\", ...)\nhist(iris$Sepal.Length, breaks = 10, col = \"blue\",\n     xlab = \"Sepal Length\", ylab = \"Frequency\",\n     main = \"Histogram of Sepal Length in Iris Dataset\")\nhist(mtcars$mpg,\n     freq=FALSE,\n     breaks=12,\n     col=\"red\",\n     xlab=\"Miles Per Gallon\",\n     main=\"Histogram, with density curve\")\n# add density curve\nlines(density(mtcars$mpg), col=\"blue\", lwd=2)"},{"path":"visualisations-in-base-r.html","id":"boxplots-and-variants-using-boxplot","chapter":"12 Visualisations in Base R","heading":"12.4 Boxplot(s) and variants using boxplot()","text":"\nFigure 12.1: Understanding boxplots\nUsing boxplot() function simple, syntax boxplot(x, ...) x numeric vector list numeric vectors plotted, ... represents additional arguments can used customize appearance box plot.Example-However, boxplots particularly useful drawn parallel several categories. draw , can directly use formula example . E.g. following code create box-plots type spray, InsectSprays14 data.","code":"\nboxplot(mtcars$mpg)\nboxplot(count ~ spray, \n        data = InsectSprays, \n        col = \"lightgray\")"},{"path":"visualisations-in-base-r.html","id":"saving-and-exporting-charts","chapter":"12 Visualisations in Base R","heading":"12.5 Saving and exporting charts","text":"See last section next chapter.","code":""},{"path":"visualisations-in-base-r.html","id":"use-of-par","chapter":"12 Visualisations in Base R","heading":"12.6 Use of par()","text":"One useful functions using base R’s graphics par() used set query graphical parameters. See following example, one argument mfrow = c(nr, nc) used inside function draw subsequent plots nr--nc array device.","code":"\npar(mfrow = c(2,2))\npurrr::walk(1:4, \n     ~plot(anscombe[[paste0('x', .x)]], \n           anscombe[[paste0('y', .x)]], \n           xlab = paste0('x', .x),\n           ylab = paste0('y', .x),\n           main = paste(\"Anscombe's Quartet Chart No.\", .x))\n     )"},{"path":"visualising-data-with-ggplot2.html","id":"visualising-data-with-ggplot2","chapter":"13 Visualising data with ggplot2","heading":"13 Visualising data with ggplot2","text":"","code":""},{"path":"visualising-data-with-ggplot2.html","id":"core-concepts-of-grammar-of-graphics","chapter":"13 Visualising data with ggplot2","heading":"13.1 Core concepts of grammar of graphics","text":"ggplot21516 package developed Hadley Wickham, based concepts laid (2005) Leland Wilkinson Grammar Graphics.17 Basically, grammar graphics framework follows layered approach describe construct visualizations graphics structured manner. Even letters gg ggplot2 stand ggrammar graphics.Hadley Wickham, paper titled Layered Grammar Graphics18(2010)19 proposed idea layered grammar graphics detail simultaneously put forward idea ggplot2 open source implementation framework building graphics. Readers/Users advised check paper describes concept grammar graphics detail. end decade package progressed20 one used popular packages R.relationship components explained grammars can illustrated image21 Figure 13.1. components left put forward Wilkinson whereas right proposed Wickham. may seen TRANS relation ggplot2 role played -built features R.\nFigure 13.1: Layers Grammar Graphics mapped GGPLOT2\nThus, build graphic one dimensions, given data, use seven major components -Data: Unarguably, graphic/visualisation start \ndata. also first argument important function \npackage .e. ggplot(data =).Aesthetics: aes() short, provide mapping \nvarious data dimensions axes provide positions \nvarious data points output plot/graphic.Geometries: geoms short, used provide \ngeometries data points may take concrete shape \nvisualisation. e.g. data points depicted bars\nscatter points else decided provided geoms.Statistics: stat short, provides statistics show\nvisualisation like measures central tendency, etc.Scale: component used decide whether dimension\nneeds scaling like logarithmic transformation, etc.Coordinate System: Though time Cartesian coordinate\nsystem used, yet times polar coordinate system\n(e.g. pie chart) spherical coordinate system (e.g. geographical\nmaps) used.Facets: Used based certain dimension, plot \ndivided sub-plots.afore-mentioned components, first three (data, aesthetics geometries) explicitly provided thus can understood mandatory components. Whilst three components mandatorily provided, others mandatory. just components defaults (e.g. default coordinate system Cartesian coordinate system). Let us dive three essential components build plot using .","code":""},{"path":"visualising-data-with-ggplot2.html","id":"building-a-basic-plot-using-key-components","chapter":"13 Visualising data with ggplot2","heading":"13.2 Building a basic plot using key components","text":"use mtcars data-sets, default dataset package, learn concepts. Let us see happens data provided ggplot function-\nFigure 13.2: Data provided ggplot2\nFigure 13.2 can see blank chart/plot space created data mtcars now mapped ggplot2. Now, let us provide aesthetic mappings using function aes(), argument mapping ggplot2 function .\nFigure 13.3: Data mapping provided ggplot2\nFigure 13.3, may now notice apart creating blank space plot, two dimensions provided, .e. wt mpg mapped x y axes respectively. Since geometry provided, plot area still blank. Now provide geometry dimension say point. use another family functions .e. geom_* (geom_point() case specifically).\nFigure 13.4: Data plotted points scatterplot\nFigure 13.4 may now notice data plotted\npoints (due geometry used geom_point) soon added\nanother layer function ggplot() using + sign earlier\ncode. Using code , actually plotted relationship\nweight vehicle (wt) mileage miles per gallon\n(mpg) vehicles available mpg dataset.plotted data box-plot used another\ngeometry say geom_boxplot . Refer Figure 13.5.\nFigure 13.5: Data plotted boxplot\n’s basic architecture construction plot \npackage. point may noted provided data\naesthetics argument function ggplot geometry \nused another function geom_* added components\nusing plus + sign. code(s) may also noted \ndata mapping first two arguments function ggplot;\nx y default first two arguments function aes \nmay draw plot Figure 13.4 using following\ncode wherein haven’t used named arguments. follow\nconvention subsequent sections.Now lets discuss aesthetics geometries using \nbuild desired plots, moving components plot\npackage.","code":"\nlibrary(tidyverse)\nggplot(data=mtcars)\nggplot(data = mtcars, mapping = aes(x=wt, y=mpg))\nggplot(data = mtcars, mapping = aes(x = wt, y = mpg)) +\n  geom_point()\nggplot(data = mtcars, mapping = aes(y = wt)) +\n  geom_boxplot()ggplot(mtcars, aes(wt, mpg)) +\n  geom_point()"},{"path":"visualising-data-with-ggplot2.html","id":"AESTH","chapter":"13 Visualising data with ggplot2","heading":"13.3 Other Aesthetic attributes (color, shape, size, etc.)","text":"previous section chapter mapped attributes data\nusing position coordinate system (x /y Cartesian\ncoordinate system). can, however, map variables data \nplot using aesthetic attributes like shape, size, color,\nalpha (transparency), etc., shown image Figure\n13.6.\nFigure 13.6: Common Aesthetic mappings. Image Source: Claus Wilke’s book Fundamentals Data Visualization\naesthetics may divided broadly two categories -aesthetics can mapped continuous data variable(s);\nandaesthetics can mapped discrete categorical data\nvariables.example, position (coordinates coordinate system), size,\ncolor, linewidth can represent continuous data; shape,\nlinetype etc. aesthetics can mapped discrete data. Numerical\ndata can used represent continuous discrete\ndata (see example shortly) mapped aesthetic \ndefault represent continuous data thus, need converted \ndiscrete data type (factor, cases suffice) \nmapping aesthetic representing discrete data.commonly used aesthetics -shape = Display point geom_point() dot, star,\ntriangle, squarefill = interior color (e.g. bar box-plot)color = exterior line bar, boxplot, etc., \npoint color using geom_point()size = Size (e.g. line thickness, point size)alpha = Transparency (1 = opaque, 0 = invisible)binwidth = Width histogram binswidth = Width “bar plot” columnslinetype = Line type (e.g. solid, dashed, dotted)","code":""},{"path":"visualising-data-with-ggplot2.html","id":"color-the-most-important-aesthetic","chapter":"13 Visualising data with ggplot2","heading":"13.3.1 Color, the most important aesthetic","text":"Data elements can colored data visualisation using aesthetic\nnamed color (Alternative British spelling colour also work \nexactly way). can use color plot/visualisation primarily\nthree purposes-highlight specific values.grouping data points .e. using color distinguish data\nelements .mapping variable, .e. using color represent different data\nelements.understand use cases, let us fill color points \nFigure @(fig:rgg3) say, \"red\" color. , can provide\nvalue color aesthetic directly inside geom_* function\n(Figure 13.7).\nFigure 13.7: Highlighting data points static color\nargument color='red' mentioned inside geom_point()\nfunction, turned every point red (.e. static color) \nFigure 13.7. requirement highlight specific\npoints plot, use color inside aes function. \nwords, use color aesthetics visualise data.let us color data points Figure 13.7 using \nvariable cyl (number cylinders vehicle), \nscatter-points colored basis number cylinders instead.\nFigure 13.8: Mapping numeric variable color aesthetic\nmay notice Figure 13.8 scatter-points now\ncolored basis number cylinders cars. Simultaneously,\ncolor scale produced legend. Since cyl column \nnumeric column, mapped continuous type aesthetic\ncolor, mapped continuous variable aesthetic default.\nNow case, though cyl numerical values, plot\nmeaningful corresponding discrete variable \nmapped color aesthetic. can convert factor type\nvariable, fly.\nFigure 13.9: Mapping discrete variable color aesthetic\nFigure 13.9 may see points grouped\nusing different color group. Readers may also \ncolor aesthetic provided aes() function second\nlayer wrapped geom_point() function. aesthetic \nwrapped ggplot() layer also. basically following\ncode also produce exactly chart-difference two? Yes, basically aesthetics \nprovided geoms, override aesthetics \nalready provided ggplot function. understand difference,\nsee result following code Figure 13.10.\nFigure 13.10: -riding aesthetics\nthird use-case, .e. using color describe variable, let us\nanalyse mean mileage cars group cars () number\ncylinders cyl (ii) number carburetors carb. good\nvisualisation plot values heat-map (sometimes also\ncalled highlight table). generate grouped summary \nproceeding, can understood using concepts explained \nchapter related data manipulation dplyr .e. Chapter\n14.draw rectangular boxes heat-map use another geometry\nnamely geom_tile() map two categorical variables x y\ncoordinates Cartesian system. fill color values \nbasis variable mpg use fill aesthetic instead color\n(understand difference fill color aesthetics\nshortly).\nFigure 13.11: using color plot variable directly\nReferring plot Figure 13.11 may see cars 4\ncylinders 1 carburetor highest mileage.now, seen map color dynamically \nvariable, pass aesthetic inside aes function; \notherwise intend use color static value, may pass\noutside aes .e. directly corresponding geom function.\nPackage ggplot2 recognises color names discussed\ncolors Appendix . pass \nstatic color value color aesthetic inside aes function? Let\nus check .\nFigure 13.12: Mapping Static color inside aes\nInteresting! GGPLOT2 mapped dummy variable called\n'blue' color points, also created legend. \ninterestingly color wanted. Actually, happened\n, mapped color aesthetic inside aes(), ggplot2 created\nnew variable fly, mapped aesthetic \nthus producing legend newly created variable.","code":"\nggplot(mtcars, aes(wt, mpg)) +\n  geom_point(color='red')\nggplot(mtcars, aes(wt, mpg)) +\n  geom_point(aes(color=cyl))\nggplot(mtcars, aes(wt, mpg)) +\n  geom_point(aes(color=as.factor(cyl)))ggplot(mtcars, aes(wt, mpg, color = as.factor(cyl))) +\n  geom_point()\nggplot(mtcars, aes(wt, mpg, color = as.factor(cyl))) +\n  geom_point(color='red')\nmtcars |> \n  summarise(mpg = mean(mpg, na.rm = TRUE),\n            .by = c(cyl, carb)) |> \n  ggplot() +\n  geom_tile(aes(x = cyl, y = carb, fill = mpg))\nggplot(mtcars, aes(wt, mpg)) +\n  geom_point(aes(color='blue'))"},{"path":"visualising-data-with-ggplot2.html","id":"color-vs.-fill","chapter":"13 Visualising data with ggplot2","heading":"13.3.2 Color Vs. Fill","text":"Till now use used color aesthetic point geom (Figure\n13.9) fill aesthetic tile geom (Figure\n13.11) map colors variables. use\ndifferent aesthetics? Typically, color aesthetic changes \noutline geom fill aesthetic changes inside.\ngeom_point() exception, used color (fill) \npoint color. Actually, exception . reason \ndefault point shape used geom_point() shape = 19: solid\ncircle.can see subtle difference override default shape \nfigure 13.9 shape = 21: circle allows us use\nfill inside color outline. (Figures\n13.13.)\nFigure 13.13: Color Vs. Fill aesthetics\n","code":"\ntheme_set(theme_bw())\n\nggplot(mtcars, aes(wt, mpg)) +\n  geom_point(aes(color=as.factor(cyl)), shape = 21, size = 4) +\n  ggtitle(\"Using color aesthetic\")\n\nggplot(mtcars, aes(wt, mpg)) +\n  geom_point(aes(fill=as.factor(cyl)), shape = 21, size = 4) +\n  ggtitle(\"Using fill aesthetic\")"},{"path":"visualising-data-with-ggplot2.html","id":"transparency-through-alpha","chapter":"13 Visualising data with ggplot2","heading":"13.3.3 Transparency through alpha","text":"ggplot2, one aesthetic used change color\ngeometries, alpha used control transparency \nelements plot. adjusting alpha value, ranges\n0 (completely transparent) 1 (fully opaque), can manage \nvisibility layering overlapping elements. particularly\nuseful dealing dense data, helps reduce -plotting\nallows better visualization distributions relationships.\nUsually, used map continuous variable . Example-\nFigure 13.14: Setting transparency number cylinders\nmay see figure 13.14 points transparency \nvaries according number cylinders .e. cyl variable \ndata. Similar aesthetics may pass static value 0 \n1 alpha setting transparency geometries desired.","code":"\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point(aes(alpha = cyl), size = 4) "},{"path":"visualising-data-with-ggplot2.html","id":"shape-aesthetic","chapter":"13 Visualising data with ggplot2","heading":"13.3.4 Shape Aesthetic","text":"figure 13.13, already saw shape aesthetic change\nshape points solid color hollow color. Actually, \nggplot2, shape aesthetic used differentiate points \nplot assigning different symbols . Moreover, \nalready discussed, aesthetic either mapped \ndiscrete variable; using shape pre-existing shapes \npackage (see ?points). ggplot2 supports variety shapes, \ncircles, triangles, squares, , represented unique\ninteger character.instance, plotting data categorical variable, can map\nvariable shape aesthetic visually separate groups.\nHowever, ’s important note shapes can less effective \ngroups many categories, distinctiveness shape may\ndiminish.plots, may map cyl variable shape instead, \nconverting factor variable.\nFigure 13.15: Mapping shape aesthetic\nmay notice different shapes used 4, 6 \n8 cylinder vehicles.shapes available shape aesthetics, fill aesthetic\nshown orange' andcolor` aesthetic shown ‘blue’ color \nfigure 13.16.\nFigure 13.16: Shapes available GGplot\n","code":"\nggplot(mtcars, aes(wt, mpg)) +\n  geom_point(aes(shape=as.factor(cyl)), size = 3)"},{"path":"visualising-data-with-ggplot2.html","id":"size-aesthetic","chapter":"13 Visualising data with ggplot2","heading":"13.3.5 Size Aesthetic","text":"already seen size aesthetic controls size \nplot elements geometries, points scatter plot. \nmapping continuous discrete variable size, can represent\nadditional dimensions data, making plot informative.example, scatter plot car weight versus fuel efficiency, \nmight use size aesthetic represent horsepower car,\nlarger points indicate powerful cars.\nFigure 13.17: Mapping size aesthetic\nfigure 13.17 may see visual layer added\nhelps identify relationships patterns across multiple\nvariables simultaneously. However, ’s essential use size\naesthetic judiciously, overly large small elements can distort \nreadability plot.","code":"\nggplot(mtcars, aes(wt, mpg)) +\n  geom_point(aes(size=hp))"},{"path":"visualising-data-with-ggplot2.html","id":"using-multiple-aesthetics-simultaneously","chapter":"13 Visualising data with ggplot2","heading":"13.3.6 Using multiple aesthetics simultaneously","text":"Multiple aesthetics can also mapped simultaneously, per\nrequirement. See example-\nFigure 13.18: Using multiple aesthetics\nlearn aesthetics like binwidth, linetype,\nwidth, etc., next section 13.4 learn \nuse geometries.","code":"\nggplot(mtcars,\n       aes(\n         x = wt,\n         y = mpg,\n         shape = as.factor(cyl),\n         color = as.factor(gear),\n         alpha = wt,\n         size = hp\n       )) +\n  geom_point()"},{"path":"visualising-data-with-ggplot2.html","id":"geoms","chapter":"13 Visualising data with ggplot2","heading":"13.4 More on Geoms","text":"previous section seen soon passed geom_*\nfunction/layer data & aesthetics layers, chart/graph \nconstructed. Actually, geom_point() function, background added\nthree layers .e. stat, geom position, geom_*\nactually shortcuts, add three layers. example,\nggplot(mtcars, aes(wt, mpg)) + geom_point() actually equivalent \n-\nFigure 13.19: Components GGPLOT2\ncommon geoms listed :Histograms - geom_histogram()Bar charts - geom_bar() geom_col()Box plots - geom_boxplot()Points (e.g. scatter plots) - geom_point()Line graphs - geom_line() geom_path()Trend lines - geom_smooth()Heat-map - geom_tile()Label charts using geom_text() /geom_label(), already seen examples geom_point, geom_boxplot\ngeom_tile. Let us discuss geoms bit detail.","code":"\nggplot() +\n  layer(\n    data = mtcars,\n    mapping = aes(wt, mpg),\n    geom = \"point\",\n    stat = \"identity\",\n    position = \"identity\"\n  )"},{"path":"visualising-data-with-ggplot2.html","id":"univariate-bar-charts-through-geom_bar","chapter":"13 Visualising data with ggplot2","heading":"13.4.1 Univariate Bar Charts through geom_bar()","text":"Bar charts though form simplest visualisations can \ndeceptive try build without understanding mechanics\nbehind bars, literally :). Bar charts can univariate \nbivariate. Even multivariate data can visualised bar charts.Simplest bar charts can plot showing distribution \ncategorical variable data. words, number data\npoints available per category variable. Example - many cars\ndifferent cylinder count available data.\nFigure 13.20: Univariate Bar Chart\nfigure 13.20 may see numbers cars available per\ncategory (cylinders therein). used numerical variable \nx-axis, numerical scale shown. Also notice data\nsummarised ggplot2 aggregated basis \nvariable passed aesthetics (position) applying count summary\nfunction. can confirmed label y axis.Readers advised note change x-axis soon variable\nconverted categorical variable, executing code-aggregating data bar-plot using count\nfunction. sometimes, aggregation methods may required. \ncan done understand mechanics behind code. Actually\naes(cyl) shortcut aes(x = cyl, y = after_stat(count)) \ncount special variable representing counts \ncategory present variable.now, let us calculate proportions instead count (frequency) \ncategories available variable. change, now let us use\nanother dataset mpg comes default ggplot2 package. \nanalyse proportion vehicles class (\ncategorical variable).\nFigure 13.21: Univariate Bar Chart representing proportions\nfigure 13.21 may see now proportions \nplotted (notice y axis).","code":"\nggplot(mtcars, aes(cyl)) +\n  geom_bar()ggplot(mtcars, aes(as.factor(cyl))) +\n  geom_bar()\nggplot(mpg, aes(class, y = after_stat(count/sum(count)))) +\n  geom_bar()"},{"path":"visualising-data-with-ggplot2.html","id":"bivariate-bar-charts-through-geom_bar","chapter":"13 Visualising data with ggplot2","heading":"13.4.2 Bivariate Bar Charts through geom_bar()","text":"learn geom_bar() carries summarisation \nun-aggregated granular data draws plots us. tweak \nsummary function, per requirement, used y position\naesthetic. granular data, may sometimes require \nperform aggregation another variable.example let us see mean city mileage cty every class\ncar mpg data-set. achieve , another aesthetics\nstat special value \"summary_bin\". Moreover, stat\naesthetics also requires fun statistic mean case.\nFigure 13.22: Bivariate Bar Chart representing mean milaege per class car\nfigure 13.22 can see subcompact class cars\nhighest mean mileage city.two sections, learnt draw plots using un-aggregated\ndata. However, can also plot pre-aggregated data bar-plots using\ngeom_bar. let us draw plot figure 13.22,\ntime aggregating data , beforehand. trick \nuse stat = \"identity\" aesthetics geom_bar() layer. see\naesthetic actually short-. change,\ntime let’s draw plot x y axes flipped.\nFigure 13.23: Bivariate Bar Chart representing mean milaege per class car\nfigure 13.23 can see desired generated.\nReaders may try -mentioned code removing stat - \"identity\"\ngeom_bar().Now, promised discuss stat aesthetic . \ngenerating summary bivariate chart used stat = \"summary_bin\"\ncreated summary using fun un-aggregated data. Whereas\nstat = \"identity\" tells ggplot2 data either already aggregated\nvalue y per category x variable. \nstat aesthetics available us? answer yes. However,\nreaders advised plot bar charts aggregated data using\ngeom_col discussed subsequent sections, instead \ntrying complex aggregations within ggplot2 gets trickier \n.","code":"\nggplot(mpg, aes(x = class, y = cty)) + \n  geom_bar(stat = \"summary_bin\", fun = mean)\naggregated_mpg <- mpg |> \n  summarise(mean_cty = mean(cty),\n            .by = class) \n\nggplot(aggregated_mpg, aes(x = mean_cty, y = class)) +\n  geom_bar(stat = \"identity\")"},{"path":"visualising-data-with-ggplot2.html","id":"stacked-bar-charts-through-geom_bar","chapter":"13 Visualising data with ggplot2","heading":"13.4.3 Stacked bar charts through geom_bar","text":"section 13.3, learnt can plot variables \ntwo-dimensional plots using aesthetic attributes like color, size,\netc. size bar, bar chart already mapped variable,\nsuitable aesthetic mapped another variable fill \ncolor.Let us aggregate cars (count) basis class . \nlet’s map fill fuel type fl variable.\nFigure 13.24: Color Stacked bar chart\nfigure 13.24 achieved desired results simply \nmapping fill additional variable. Actually, possible\ndue default value position aesthetic geom_bar() layer\n\"stack\" matches requirement. default, multiple bars occupying\nx position stacked atop one another \nposition_stack().useful argument reverse position_stack() also helpful\nreversing order fill values. E.g. plot drawn\ny-axis instead.\nFigure 13.25: Color Stacked bar chart Y axis\nfigure 13.25 can see legend values now align \nvalues represented bar chart.use bars, side side, plot can use another position\nstat function position_dodge(). Redrawing plot \nbars side side-\nFigure 13.26: Dodged Bar Chart\nfigure 13.26 may see separate bar fill\naxis now drawn. bars’ widths preserved \ndefault parameter preserve inside position_dodge() total.\nmay change \"single\" want bars equal width\nirrespective fact whether fill category available\nx value. Refer figure 13.27 wherein bars’\nwidths equal.\nFigure 13.27: Dodged Bar Chart equal bar widths\nSimilar position_dodge another position_dodge2()\nfunction works better bar charts. may tweak padding\nbars (Refer figure 13.28).\nFigure 13.28: Dodged Bar Chart padding bars\nFinally, one position namely position_fill() \nstacks bars standardises stack constant height. Refer\nfigure 13.29.\nFigure 13.29: 100% stacked bar chart\n","code":"\nggplot(mpg, aes(x = class, fill = fl)) +\n  geom_bar()\nggplot(mpg, aes(y = class, fill = fl)) +\n  geom_bar(position = position_stack(reverse = TRUE)) +\n  theme(legend.position = \"top\")\nggplot(mpg, aes(x = class, fill = fl)) +\n  geom_bar(position = position_dodge()) +\n  theme(legend.position = \"top\")\nggplot(mpg, aes(x = class, fill = fl)) +\n  geom_bar(position = position_dodge(preserve = \"single\")) +\n  theme(legend.position = \"top\")\nggplot(mpg, aes(x = class, fill = fl)) +\n  geom_bar(position = position_dodge2(preserve = \"single\", padding = 0.2)) +\n  theme(legend.position = \"top\")\nggplot(mpg, aes(x = class, fill = fl)) +\n  geom_bar(position = position_fill()) +\n  theme(legend.position = \"top\")"},{"path":"visualising-data-with-ggplot2.html","id":"bar-charts-through-geom_col","chapter":"13 Visualising data with ggplot2","heading":"13.4.4 Bar Charts through geom_col()","text":"stated earlier, plotting bar charts tedious aggregations \nggplot2 gets trickier, always advisable plot bar charts \ngeom_col() cases aggregating thee data . Since\ndifference geom_bar geom_col former uses \ndefault: counts number cases x position. \nhand, latter uses stat = “identity” default. draw plot \nfigure 13.23 geom_col() may use\nstat explicitly. (Refer figure 13.30.)\nFigure 13.30: Plotting geom col\nreaders understood functioning position \nstat arguments geom_bar now pretty easy draw stacked bar\ncharts, dodged bar charts 100 percent stacked bar charts \ngeom_col much easier way. Readers may try drawing \ncharts using pre-aggregated data keeping mind geom_col \nusing stat = “identity” default thus performing \naggregation.","code":"\naggregated_mpg <- mpg |> \n  summarise(mean_cty = mean(cty),\n            .by = class) \n\nggplot(aggregated_mpg, aes(x = mean_cty, y = class)) +\n  geom_col()"},{"path":"visualising-data-with-ggplot2.html","id":"adding-labels-to-charts-using-geom_text-or-geom_label","chapter":"13 Visualising data with ggplot2","heading":"13.4.5 Adding labels to charts using geom_text or geom_label","text":"laerning draw plots using geom_* family \nfunctions, right time learn labelling geometries \nplots.label data points ggplot2, can use either functions ()\ngeom_text(); (ii)geom_label(). geom_text() adds text \nplot; whereas geom_label() draws rectangle behind text, making\neasier read.functions adds text provided label aesthetics, \nplot specified x y coordinates. Moreover, can\ncustomize appearance labels adding additional arguments \ngeom_text() -size set font sizecolor color fontshjust vjust adjust labels vertically horizontally,\nrespectively. can modify text alignment aesthetics.\ncan either number 0 (right/bottom) 1\n(top/left) character (\"left\", \"middle\", \"right\",\n\"bottom\", \"center\", \"top\"). two special alignments:\n\"inward\" \"outward\". Inward always aligns text towards \ncenter, outward aligns away center.family font family [options “sans” (default),\n“serif”, “mono”]fontface face font [options: “plain” (default),\n“bold”, \"italic\" “bold.italic”]Example-\nFigure 13.31: Adding labels geoms\nfigure 13.31 can see geometries (.e points) \nlabelled slighly points (due vjust = -1). may\nobserve labels overlapped. fantastic package\nggrepel works ggplot2 plots places overlapped\nlabels nicer way. See figure 13.32.\nFigure 13.32: Adding labels geoms external ggrepel package\nlabelling bar charts generated granular data using geom_bar()\nmay sometimes tricky use stat functions used \ngenerate summary/aggregation. See Example figure 13.33\nFigure 13.33: Labelled bar chart\nwant label chart 13.22, provide \nspecial values aesthetics geom_text (geom_label). See figure\n13.34.\nFigure 13.34: Bivariate bar chart labelled\nOne example labelling boxplots maximum value \ncategory may -\nFigure 13.35: Upper Hinge labelled Boxplot\nLabeling color stacked bar charts can also get trickier, \nprovide appropriate position argument geom_text layer also. See\nexample figure 13.36.\nFigure 13.36: Colored bar chart labelled\ndiscussed difference geom_bar geom_col \ndetails, readers may find pretty easier draw chart\n(Figure 13.36) using geom_col pre-aggregated data. Refer\nfigure 13.37 wherein handle placements \nlabels position_stack argument.\nFigure 13.37: Colored bar chart labelled\n","code":"\nggplot(mtcars, aes(x = hp, y = mpg)) +\n  geom_point() +\n  geom_text(aes(label = rownames(mtcars)),\n            size = 3,\n            color = \"dodgerblue\",\n            vjust = -1) # -1 pushes he value further upwards (vjust)\n# Load package\nlibrary(ggrepel)\n# Set global options for max overlaps\noptions(ggrepel.max.overlaps = Inf)\n\nggplot(mtcars, aes(x = hp, y = mpg)) +\n  geom_point() +\n  geom_text_repel(aes(label = rownames(mtcars)),\n            size = 3,\n            color = \"seagreen\",\n            vjust = -1,\n            fontface = \"bold.italic\")\n# Labelling a bar plot\nggplot(mpg, aes(class)) +\n  geom_bar() +\n  geom_text(\n    aes(\n      y = after_stat(count + 2), # shift the label slightly\n      label = after_stat(count)\n    ),\n    stat = \"count\"\n  )\nggplot(mpg, aes(class, cty)) +\n  geom_bar(stat = \"summary_bin\", fun = mean) +\n  geom_text(\n    aes(label = after_stat(round(y, 2))),\n    stat = \"summary_bin\",\n    fun = mean,\n    vjust = -0.5\n  )\n# Labelling the upper hinge of a boxplot,\n# inspired by June Choe\nggplot(mpg, aes(displ, class)) +\n  geom_boxplot(outlier.shape = NA) +\n  geom_label(\n    aes(\n      label = after_stat(xmax),\n      x = stage(displ, after_stat = xmax)\n    ),\n    stat = \"boxplot\", hjust = -0.5\n  )\nggplot(mpg, aes(class, fill = fl)) +\n  geom_bar() +\n  geom_text(\n    aes(label = after_stat(count)),\n    stat = \"count\",\n    position = position_stack(vjust = 0.5)\n  )\nmpg_agg <- mpg |> \n  count(class, fl)\n\nggplot(mpg_agg, aes(\n  x = class,\n  y = n,\n  fill = fl,\n  label = n # provided globally\n)) +\n  geom_col() +\n  geom_text(\n    position = position_stack(vjust = 0.5) #labels centered vertically\n    )"},{"path":"visualising-data-with-ggplot2.html","id":"histograms-and-related-plots","chapter":"13 Visualising data with ggplot2","heading":"13.4.6 Histograms and related plots","text":"Readers now, mayhave understood ggplot2 vast topic \noften requires separate book. Full coverage single chapter \nnearly impossible. However, several geoms \ntrickier geom_bar simultaneously useful analysing \ndata. One geom_histogram plots histogram \nunivariate numerical (continuous variable) data.Like geom_bar also aggregates data using default\nstatistic stat = \"bin\". change let us now use diamonds\ndata-set, also loaded default ggplot2 package. \ncontains 50,000 observations round-cut diamonds. example\nlet us visualise distribution carat variable.\nFigure 13.38: simple histogram\nfigure 13.38 histogram generated warning\npick better value bin-width. let us modify bin-width \nsay, 0.025.\nFigure 13.39: Histogram custom binwidth\nmay use bins aesthetic choose number bins histogram.\naestheics like fill color may also used.\nFigure 13.40: Filled Histogram\nRelated geom geom_freqpoly (short frequency polygon) \ndisplay counts lines. histogram figure 13.40\nmay drawn frequency polygon usingthis geom layer. Refer figure\n13.41.\nFigure 13.41: Filled frequency polygon\nmake easier compare distributions different counts,\nmay put density y axis instead default count. Refer\nfigure 13.42.\nFigure 13.42: Filled frequency polygon density\n","code":"\nggplot(diamonds, aes(carat)) +\n  geom_histogram()## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\nggplot(diamonds, aes(carat)) +\n  geom_histogram(binwidth = 0.025)\nggplot(diamonds, aes(price, fill = cut)) +\n  geom_histogram(binwidth = 500)\nggplot(diamonds, aes(price, color = cut)) +\n  geom_freqpoly(binwidth = 500)\nggplot(diamonds, aes(price, after_stat(density), colour = cut)) +\n  geom_freqpoly(binwidth = 500)"},{"path":"visualising-data-with-ggplot2.html","id":"line-charts","chapter":"13 Visualising data with ggplot2","heading":"13.4.7 Line Charts","text":"Since almost geoms ggplot2 named intuitively, can\ncorrect guess line charts canbe drawn using geom_line().\nHowever, unlike geoms seen till now, geom_line() special\ngeom works correctly groups. thus sometimes referred \ngrouped collective geom.understand concept group, let us construct simple data,\nindex variable (x axis), another numerical variable\nvalues also categorical variable say gr. Let us plot\nvalues vs. index line plot.\nFigure 13.43: simple line chart\nfigure 13.43, can see values plotted \nindex joined first moving onto another\nindex. inadvertant thing can fixed use aesthetic group.\ngroup aesthetic determines observations connected. See\nfigure 13.44.\nFigure 13.44: line chart without legend\nfigure 13.44, got two different lines \nintended, corresponding legend identify group .\n, map color aesthetic group variable can get \nlegend. Moreover, mapping aesthetic may -riding effect (\ncase) group aesthetic, aesthetic kind \nredundant. Refer plot figure 13.45.\nFigure 13.45: line chart legend\ngroup isn’t defined single variable, instead \ncombination multiple variables, may use interaction() combine\n.Now use two data-sets () economics economics_long; \npart tidyr package. Readers using ggplot2 library\nthus, advised load package tidyr (alternatively\ntidyverse contains packages). data-sets\ncontains economic parameters, monthly basis US.figure 13.47, separate lines variable \nmonths across years plotted.\nFigure 13.46: Groups multiple variables\nanother related geom .e. geom_path() also draws line\nchart. geom_line() connects points left right;\ngeom_path() connects points order appear data. \nfigure 13.47 exdata created earlier \nre-arranged show difference. geom_line() geom_path()\nalso understand aesthetic linetype, maps categorical\nvariable 'solid' (default), 'dotted' , 'dashed' 'dotdash'\nlines.\nFigure 13.47: Path vs Line geoms\n","code":"\n# Constructing example data\nset.seed(10)\nexdata <- data.frame(\n  gr = rep(c(\"G1\", \"G2\"), 5),\n  index = rep(1:5, each = 2),\n  values = sample(100:200, 10)\n)\n# print the data\nexdata##    gr index values\n## 1  G1     1    108\n## 2  G2     1    173\n## 3  G1     2    175\n## 4  G2     2    154\n## 5  G1     3    171\n## 6  G2     3    153\n## 7  G1     4    138\n## 8  G2     4    182\n## 9  G1     5    187\n## 10 G2     5    114\n# Line chart\nggplot(exdata, aes(index, values)) +\n  geom_line()\nggplot(exdata, aes(index, values, group = gr)) +\n  geom_line()\nggplot(exdata, aes(index, values, color = gr)) +\n  geom_line()\neconomics_long |> \n  mutate(year = year(date),\n         month = month(date)) |> \n  ggplot(aes(year, value01, group = interaction(month, variable))) +\n  geom_line()\nexdata |>\n  # Rearranging the data points\n  arrange(values) |>\n  ggplot(aes(index, values)) +\n  geom_line(linetype = \"dotdash\", linewidth = 2) +\n  ggtitle(\"Using line geom\")\n\nexdata |>\n  # Rearranging the data points\n  arrange(values) |>\n  ggplot(aes(index, values)) +\n  geom_path(linetype = \"dotted\", linewidth = 2) +\n  ggtitle(\"Using path geom\")"},{"path":"visualising-data-with-ggplot2.html","id":"smoothing-through-geom_smooth","chapter":"13 Visualising data with ggplot2","heading":"13.4.8 Smoothing through geom_smooth","text":"Essentially, geom_smooth() adds trend line existing plot, \nscatter plot line plot. e.g. draw trend \nunemployment US, can use geom_smooth see smoothed trend \nperiod. Refer plot figure 13.48.\nFigure 13.48: Line Chart smoothed trend\nwarning shows method argument used smooth curve \nloess. methods available lm, glm , gam etc. \nrequire formula provided. may also smooth scatter plot\nusing function, see regression (linear) line. Refer plot \nfigure 13.49.\nFigure 13.49: Scatter plot regression line\nReaders may note, figure 13.49 much lesser\npoints seen plot actually available due overlapping \npoints. overlapping issue can either solved using alpha\naesthetic using jitter method scatter plots. geom_jitter\nadds random noise data, overlapping points seen\nclearly. Jittered scatterplot may seen 13.50.\nFigure 13.50: Scatter plot regression line\n","code":"\nggplot(economics, aes(date, unemploy)) +\n  geom_line(color = \"indianred4\", linewidth = 1) +\n  geom_smooth()## `geom_smooth()` using method = 'loess' and formula = 'y ~ x'\nggplot(mpg, aes(cty, hwy)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = \"y ~ x\")\nggplot(mpg, aes(cty, hwy)) +\n  geom_jitter() +\n  geom_smooth(method = \"lm\", formula = \"y ~ x\")"},{"path":"visualising-data-with-ggplot2.html","id":"combining-multiple-geoms","chapter":"13 Visualising data with ggplot2","heading":"13.4.9 Combining multiple geoms","text":"already used multiple geoms plot labeling \nwell smoothing trend. Multiple geoms can used \nplot, add layers existing layers produced \nearlier geoms. example let us add data points figure\n13.43.\nFigure 13.51: Combining Geoms\nfigure 13.51 can see points added \nline. reverse order, can see figure #ref(fig:rgg51)\nline (latter layer) drawn points (former layer).\nFigure 13.52: Understanding overlapping geoms\n","code":"\nggplot(exdata, aes(index, values)) +\n  # layer with line\n  geom_line(linewidth = 1, color = \"dodgerblue\") +\n  # layer with points\n  geom_point(shape = 21, size = 7, fill = \"orange\", stroke = 2)\nggplot(exdata, aes(index, values))  +\n  # layer with points\n  geom_point(shape = 21, size = 7, fill = \"orange\", stroke = 2) +\n  # layer with line\n  geom_line(linewidth = 1, color = \"dodgerblue\")"},{"path":"visualising-data-with-ggplot2.html","id":"other-geoms","chapter":"13 Visualising data with ggplot2","heading":"13.4.10 Other geoms","text":"Several geoms useful depict meaningful information \nafore-mentioned charts -geom_vline(), geom_hline geom_abline() add vertical,\nhorizontal reference lines plot.geom_rect() draw constant/reference rectangle data\nusing four corners rectangle, xmin, ymin, xmax \nymax.geom_area() draws area plot, line plot filled\ny-axis (filled lines). Multiple groups stacked top\n.Example\nFigure 13.53: Adding reference lines\n","code":"\nggplot(mtcars, aes(wt, mpg)) +\n  geom_point() +\n  geom_abline(intercept = 37, slope = -5, color = \"dodgerblue\") +\n  geom_vline(xintercept = 3.5, color = \"orange\")"},{"path":"visualising-data-with-ggplot2.html","id":"list-of-geoms-available-in-ggplot2","chapter":"13 Visualising data with ggplot2","heading":"13.4.11 List of geoms available in ggplot2","text":"Apart geoms discussed, many geoms available \nggplot2. discussed subsequent\nchapters/sections, per use case. However, readers may explore\ngeoms want explore unchartered\nterritories.reference, geoms available ggplot2 version used \ncompile book, listed table 13.1 reference.Table 13.1: List Geoms available ggplot2","code":""},{"path":"visualising-data-with-ggplot2.html","id":"modifying-scales","chapter":"13 Visualising data with ggplot2","heading":"13.5 Modifying scales","text":"Scales ggplot2 control mapping data aesthetics \ndata can seen. words, provide us way \ncustomise aesthetics size, color, position, shape, etc.charts generated far, aesthetic mappings data\ndefault haven’t customised default scales. may\ndivide customisation requirements scales three\ncategories, learn section.position scales,color (fill) scalesother scales.","code":""},{"path":"visualising-data-with-ggplot2.html","id":"position-scales-i.e.-transforming-axes","chapter":"13 Visualising data with ggplot2","heading":"13.5.1 Position scales i.e. transforming axes","text":"seen default coordinate system used ggplot2 Cartesian\nplot data two position aesthetics x y required.\ndrawing figure 13.3 provided aesthetics\nexplicitly, whereas time drawing bar chart 13.20\nprovided one x aesthetic ggplot2 generated y aesthetic \n(remember after_stat(count)).customise position scales scale_*_#() group functions,\n* represent position aesthetic usually x y; #\nrepresents variable type. example two functions \ncontinuous axis/variables.arguments functions, can see axis title (name), axis\nbreaks, axis labels, axis limits, transformations can dealt .\nSee following examples wherein changed limits axes,\nrenamed using scale functions (Refer 13.54).\nFigure 13.54: Modifying Scales GGplot2\ncan also applying transformation axes using trans argument\n(refer figure 13.55).\nFigure 13.55: Transforming Axes GGplot2\ntransformation actually carried \"transformer\", describes transformation, inverse, draw labels. transformations listed table 13.2 following.Table 13.2:  transformationsHowever, certain scale functions dedicated transform axes ggplot2. listed . Obviously functions corresponding y variants available.scale_x_log10()scale_x_reverse()scale_x_sqrt(), scale_x_reverse y variant sometimes really useful.examples, though seen scale functions dealing numerical data, plenty functions deal data. ,scale_x_datescale_x_datetime()scale_x_discrete()scale_x_binned()One example using date scale can economics data. (refer figure 13.56).\nFigure 13.56: Transforming Axes GGplot2\nfigure 13.56, modified () name x axis, (ii) breaks, places label, (iii) label 2 digits year, (iv) limited data plotted, (v) name y axis (vi) modified bales y axis percentages; using scale functions.Readers may noticed modified axes lables plot figure 13.56 using scales package. Using scales library may modify format labels (refer plot figure 13.57).\nFigure 13.57: Transforming Axes GGplot2\nfunctions scales library useful displaying numerical data axes -scales::label_bytes() formats numbers kilobytes, megabytes etc.scales::label_dollar() formats numbers currency.scales::label_ordinal() formats numbers rank order: 1st, 2nd, 3rd etc.scales::label_pvalue() formats numbers p-values: <.05, <.01,\n.34, etc.scales::label_date()scales::label_date_short() formats datesscale::label_wrap() useful wrap long strings across multiple\nlines.","code":"scale_x_continuous(name, breaks, labels, limits, trans)\nscale_y_continuous(name, breaks, labels, limits, trans)\n# Basic Scatter Plot\nggplot(cars, aes(x = speed, y = dist)) + \n  geom_point()\n# Modifying scales both axis title and axis limits\nggplot(cars, aes(x = speed, y = dist)) + \n  geom_point() + \n  scale_x_continuous(name=\"Speed of cars\", limits=c(10, 15)) +\n  scale_y_continuous(name=\"Stopping distance\", limits=c(0, 150))\nggplot(cars, aes(x = speed, y = dist)) + \n  geom_point()+\n  scale_x_continuous(trans='log10') +\n  scale_y_continuous(trans='log10')\nggplot(economics, aes(date, psavert)) +\n  geom_line() +\n  scale_x_date(name = \"Year\",\n               date_breaks = \"2 years\",\n               date_labels = \"%y\",\n               limits = c(ymd(\"19800101\"), ymd(\"19991231\"))) +\n  scale_y_continuous(name = \"Personal Savings Rate\",\n                     labels = scales::label_percent(scale = 1))\nstate.x77 %>% \n  as.data.frame() %>% \n  ggplot(aes(Area, Illiteracy/100)) +\n  geom_point(size = 3, color = \"dodgerblue\") +\n  scale_x_continuous(name = \"Area in Square Miles\", labels = scales::label_comma()) +\n  scale_y_continuous(name = \"Illiteracy as % of Population\", labels = scales::label_percent())"},{"path":"visualising-data-with-ggplot2.html","id":"customising-color-or-fill-mappings","chapter":"13 Visualising data with ggplot2","heading":"13.5.2 Customising color (or fill) mappings","text":"Readers advised read appendix wherein \ndiscussed color aesthetic pretty details. far know \ncontinuous data mapped color aesthetic gives us gradient color\nscale discrete data hand, provides us discrete color\nvalues.continuous variables thus scale_fill_continuous() function\nturn defaulting scale_fill_gradient(). words, \ndefault colors picked scale_fill_gradient function, uses\nfollowing mentioned colors. Readers may also note fill scale\nfunctions corresponding color scale function used \ncolor aesthetic.may change desired colors color gradient. two\nrelated scales.scale_fill_gradient2() produces three-colour gradient \nspecified midpointscale_fill_gradientn() produces n-colour gradient.usage may clear following examples.\nFigure 13.58: Customising Fill scale\nhowever, variable mapped fill color scale \ndiscrete, default scale picked ggplot2 \nscale_fill_discrete picks values corresponding\nscale_fill_hue() default.However, discrete color/fill scales advised use brewer\npalettes described Appendix , due slight\ncomplexity involved hue scales. Moreover, want pick \ncolors manually, may use scale_fill_manual() takes color\nvalues named vector names categories available \nvariable. See following example figure 13.59.\nFigure 13.59: Customising Fill scale manually\nAnother set scales ggplot2, sometimes useful \nscale_color_identity() useful data already \nscaled, .e. already represents aesthetic values ggplot2 can\nhandle directly. See following example figure 13.60.\nFigure 13.60: Identity Color Scale\n","code":"scale_colour_gradient(\n  name = waiver(),\n  ...,\n  low = \"#132B43\",\n  high = \"#56B1F7\",\n  space = \"Lab\",\n  na.value = \"grey50\",\n  guide = \"colourbar\",\n  aesthetics = \"colour\"\n)\neconomics |> \n  mutate(month = month(date, label = TRUE, abbr = TRUE),\n         year = year(date)) |> \n  ggplot(aes(year, month)) +\n  geom_tile(aes(fill = psavert)) +\n  scale_fill_gradient2(low = \"red\",\n                       high = \"seagreen\",\n                       midpoint = mean(economics$psavert))\nggplot(mpg, aes(fl, fill = fl)) +\n  geom_bar()\n\nggplot(mpg, aes(fl, fill = fl)) +\n  geom_bar() +\n  scale_fill_manual(values = c(c = \"seagreen\", d = \"tomato4\",\n                               e = \"dodgerblue\", p = \"cadetblue3\",\n                               r = \"indianred\"))\ndf <- data.frame(\n  x = 1:4,\n  y = 1:4,\n  colour = c(\"red\", \"green\", \"blue\", \"yellow\")\n)\n\nggplot(df, aes(x, y)) + \n  geom_tile(aes(fill = colour))\n\nggplot(df, aes(x, y)) +\n  geom_tile(aes(fill = colour)) +\n  scale_fill_identity()"},{"path":"visualising-data-with-ggplot2.html","id":"other-scale-aesthetics","chapter":"13 Visualising data with ggplot2","heading":"13.5.3 Other scale aesthetics","text":"Similar position color/fill scales scale functions \naesthetics appropriately named can used modify\nmappings per requirement. e.g. change shape aesthetic \nvalues available data, scale_shape_identity() \nchange shapes manually scale_shape_manual takes named\nvector. Readers advised go functions \nhelp pages use-cases .","code":""},{"path":"visualising-data-with-ggplot2.html","id":"coordinate-systems","chapter":"13 Visualising data with ggplot2","heading":"13.6 Coordinate systems","text":"far, know default coordinate system adopted ggplot2 \nCartesian, requires x y position aesthetics map data \ntwo-dimensional plots. uses coord_cartesian() function along \ndefault values. linear coordinate systems coord_flip(),\nflips x y axes; coord_fixed() preserves fixed\naspect ratio.coord_cartesian() arguments xlim ylim can used \nset limits x y axes respectively, unlike limits\nargument scale_*_* function discard data, \nlimits. example, refer three plots Figure\n13.61, wherein scale_x_continuous changed shape \nsmoothed curve data plot discarded.\nFigure 13.61: Setting limits\nAnother coordinate system coord_flip, flips axes useful \nflip axes. One thing note x axis drawn vertically\nthus, geom drawn takes x value take \nvalues vertical axis horizontal axis.However coordinate systems available ggplot2, \nlisted table 13.3 ready reference.Table 13.3: List Coordinate Systems available ggplot2Out listed coordinate systems, really important \nskipped gaining basic architecture ggplot2. \nknow pie chart basically bar chart drawn polar coordinate\nsystem? answer yes. let’s understand polar coordinate system\nrecently introduced radial coordinate system help us draw \npie chart.following code, 100% stacked bar chart (flipped y axis) \nconverted pie chart adopting coord_polar() bulls-eye\nchart setting theta position aesthetic \"y\" position. Refer\nthree plots figure 13.62.\nFigure 13.62: Polar Coordinates\nrecently introduced coord_radial specifically designed \npie charts, base layer (.e. 100% stacked bar chart) can \nconverted pie chart. (Refer plots figure 13.63.)\nFigure 13.63: Using Radial Coordinates\ncoord_radial can also used create donut charts easily \nsetting parameter inner.radius can seen Figure\n13.64.\nFigure 13.64: Donut Charts Radial Coordinates\ncoord_radial also places labels automatically, adjusting\ninner.radius. Refer plots 13.65.\nFigure 13.65: Labels adjustments Donut Charts Radial Coordinates\nThough lot can done coord_radial , one\nfinal example using coord_radial can drawing bar plots \ncircle, shown figure 13.66.\nFigure 13.66: Bar plot circle\n","code":"\nbase <- ggplot(mpg, aes(displ, hwy)) + \n  geom_point() + \n  geom_smooth()\n\n# Full dataset\nbase\n\n# Scaling to 4--6 throws away data outside that range\nbase + scale_x_continuous(limits = c(4, 6))\n\n# Zooming to 4--6 keeps all the data but only shows some of it\nbase + coord_cartesian(xlim = c(4, 6))\nbase <- ggplot(mtcars, aes(y = factor(1), fill = factor(cyl))) +\n  geom_bar(width = 1) + \n  scale_y_discrete(name = NULL,\n                   guide = \"none\", \n                   expand = c(0, 0)) +\n  scale_fill_discrete(guide = \"none\")\n\n# Stacked barchart\nbase\n\n# Pie chart\nbase + coord_polar()\n\n# The bullseye chart\nbase + coord_polar(theta = \"y\")\n# With default value\nbase + coord_radial()\n\n# Without expansion\nbase + coord_radial(expand = FALSE)\n# Donut Chart\nbase + coord_radial(expand = FALSE,\n                    inner.radius = 0.5)\nbase2 <- mpg |> \n    count(class) |> \n  ggplot(aes(y = n, x = \"\", fill = class)) +\n  geom_col() +\n  geom_text(aes(label = n), \n                 position = position_stack(vjust = 0.5))\n\nbase2 + coord_polar(theta = \"y\")\n\nbase2 +\n  coord_radial(expand = FALSE, \n               theta = \"y\", \n               inner.radius = 0.5)\nmtcars |>\n  #arrange(-mpg) |>\n  rownames_to_column('Car') |> \n  ggplot(aes(seq_along(mpg), mpg, fill = factor(cyl))) +\n  geom_col(width = 1,\n           color = \"white\") +\n  geom_text(aes(y = 32, label = Car),\n            angle = 90,\n            hjust = 1) +\n  geom_text(\n    aes(y = 32, label = mpg),\n    angle = 90,\n    hjust = -1,\n    fontface = \"bold\"\n  ) +\n  coord_radial(rotate.angle = TRUE, expand = FALSE)  +\n  scale_fill_manual(values = c(\"dodgerblue\", \"seagreen\", \"orange\"), guide = \"none\") +\n  theme_void() +\n  theme(panel.background = element_rect(fill = \"aliceblue\")) +\n  ggtitle(\"Miles per gallon for different cars\") +\n  theme(plot.title = element_text(size = 15,\n                                  face = \"bold\", hjust = 0.5))"},{"path":"visualising-data-with-ggplot2.html","id":"faceting","chapter":"13 Visualising data with ggplot2","heading":"13.7 Faceting","text":"amount data also makes difference: lot data \ncan hard distinguish different groups. alternative solution \nuse faceting, described next. Facets, “small-multiples”, \nused split one plot multi-panel figure, one panel\n(“facet”) per group data. type plot created multiple\ntimes, one using sub-group dataset.ggplot2 faceting can achieved using either functions -facet_grid() creates grid plots, plot showing \nsubset data. may also specify number columns use\ngrid using ncol argument.facet_wrap() creates grid plots different variables \naxis. may also specify scales use axis using\nscales argument.Let us understand , following examples. figure\n13.67 facet_grid() used. Notice \nfacet_grid() arranges plots grid different variables \naxis. specify variables use faceting using ~\noperator. example, facet_grid(variable1 ~ variable2) create \ngrid plots variable1 y-axis variable2 \nx-axis. useful want compare relationship \ntwo variables across different levels third variable.\nFigure 13.67: Wrapping sub plots facets\nhand, facet_wrap() creates grid plots, showing\nsubset data based single variable. specify \nvariable use faceting using ~ operator . \nexample, facet_wrap(~ variable) create grid plots, \nshowing different level variable. useful \nsingle categorical variable want use faceting. See\nexample figure 13.68.\nFigure 13.68: Grid alignment sub plots facets\n","code":"\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point() + \n  facet_wrap(~ class, ncol = 2)\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point() + \n  facet_grid(year ~ class)"},{"path":"visualising-data-with-ggplot2.html","id":"labeling-and-annotating-charts","chapter":"13 Visualising data with ggplot2","heading":"13.8 Labeling and Annotating Charts","text":"Labeling essential aspect data visualization provides context information data presented. Labels can include titles, axis labels, legends, annotations describe data provide important information helps viewer understand looking . Proper labeling can help make data understandable, clear, accessible, enhances overall value impact.","code":""},{"path":"visualising-data-with-ggplot2.html","id":"annotations","chapter":"13 Visualising data with ggplot2","heading":"13.8.1 Annotations","text":"already covered use geoms like geom_text, geom_label, geom_vline, geom_abline, etc., used label annotate geometries plot. several geoms used annotate charts ggplot2. Let us discuss .geom_rect() used draw rectangle plot area using aesthetics xmin, xmax, ymin ymaxgeom_segment() can create line segment.Moreover, helper function annotate also adds geoms plot, unlike typical geom function, properties geoms mapped variables data frame, instead passed vectors. useful adding small annotations (text labels) data vectors, reason don’t want put data frame. See example plot Figure 13.69 wherein added five annotation elements () rectangle, (ii) two text labels (iii) two arrows.\nFigure 13.69: Annotating charts\n","code":"\nggplot(economics, aes(date, unemploy)) +\n  annotate(\n    geom = \"rect\",\n    xmin = ymd(\"20071201\"),\n    xmax = ymd(\"20091231\"),\n    ymin = -Inf,\n    ymax = Inf,\n    fill = \"orange\",\n    alpha = 0.5\n  ) +\n  annotate(\n    geom = \"label\",\n    x = ymd(\"20070801\"),\n    y = 2500,\n    label = \"Global Financial Crisis\",\n    hjust = 1\n  ) +\n  annotate(\n    geom = \"curve\",\n    xend = ymd(\"20080101\"),\n    yend = 4000,\n    x = ymd(\"20000101\"),\n    y = 3000,\n    curvature = -0.5,\n    arrow = arrow(length = unit(0.5, 'cm'))\n  ) +\n  annotate(\n    geom = \"text\",\n    x = ymd(\"20070801\"),\n    y = 14000,\n    label = \"Sharp rise in unemployment\",\n    hjust = 1,\n    fontface = \"bold.italic\"\n  ) +\n  annotate(\n    geom = \"curve\",\n    xend = ymd(\"20080701\"),\n    yend = 12000,\n    x = ymd(\"20000101\"),\n    y = 13500,\n    curvature = 0.5,\n    arrow = arrow(length = unit(0.5, 'cm'))\n  ) +\n  geom_line(color = \"indianred4\")"},{"path":"visualising-data-with-ggplot2.html","id":"labels","chapter":"13 Visualising data with ggplot2","heading":"13.8.2 Labels","text":"Effective labeling crucial ensuring plots accessible comprehensible wider audience. achieve , ’s important use full variable names axis legend labels, enhances clarity. plot’s title subtitle employed communicate primary insights key takeaways, making information digestible glance. caption serves valuable space include details data source, providing context credibility. Furthermore, tag feature can utilized add identification markers, particularly helpful comparing displaying multiple plots within project.several ways add labels ggplot2 charts, focus using labs() function, allows us add titles, subtitles, axis labels, annotations like caption, etc., plot. Example -\nFigure 13.70: properly labelled chart\n","code":"\nggplot(mtcars, aes(x = hp, y = mpg, color = factor(cyl))) +\n  geom_point() +\n  labs(title = \"Scatter plot of mpg vs. hp\",\n       subtitle = \"Data from mtcars dataset\",\n       x = \"Horsepower\",\n       y = \"Miles per gallon\",\n       color = \"Cylinders\",\n       caption = \"Source: R datasets\", \n       tag = \"Chart-1\")"},{"path":"visualising-data-with-ggplot2.html","id":"themes","chapter":"13 Visualising data with ggplot2","heading":"13.9 Themes","text":"Till now (except cases) used default themes plots generated ggplot2. Themes can however used customise appearance, visual aesthetics exercising fine control non-data elements plot. can customize appearance plots, axis labels, titles, background colors, font sizes, styles, etc. applying themes plot.certain complete themes available ggplot2 set theme elements values designed work harmoniously. Defualt theme applied plot ggplot2 theme_gray(). complete list themes available ggplot2 given table 13.4 :Table 13.4:  Themes ggplot2Example plots four themes can seen figure 13.71.\nFigure 13.71: complete themes available ggplot2\nIndividual elements can also, however, modified function theme available ggplot2. theme function consist element name modified, argument function. element can modified providing values argument, times provided element function like element_text, etc. elements commonly requiring customisation theme function explained figure 13.72.\nFigure 13.72: theme elements\nexample refer plot figure 13.73 wherein certain plot elements modified using theme function.\nFigure 13.73: Customising Themes GGplot2\ncan combine multiple customization options together create customized theme fits specific needs using theme_set(). possibilities customization endless, feel free experiment create unique theme!","code":"\nmyplot<- ggplot(mpg, aes(x = cty, y = hwy)) +\n  geom_point(size = 2, position = position_jitter(seed = 42), aes(color = factor(class))) +\n  geom_point(shape=21, size = 4.5, position = position_jitter(seed = 42), aes(color = factor(class))) +\n  geom_smooth(method = \"lm\", se = FALSE, formula = \"y ~ x\", linewidth = 0.5) +\n  labs(title = \"City Vs. Highway Mileages\",\n       caption = \"Data Source: MPG Dataset\",\n       x = \"Mileage in City (Miles per Gallon)\",\n       y = \"Milaege in Highway (Miles per Gallon)\",\n       color = \"Class\") +\n  theme(\n    panel.background = element_rect(fill = NA),\n    axis.line = element_line(colour = \"seagreen4\",\n                             linetype = \"solid\"),\n    axis.ticks = element_line(colour = \"darkorchid4\"),\n    axis.text.x = element_text(face = \"bold\", size = 10),\n    axis.text.y = element_text(face = \"bold\", size = 10),\n    axis.title = element_text(face = \"bold\", size = 12),\n    plot.title = element_text(face = \"bold\", hjust = 0.5, size = 18),\n    plot.caption = element_text(face = \"italic\", size = 12),\n    legend.title = element_text(face = \"bold.italic\", size = 12),\n    legend.text = element_text(size = 12),\n    legend.position = c(1, 1),\n    legend.justification = c(1, 1),\n    legend.background = element_rect(size = 1, fill = \"white\", colour = \"grey\")\n  )\n\nmyplot"},{"path":"visualising-data-with-ggplot2.html","id":"savingexporting-plots","chapter":"13 Visualising data with ggplot2","heading":"13.10 Saving/exporting plots","text":"course, creating charts/plots like save usage reports/documents, etc. Though may many options save plot disk, focusing three different methods.","code":""},{"path":"visualising-data-with-ggplot2.html","id":"saving-through-rstudio-menu","chapter":"13 Visualising data with ggplot2","heading":"Saving through Rstudio menu","text":"save graph using RStudio menus, go Plots tab choose Export.\nFigure 13.74: Exporting Charts\nThree options available .Save ImageSave PDFCopy clipboard.can use options per requirement.","code":""},{"path":"visualising-data-with-ggplot2.html","id":"saving-through-code","chapter":"13 Visualising data with ggplot2","heading":"Saving through code","text":"may also save plots using function ggsave() . syntax simpleAll arguments simple understand. Thus example need save plot generated figure 13.73, can use ggsave.","code":"ggsave(\n  filename,\n  plot = last_plot(),\n  device = NULL,\n  path = NULL,\n  scale = 1,\n  width = NA,\n  height = NA,\n  units = c(\"in\", \"cm\", \"mm\", \"px\"),\n  dpi = 300,\n  limitsize = TRUE,\n  bg = NULL,\n  ...\n)ggsave('Mileages.png', myplot, height = 10, width = 8)"},{"path":"visualising-data-with-ggplot2.html","id":"graphics-devices-base-r-plots","chapter":"13 Visualising data with ggplot2","heading":"Graphics Devices (Base R Plots)","text":"created plots outside ggplot (plot(), hist(),\nboxplot(), etc.), use ggsave() save plots since \nsupports plots made ggplot.Base R provides way save plots graphic device\nfunctions. three steps involved process-Specify file extension properties (size, resolution, etc.)\nalong unitscreate plot, base R /ggplot2Signal plot finished save running dev.().\nThus, using way can insert many charts single pdf\nwithout turning device till pdf ready.Example-","code":"\n# Creates a png file\npng(\n  filename = \"scatter.png\",\n  width = 5,\n  height = 3,\n  units = \"in\",\n  res = 300\n)\n# Prints a ggplot2 in it\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  geom_abline(intercept = 5,\n              slope = 3,\n              color = \"seagreen\")\n# Device is off\ndev.off()## png \n##   2\n# Creates a new PDF file\npdf(file = \"two_page.pdf\",\n    width = 6,\n    height = 4)\n#first plot\nplot(mtcars$wt, mtcars$mpg)\nabline(a = 5, b = 3, col = \"red\")\n# Second Plot\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  geom_abline(intercept = 5,\n              slope = 3,\n              color = \"seagreen\")\n# Device Off\ndev.off()## png \n##   2"},{"path":"DPLYRR.html","id":"DPLYRR","chapter":"14 Data Transformation in dplyr","heading":"14 Data Transformation in dplyr","text":"Prerequisites\nObviously dplyr22 needed. package also comes matrittr pipe .e. %>% therefore dplyr syntax using pipes. library(magrittr) needed.package dplyr (14.1) calls functions ‘verbs’ actually action. dplyr verbs can divided three classifications depending upon operate -‘Row’ verbs operate Rows‘Column’ verbs‘group’ verbs operate table split different groups.\nFigure 14.1: Package Dplyr\nLet’s learn -","code":"\nlibrary(dplyr)\nlibrary(knitr)"},{"path":"DPLYRR.html","id":"column-verb-select","chapter":"14 Data Transformation in dplyr","heading":"14.1 Column verb select()","text":"real world data sets often come across data frames numerous columns. However many data analysis tasks, columns needed. already stated select (figure 14.2) operates columns. Like SELECT SQL, just select column(s) given data frame. basic syntax - select(data_frame, column_names, ...). pipes syntax goes like \nFigure 14.2: Illustration dplyr::select()\nexample, let’s try hands mtcars dataset.\nExample-1Note output still data frame unlike mtcars[['mpg']] returns vector. can subset multiple columns . Example-2We can also provide column numbers instead names. Example-3We can also use select reorder columns output, using dplyr helping verb everything() basically everything else. See example-may also use mix match column names column numbers. SeeOperator : can also used column names. Ex-helping verbs\nhelping verbs, apart everything() can used within select() just eliminate need type column names select columns based conditions. verbs self explanatory-starts_with('ABC') select columns names starts string ABCends_with('ABC') select columns names ends string ABCcontains('ABC') select columns names contains string ABCnum_range('', 1:3) select columns named A1, A2 A3Some Examples.syntax select columns ends \"color\" column names.Example-2: syntax select columns contains characters \"\" column names.Using (): selection helper selects variables function applied column names, returns TRUE.Example-3: syntax select numeric columns dataset.","code":"data_frame %>% \n  select(column_name)\nmtcars %>% \n  select(mpg)##                      mpg\n## Mazda RX4           21.0\n## Mazda RX4 Wag       21.0\n## Datsun 710          22.8\n## Hornet 4 Drive      21.4\n## Hornet Sportabout   18.7\n## Valiant             18.1\n## Duster 360          14.3\n## Merc 240D           24.4\n## Merc 230            22.8\n## Merc 280            19.2\n## Merc 280C           17.8\n## Merc 450SE          16.4\n## Merc 450SL          17.3\n## Merc 450SLC         15.2\n## Cadillac Fleetwood  10.4\n## Lincoln Continental 10.4\n## Chrysler Imperial   14.7\n## Fiat 128            32.4\n## Honda Civic         30.4\n## Toyota Corolla      33.9\n## Toyota Corona       21.5\n## Dodge Challenger    15.5\n## AMC Javelin         15.2\n## Camaro Z28          13.3\n## Pontiac Firebird    19.2\n## Fiat X1-9           27.3\n## Porsche 914-2       26.0\n## Lotus Europa        30.4\n## Ford Pantera L      15.8\n## Ferrari Dino        19.7\n## Maserati Bora       15.0\n## Volvo 142E          21.4\nmtcars %>% \n  select(mpg, qsec) %>% \n  head()##                    mpg  qsec\n## Mazda RX4         21.0 16.46\n## Mazda RX4 Wag     21.0 17.02\n## Datsun 710        22.8 18.61\n## Hornet 4 Drive    21.4 19.44\n## Hornet Sportabout 18.7 17.02\n## Valiant           18.1 20.22\nmtcars %>% \n  select(4, 6) %>% \n  tail()##                 hp    wt\n## Porsche 914-2   91 2.140\n## Lotus Europa   113 1.513\n## Ford Pantera L 264 3.170\n## Ferrari Dino   175 2.770\n## Maserati Bora  335 3.570\n## Volvo 142E     109 2.780\nmtcars %>% \n  select(qsec, mpg, everything()) %>% \n  names()##  [1] \"qsec\" \"mpg\"  \"cyl\"  \"disp\" \"hp\"   \"drat\" \"wt\"   \"vs\"   \"am\"   \"gear\"\n## [11] \"carb\"\nmtcars %>% \n  select(5, 7, mpg, everything()) %>% \n  names()##  [1] \"drat\" \"qsec\" \"mpg\"  \"cyl\"  \"disp\" \"hp\"   \"wt\"   \"vs\"   \"am\"   \"gear\"\n## [11] \"carb\"\nmtcars %>% \n  select(mpg:drat) %>% \n  head(n=4)##                 mpg cyl disp  hp drat\n## Mazda RX4      21.0   6  160 110 3.90\n## Mazda RX4 Wag  21.0   6  160 110 3.90\n## Datsun 710     22.8   4  108  93 3.85\n## Hornet 4 Drive 21.4   6  258 110 3.08\nstarwars %>% \n  select(ends_with('color'))## # A tibble: 87 × 3\n##    hair_color    skin_color  eye_color\n##    <chr>         <chr>       <chr>    \n##  1 blond         fair        blue     \n##  2 <NA>          gold        yellow   \n##  3 <NA>          white, blue red      \n##  4 none          white       yellow   \n##  5 brown         light       brown    \n##  6 brown, grey   light       blue     \n##  7 brown         light       blue     \n##  8 <NA>          white, red  red      \n##  9 black         light       brown    \n## 10 auburn, white fair        blue-gray\n## # ℹ 77 more rows\nstarwars %>% \n  select(contains('or'))## # A tibble: 87 × 4\n##    hair_color    skin_color  eye_color homeworld\n##    <chr>         <chr>       <chr>     <chr>    \n##  1 blond         fair        blue      Tatooine \n##  2 <NA>          gold        yellow    Tatooine \n##  3 <NA>          white, blue red       Naboo    \n##  4 none          white       yellow    Tatooine \n##  5 brown         light       brown     Alderaan \n##  6 brown, grey   light       blue      Tatooine \n##  7 brown         light       blue      Tatooine \n##  8 <NA>          white, red  red       Tatooine \n##  9 black         light       brown     Tatooine \n## 10 auburn, white fair        blue-gray Stewjon  \n## # ℹ 77 more rows\nstarwars %>% \n  select(where(is.numeric))## # A tibble: 87 × 3\n##    height  mass birth_year\n##     <int> <dbl>      <dbl>\n##  1    172    77       19  \n##  2    167    75      112  \n##  3     96    32       33  \n##  4    202   136       41.9\n##  5    150    49       19  \n##  6    178   120       52  \n##  7    165    75       47  \n##  8     97    32       NA  \n##  9    183    84       24  \n## 10    182    77       57  \n## # ℹ 77 more rows"},{"path":"DPLYRR.html","id":"column-verbmutate","chapter":"14 Data Transformation in dplyr","heading":"14.2 Column verbmutate()","text":"perhaps one important functions dplyr kitty. enables us create new column(s) functions one existing columns. Refer figure 14.3\nFigure 14.3: Illustration dplyr::mutate()\none column can added simultaneously. Newly created column may also used creation another new column. See example.default new column added last data frame. shown example, operations can combined one using %>%. cousin transmute() mutate drops old columns keeps newly created columns. ExampleOther useful dplyr functions Another good use mutate generate summarised result display corresponding row data. example requirement calculate proportion say wt column mtcars data.n() used count number rowsn_distinct() used count number distinct values given variable","code":"\nstarwars %>% \n  select(name:mass) %>% \n  mutate(name_upper = toupper(name),\n         BMI = mass/(height/100)^2)## # A tibble: 87 × 5\n##    name               height  mass name_upper           BMI\n##    <chr>               <int> <dbl> <chr>              <dbl>\n##  1 Luke Skywalker        172    77 LUKE SKYWALKER      26.0\n##  2 C-3PO                 167    75 C-3PO               26.9\n##  3 R2-D2                  96    32 R2-D2               34.7\n##  4 Darth Vader           202   136 DARTH VADER         33.3\n##  5 Leia Organa           150    49 LEIA ORGANA         21.8\n##  6 Owen Lars             178   120 OWEN LARS           37.9\n##  7 Beru Whitesun Lars    165    75 BERU WHITESUN LARS  27.5\n##  8 R5-D4                  97    32 R5-D4               34.0\n##  9 Biggs Darklighter     183    84 BIGGS DARKLIGHTER   25.1\n## 10 Obi-Wan Kenobi        182    77 OBI-WAN KENOBI      23.2\n## # ℹ 77 more rows\nstarwars %>% \n  transmute(name_upper = toupper(name))## # A tibble: 87 × 1\n##    name_upper        \n##    <chr>             \n##  1 LUKE SKYWALKER    \n##  2 C-3PO             \n##  3 R2-D2             \n##  4 DARTH VADER       \n##  5 LEIA ORGANA       \n##  6 OWEN LARS         \n##  7 BERU WHITESUN LARS\n##  8 R5-D4             \n##  9 BIGGS DARKLIGHTER \n## 10 OBI-WAN KENOBI    \n## # ℹ 77 more rows\nmtcars %>% \n  head() %>% \n  select(wt) %>% \n  mutate(total_wt = sum(wt),\n         wt_proportion = wt*100/total_wt) ##                      wt total_wt wt_proportion\n## Mazda RX4         2.620    17.93      14.61238\n## Mazda RX4 Wag     2.875    17.93      16.03458\n## Datsun 710        2.320    17.93      12.93921\n## Hornet 4 Drive    3.215    17.93      17.93084\n## Hornet Sportabout 3.440    17.93      19.18572\n## Valiant           3.460    17.93      19.29727\nmtcars %>% \n  select(1:5) %>% \n  mutate(total_cars = n()) %>% \n  head()##                    mpg cyl disp  hp drat total_cars\n## Mazda RX4         21.0   6  160 110 3.90         32\n## Mazda RX4 Wag     21.0   6  160 110 3.90         32\n## Datsun 710        22.8   4  108  93 3.85         32\n## Hornet 4 Drive    21.4   6  258 110 3.08         32\n## Hornet Sportabout 18.7   8  360 175 3.15         32\n## Valiant           18.1   6  225 105 2.76         32"},{"path":"DPLYRR.html","id":"column-verb-rename","chapter":"14 Data Transformation in dplyr","heading":"14.3 Column verb rename()","text":"used rename column names. Refer figure 14.4 illustration.\nFigure 14.4: Illustration dplyr::rename()\nSee exampleNote select can also rename columns drop unselected columns. Check ","code":"\nmtcars %>% \n  rename(miles_per_gallon = mpg) %>% \n  head(3)##               miles_per_gallon cyl disp  hp drat    wt  qsec vs am gear carb\n## Mazda RX4                 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag             21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n## Datsun 710                22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nmtcars %>% \n  select(miles_per_gallon = mpg) %>% \n  head(3)##               miles_per_gallon\n## Mazda RX4                 21.0\n## Mazda RX4 Wag             21.0\n## Datsun 710                22.8"},{"path":"DPLYRR.html","id":"column-verb-relocate","chapter":"14 Data Transformation in dplyr","heading":"14.4 Column verb relocate()","text":"relocates column block columns simultaneosly either column mentioned argument .mentioned .. See example-","code":"\nstarwars %>% \n  relocate(ends_with('color'), .after = name) %>% \n  head(5)## # A tibble: 5 × 14\n##   name      hair_color skin_color eye_color height  mass birth_year sex   gender\n##   <chr>     <chr>      <chr>      <chr>      <int> <dbl>      <dbl> <chr> <chr> \n## 1 Luke Sky… blond      fair       blue         172    77       19   male  mascu…\n## 2 C-3PO     <NA>       gold       yellow       167    75      112   none  mascu…\n## 3 R2-D2     <NA>       white, bl… red           96    32       33   none  mascu…\n## 4 Darth Va… none       white      yellow       202   136       41.9 male  mascu…\n## 5 Leia Org… brown      light      brown        150    49       19   fema… femin…\n## # ℹ 5 more variables: homeworld <chr>, species <chr>, films <list>,\n## #   vehicles <list>, starships <list>"},{"path":"DPLYRR.html","id":"row-verb-filter","chapter":"14 Data Transformation in dplyr","heading":"14.5 Row verb filter","text":"verb/function used subset data, words filter rows data frame based certain condition. Refer figure 14.5 illustration.\nFigure 14.5: Illustration dplyr::filter()\nSee example-Multiple conditions can passed simultaneously. ExampleNote conditions act simultaneously operator used. used, use | explicitly","code":"\nstarwars %>% \n  filter(eye_color %in% c('red', 'yellow'))## # A tibble: 16 × 14\n##    name     height  mass hair_color skin_color eye_color birth_year sex   gender\n##    <chr>     <int> <dbl> <chr>      <chr>      <chr>          <dbl> <chr> <chr> \n##  1 C-3PO       167    75 <NA>       gold       yellow         112   none  mascu…\n##  2 R2-D2        96    32 <NA>       white, bl… red             33   none  mascu…\n##  3 Darth V…    202   136 none       white      yellow          41.9 male  mascu…\n##  4 R5-D4        97    32 <NA>       white, red red             NA   none  mascu…\n##  5 Palpati…    170    75 grey       pale       yellow          82   male  mascu…\n##  6 IG-88       200   140 none       metal      red             15   none  mascu…\n##  7 Bossk       190   113 none       green      red             53   male  mascu…\n##  8 Nute Gu…    191    90 none       mottled g… red             NA   male  mascu…\n##  9 Watto       137    NA black      blue, grey yellow          NA   male  mascu…\n## 10 Darth M…    175    80 none       red        yellow          54   male  mascu…\n## 11 Dud Bolt     94    45 none       blue, grey yellow          NA   male  mascu…\n## 12 Ki-Adi-…    198    82 white      pale       yellow          92   male  mascu…\n## 13 Yarael …    264    NA none       white      yellow          NA   male  mascu…\n## 14 Poggle …    183    80 none       green      yellow          NA   male  mascu…\n## 15 Zam Wes…    168    55 blonde     fair, gre… yellow          NA   fema… femin…\n## 16 Dexter …    198   102 none       brown      yellow          NA   male  mascu…\n## # ℹ 5 more variables: homeworld <chr>, species <chr>, films <list>,\n## #   vehicles <list>, starships <list>\nstarwars %>% \n  filter(skin_color == 'white',\n         height >= 150)## # A tibble: 2 × 14\n##   name      height  mass hair_color skin_color eye_color birth_year sex   gender\n##   <chr>      <int> <dbl> <chr>      <chr>      <chr>          <dbl> <chr> <chr> \n## 1 Darth Va…    202   136 none       white      yellow          41.9 male  mascu…\n## 2 Yarael P…    264    NA none       white      yellow          NA   male  mascu…\n## # ℹ 5 more variables: homeworld <chr>, species <chr>, films <list>,\n## #   vehicles <list>, starships <list>\nstarwars %>% \n  filter(skin_color == 'white' | height >= 150) %>% \n  nrow()## [1] 71"},{"path":"DPLYRR.html","id":"slice_func","chapter":"14 Data Transformation in dplyr","heading":"14.6 Row verbs slice() / slice_*()","text":"slice() cousins also filters rows based rows placement. , data_fr %>% slice(1:5) filter first five rows data_fr. See exampleOther slice() cousins -slice_head(5) slice first 5 rowsslice_tail(10) slice last 10 rowsslice_min() slice_max() slice rows highest lowest values given variable. full syntax slice_max(.data, order_by, ..., n, prop, with_ties = TRUE) equivalentslice_sample() randomly select rows. syntax slice_sample(.data, ..., n, prop, weight_by = NULL, replace = FALSE)Example-1:Example-2:","code":"\nstarwars %>% \n  slice(4:10) # filter 4 to 10th row## # A tibble: 7 × 14\n##   name      height  mass hair_color skin_color eye_color birth_year sex   gender\n##   <chr>      <int> <dbl> <chr>      <chr>      <chr>          <dbl> <chr> <chr> \n## 1 Darth Va…    202   136 none       white      yellow          41.9 male  mascu…\n## 2 Leia Org…    150    49 brown      light      brown           19   fema… femin…\n## 3 Owen Lars    178   120 brown, gr… light      blue            52   male  mascu…\n## 4 Beru Whi…    165    75 brown      light      blue            47   fema… femin…\n## 5 R5-D4         97    32 <NA>       white, red red             NA   none  mascu…\n## 6 Biggs Da…    183    84 black      light      brown           24   male  mascu…\n## 7 Obi-Wan …    182    77 auburn, w… fair       blue-gray       57   male  mascu…\n## # ℹ 5 more variables: homeworld <chr>, species <chr>, films <list>,\n## #   vehicles <list>, starships <list>\nstarwars %>% \n  slice_min(height, n=3)## # A tibble: 3 × 14\n##   name      height  mass hair_color skin_color eye_color birth_year sex   gender\n##   <chr>      <int> <dbl> <chr>      <chr>      <chr>          <dbl> <chr> <chr> \n## 1 Yoda          66    17 white      green      brown            896 male  mascu…\n## 2 Ratts Ty…     79    15 none       grey, blue unknown           NA male  mascu…\n## 3 Wicket S…     88    20 brown      brown      brown              8 male  mascu…\n## # ℹ 5 more variables: homeworld <chr>, species <chr>, films <list>,\n## #   vehicles <list>, starships <list>\nset.seed(2022)\nstarwars %>% \n  slice_sample(prop = 0.1) #sample 10% rows## # A tibble: 8 × 14\n##   name      height  mass hair_color skin_color eye_color birth_year sex   gender\n##   <chr>      <int> <dbl> <chr>      <chr>      <chr>          <dbl> <chr> <chr> \n## 1 Ki-Adi-M…    198  82   white      pale       yellow            92 male  mascu…\n## 2 Grievous     216 159   none       brown, wh… green, y…         NA male  mascu…\n## 3 Saesee T…    188  NA   none       pale       orange            NA male  mascu…\n## 4 Wat Tamb…    193  48   none       green, gr… unknown           NA male  mascu…\n## 5 Jango Fe…    183  79   black      tan        brown             66 male  mascu…\n## 6 Owen Lars    178 120   brown, gr… light      blue              52 male  mascu…\n## 7 Luminara…    170  56.2 black      yellow     blue              58 fema… femin…\n## 8 BB8           NA  NA   none       none       black             NA none  mascu…\n## # ℹ 5 more variables: homeworld <chr>, species <chr>, films <list>,\n## #   vehicles <list>, starships <list>"},{"path":"DPLYRR.html","id":"row-verb-arrange","chapter":"14 Data Transformation in dplyr","heading":"14.7 Row verb arrange()","text":"verb also act upon rows actually rearranges basis condition. Refer figure 14.6 illustration.\nFigure 14.6: Illustration dplyr::arrange()\nExample-","code":"\nstarwars %>% \n  arrange(height) %>% \n  slice(1:5)## # A tibble: 5 × 14\n##   name      height  mass hair_color skin_color eye_color birth_year sex   gender\n##   <chr>      <int> <dbl> <chr>      <chr>      <chr>          <dbl> <chr> <chr> \n## 1 Yoda          66    17 white      green      brown            896 male  mascu…\n## 2 Ratts Ty…     79    15 none       grey, blue unknown           NA male  mascu…\n## 3 Wicket S…     88    20 brown      brown      brown              8 male  mascu…\n## 4 Dud Bolt      94    45 none       blue, grey yellow            NA male  mascu…\n## 5 R2-D2         96    32 <NA>       white, bl… red               33 none  mascu…\n## # ℹ 5 more variables: homeworld <chr>, species <chr>, films <list>,\n## #   vehicles <list>, starships <list>"},{"path":"DPLYRR.html","id":"group-verb-group_by","chapter":"14 Data Transformation in dplyr","heading":"14.8 Group verb group_by()","text":"data analyst hard find using group_by. basically groups rows basis values given variable block variables. returned result still data frame (one ) now rows grouped. Refer figure 14.7 illustration. functions learnt give different result group .\nFigure 14.7: Illustration Grouped Operations dplyr\nNote output simple exampleNote output now 5 groups, though nothing different seen displayed data.operation/verb thus useful used combination verbs.Example-1: many total characters skin_color?Example- 2: Sample 2 rows cyl size mtcars?Also note grouped varaible(s) always available output.","code":"\nstarwars %>% \n  group_by(sex)## # A tibble: 87 × 14\n## # Groups:   sex [5]\n##    name     height  mass hair_color skin_color eye_color birth_year sex   gender\n##    <chr>     <int> <dbl> <chr>      <chr>      <chr>          <dbl> <chr> <chr> \n##  1 Luke Sk…    172    77 blond      fair       blue            19   male  mascu…\n##  2 C-3PO       167    75 <NA>       gold       yellow         112   none  mascu…\n##  3 R2-D2        96    32 <NA>       white, bl… red             33   none  mascu…\n##  4 Darth V…    202   136 none       white      yellow          41.9 male  mascu…\n##  5 Leia Or…    150    49 brown      light      brown           19   fema… femin…\n##  6 Owen La…    178   120 brown, gr… light      blue            52   male  mascu…\n##  7 Beru Wh…    165    75 brown      light      blue            47   fema… femin…\n##  8 R5-D4        97    32 <NA>       white, red red             NA   none  mascu…\n##  9 Biggs D…    183    84 black      light      brown           24   male  mascu…\n## 10 Obi-Wan…    182    77 auburn, w… fair       blue-gray       57   male  mascu…\n## # ℹ 77 more rows\n## # ℹ 5 more variables: homeworld <chr>, species <chr>, films <list>,\n## #   vehicles <list>, starships <list>\nstarwars %>% \n  select(name, skin_color) %>% \n  group_by(skin_color) %>% \n  mutate(total_with_s_c = n())## # A tibble: 87 × 3\n## # Groups:   skin_color [31]\n##    name               skin_color  total_with_s_c\n##    <chr>              <chr>                <int>\n##  1 Luke Skywalker     fair                    17\n##  2 C-3PO              gold                     1\n##  3 R2-D2              white, blue              2\n##  4 Darth Vader        white                    2\n##  5 Leia Organa        light                   11\n##  6 Owen Lars          light                   11\n##  7 Beru Whitesun Lars light                   11\n##  8 R5-D4              white, red               1\n##  9 Biggs Darklighter  light                   11\n## 10 Obi-Wan Kenobi     fair                    17\n## # ℹ 77 more rows\nset.seed(123)\nmtcars %>% \n  group_by(cyl) %>% \n  slice_sample(n=2)## # A tibble: 6 × 11\n## # Groups:   cyl [3]\n##     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n##   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n## 1  22.8     4  141.    95  3.92  3.15  22.9     1     0     4     2\n## 2  21.4     4  121    109  4.11  2.78  18.6     1     1     4     2\n## 3  21       6  160    110  3.9   2.88  17.0     0     1     4     4\n## 4  19.7     6  145    175  3.62  2.77  15.5     0     1     5     6\n## 5  10.4     8  472    205  2.93  5.25  18.0     0     0     3     4\n## 6  13.3     8  350    245  3.73  3.84  15.4     0     0     3     4\nmtcars %>% \n  group_by(cyl) %>% \n  select(drat) %>% # despite not selecting cyl\n  head() # it is available in output## Adding missing grouping variables: `cyl`## # A tibble: 6 × 2\n## # Groups:   cyl [3]\n##     cyl  drat\n##   <dbl> <dbl>\n## 1     6  3.9 \n## 2     6  3.9 \n## 3     4  3.85\n## 4     6  3.08\n## 5     8  3.15\n## 6     6  2.76"},{"path":"DPLYRR.html","id":"group-verb-summarise","chapter":"14 Data Transformation in dplyr","heading":"14.9 Group verb summarise()","text":"verb creates summary row group grouped data frame input, otherwise one single complete operation.\nExample-1:Example-2:","code":"\nmtcars %>% \n  summarise(total_wt = sum(wt))##   total_wt\n## 1  102.952\nmtcars %>% \n  group_by(cyl) %>% \n  summarise(total_wt = sum(wt))## # A tibble: 3 × 2\n##     cyl total_wt\n##   <dbl>    <dbl>\n## 1     4     25.1\n## 2     6     21.8\n## 3     8     56.0"},{"path":"DPLYRR.html","id":"group-verb-ungroup","chapter":"14 Data Transformation in dplyr","heading":"14.10 Group verb ungroup()","text":"now, may noticed grouping data group_by thereafter performing operation like mutate slice returns grouped data. requirement next steps ungrouped data. specially designed function dplyr ungroups grouped data.","code":"\nset.seed(123)\nmtcars %>% \n  group_by(cyl) %>% \n  slice_sample(n=2) %>% \n  ungroup()## # A tibble: 6 × 11\n##     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n##   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n## 1  22.8     4  141.    95  3.92  3.15  22.9     1     0     4     2\n## 2  21.4     4  121    109  4.11  2.78  18.6     1     1     4     2\n## 3  21       6  160    110  3.9   2.88  17.0     0     1     4     4\n## 4  19.7     6  145    175  3.62  2.77  15.5     0     1     5     6\n## 5  10.4     8  472    205  2.93  5.25  18.0     0     0     3     4\n## 6  13.3     8  350    245  3.73  3.84  15.4     0     0     3     4"},{"path":"DPLYRR.html","id":"new-argument-.by-in-mutate-summarise-filter-etc.","chapter":"14 Data Transformation in dplyr","heading":"14.11 New argument .by in mutate, summarise, filter, etc.","text":"Since grouping data one step, performing required step ungrouping somewhat cumbersome, coming dplyr 1.1.0 version, functions mutate(), summarise, slice etc. gained additional argument .wherein variable names performing grouped operations may provided directly, thus eliminating need grouping ungrouping . following two syntax produce exactly results.","code":"\n# Syntax - 1 (old style)\n\nmtcars %>% \n  group_by(cyl) %>% \n  slice(2) %>% \n  ungroup()## # A tibble: 3 × 11\n##     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n##   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n## 1  24.4     4  147.    62  3.69  3.19  20       1     0     4     2\n## 2  21       6  160    110  3.9   2.88  17.0     0     1     4     4\n## 3  14.3     8  360    245  3.21  3.57  15.8     0     0     3     4\n# Syntax 2 (Dplyr 1.1.0 +)\n\nmtcars %>% \n  slice(2, .by = cyl)##                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\n## Mazda RX4 Wag 21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\n## Merc 240D     24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\n## Duster 360    14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4"},{"path":"DPLYRR.html","id":"group-verb-reframe","chapter":"14 Data Transformation in dplyr","heading":"14.12 Group verb reframe()","text":"content development.","code":""},{"path":"DPLYRR.html","id":"other-useful-functions-in-dplyr","chapter":"14 Data Transformation in dplyr","heading":"14.13 Other Useful functions in dplyr","text":"","code":""},{"path":"DPLYRR.html","id":"if_else","chapter":"14 Data Transformation in dplyr","heading":"if_else()","text":"function operates nearly similar base R’s ifelse() two exceptions-extra argument provide values missing values encountered. (See example-1)NA provided specifically. (See Example-2)See examples. Example-1:Example-2:Due additional restrictions, function sometimes faster base R alternative may also useful prevention bugs code output known beforehand.","code":"\nx <- c(-2:2, NA)\nif_else(x>=0, \"positive\", \"negative\", \"missing\")## [1] \"negative\" \"negative\" \"positive\" \"positive\" \"positive\" \"missing\"\nx <- c(-2:2, NA)\nif_else(x>=0, \"positive\", \"negative\", NA_character_)## [1] \"negative\" \"negative\" \"positive\" \"positive\" \"positive\" NA"},{"path":"DPLYRR.html","id":"case_when","chapter":"14 Data Transformation in dplyr","heading":"case_when()","text":"Though ifelse if_else variants provide nesting multiple conditions, yet case_when provides simpler alternative conditions multiple conditions can provided simultaneously. Syntax follows style-See example.","code":"case_when(\n  condition1 ~ value_if_true,\n  condition2 ~ value_if_true,\n  ...,\n  TRUE ~ value_if_all_above_are_false\n)\nset.seed(123)\n# Generate Incomes from random (uniform) distribution\nincome <- runif(7, 1, 9)*100000\nincome## [1] 330062.0 730644.1 427181.5 806413.9 852373.8 136445.2 522484.4\n# tax brackets say 0% upto 2 lakh, then 10% upto 5 Lakh\n# then 20% upto 7.5 lakh otherwise 30%\ntax_slab <- case_when(\n  income <= 200000 ~ 0,\n  income <= 500000 ~ 10,\n  income <= 750000 ~ 20,\n  TRUE ~ 30\n)\n\n# check tax_slab\ndata.frame(\n  income=income,\n  tax_slab = tax_slab\n)##     income tax_slab\n## 1 330062.0       10\n## 2 730644.1       20\n## 3 427181.5       10\n## 4 806413.9       30\n## 5 852373.8       30\n## 6 136445.2        0\n## 7 522484.4       20"},{"path":"DPLYRR.html","id":"case_match","chapter":"14 Data Transformation in dplyr","heading":"case_match()","text":"function somewhat similar case_when. case_when() uses logical expressions left-hand side formula, case_match() uses values match . Syntax likeSo, example rewritten ","code":"case_match(\n  .x,  # A vector whose values are to be matched\n  ..., # Basically for a condition like .x %in% Y, only Y has to be specified\n  .default = NULL, # Default values to be specified for if no match is found\n)\ntax_slab <- case_match(\n  floor(income), # Vector to be checked\n  0:200000 ~ 0, # slab 1\n  200001:500000 ~ 10, # slab 2\n  500001:750000 ~ 20, # slab 3\n  .default = 30 # Slab 4\n)\n\n# check tax_slab\ndata.frame(\n  income=income,\n  tax_slab = tax_slab\n)##     income tax_slab\n## 1 330062.0       10\n## 2 730644.1       20\n## 3 427181.5       10\n## 4 806413.9       30\n## 5 852373.8       30\n## 6 136445.2        0\n## 7 522484.4       20"},{"path":"DPLYRR.html","id":"across","chapter":"14 Data Transformation in dplyr","heading":"14.14 Functional programming in dplyr through across()","text":"content development.","code":""},{"path":"DPLYRR.html","id":"window-functionsoperations","chapter":"14 Data Transformation in dplyr","heading":"14.15 Window functions/operations","text":"learnt using group_by function can create windows data can make calculations separate window specifically.Dplyr provides us useful window functions operate windows.row_number() can used generate row numberdense_rank / min_rank / percent_rank() / ntile() / cume_dist() windowed functions dplyr. Check ?dplyr::ranking complete reference.lead() lag() give leading/lagging value window.consecutive_id() generates unique identifier increments every time variable (combination variables) changes.functions can helpful analyzing time series data.Example-1: Generating row numbers ranksExample-2: Using previous next valuesExample-3: generating run length encoding/consecutive ID","code":"\n# example data\ndf <- data.frame(\n  val = c(10, 2, 3, 2, NA)\n)\n\ndf %>% \n  mutate(\n    row = row_number(),\n    min_rank = min_rank(val),\n    dense_rank = dense_rank(val),\n    perc_rank = percent_rank(val),\n    cume_dist = cume_dist(val)\n  )##   val row min_rank dense_rank perc_rank cume_dist\n## 1  10   1        4          3 1.0000000      1.00\n## 2   2   2        1          1 0.0000000      0.50\n## 3   3   3        3          2 0.6666667      0.75\n## 4   2   4        1          1 0.0000000      0.50\n## 5  NA   5       NA         NA        NA        NA\nOrange %>% \n  group_by(Tree) %>% \n  mutate(prev_circ = lag(circumference),\n         next_circ = lead(circumference))## # A tibble: 35 × 5\n## # Groups:   Tree [5]\n##    Tree    age circumference prev_circ next_circ\n##    <ord> <dbl>         <dbl>     <dbl>     <dbl>\n##  1 1       118            30        NA        58\n##  2 1       484            58        30        87\n##  3 1       664            87        58       115\n##  4 1      1004           115        87       120\n##  5 1      1231           120       115       142\n##  6 1      1372           142       120       145\n##  7 1      1582           145       142        NA\n##  8 2       118            33        NA        69\n##  9 2       484            69        33       111\n## 10 2       664           111        69       156\n## # ℹ 25 more rows\ndf <- data.frame(\n  grp = c(\"A\", \"B\", \"B\", \"A\", \"A\", \"B\"),\n  value = c(1, 2, 3 , 4, 5, 6)\n)\n\ndf %>% \n  mutate(RLE = consecutive_id(grp))##   grp value RLE\n## 1   A     1   1\n## 2   B     2   2\n## 3   B     3   2\n## 4   A     4   3\n## 5   A     5   3\n## 6   B     6   4"},{"path":"combining-tablestabular-data.html","id":"combining-tablestabular-data","chapter":"15 Combining Tables/tabular data","heading":"15 Combining Tables/tabular data","text":"\nFigure 15.1: times, joining two tables required perform analytics\nreal world scenarios, may hardly case analyse one single table. may cases either join tables split multiple smaller tables (e.g. can smaller tables split States-wise), tables may divided various smaller master transaction tables (relational databases).may thus divide data tables joining requirements three broad categories-Simple joins concatenationRelational JoinsFiltering JoinsLet us discuss examples.","code":""},{"path":"combining-tablestabular-data.html","id":"simple-joinsconcatenation","chapter":"15 Combining Tables/tabular data","heading":"15.1 Simple joins/concatenation","text":"Many times tables split smaller tables joined back proceeding data analytics. may join two tables either columnwise (e.g. features rows split separate table) row wise (e.g. fields/columns split smaller tables like separate table State). Diagramatically joins may depicted shown figure 15.2.\nFigure 15.2: Illustration Simple joins/concatenation\n","code":""},{"path":"combining-tablestabular-data.html","id":"column-binding","chapter":"15 Combining Tables/tabular data","heading":"15.1.1 Column binding","text":"already seen data frames act like matrices many ways except fact support heterogeneous data unlike matrices. also discussed ways two matrices can joined. Base R two dedicated functions .e. cbind() rbind() operations.tidyverse (dplyr specifically) two similar functions bind_cols() bind_rows respectively provide us better functionality use cases. syntax finction bind_cols() used concatenating two tables column wise --... represent data frames combined.name_repair argument chooses method rename duplicate column names, .Example-1:Note: data frames merged row-consistent column binding. Try bind_cols(iris[1:3, 1:2], iris[1:4, 3:4]) see results.","code":"bind_cols(\n  ...,\n  .name_repair = c(\"unique\", \"universal\", \"check_unique\")\n)\ndf1 <- iris[1:3, c(1,2,5)]\ndf2 <- iris[1:3, 3:5]\n\nbind_cols(df1, df2, .name_repair = 'universal')## New names:\n## • `Species` -> `Species...3`\n## • `Species` -> `Species...6`##   Sepal.Length Sepal.Width Species...3 Petal.Length Petal.Width Species...6\n## 1          5.1         3.5      setosa          1.4         0.2      setosa\n## 2          4.9         3.0      setosa          1.4         0.2      setosa\n## 3          4.7         3.2      setosa          1.3         0.2      setosa"},{"path":"combining-tablestabular-data.html","id":"row-binding","chapter":"15 Combining Tables/tabular data","heading":"15.1.2 Row binding","text":"syntax used appending rows one tables together one table --... represent data frames combined.id argument creates new column identifiers.understand better, let us see exampleNote: example requirement store data tables names new identifier column just list convert databases list .id take element_names values identifiers. Try bind_rows(list(setosa=setosa, versicolor=versicolor, virginica=virginica), .id = 'Species')","code":"bind_rows(..., .id = NULL)\nsetosa <- iris[1:3, 1:4]\nversicolor <- iris[51:53, 1:4]\nvirginica <- iris[101:103, 1:4]\n\nbind_rows(setosa, versicolor, virginica, .id = 'groups')##   groups Sepal.Length Sepal.Width Petal.Length Petal.Width\n## 1      1          5.1         3.5          1.4         0.2\n## 2      1          4.9         3.0          1.4         0.2\n## 3      1          4.7         3.2          1.3         0.2\n## 4      2          7.0         3.2          4.7         1.4\n## 5      2          6.4         3.2          4.5         1.5\n## 6      2          6.9         3.1          4.9         1.5\n## 7      3          6.3         3.3          6.0         2.5\n## 8      3          5.8         2.7          5.1         1.9\n## 9      3          7.1         3.0          5.9         2.1"},{"path":"combining-tablestabular-data.html","id":"relational-joins","chapter":"15 Combining Tables/tabular data","heading":"15.2 Relational joins","text":"Relational Joins usually needed join multiple tables based primary key secondary keys. joins may either one--one key join one--many key joins. Broadly can either inner joins outer joins. Diagrammatically may represented shown figure 15.3 wherein may noticed additional columns right data frame gets appended left data frame. reason, four joins also known mutating joins.\nFigure 15.3: Illustration Mutating Joins dplyr\nsyntax joins nearly --x y data frames joinedby character vector column names joined bysuffix argument provides suffixes added column names, duplicatekeep argument decides whether join keys x y preserved output?Let’s discuss joins individually.","code":"*_join(x, y, by = NULL, copy = FALSE, suffix = c('.x', '.y'), ... , keep = FALSE)"},{"path":"combining-tablestabular-data.html","id":"inner-joins","chapter":"15 Combining Tables/tabular data","heading":"15.2.1 Inner Joins","text":"Inner Joins keeps rows matching keys present data frames.Example-1:","code":"\nband_members## # A tibble: 3 × 2\n##   name  band   \n##   <chr> <chr>  \n## 1 Mick  Stones \n## 2 John  Beatles\n## 3 Paul  Beatles\nband_instruments## # A tibble: 3 × 2\n##   name  plays \n##   <chr> <chr> \n## 1 John  guitar\n## 2 Paul  bass  \n## 3 Keith guitar\ninner_join(band_members, band_instruments)## Joining with `by = join_by(name)`## # A tibble: 2 × 3\n##   name  band    plays \n##   <chr> <chr>   <chr> \n## 1 John  Beatles guitar\n## 2 Paul  Beatles bass"},{"path":"combining-tablestabular-data.html","id":"left-joins","chapter":"15 Combining Tables/tabular data","heading":"15.2.2 Left Joins","text":"Left Joins hand preserves rows data frame passed x .e. first argument irrespective fact matching key record available second data table .Example-1:","code":"\nleft_join(band_members, band_instruments)## Joining with `by = join_by(name)`## # A tibble: 3 × 3\n##   name  band    plays \n##   <chr> <chr>   <chr> \n## 1 Mick  Stones  <NA>  \n## 2 John  Beatles guitar\n## 3 Paul  Beatles bass"},{"path":"combining-tablestabular-data.html","id":"right-joins","chapter":"15 Combining Tables/tabular data","heading":"15.2.3 Right Joins","text":"Right join, similar left join preserves rows data frame passed y .e. second argument irrespective fact matching key record available first data table .Example-1:","code":"\nright_join(band_members, band_instruments)## Joining with `by = join_by(name)`## # A tibble: 3 × 3\n##   name  band    plays \n##   <chr> <chr>   <chr> \n## 1 John  Beatles guitar\n## 2 Paul  Beatles bass  \n## 3 Keith <NA>    guitar"},{"path":"combining-tablestabular-data.html","id":"full-joins","chapter":"15 Combining Tables/tabular data","heading":"15.2.4 Full Joins","text":"Full join returns rows data tables despite non-availability matching key either tables.Example-1:must noticed examples shown thrown warning join performed variable name. may override warning specifically providing joining key column name(s) argument .e. = \"name\".may cases joining key column(s) two data frames different names. cases can also handled using argument.Example-Note: *_join() can joined multiple keys/columns (.e. one) using argument explained .","code":"\nfull_join(band_members, band_instruments)## Joining with `by = join_by(name)`## # A tibble: 4 × 3\n##   name  band    plays \n##   <chr> <chr>   <chr> \n## 1 Mick  Stones  <NA>  \n## 2 John  Beatles guitar\n## 3 Paul  Beatles bass  \n## 4 Keith <NA>    guitar\nband_instruments2## # A tibble: 3 × 2\n##   artist plays \n##   <chr>  <chr> \n## 1 John   guitar\n## 2 Paul   bass  \n## 3 Keith  guitar\nleft_join(band_members, band_instruments2, by = c('name' = 'artist'))## # A tibble: 3 × 3\n##   name  band    plays \n##   <chr> <chr>   <chr> \n## 1 Mick  Stones  <NA>  \n## 2 John  Beatles guitar\n## 3 Paul  Beatles bass"},{"path":"combining-tablestabular-data.html","id":"many-to-many-joins","chapter":"15 Combining Tables/tabular data","heading":"Many-to-many Joins","text":"Example four -mentioned joins many many joins thus need used carefully. See following exampleIn fact, can use argument relationship explicitly silence warning avoid errors. argument can take one valuesNULL (default)\"one--one\"\"one--many\"\"many--one\"\"many--many\"Example re-coded","code":"\ndf1 <- data.frame(\n  x = c(1, 1, NA),\n  y = c(11, 12, 21)\n)\ndf1##    x  y\n## 1  1 11\n## 2  1 12\n## 3 NA 21\ndf2 <- data.frame(\n  x = c(1, 1, NA, NA),\n  z = c(101, 102, 201, 202)\n)\ndf2##    x   z\n## 1  1 101\n## 2  1 102\n## 3 NA 201\n## 4 NA 202\ndf1 %>% left_join(df2, by = 'x')## Warning in left_join(., df2, by = \"x\"): Detected an unexpected many-to-many relationship between `x` and `y`.\n## ℹ Row 1 of `x` matches multiple rows in `y`.\n## ℹ Row 1 of `y` matches multiple rows in `x`.\n## ℹ If a many-to-many relationship is expected, set `relationship =\n##   \"many-to-many\"` to silence this warning.##    x  y   z\n## 1  1 11 101\n## 2  1 11 102\n## 3  1 12 101\n## 4  1 12 102\n## 5 NA 21 201\n## 6 NA 21 202\ndf1 %>% left_join(df2, by = 'x', relationship = \"many-to-many\")##    x  y   z\n## 1  1 11 101\n## 2  1 11 102\n## 3  1 12 101\n## 4  1 12 102\n## 5 NA 21 201\n## 6 NA 21 202"},{"path":"combining-tablestabular-data.html","id":"filtering-joins","chapter":"15 Combining Tables/tabular data","heading":"15.3 Filtering Joins","text":"joins available dplyr basically filtering joins. joins result appending additional columns present right data frame left data frame. common keys (columns) right data frame used filter left data frame.","code":""},{"path":"combining-tablestabular-data.html","id":"semi-joins","chapter":"15 Combining Tables/tabular data","heading":"15.3.1 Semi Joins","text":"First semi_join essentially filters rows data frame, based another data frame. See example-","code":"\ndf1 <- data.frame(\n  x = c(1, 2, NA),\n  y = c(11, 21, 100)\n)\ndf1##    x   y\n## 1  1  11\n## 2  2  21\n## 3 NA 100\ndf2 <- data.frame(\n  x = c(1, 3, 4, NA),\n  z = c(101, 301, 401, 501)\n)\ndf2##    x   z\n## 1  1 101\n## 2  3 301\n## 3  4 401\n## 4 NA 501\ndf1 %>% semi_join(df2, by = 'x')##    x   y\n## 1  1  11\n## 2 NA 100"},{"path":"combining-tablestabular-data.html","id":"anti-joins","chapter":"15 Combining Tables/tabular data","heading":"15.3.2 Anti Joins","text":"anti_join basically opposite semi_join. keeps records left data-frame available right data-frame. Example","code":"\ndf1 %>% anti_join(df2, by = 'x')##   x  y\n## 1 2 21"},{"path":"combining-tablestabular-data.html","id":"relational-but-non-equi-joins","chapter":"15 Combining Tables/tabular data","heading":"15.4 Relational, but non-equi joins","text":"Till now, learnt joins use syntax dplyr version 1.1.0. However, dplyr newest version, 1.1.0, released January 2023, new helper function join_by() introduced. actually, constructs specification describes join two tables using small domain specific language. result can supplied argument join functions learnt . equal joins, learnt , two slight easier implement changes use join_by() -may pass variable names argument instead using characters described .instead using = use == actually equality operator opposed assignment operator using earlier.syntax left_join example isSimple, isn’t . now using potential new helper function, can hand join two (data frames) using inequality conditions.","code":"\nband_instruments2## # A tibble: 3 × 2\n##   artist plays \n##   <chr>  <chr> \n## 1 John   guitar\n## 2 Paul   bass  \n## 3 Keith  guitar\nleft_join(band_members, band_instruments2, by = join_by(name == artist))## # A tibble: 3 × 3\n##   name  band    plays \n##   <chr> <chr>   <chr> \n## 1 Mick  Stones  <NA>  \n## 2 John  Beatles guitar\n## 3 Paul  Beatles bass"},{"path":"combining-tablestabular-data.html","id":"inequality-joins","chapter":"15 Combining Tables/tabular data","heading":"15.4.1 Inequality Joins","text":"Inequality joins match inequality, >, >=, <, <=, common time series analysis genomics. construct inequality join using join_by(), supply two column names separated one mentioned inequalities. See following example better understanding inequality joins.Example- Suppose two tables, one containing product-wise sales another containing rates revised. , order total sales amount, can use inequality join.got results, results/rates obtained fixed earlier dates. solve issue, rolling joins. See next section.","code":"\n# sales\nsales <- data.frame(\n  product.id = c(\"A\", \"A\", \"A\", \"A\", \"A\", \"B\", \"B\"),\n  sales.date = as.Date(c(\"01-01-2022\",\n                 \"02-02-2022\",\"05-11-2022\",\"05-04-2022\",\n                 \"05-10-2022\",\"01-02-2022\",\"01-01-2023\"), format = \"%d-%m-%Y\"),\n  quantity = c(5L, 10L, 15L, 5L, 15L, 1L, 2L)\n)\n# Print it\nsales##   product.id sales.date quantity\n## 1          A 2022-01-01        5\n## 2          A 2022-02-02       10\n## 3          A 2022-11-05       15\n## 4          A 2022-04-05        5\n## 5          A 2022-10-05       15\n## 6          B 2022-02-01        1\n## 7          B 2023-01-01        2\n# rates\nrates <- data.frame(\n  product.id = c(\"A\", \"A\", \"A\", \"B\", \"B\"),\n  revision.date = as.Date(c(\"01-02-2022\",\n                    \"03-04-2022\",\"05-10-2022\",\"01-01-2022\",\n                    \"05-10-2022\"), format = \"%d-%m-%Y\"),\n  rate = c(50L, 60L, 70L, 500L, 1500L)\n)\n#print it\nrates##   product.id revision.date rate\n## 1          A    2022-02-01   50\n## 2          A    2022-04-03   60\n## 3          A    2022-10-05   70\n## 4          B    2022-01-01  500\n## 5          B    2022-10-05 1500\n# total sale(1st attempt)\nsales %>% \n  left_join(rates,\n            by = join_by(product.id,\n                         sales.date >= revision.date))##    product.id sales.date quantity revision.date rate\n## 1           A 2022-01-01        5          <NA>   NA\n## 2           A 2022-02-02       10    2022-02-01   50\n## 3           A 2022-11-05       15    2022-02-01   50\n## 4           A 2022-11-05       15    2022-04-03   60\n## 5           A 2022-11-05       15    2022-10-05   70\n## 6           A 2022-04-05        5    2022-02-01   50\n## 7           A 2022-04-05        5    2022-04-03   60\n## 8           A 2022-10-05       15    2022-02-01   50\n## 9           A 2022-10-05       15    2022-04-03   60\n## 10          A 2022-10-05       15    2022-10-05   70\n## 11          B 2022-02-01        1    2022-01-01  500\n## 12          B 2023-01-01        2    2022-01-01  500\n## 13          B 2023-01-01        2    2022-10-05 1500"},{"path":"combining-tablestabular-data.html","id":"rolling-joins","chapter":"15 Combining Tables/tabular data","heading":"15.4.2 Rolling Joins","text":"Rolling joins variant inequality joins limit results returned inequality join condition. useful “rolling” closest match forward/backwards isn’t exact match. construct rolling join, wrap inequality condition closest().example solved wrapping date condition closest()may see rates product sales made 01-01-2022.","code":"\nsales %>% \n  left_join(rates,\n            by = join_by(product.id,\n                         closest(sales.date >= revision.date)))##   product.id sales.date quantity revision.date rate\n## 1          A 2022-01-01        5          <NA>   NA\n## 2          A 2022-02-02       10    2022-02-01   50\n## 3          A 2022-11-05       15    2022-10-05   70\n## 4          A 2022-04-05        5    2022-04-03   60\n## 5          A 2022-10-05       15    2022-10-05   70\n## 6          B 2022-02-01        1    2022-01-01  500\n## 7          B 2023-01-01        2    2022-10-05 1500"},{"path":"combining-tablestabular-data.html","id":"overlap-joins","chapter":"15 Combining Tables/tabular data","heading":"15.4.3 Overlap Joins","text":"Overlap joins special case inequality joins involving one two columns left-hand table overlapping range defined two columns right-hand table. three helpers join_by() recognizes assist constructing overlap joins, can constructed simpler inequalities.(x, y_lower, y_upper, ..., bounds = \"[]\") just short x >= y_lower, x <= y_upperbetween(x, y_lower, y_upper, ..., bounds = \"[]\") just short x >= y_lower, x <= y_upperwithin(x_lower, x_upper, y_lower, y_upper) just short x_lower >= y_lower, x_upper <= y_upperwithin(x_lower, x_upper, y_lower, y_upper) just short x_lower >= y_lower, x_upper <= y_upperoverlaps(x_lower, x_upper, y_lower, y_upper, ..., bounds = \"[]\") short x_lower <= y_upper, x_upper >= y_loweroverlaps(x_lower, x_upper, y_lower, y_upper, ..., bounds = \"[]\") short x_lower <= y_upper, x_upper >= y_lowerWe can make use functions per need.Example-2: Suppose orders data customer-wise orders, corresponsing order dates ship dates. Let us suppose want know customers placed another order, without waiting shipping previous/earlier order.","code":"\norders <- structure(list(customer_id = c(\"A\", \"A\", \"A\", \"B\", \"B\", \"C\", \n\"C\"), order_id = 1:7, order_date = c(\"2019-01-01\", \"2019-01-05\", \n\"2019-01-16\", \"2019-01-05\", \"2019-01-06\", \"2019-01-07\", \"2019-01-09\"\n), ship_date = c(\"2019-01-30\", \"2019-02-10\", \"2019-01-18\", \"2019-01-08\", \n\"2019-01-08\", \"2019-01-08\", \"2019-01-10\")), row.names = c(NA, \n-7L), class = \"data.frame\")\n\n# print \norders##   customer_id order_id order_date  ship_date\n## 1           A        1 2019-01-01 2019-01-30\n## 2           A        2 2019-01-05 2019-02-10\n## 3           A        3 2019-01-16 2019-01-18\n## 4           B        4 2019-01-05 2019-01-08\n## 5           B        5 2019-01-06 2019-01-08\n## 6           C        6 2019-01-07 2019-01-08\n## 7           C        7 2019-01-09 2019-01-10\n# customers who placed the orders without\n# waiting to ship their earlier order\norders %>% \n  inner_join(orders,\n             by = join_by(customer_id,\n                          overlaps(order_date, ship_date, order_date, ship_date),\n                          order_id < order_id)) ##   customer_id order_id.x order_date.x ship_date.x order_id.y order_date.y\n## 1           A          1   2019-01-01  2019-01-30          2   2019-01-05\n## 2           A          1   2019-01-01  2019-01-30          3   2019-01-16\n## 3           A          2   2019-01-05  2019-02-10          3   2019-01-16\n## 4           B          4   2019-01-05  2019-01-08          5   2019-01-06\n##   ship_date.y\n## 1  2019-02-10\n## 2  2019-01-18\n## 3  2019-01-18\n## 4  2019-01-08"},{"path":"combining-tablestabular-data.html","id":"example-uses-of-rolling-joins.","chapter":"15 Combining Tables/tabular data","heading":"15.4.4 Example uses of rolling joins.","text":"content development.","code":""},{"path":"combining-tablestabular-data.html","id":"cross-joins","chapter":"15 Combining Tables/tabular data","heading":"15.4.5 Cross Joins","text":"join matches everything everything therefore can thought cross product two data frames. can use cross_join special join. m n rows two data frames, result cross join m*n rows.Example-","code":"\ndf <- data.frame(products = c('A', 'B', 'C'))\n\ndf %>% \n  cross_join(df)##   products.x products.y\n## 1          A          A\n## 2          A          B\n## 3          A          C\n## 4          B          A\n## 5          B          B\n## 6          B          C\n## 7          C          A\n## 8          C          B\n## 9          C          C"},{"path":"data-wrangling-in-tidyr.html","id":"data-wrangling-in-tidyr","chapter":"16 Data Wrangling in tidyr","heading":"16 Data Wrangling in tidyr","text":"chapter learn reshaping data format suitable data analysis work. reshape data, use tidyr package23 part core tidyverse can loaded either calling library(tidyr) library(tidyverse).Prerequisites","code":"\nlibrary(tidyverse)"},{"path":"data-wrangling-in-tidyr.html","id":"concepts-of-tidy-data","chapter":"16 Data Wrangling in tidyr","heading":"16.1 Concepts of tidy data","text":"Hadley Wickham, chief scientist behind development RStudio, tidyverse, much , introduced concept tidy data paper24 published Journal Statistical Software.25 Tidy data framework structure data sets can easily analyzed visualized. can thought goal one aim cleaning data. understand tidy data , knowledge make data analysis, visualization, collection much easier.26A tidy data-set following properties:variable forms column.observation forms row.type observational unit forms table.Diagrammatically27 can represented figure 16.1.\nFigure 16.1: Diagrammatic representation tidy data\ndataset tidy, can used input variety functions may transform, model, visualize data.Consider five examples28. examples29 represent data shown different formats-Let us discuss example one one -table1 fulfills three rules stated thus, tidy format. Notice every observation row, variable stored separate column.table2 stores one observation two columns (separately cases population) thus tidy.table3 stores two variables one column (cases population together) thus tidy.table4a table4b clearly stores one observation two different tables thus tidy. may notice tables use values column headers, also violate rule .3 stated .table5 stores one variable .e. year two separate columns, thus follow rule .2 stated .","code":"\n#Example-1\ntidyr::table1## # A tibble: 6 × 4\n##   country      year  cases population\n##   <chr>       <dbl>  <dbl>      <dbl>\n## 1 Afghanistan  1999    745   19987071\n## 2 Afghanistan  2000   2666   20595360\n## 3 Brazil       1999  37737  172006362\n## 4 Brazil       2000  80488  174504898\n## 5 China        1999 212258 1272915272\n## 6 China        2000 213766 1280428583\n# Example-2\ntidyr::table2## # A tibble: 12 × 4\n##    country      year type            count\n##    <chr>       <dbl> <chr>           <dbl>\n##  1 Afghanistan  1999 cases             745\n##  2 Afghanistan  1999 population   19987071\n##  3 Afghanistan  2000 cases            2666\n##  4 Afghanistan  2000 population   20595360\n##  5 Brazil       1999 cases           37737\n##  6 Brazil       1999 population  172006362\n##  7 Brazil       2000 cases           80488\n##  8 Brazil       2000 population  174504898\n##  9 China        1999 cases          212258\n## 10 China        1999 population 1272915272\n## 11 China        2000 cases          213766\n## 12 China        2000 population 1280428583\n# Example-3\ntidyr::table3## # A tibble: 6 × 3\n##   country      year rate             \n##   <chr>       <dbl> <chr>            \n## 1 Afghanistan  1999 745/19987071     \n## 2 Afghanistan  2000 2666/20595360    \n## 3 Brazil       1999 37737/172006362  \n## 4 Brazil       2000 80488/174504898  \n## 5 China        1999 212258/1272915272\n## 6 China        2000 213766/1280428583\n# Example-4 (Same data in 2 data tables now)\ntidyr::table4a## # A tibble: 3 × 3\n##   country     `1999` `2000`\n##   <chr>        <dbl>  <dbl>\n## 1 Afghanistan    745   2666\n## 2 Brazil       37737  80488\n## 3 China       212258 213766\ntidyr::table4b## # A tibble: 3 × 3\n##   country         `1999`     `2000`\n##   <chr>            <dbl>      <dbl>\n## 1 Afghanistan   19987071   20595360\n## 2 Brazil       172006362  174504898\n## 3 China       1272915272 1280428583\n# Example-5\ntidyr::table5## # A tibble: 6 × 4\n##   country     century year  rate             \n##   <chr>       <chr>   <chr> <chr>            \n## 1 Afghanistan 19      99    745/19987071     \n## 2 Afghanistan 20      00    2666/20595360    \n## 3 Brazil      19      99    37737/172006362  \n## 4 Brazil      20      00    80488/174504898  \n## 5 China       19      99    212258/1272915272\n## 6 China       20      00    213766/1280428583"},{"path":"data-wrangling-in-tidyr.html","id":"reshaping-data","chapter":"16 Data Wrangling in tidyr","heading":"Reshaping data","text":"real world problems mostly come across data-sets tidy formats data analysis, visualisation need tidying datasets. first need understand actually value, variable/field observation. second step tidying require reshape data either -re-organising observation originally spread multiple rows (e.g. table2), one row; ORre-organising variable spread multiple columns (e.g. table3, etc.), one single column.perform tidying exercise, need two important functions tidyr .e. pivot_longer pivot_wider. let us understand functioning .","code":""},{"path":"data-wrangling-in-tidyr.html","id":"longer-format-through-function-pivot_longer","chapter":"16 Data Wrangling in tidyr","heading":"16.2 LONGER format through function pivot_longer()","text":"Often come across data sets values (instead actual variable name) column headers. Let us take example table4a table4b shown . tables values variable year column names. need re-structure tables longer format values form part columns instead column names. use pivot_longer() function . Th basic syntax -Note many useful arguments function, first let us consider .cols indicate names columns (character vector) converted longer formatnames_to = \"name\" argument actually convert values used column headers back column given “name”values_to = \"value\" argument convert values columns back one column given name “value” (e.g. population table4b).can understood gif (animated) image 16.2.\nFigure 16.2: Data Reshaping longer format\nBasic functionality can understood using following example-Using pivot_wider can convert column headers values. Check diagram figure 16.3.\nFigure 16.3: Diagrammatic representation pivot_longer\nNow ready convert table4a tidier format.","code":"pivot_longer(\n  data,\n  cols,\n  names_to = \"name\",\n  values_to = \"value\",\n  ...\n)\niris_summary <- iris %>% \n  group_by(Species) %>% \n  summarise(mean = mean(Sepal.Width),\n            st_dev = sd(Sepal.Width))\niris_summary## # A tibble: 3 × 3\n##   Species     mean st_dev\n##   <fct>      <dbl>  <dbl>\n## 1 setosa      3.43  0.379\n## 2 versicolor  2.77  0.314\n## 3 virginica   2.97  0.322"},{"path":"data-wrangling-in-tidyr.html","id":"case-i-when-values-are-in-column-headers","chapter":"16 Data Wrangling in tidyr","heading":"Case-I when values are in column headers","text":"pipes little tweaking, syntax written -","code":"\npivot_longer(\n  table4a, \n  cols = c('1999', '2000'), \n  names_to = 'year', \n  values_to = 'cases')## # A tibble: 6 × 3\n##   country     year   cases\n##   <chr>       <chr>  <dbl>\n## 1 Afghanistan 1999     745\n## 2 Afghanistan 2000    2666\n## 3 Brazil      1999   37737\n## 4 Brazil      2000   80488\n## 5 China       1999  212258\n## 6 China       2000  213766\ntidyr::table4a %>%                         # first argument passed through pipe \n  pivot_longer(cols = -country,     # all columns except country\n               names_to = \"year\",\n               values_to = \"cases\")"},{"path":"data-wrangling-in-tidyr.html","id":"case-ii-when-both-variables-and-variable-names-are-combined-together-as-column-names","chapter":"16 Data Wrangling in tidyr","heading":"Case-II when both variables and variable names are combined together as column names","text":"seen simple case tidy table values (e.g. years) depicted column names instead variables .e. actual data. may cases column names combination .Example - Say table table6 asWe may use names_sep argument case, separate combined variables column names -Note two column names argument names_to.Though table still tidy format, yet example taken show functioning arguments pivot_longer. provided two static column names related argument variables created splitting headers sep _. actually require one dynamic value retained column name (cases pop ) need convert year variables.use special value \".value\" related argument. Understanding special value becomes clear animated figure 16.4.\nFigure 16.4: headers contain headers data combined together\nExample-Note using .value argument values_to becomes meaning less.","code":"## # A tibble: 3 × 5\n##   country     cases_1999 cases_2000   pop_1999   pop_2000\n##   <chr>            <dbl>      <dbl>      <dbl>      <dbl>\n## 1 Afghanistan        745       2666   19987071   20595360\n## 2 Brazil           37737      80488  172006362  174504898\n## 3 China           212258     213766 1272915272 1280428583\ntable6 %>% \n  pivot_longer(cols = !country,\n               names_sep = \"_\",\n               names_to = c(\"count_type\", \"year\"),\n               values_to = \"count\")## # A tibble: 12 × 4\n##    country     count_type year       count\n##    <chr>       <chr>      <chr>      <dbl>\n##  1 Afghanistan cases      1999         745\n##  2 Afghanistan cases      2000        2666\n##  3 Afghanistan pop        1999    19987071\n##  4 Afghanistan pop        2000    20595360\n##  5 Brazil      cases      1999       37737\n##  6 Brazil      cases      2000       80488\n##  7 Brazil      pop        1999   172006362\n##  8 Brazil      pop        2000   174504898\n##  9 China       cases      1999      212258\n## 10 China       cases      2000      213766\n## 11 China       pop        1999  1272915272\n## 12 China       pop        2000  1280428583\ntable6 %>% \n  pivot_longer(cols = !country,\n               names_sep = \"_\",\n               names_to = c(\".value\", \"year\"),\n               values_to = \"count\")## # A tibble: 6 × 4\n##   country     year   cases        pop\n##   <chr>       <chr>  <dbl>      <dbl>\n## 1 Afghanistan 1999     745   19987071\n## 2 Afghanistan 2000    2666   20595360\n## 3 Brazil      1999   37737  172006362\n## 4 Brazil      2000   80488  174504898\n## 5 China       1999  212258 1272915272\n## 6 China       2000  213766 1280428583"},{"path":"data-wrangling-in-tidyr.html","id":"wider-format-through-pivot_wider","chapter":"16 Data Wrangling in tidyr","heading":"16.3 WIDER format through pivot_wider()","text":"name suggests, pivot_wider() exactly opposite pivot_longer . Additionally, function used create summary reports, pivot functionality MS Excel, values_fn argument. Diagrammatically can represented figure 16.6. basic syntax (commonly used arguments) --id_cols vector columns uniquely identifies observationnames_from vector columns get name output columnvalues_from similarly provides columns get cell values fromvalues_fill provides value filled missingvalues_fn named list - apply different aggregations different values_from columnsAlso note many arguments function, may used deal complicated tables.\nFigure 16.5: Basic functionality Pivot wider\ndiagrammtic representation mechanics behind function also shown Figure 16.6.\nFigure 16.6: Diagrammatic representation pivot_wider\nExample-1:Example-2:Example-3: SummarisationExample-4: Summarisation different id_colsExample-5: Use multiple columns names_from argumentWhat order reversed names_from arg. See Example-6:Example-7: Multiple columns values_fromExample-8: Use names_vary argument control order output columnsFor details please refer package vignette Chapter-12 R Data Science book.","code":"pivot_wider(\n  data,\n  id_cols = NULL,\n  names_from = name,\n  values_from = value,\n  values_fill = NULL,\n  values_fn = NULL,\n  ...\n)\ntidyr::table2 %>% \n  pivot_wider(names_from = \"type\",\n              values_from = \"count\")## # A tibble: 6 × 4\n##   country      year  cases population\n##   <chr>       <dbl>  <dbl>      <dbl>\n## 1 Afghanistan  1999    745   19987071\n## 2 Afghanistan  2000   2666   20595360\n## 3 Brazil       1999  37737  172006362\n## 4 Brazil       2000  80488  174504898\n## 5 China        1999 212258 1272915272\n## 6 China        2000 213766 1280428583\ntidyr::table2 %>% \n  pivot_wider(names_from = year,\n              values_from = count)## # A tibble: 6 × 4\n##   country     type           `1999`     `2000`\n##   <chr>       <chr>           <dbl>      <dbl>\n## 1 Afghanistan cases             745       2666\n## 2 Afghanistan population   19987071   20595360\n## 3 Brazil      cases           37737      80488\n## 4 Brazil      population  172006362  174504898\n## 5 China       cases          212258     213766\n## 6 China       population 1272915272 1280428583\ntidyr::table2 %>% \n  pivot_wider(id_cols = country,\n              names_from = type,\n              values_from = count,\n              values_fn = mean)## # A tibble: 3 × 3\n##   country       cases  population\n##   <chr>         <dbl>       <dbl>\n## 1 Afghanistan   1706.   20291216.\n## 2 Brazil       59112.  173255630 \n## 3 China       213012  1276671928.\ntidyr::table2 %>% \n  pivot_wider(id_cols = year,\n              names_from = type,\n              values_from = count,\n              values_fn = sum)## # A tibble: 2 × 3\n##    year  cases population\n##   <dbl>  <dbl>      <dbl>\n## 1  1999 250740 1464908705\n## 2  2000 296920 1475528841\ntidyr::table2 %>% \n  pivot_wider(names_from = c(year, type),\n               values_from = count)## # A tibble: 3 × 5\n##   country     `1999_cases` `1999_population` `2000_cases` `2000_population`\n##   <chr>              <dbl>             <dbl>        <dbl>             <dbl>\n## 1 Afghanistan          745          19987071         2666          20595360\n## 2 Brazil             37737         172006362        80488         174504898\n## 3 China             212258        1272915272       213766        1280428583\ntidyr::table2 %>% \n  pivot_wider(names_from = c(type, year),\n               values_from = count)## # A tibble: 3 × 5\n##   country     cases_1999 population_1999 cases_2000 population_2000\n##   <chr>            <dbl>           <dbl>      <dbl>           <dbl>\n## 1 Afghanistan        745        19987071       2666        20595360\n## 2 Brazil           37737       172006362      80488       174504898\n## 3 China           212258      1272915272     213766      1280428583\ntidyr::table1 %>% \n  pivot_wider(names_from = year,\n              values_from = c(cases, population))## # A tibble: 3 × 5\n##   country     cases_1999 cases_2000 population_1999 population_2000\n##   <chr>            <dbl>      <dbl>           <dbl>           <dbl>\n## 1 Afghanistan        745       2666        19987071        20595360\n## 2 Brazil           37737      80488       172006362       174504898\n## 3 China           212258     213766      1272915272      1280428583\ntidyr::table1 %>% \n  pivot_wider(names_from = year,\n              values_from = c(cases, population),\n              names_vary = \"slowest\")## # A tibble: 3 × 5\n##   country     cases_1999 population_1999 cases_2000 population_2000\n##   <chr>            <dbl>           <dbl>      <dbl>           <dbl>\n## 1 Afghanistan        745        19987071       2666        20595360\n## 2 Brazil           37737       172006362      80488       174504898\n## 3 China           212258      1272915272     213766      1280428583"},{"path":"data-wrangling-in-tidyr.html","id":"separate-columns-into-multiple-columns-join-columns-into-one-column","chapter":"16 Data Wrangling in tidyr","heading":"16.4 Separate Column(s) into multiple columns/ Join columns into one column","text":"","code":""},{"path":"data-wrangling-in-tidyr.html","id":"separate-a-character-column-into-multiple-with-separate","chapter":"16 Data Wrangling in tidyr","heading":"Separate a character column into multiple with separate()","text":"name suggests, separate() function used separate given character column multiple columns either using regular expression vector character positions.\nsyntax -Explanation purpose different arguments syntax -data usual name data framecol name column required separated.character vector, usually equal length maximum number new columns created separation. (Refer examples nos. 1)sep provides separator value. (Refer Example -3 ).remove FALSE, original column removed output. (Refer Example -3 ).convert TRUE, component columns converted double/integer/logical/NA, possible. useful component columns integer, numeric logical. (Refer Example-1 ).extra argument used control number desired component columns less maximum possible count. (Refer Example-4 ).fill argument hand, useful number components different row. (Refer Example-2 )Example-1:Example-2:Example-3:Example-4:","code":"separate(\n  data,\n  col,\n  into,\n  sep = \"[^[:alnum:]]+\",\n  remove = TRUE,\n  convert = FALSE,\n  extra = \"warn\",\n  fill = \"warn\",\n  ...\n)\ntidyr::table3 %>% \n  separate(rate, into = c(\"cases\", \"population\"),\n           convert = TRUE) # optional - will convert the values## # A tibble: 6 × 4\n##   country      year  cases population\n##   <chr>       <dbl>  <int>      <int>\n## 1 Afghanistan  1999    745   19987071\n## 2 Afghanistan  2000   2666   20595360\n## 3 Brazil       1999  37737  172006362\n## 4 Brazil       2000  80488  174504898\n## 5 China        1999 212258 1272915272\n## 6 China        2000 213766 1280428583\ndata.frame(\n  x = c(\"a\", \"a+b\", \"c+d+e\")\n) %>% \n  separate(x,\n           into=c('X1', 'X2', 'X3'),\n           fill = \"left\")##     X1   X2 X3\n## 1 <NA> <NA>  a\n## 2 <NA>    a  b\n## 3    c    d  e\ndata.frame(\n  x = c(\"A$B\", \"C+D\", \"E-F\")\n) %>% \n  separate(x, \n           sep = \"\\\\-|\\\\$\",\n           into = c('X1', 'X2'),\n           remove = FALSE)## Warning: Expected 2 pieces. Missing pieces filled with `NA` in 1 rows [2].##     x  X1   X2\n## 1 A$B   A    B\n## 2 C+D C+D <NA>\n## 3 E-F   E    F\ndata.frame(\n  x = c(\"a\", \"a+b\", \"c+d+e\")\n) %>% \n  separate(x,\n           into=c('X1', 'X2'),\n           extra = \"merge\")## Warning: Expected 2 pieces. Missing pieces filled with `NA` in 1 rows [1].##   X1   X2\n## 1  a <NA>\n## 2  a    b\n## 3  c  d+e"},{"path":"data-wrangling-in-tidyr.html","id":"unite-multiple-character-columns-into-one-using-unite","chapter":"16 Data Wrangling in tidyr","heading":"Unite multiple character columns into one using unite()","text":"complements separate uniting columns one. syntax isExplanation arguments syntax-data usual name data frame.col name new column formed (string),... names columns united providedsep separator used unitingremove FALSE, remove original component columnsna.rm TRUE, missing values removed beforehand.Example-1a:Example-1b:\nmay complete tidying process next stepExample-","code":"unite(data, \n      col, \n      ..., \n      sep = \"_\", \n      remove = TRUE, \n      na.rm = FALSE)\ntidyr::table5 %>% \n  unite(\"Year\",\n        c(\"century\", \"year\"), \n        sep = \"\",\n        remove = FALSE)## # A tibble: 6 × 5\n##   country     Year  century year  rate             \n##   <chr>       <chr> <chr>   <chr> <chr>            \n## 1 Afghanistan 1999  19      99    745/19987071     \n## 2 Afghanistan 2000  20      00    2666/20595360    \n## 3 Brazil      1999  19      99    37737/172006362  \n## 4 Brazil      2000  20      00    80488/174504898  \n## 5 China       1999  19      99    212258/1272915272\n## 6 China       2000  20      00    213766/1280428583\ntidyr::table5 %>% \n  unite(\"Year\",\n        c(\"century\", \"year\"), \n        sep = \"\") %>% \n  separate(rate, \n           into = c(\"cases\", \"population\"),\n           convert = TRUE)## # A tibble: 6 × 4\n##   country     Year   cases population\n##   <chr>       <chr>  <int>      <int>\n## 1 Afghanistan 1999     745   19987071\n## 2 Afghanistan 2000    2666   20595360\n## 3 Brazil      1999   37737  172006362\n## 4 Brazil      2000   80488  174504898\n## 5 China       1999  212258 1272915272\n## 6 China       2000  213766 1280428583"},{"path":"data-wrangling-in-tidyr.html","id":"separate-rows-into-multiple-rows","chapter":"16 Data Wrangling in tidyr","heading":"16.5 Separate row(s) into multiple rows","text":"","code":""},{"path":"data-wrangling-in-tidyr.html","id":"split-data-into-multiple-rows-with-separate_rows","chapter":"16 Data Wrangling in tidyr","heading":"Split data into multiple rows with separate_rows()","text":"function used separate delimited values placed one single cell/column multiple rows (rows using separate).\nSee Example-create type however, add extra step example-","code":"\ntidyr::table3 %>% \n  separate_rows(\n    rate,\n    sep = \"/\",\n    convert = TRUE\n  )## # A tibble: 12 × 3\n##    country      year       rate\n##    <chr>       <dbl>      <int>\n##  1 Afghanistan  1999        745\n##  2 Afghanistan  1999   19987071\n##  3 Afghanistan  2000       2666\n##  4 Afghanistan  2000   20595360\n##  5 Brazil       1999      37737\n##  6 Brazil       1999  172006362\n##  7 Brazil       2000      80488\n##  8 Brazil       2000  174504898\n##  9 China        1999     212258\n## 10 China        1999 1272915272\n## 11 China        2000     213766\n## 12 China        2000 1280428583\ntidyr::table3 %>% \n  separate_rows(\n    rate,\n    sep = \"/\",\n    convert = TRUE\n  ) %>% \n  group_by(country, year) %>% \n  mutate(type = c(\"cases\", \"pop\"))## # A tibble: 12 × 4\n## # Groups:   country, year [6]\n##    country      year       rate type \n##    <chr>       <dbl>      <int> <chr>\n##  1 Afghanistan  1999        745 cases\n##  2 Afghanistan  1999   19987071 pop  \n##  3 Afghanistan  2000       2666 cases\n##  4 Afghanistan  2000   20595360 pop  \n##  5 Brazil       1999      37737 cases\n##  6 Brazil       1999  172006362 pop  \n##  7 Brazil       2000      80488 cases\n##  8 Brazil       2000  174504898 pop  \n##  9 China        1999     212258 cases\n## 10 China        1999 1272915272 pop  \n## 11 China        2000     213766 cases\n## 12 China        2000 1280428583 pop"},{"path":"data-wrangling-in-tidyr.html","id":"expand-table-to-handle-missing-rowsvalues","chapter":"16 Data Wrangling in tidyr","heading":"16.6 Expand table to handle missing rows/values","text":"Sometimes, deal missing data, require handle implicit missing values well. Either turn values explicit missing values fill appropriate values. cases, two functions namely complete fill package extremely useful. Let’s learn well.","code":""},{"path":"data-wrangling-in-tidyr.html","id":"turn-implicit-missing-values-to-explicit-using-complete","chapter":"16 Data Wrangling in tidyr","heading":"Turn implicit missing values to explicit using complete()","text":"th name suggests, function used turn implicit missing values (invisible rows) explicit missing values (visible rows NA).example, let’s suppose fuel prices revised randomly. Say, revision 1 January 2020 prices revise 20 January 2020. 2 rows data say Janaury 2020. Thus, 29 implicit missing values data.syntax isWhere -data usual argument provide data frame... meant provide column names completedfill provides list supply single value can provided instead NA missing combinations.example mentioned can proceed asThough example created dates per criteria given, complete function can find unique combinations set columns provided return complete set observations. See exampleNote: fill argument used, value sales missing columns NA instead provided value.","code":"complete(data, \n         ..., \n         fill = list(), \n         explicit = TRUE)\n# First create a sample data\ndf <- data.frame(\n  date = c(as.Date(\"2020-01-01\"), as.Date(\"2020-01-20\")),\n  price = c(75.12, 78.32)\n)\ndf ##         date price\n## 1 2020-01-01 75.12\n## 2 2020-01-20 78.32\n# Use tidyr::complete to see explicit missing values\ndf %>% \n  complete(date = seq.Date(as.Date(\"2020-01-01\"), as.Date(\"2020-01-31\"), by = \"day\"))## # A tibble: 31 × 2\n##    date       price\n##    <date>     <dbl>\n##  1 2020-01-01  75.1\n##  2 2020-01-02  NA  \n##  3 2020-01-03  NA  \n##  4 2020-01-04  NA  \n##  5 2020-01-05  NA  \n##  6 2020-01-06  NA  \n##  7 2020-01-07  NA  \n##  8 2020-01-08  NA  \n##  9 2020-01-09  NA  \n## 10 2020-01-10  NA  \n## # ℹ 21 more rows\n#Let's create a sample data\nset.seed(123)\ndf2 <- data.frame(\n  year = c(2020, 2020, 2020, 2021, 2021),\n  qtr = c(1,3,4,2,3),\n  sales = runif(5, 100, 200)\n)\ndf2##   year qtr    sales\n## 1 2020   1 128.7578\n## 2 2020   3 178.8305\n## 3 2020   4 140.8977\n## 4 2021   2 188.3017\n## 5 2021   3 194.0467\n# use complete to find all combination\ndf2 %>% \n  complete(year, qtr, # cols provided\n           fill = list(sales = 0))## # A tibble: 8 × 3\n##    year   qtr sales\n##   <dbl> <dbl> <dbl>\n## 1  2020     1  129.\n## 2  2020     2    0 \n## 3  2020     3  179.\n## 4  2020     4  141.\n## 5  2021     1    0 \n## 6  2021     2  188.\n## 7  2021     3  194.\n## 8  2021     4    0"},{"path":"data-wrangling-in-tidyr.html","id":"fill-missing-values-based-on-criteria-fill","chapter":"16 Data Wrangling in tidyr","heading":"Fill missing values based on criteria fill()","text":"function helps filling missing values using previous next entry. Think paper sheet many cells table filled \"--\". Syntax -arguments pretty simple. time \"\" method used. example see example fuel prices mentioned .","code":"\nfill(data, # used to provide data\n     ..., # provide columns to be filled\n     .direction = c(\"down\", \"up\", \"downup\", \"updown\"))\nfill_df <- df %>% \n  complete(date = seq.Date(as.Date(\"2020-01-01\"), as.Date(\"2020-01-31\"), by = \"day\")) %>% \n  fill(price, .direction = \"down\")\n\nhead(fill_df)## # A tibble: 6 × 2\n##   date       price\n##   <date>     <dbl>\n## 1 2020-01-01  75.1\n## 2 2020-01-02  75.1\n## 3 2020-01-03  75.1\n## 4 2020-01-04  75.1\n## 5 2020-01-05  75.1\n## 6 2020-01-06  75.1\ntail(fill_df)## # A tibble: 6 × 2\n##   date       price\n##   <date>     <dbl>\n## 1 2020-01-26  78.3\n## 2 2020-01-27  78.3\n## 3 2020-01-28  78.3\n## 4 2020-01-29  78.3\n## 5 2020-01-30  78.3\n## 6 2020-01-31  78.3"},{"path":"generating-descriptive-statistics.html","id":"generating-descriptive-statistics","chapter":"17 Generating Descriptive statistics","heading":"17 Generating Descriptive statistics","text":"Exploratory Data Analysis often abbreviated EDA, mostly first foremost step carrying data analytics task, used analyze investigate data sets summarize main characteristics, often employing data visualization methods. EDA primarily used see data can reveal beyond formal modeling hypothesis testing task provides provides better understanding data set variables relationships . can also help determine statistical techniques considering data analysis appropriate. Originally developed American mathematician John Tukey 1960s, EDA techniques continue widely used method data discovery process today.","code":""},{"path":"generating-descriptive-statistics.html","id":"using-base-r","chapter":"17 Generating Descriptive statistics","heading":"17.1 Using base R","text":"Base R provides us two functions used ato ascertain structure summary statistics data frame. First str short structure (confused string) full name suggests gives us structure data. usage simpleAs can seen gives us number variables (columns) well observations (rows) available given data. thereafter presents us names columns/variables data along types. ’s . also prints first values columns. factor columns also gives us available levels factor variables.Another function base R summary can used generate summary statistics given data frame. Let’s see can get function.can see nicely gives us five-point summary numeric variables count values present factor variables. Apart five point summary .e. (1) minimum, (2) 1st quartile, (3) Median, (4) third quartile (5) maximum; also get mean (arithmetic) numeric variables.moving forward, can discuss table() function used genrate counts factor/character variable(s) base R.","code":"\nstr(iris)## 'data.frame':    150 obs. of  5 variables:\n##  $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n##  $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n##  $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n##  $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n##  $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\nsummary(iris)##   Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n##  Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n##  1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n##  Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n##  Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n##  3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n##  Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n##        Species  \n##  setosa    :50  \n##  versicolor:50  \n##  virginica :50  \n##                 \n##                 \n## \nwith(iris, table(Species))## Species\n##     setosa versicolor  virginica \n##         50         50         50"},{"path":"generating-descriptive-statistics.html","id":"dplyr-functions","chapter":"17 Generating Descriptive statistics","heading":"17.2 Dplyr functions","text":"calculating statistics can use dplyr::summarise combination across. Example calculate mean, sd, variance numeric variables say iris data, can -trying understand output let’s learn use dplyr::across. Actually across used inside dplyr verbs mostly mutate summarise can mutate/summarise multiple variables (columns) simultaneously. , least two arguments needed; first variable names can provided type checking variable, str detecting function, etc.; second argument either function name list functions together. example summarised numeric columns (see first argument function .numeric operates column names) second argument list three functions lambda style notation. example 4 numeric columns three aggregating functions, 12 columns getting output.can reshape/transform data using tidyr::pivot_longer. SeeLet us also discuss one data summary statistics function dplyr glimpse. basically pipe friendly version str(). SeeTo calculate counts factor variable (generated table base R), can use dplyr::count pipe friendly function.can generate counts multiple combinations variables","code":"\nlibrary(dplyr)\niris %>%\n  summarise(across(where(is.numeric),\n                   .fns = list(\n                     Mean = ~ mean(.),\n                     SD = ~ sd(.),\n                     Var = ~ var(.)\n                   )))##   Sepal.Length_Mean Sepal.Length_SD Sepal.Length_Var Sepal.Width_Mean\n## 1          5.843333       0.8280661        0.6856935         3.057333\n##   Sepal.Width_SD Sepal.Width_Var Petal.Length_Mean Petal.Length_SD\n## 1      0.4358663       0.1899794             3.758        1.765298\n##   Petal.Length_Var Petal.Width_Mean Petal.Width_SD Petal.Width_Var\n## 1         3.116278         1.199333      0.7622377       0.5810063\nlibrary(tidyr)\niris %>%\n  summarise(across(where(is.numeric),\n                   .fns = list(\n                     Mean = ~ mean(.),\n                     SD = ~ sd(.),\n                     Var = ~ var(.)\n                   ))) %>%\n  pivot_longer(everything(),\n               names_sep = \"_\",\n               names_to = c(\".value\", \"Function\"))## # A tibble: 3 × 5\n##   Function Sepal.Length Sepal.Width Petal.Length Petal.Width\n##   <chr>           <dbl>       <dbl>        <dbl>       <dbl>\n## 1 Mean            5.84        3.06          3.76       1.20 \n## 2 SD              0.828       0.436         1.77       0.762\n## 3 Var             0.686       0.190         3.12       0.581\niris %>%\n  summarise(across(where(is.numeric),\n                   .fns = list(\n                     Mean = ~ mean(.),\n                     SD = ~ sd(.),\n                     Var = ~ var(.)\n                   ))) %>%\n  pivot_longer(everything(),\n               names_sep = \"_\",\n               names_to = c(\"Variable\", \".value\"))## # A tibble: 4 × 4\n##   Variable      Mean    SD   Var\n##   <chr>        <dbl> <dbl> <dbl>\n## 1 Sepal.Length  5.84 0.828 0.686\n## 2 Sepal.Width   3.06 0.436 0.190\n## 3 Petal.Length  3.76 1.77  3.12 \n## 4 Petal.Width   1.20 0.762 0.581\niris %>% \n  glimpse()## Rows: 150\n## Columns: 5\n## $ Sepal.Length <dbl> 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.…\n## $ Sepal.Width  <dbl> 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.…\n## $ Petal.Length <dbl> 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.…\n## $ Petal.Width  <dbl> 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.…\n## $ Species      <fct> setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\niris %>% \n  count(Species)##      Species  n\n## 1     setosa 50\n## 2 versicolor 50\n## 3  virginica 50\nggplot2::diamonds %>% \n  count(cut, color, name = \"count\")## # A tibble: 35 × 3\n##    cut   color count\n##    <ord> <ord> <int>\n##  1 Fair  D       163\n##  2 Fair  E       224\n##  3 Fair  F       312\n##  4 Fair  G       314\n##  5 Fair  H       303\n##  6 Fair  I       175\n##  7 Fair  J       119\n##  8 Good  D       662\n##  9 Good  E       933\n## 10 Good  F       909\n## # ℹ 25 more rows"},{"path":"generating-descriptive-statistics.html","id":"using-psych","chapter":"17 Generating Descriptive statistics","heading":"17.3 Using psych","text":"indeed beautiful packages R, creates beautiful EDA summaries us without much ado. Package psych one .Note output data.frame format ready use. Another function psych describeBy creates grouped summaries.one function describeData package also results first well last four (default) values.","code":"\nlibrary(psych)## \n## Attaching package: 'psych'## The following objects are masked from 'package:ggplot2':\n## \n##     %+%, alpha\ndescribe(USArrests)##          vars  n   mean    sd median trimmed    mad  min   max range  skew\n## Murder      1 50   7.79  4.36   7.25    7.53   5.41  0.8  17.4  16.6  0.37\n## Assault     2 50 170.76 83.34 159.00  168.48 110.45 45.0 337.0 292.0  0.22\n## UrbanPop    3 50  65.54 14.47  66.00   65.88  17.79 32.0  91.0  59.0 -0.21\n## Rape        4 50  21.23  9.37  20.10   20.36   8.60  7.3  46.0  38.7  0.75\n##          kurtosis    se\n## Murder      -0.95  0.62\n## Assault     -1.15 11.79\n## UrbanPop    -0.87  2.05\n## Rape         0.08  1.32\ndescribeBy(ggplot2::diamonds, group = \"cut\")## \n##  Descriptive statistics by group \n## cut: 1\n##         vars    n    mean      sd  median trimmed     mad    min      max\n## carat      1 1610    1.05    0.52    1.00    0.98    0.43   0.22     5.01\n## cut        2 1610    1.00    0.00    1.00    1.00    0.00   1.00     1.00\n## color      3 1610    3.85    1.71    4.00    3.85    1.48   1.00     7.00\n## clarity    4 1610    3.02    1.45    3.00    2.93    1.48   1.00     8.00\n## depth      5 1610   64.04    3.64   65.00   64.48    1.33  43.00    79.00\n## table      6 1610   59.05    3.95   58.00   58.64    2.97  49.00    95.00\n## price      7 1610 4358.76 3560.39 3282.00 3695.65 2183.13 337.00 18574.00\n## x          8 1610    6.25    0.96    6.18    6.21    0.81   0.00    10.74\n## y          9 1610    6.18    0.96    6.10    6.14    0.79   0.00    10.54\n## z         10 1610    3.98    0.65    3.97    3.95    0.52   0.00     6.98\n##            range  skew kurtosis    se\n## carat       4.79  1.68     5.31  0.01\n## cut         0.00   NaN      NaN  0.00\n## color       6.00  0.06    -0.86  0.04\n## clarity     7.00  0.68     0.14  0.04\n## depth      36.00 -1.17     2.20  0.09\n## table      46.00  1.34     4.83  0.10\n## price   18237.00  1.78     3.07 88.73\n## x          10.74  0.36     1.58  0.02\n## y          10.54  0.36     1.53  0.02\n## z           6.98  0.34     1.43  0.02\n## ------------------------------------------------------------ \n## cut: 2\n##         vars    n    mean      sd  median trimmed     mad    min      max\n## carat      1 4906    0.85    0.45    0.82    0.80    0.43   0.23     3.01\n## cut        2 4906    2.00    0.00    2.00    2.00    0.00   2.00     2.00\n## color      3 4906    3.57    1.76    3.00    3.51    1.48   1.00     7.00\n## clarity    4 4906    3.60    1.47    3.00    3.44    1.48   1.00     8.00\n## depth      5 4906   62.37    2.17   63.40   62.70    0.74  54.30    67.00\n## table      6 4906   58.69    2.85   58.00   58.57    2.97  51.00    66.00\n## price      7 4906 3928.86 3681.59 3050.50 3251.51 2853.26 327.00 18788.00\n## x          8 4906    5.84    1.06    5.98    5.80    1.10   0.00     9.44\n## y          9 4906    5.85    1.05    5.99    5.82    1.08   0.00     9.38\n## z         10 4906    3.64    0.65    3.70    3.62    0.68   0.00     5.79\n##            range  skew kurtosis    se\n## carat       2.78  1.03     1.22  0.01\n## cut         0.00   NaN      NaN  0.00\n## color       6.00  0.25    -0.93  0.03\n## clarity     7.00  0.81     0.29  0.02\n## depth      12.70 -1.20     0.17  0.03\n## table      15.00  0.31    -0.64  0.04\n## price   18461.00  1.72     3.04 52.56\n## x           9.44  0.15    -0.15  0.02\n## y           9.38  0.14    -0.17  0.02\n## z           5.79  0.09     0.12  0.01\n## ------------------------------------------------------------ \n## cut: 3\n##         vars     n    mean      sd  median trimmed     mad   min      max\n## carat      1 12082    0.81    0.46    0.71    0.75    0.46   0.2     4.00\n## cut        2 12082    3.00    0.00    3.00    3.00    0.00   3.0     3.00\n## color      3 12082    3.57    1.72    3.00    3.51    1.48   1.0     7.00\n## clarity    4 12082    4.00    1.59    4.00    3.87    1.48   1.0     8.00\n## depth      5 12082   61.82    1.38   62.10   61.95    1.48  56.8    64.90\n## table      6 12082   57.96    2.12   58.00   57.88    1.48  44.0    66.00\n## price      7 12082 3981.76 3935.86 2648.00 3243.22 2855.49 336.0 18818.00\n## x          8 12082    5.74    1.10    5.74    5.69    1.25   0.0    10.01\n## y          9 12082    5.77    1.10    5.77    5.72    1.25   0.0     9.94\n## z         10 12082    3.56    0.73    3.56    3.53    0.76   0.0    31.80\n##            range  skew kurtosis    se\n## carat       3.80  0.99     0.89  0.00\n## cut         0.00   NaN      NaN  0.00\n## color       6.00  0.25    -0.89  0.02\n## clarity     7.00  0.57    -0.43  0.01\n## depth       8.10 -0.71    -0.30  0.01\n## table      22.00  0.28     0.04  0.02\n## price   18482.00  1.60     2.24 35.81\n## x          10.01  0.23    -0.65  0.01\n## y           9.94  0.23    -0.66  0.01\n## z          31.80  4.96   183.94  0.01\n## ------------------------------------------------------------ \n## cut: 4\n##         vars     n    mean      sd  median trimmed     mad   min      max\n## carat      1 13791    0.89    0.52    0.86    0.83    0.56   0.2     4.01\n## cut        2 13791    4.00    0.00    4.00    4.00    0.00   4.0     4.00\n## color      3 13791    3.70    1.71    4.00    3.67    1.48   1.0     7.00\n## clarity    4 13791    3.74    1.50    4.00    3.60    1.48   1.0     8.00\n## depth      5 13791   61.26    1.16   61.40   61.36    1.19  58.0    63.00\n## table      6 13791   58.75    1.48   59.00   58.77    1.48  51.0    62.00\n## price      7 13791 4584.26 4349.20 3185.00 3822.23 3371.43 326.0 18823.00\n## x          8 13791    5.97    1.19    6.11    5.92    1.42   0.0    10.14\n## y          9 13791    5.94    1.26    6.06    5.89    1.41   0.0    58.90\n## z         10 13791    3.65    0.73    3.72    3.62    0.86   0.0     8.06\n##            range  skew kurtosis    se\n## carat       3.81  0.86     0.43  0.00\n## cut         0.00   NaN      NaN  0.00\n## color       6.00  0.12    -0.88  0.01\n## clarity     7.00  0.69     0.06  0.01\n## depth       5.00 -0.61    -0.37  0.01\n## table      11.00 -0.37     1.33  0.01\n## price   18497.00  1.33     1.07 37.03\n## x          10.14  0.17    -0.85  0.01\n## y          58.90  5.53   225.05  0.01\n## z           8.06  0.11    -0.44  0.01\n## ------------------------------------------------------------ \n## cut: 5\n##         vars     n    mean      sd  median trimmed     mad   min      max\n## carat      1 21551    0.70    0.43    0.54    0.64    0.33   0.2     3.50\n## cut        2 21551    5.00    0.00    5.00    5.00    0.00   5.0     5.00\n## color      3 21551    3.53    1.66    4.00    3.48    1.48   1.0     7.00\n## clarity    4 21551    4.46    1.71    4.00    4.39    1.48   1.0     8.00\n## depth      5 21551   61.71    0.72   61.80   61.76    0.59  43.0    66.70\n## table      6 21551   55.95    1.25   56.00   55.97    1.48  43.0    63.00\n## price      7 21551 3457.54 3808.40 1810.00 2656.14 1630.86 326.0 18806.00\n## x          8 21551    5.51    1.06    5.25    5.41    1.19   0.0     9.65\n## y          9 21551    5.52    1.07    5.26    5.42    1.19   0.0    31.80\n## z         10 21551    3.40    0.66    3.23    3.34    0.73   0.0     6.03\n##            range  skew kurtosis    se\n## carat       3.30  1.34     1.63  0.00\n## cut         0.00   NaN      NaN  0.00\n## color       6.00  0.19    -0.82  0.01\n## clarity     7.00  0.36    -0.71  0.01\n## depth      23.70 -1.44    22.33  0.00\n## table      20.00  0.20     1.70  0.01\n## price   18480.00  1.84     2.98 25.94\n## x           9.65  0.66    -0.42  0.01\n## y          31.80  1.30    15.99  0.01\n## z           6.03  0.65    -0.36  0.00\ndescribeData(ggplot2::diamonds)## n.obs =  53940 of which  53940   are complete cases.   Number of variables =  10  of which all are numeric  TRUE  \n##          variable # n.obs type    H1      H2   H3      H4   T1        T2\n## carat*            1 53940    4  0.23    0.21 0.23    0.29 0.72      0.70\n## cut*              2 53940    4 Ideal Premium Good Premium Good Very Good\n## color*            3 53940    4     E       E    E       I    D         D\n## clarity*          4 53940    4   SI2     SI1  VS1     VS2  SI1       SI1\n## depth*            5 53940    4  61.5    59.8 56.9    62.4 63.1      62.8\n## table*            6 53940    4    55      61   65      58   55        60\n## price*            7 53940    4   326     326  327     334 2757      2757\n## x*                8 53940    4  3.95    3.89 4.05    4.20 5.69      5.66\n## y*                9 53940    4  3.98    3.84 4.07    4.23 5.75      5.68\n## z*               10 53940    4  2.43    2.31 2.31    2.63 3.61      3.56\n##               T3    T4\n## carat*      0.86  0.75\n## cut*     Premium Ideal\n## color*         H     D\n## clarity*     SI2   SI2\n## depth*      61.0  62.2\n## table*        58    55\n## price*      2757  2757\n## x*          6.15  5.83\n## y*          6.12  5.87\n## z*          3.74  3.64"},{"path":"generating-descriptive-statistics.html","id":"using-skimr","chapter":"17 Generating Descriptive statistics","heading":"17.4 Using skimr","text":"Package skimr generates beautiful data EDA summary reports can customised per one’s taste. Full descriptions package may seen . basic purposes can use function skim package get data EDA summary reports.","code":"library(skimr)\nskim(iris)"},{"path":"generating-descriptive-statistics.html","id":"viewing-relationships-between-different-variables","chapter":"17 Generating Descriptive statistics","heading":"17.5 Viewing relationships between different variables","text":"can use package PerformanceAnalytics generate view relationships different variables data. purpose function PerformanceAnalytics::chart.Correlation() may used shown .\nFigure 17.1: Viewing relationships PerformanceAnalytics\ncan seen generates visualization Correlation Matrix numeric variables given data.one package GGally also creates beautiful charts viewing relationships. two functions package particularly useful.ggpairs() function GGally package allows build great scatterplot matrix. Scatterplots pair numeric variable drawn left part figure. Pearson correlation displayed right. Variable distribution available diagonal.ggpairs() function GGally package allows build great scatterplot matrix. Scatterplots pair numeric variable drawn left part figure. Pearson correlation displayed right. Variable distribution available diagonal.ggcorr() function allows visualize correlation pair variable square. Note method argument allows pick correlation type desire.ggcorr() function allows visualize correlation pair variable square. Note method argument allows pick correlation type desire.See following example-\nFigure 17.2: Scatterplot Matrix (Left) Correlation plot (Right) produced GGally\n","code":"\nsuppressMessages(library(PerformanceAnalytics))\n\nUSArrests %>% \n  select(where(is.numeric)) %>% \n  PerformanceAnalytics::chart.Correlation()\nsuppressMessages(library(GGally))\nUSArrests %>% \n  select_if(is.numeric) %>% \n  ggcorr(label = TRUE)\n\nUSArrests %>% \n  select_if(is.numeric) %>%\n  ggpairs()"},{"path":"part-iii-probability-and-sampling-in-r.html","id":"part-iii-probability-and-sampling-in-r","chapter":"Part-III: Probability and Sampling in R","heading":"Part-III: Probability and Sampling in R","text":"","code":""},{"path":"probability-in-r.html","id":"probability-in-r","chapter":"18 Probability in R","heading":"18 Probability in R","text":"keep short . Instead learning concepts probability, see calculate probability, densities, quantiles nearly type distribution. R’s powerhorse four types functions distributions associated called pqdr functions. Actually prefixes. Consider probability function \\(P(X=x) = p\\) variable \\(x\\) \\(p\\) associated probability.functions vectorised. Let us explore one one.","code":""},{"path":"probability-in-r.html","id":"p-set-of-functions","chapter":"18 Probability in R","heading":"18.1 p*() set of functions","text":"set functions give cumulative probability distribution probability function.Example-1. probability number less equal 25 Normal distribution mean = 50 sd = 10.contrary, probability number greater equal 25 distribution -Example-2: probability one heads two tosses fair coin (binomial distribution p = 0.5).","code":"\npnorm(25, mean = 50, sd = 10)## [1] 0.006209665\n# Either deduct probability from 1 \n1 - pnorm(25, mean = 50, sd = 10)## [1] 0.9937903\n# Or provide FALSE to lower.tail argument\npnorm(25, mean = 50, sd = 10, lower.tail = FALSE)## [1] 0.9937903\npbinom(1, size = 2, p = 0.5)## [1] 0.75"},{"path":"probability-in-r.html","id":"q-set-of-functions","chapter":"18 Probability in R","heading":"18.2 q*() set of functions","text":"set functions, give quantile inverse cumulative probability function. \\(f\\) cdf (cumulative distribution function) given probability distribution \\(F\\) quantile inverse f .e. \\(F = f^{-1}\\). related \\[\\begin{equation}\np = f(x)\n\\tag{18.1}\n\\end{equation}\\]\\[\\begin{equation}\nx = F(x) = f^{-1}(x)\n\\tag{18.2}\n\\end{equation}\\]Example- normal distribution (mean = 50 sd = 10) number 90% population distributed.Similar cdf may use lower.tail argument find number population percent distributed.","code":"\nqnorm(0.9, mean = 50, sd = 10)## [1] 62.81552\nqnorm(0.9, mean = 50, sd = 10, lower.tail = FALSE)## [1] 37.18448"},{"path":"probability-in-r.html","id":"d-set-of-functions","chapter":"18 Probability in R","heading":"18.3 d*() set of functions","text":"saw p group denotes cdf, q group denotes inverse cdf, d group actually denotes probability density function given distribution. Simply stating, returns height probability distribution function given x value.expected probability drawing exactly 2 heads two tosses single fair coin (.e. binomial distribution probability p = 0.5).","code":"\ndbinom(2, 2, prob = 0.5)## [1] 0.25"},{"path":"probability-in-r.html","id":"r-set-of-functions","chapter":"18 Probability in R","heading":"18.4 r*() set of functions","text":"set functions used generate random numbers Statistical distribution. generate 10 random numbers Normal distribution mean = 50 sd = 10, can use rnorm.can actually check using histogram.\nFigure 18.1: Histogram Random numbers generated Normal distribution\n","code":"\nrnorm(10, mean = 50, sd = 10)##  [1] 33.10444 62.39496 48.91034 48.82758 51.83083 62.80555 32.72729 66.90184\n##  [9] 55.03812 75.28337\nset.seed(1234)\nhist(rnorm(10000, 50, 10), breaks = 50)"},{"path":"random-sampling-in-r.html","id":"random-sampling-in-r","chapter":"19 Random sampling in R","heading":"19 Random sampling in R","text":"International Standard Auditing - 530 defines30 audit sampling application audit procedures less 100% items within population audit relevance sampling units chance selection order provide auditor reasonable basis draw conclusions entire population. Statistical sampling defines approach sampling two characteristics - random selection samples, use probability theory evaluate sample results, including measurement sampling risk.Appendix 4 ISA 53 prescribes different statistical methods sample selection. discuss type sampling methodology used sample records audit.PrerequisitesLoad tidyverse","code":"\nlibrary(tidyverse)"},{"path":"random-sampling-in-r.html","id":"simple-random-sampling-with-and-without-replacement","chapter":"19 Random sampling in R","heading":"19.1 Simple Random Sampling (With and without replacement)","text":"method, records selected completely random, generating random numbers e.g. using random number tables, etc. Refer figure 19.1 illustration. can replicate method random number generation R. Even method random number generation can reproducible, fixing random number seed. Mainly two functions used sample() set.seed() already discussed section 4.9. Since sample() function takes vector input gives vector output , can make use dplyr::slice_sample() function, discussed section 4.9, operates data frames instead.\nFigure 19.1: Illustration Simple Random Sampling\nLet’s see sampling iris data. Suppose select sample n=12 records, without replacement-syntax simple. first step fixed random number seed reproducibility. Using slice_sample() selected n=12 records without replacement (replace = FALSE).sample size based proportion, use prop = .10 (say 10%) instead n argument. Moreover, sampling replacement, use replace = TRUE.","code":"dat <- iris # input data\n# set the seed\nset.seed(123)\n# sample n records\ndat %>% \n  slice_sample(n = 12, replace = FALSE)"},{"path":"random-sampling-in-r.html","id":"srs","chapter":"19 Random sampling in R","heading":"19.2 Systematic random sampling","text":"ISA 530 defines sampling approach ‘Systematic selection, number sampling units population divided sample size give sampling interval, example 50, determined starting point within first 50, 50th sampling unit thereafter selected. Although starting point may determined haphazardly, sample likely truly random determined use computerized random number generator random number tables. using systematic selection, auditor need determine sampling units within population structured way sampling interval corresponds particular pattern population.’ Refer figure 19.2 illustration.\nFigure 19.2: Illustration Systematic Random Sampling\ncan replicate approach following two steps-Step-1: Select n sample size. generate maximum starting point say s dividing number rows data n. Thereafter choose starting point 1:s. can use sample function . Let’s say starting number s1. generate arithmetic sequence, say rand_seq starting s1 increasing every s steps thereafter total n terms.Step-2: next step shuffle data using slice_sample select sample using function filter.methodology replicated ","code":"set.seed(123)\nn <- 15 # sample size\ns <- floor(nrow(dat)/n)\ns1 <- sample(1:s, 1, replace = FALSE)\nrand_seq <- seq(s1, by = s, length.out = n)\ndat %>% \n  slice_sample(prop = 1) %>% \n  filter(row_number() %in% rand_seq)\n  "},{"path":"random-sampling-in-r.html","id":"probability-proportionate-to-size-with-or-without-replacement-a.k.a-monetary-unit-sampling","chapter":"19 Random sampling in R","heading":"19.3 Probability Proportionate to size (with or without replacement) a.k.a monetary unit sampling","text":"sampling approach defined ISA-530 “type value-weighted selection sample size, selection evaluation results conclusion monetary amounts.”methodology much difference methodology adopted section 19.2 except make use weight_by = argument now.Let’s use state.x77 data comes base R. Since data matrix format, let’s first convert data frame using .data.frame() first.steps simple.","code":"\ndat <- as.data.frame(state.x77)set.seed(123)\ndat %>% \n  slice_sample(n=12, weight_by = Population)"},{"path":"random-sampling-in-r.html","id":"stratified-random-sampling","chapter":"19 Random sampling in R","heading":"19.4 Stratified random sampling","text":"Stratification defined ISA-530 process dividing population sub-populations, group sampling units similar characteristics (often monetary value). Thus, stratified random sampling may imply afore-mentioned sampling techniques applied individual strata instead whole population. Refer figure 19.3 illustration.\nFigure 19.3: Illustration Stratified Random Sampling\nfunction dplyr::group_by() used stratification. Thereafter can proceed sampling described .Example Data: - Let’s include region state.x77 data using dplyr::bind_cols.Let’s see first 6 rows dataWe can check summary number States per regionCase-1: sample size constant strata. Say 2 records per region.Case-2: sample size proportion different among strata.\ntime let us assume column stratum directly available data.\n- Say, 20% States Population upto 1000;\n- 30% States population greater 1000 upto 5000 finally;\n- 50% states population 5000 sampled.scenario, strategy use purrr::map2_dfr() function splitting data group_split() function.Syntax beWe may check sample selected across stratum","code":"\ndat <- bind_cols(\n  as.data.frame(state.x77),\n  as.data.frame(state.region)\n)\ndat %>% \n  tibble::rownames_to_column('State') %>% # this step will not be \n                                  # used in databases without row names\n  group_by(state.region) %>% \n  summarise(states = n())## # A tibble: 4 × 2\n##   state.region  states\n##   <fct>          <int>\n## 1 Northeast          9\n## 2 South             16\n## 3 North Central     12\n## 4 West              13set.seed(123)\nn <- 2\ndat %>% \n  tibble::rownames_to_column('State') %>% # this step will not be used in databases without row names\n  group_by(state.region) %>% \n  slice_sample(n=n) %>% \n  ungroup()# define proportions\nprops <- c(0.2, 0.3, 0.5)\n\n# set seed\nset.seed(123)\n\n# take data\ndat %>% \n  # reduntant step where data has no column names\n  tibble::rownames_to_column('State') %>%\n  # create column according to stratums\n  mutate(stratum = cut(Population, c(0, 1000, 5000, max(Population)),\n                      labels = c(\"Low\", \"Mid\", \"High\"))) %>% \n  # split data into groups\n  group_split(stratum) %>% \n  # sample in each group\n  map2_dfr(props,\n           .f = function(d, w) slice_sample(d, prop = w))"},{"path":"random-sampling-in-r.html","id":"cluster-sampling","chapter":"19 Random sampling in R","heading":"19.5 Cluster sampling","text":"ISA 530 explicitly define cluster sampling. Actually sampling sampling strata can apply mentioned techniques easily sample clusters. E.g. sample data , can sample say, 2 clusters (regions).Thus, strategy first sample groups unique available values thereafter filter records.","code":"\n# set the seed\nset.seed(123)\n# sample clusters\nclusters <- sample(\n  unique(dat$state.region),\n  size = 2\n)\n# filter all records in above clusters\nclust_samp <- dat %>% \n  filter(state.region %in% clusters)\n# check number of records\nclust_samp$state.region %>% table()## .\n##     Northeast         South North Central          West \n##             9             0            12             0"},{"path":"part-iv-machine-learning-in-r.html","id":"part-iv-machine-learning-in-r","chapter":"Part-IV: Machine Learning in R","heading":"Part-IV: Machine Learning in R","text":"","code":""},{"path":"linear-regression.html","id":"linear-regression","chapter":"20 Linear Regression","heading":"20 Linear Regression","text":"Regression models class statistical models let us explore relationship response variable one explanatory variables. relationship exists detected, can make predictions value response variable given explanatory variables. example, relationship number employees office monthly expenses salary established, can predict monthly expenses incurred office salary know number employees. lets us thought experiments like asking much new company pay say 10 employees salary expenses monthly.Explanatory variables sometimes also referred regressors independent predictor variables whereas response variable also called dependent outcome variable. response variable numeric relationship linear, regression called linear regression. course, certain assumptions, discuss later chapter.","code":""},{"path":"linear-regression.html","id":"basic-concepts","chapter":"20 Linear Regression","heading":"20.1 Basic concepts","text":"start creating regression models, ’s always good practice visualize data. help us ascertaining nature relationship predictors response variable, . visualize relationship two numeric variables, can use scatter plot. See two examples figure 20.1. first example (plot) can see near perfect linear relationship GNP Population countries, whereas moderate negative relationship two variables seen second example.\nFigure 20.1: Linear Regression - Intuition\nLinear regression perfect model predict outcome response variable linear relationship explanatory variables. case predicted values lie assumed line (linear relationship) ideally. Maths behind estimating predicting outcomes thus, finding following algebraic equation (20.1).\\[\\begin{equation}\ny = mx + c\n\\tag{20.1}\n\\end{equation}\\]-m slope line (linear relationship)c intercept Y-axis. (value \\(y\\) \\(x\\) 0 )Interpreting equation real world like estimating coefficients .e. \\(m\\) \\(c\\) equation. goal exercise thus estimate best values \\(m\\) \\(c\\). two parameters, \\(m\\) \\(c\\) sometimes also referred \\(\\beta_1\\) \\(\\beta_0\\) respectively. familiar mathematics behind fitting equation line (two dimensional space), may know require two variables (data points .e. \\(x\\) \\(y\\) value pairs) values find parameters. means, fair amount data points available (rarest circumstances collinear .e. lying one line), can actually get many regression lines. goal thus, find best lines. , ?answer question, let us also understand values \\(x\\) \\(y\\) pairs actual practice, don’t lie regression line points rarely collinear (See plots Figure 20.1 note none data point actually lie line). data point response variable, actual value one fitted value (one falling regression line). difference values called error term residual. Technically errors random noise model able explain. Mathematically, \\(\\hat{y}\\) fitted value, \\(y\\) actual value, difference also called error (prediction) term say \\(\\epsilon\\) can denoted -\\[\\begin{equation}\n\\epsilon = y - \\hat{y}\n\\tag{20.2}\n\\end{equation}\\], can say \\[\\begin{equation}\ny = \\beta_0 + \\beta_1x + \\epsilon\n\\tag{20.3}\n\\end{equation}\\]Now, one method find best fit regression line minimize error terms. Theoretically, means capture pattern/relationship data points much possible ’s left behind true random noise. One way minimise mean error terms. error terms can positive negative. See figure 20.2. ensure cancelled taking mean, can minimise either mean absolute values squares. commonly accepted method take mean squares minimise . One benefit adopting another, squaring errors residuals, large residuals get higher weight lower residuals. ’s linear regression technique also sometimes referred Ordinary Least Squares OLS regression.\nFigure 20.2: Residuals - Intuition Concept\n","code":""},{"path":"linear-regression.html","id":"simple-linear-regression-in-r","chapter":"20 Linear Regression","heading":"20.2 Simple Linear Regression in R","text":"Don’t worry, R minimisation job . fact, base R fantastic function lm can fit best regression line, given set variables, us. syntax simple.-formula object class “formula” (one can coerced class): symbolic description model fitted. example , can write y ~ xdata optional data frame.actually returns special object, can printed directly like R objects. However, best printed summary function. Firstly see example simple linear regression linear regression one independent variable.Example-1: Problem Statement: iris famous (Fisher’s Anderson’s) data set, loaded default R, gives measurements centimeters variables sepal length (Sepal.Length) width (Sepal.Width) petal length (Petal.Length) width (Petal.Width), respectively, 50 flowers 3 species (Species) iris. species Iris setosa, versicolor, virginica. Let us try establish relationship Sepal.Length Sepal.Width variables setosa Species .e. iris` data-set, (first 50 records ).already stated , always good practice visualize data, possible. let’s make scatter-plot, seen Figure 20.3.\nFigure 20.3: Relationship sepal widths lengths setosa species\nrelationship seem fairly linear (Figure 20.3), let’s build model.Observing outputs , can notice simply printing object returns coefficients whereas printing summary gives us lot information. interpret information? proceeding interpret output, let us understand concepts essential . concepts basically assumptions, made finding best fit line words estimating parameters statistically.","code":"lm(formula, data, ...)\niris %>% \n  # Extract First 50 records\n  head(50) %>% \n  ggplot(aes(Sepal.Length, Sepal.Width)) +\n  # Scatter plot\n  geom_point() +\n  # adding a trend line without error bands\n  geom_smooth(method = 'lm', se=FALSE, formula = \"y~x\") +\n  # Theme Black and White\n  theme_bw()\nlin_reg1 <- lm(formula = Sepal.Width ~ Sepal.Length, data = iris[1:50,])\n\n# Let us print the object directly\nlin_reg1## \n## Call:\n## lm(formula = Sepal.Width ~ Sepal.Length, data = iris[1:50, ])\n## \n## Coefficients:\n##  (Intercept)  Sepal.Length  \n##      -0.5694        0.7985\n# Try printing it with summary()\nsummary(lin_reg1)## \n## Call:\n## lm(formula = Sepal.Width ~ Sepal.Length, data = iris[1:50, ])\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.72394 -0.18273 -0.00306  0.15738  0.51709 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)   -0.5694     0.5217  -1.091    0.281    \n## Sepal.Length   0.7985     0.1040   7.681 6.71e-10 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.2565 on 48 degrees of freedom\n## Multiple R-squared:  0.5514, Adjusted R-squared:  0.542 \n## F-statistic: 58.99 on 1 and 48 DF,  p-value: 6.71e-10"},{"path":"linear-regression.html","id":"assumptions-of-linear-regression","chapter":"20 Linear Regression","heading":"20.3 Assumptions of Linear Regression","text":"Linear regression makes several assumptions data, :Linearity data. relationship predictor (\\(x\\)) outcome (\\(y\\)) assumed linear. Obviously, relationship linear. try fit non-linear relationship linear regression results wouldn’t correct. Also use multiple predictors, see shortly, make another assumption model additive nature besides linear. Refer figure 20.4. clear try establish linear relationship (red line) actually cubic (green dashed line) model give erroneous results.\nFigure 20.4: relationship linear?\nNormality residuals. residuals assumed normally distributed. Actually, assumption followed assumption dependent variable normally distributed concentrated anywhere. Thus, dependent variable normally distributed, able capture relationship available, left must true noise normally distributed mean \\(0\\).Normality residuals. residuals assumed normally distributed. Actually, assumption followed assumption dependent variable normally distributed concentrated anywhere. Thus, dependent variable normally distributed, able capture relationship available, left must true noise normally distributed mean \\(0\\).Homogeneity residuals variance. residuals assumed constant variance, statistically known homoscedasticity. shows residuals left regression model true noise related fitted values, case meant model insufficient capture actual relationship. Heteroscedasticity (violation homoskedasticity) present size error term differs across values independent variable. can best understood plots Figures 20.5 residuals left plot indicate equal variance thus homoskedasticity whereas right plot heteroskedasticity indicated clearly.Homogeneity residuals variance. residuals assumed constant variance, statistically known homoscedasticity. shows residuals left regression model true noise related fitted values, case meant model insufficient capture actual relationship. Heteroscedasticity (violation homoskedasticity) present size error term differs across values independent variable. can best understood plots Figures 20.5 residuals left plot indicate equal variance thus homoskedasticity whereas right plot heteroskedasticity indicated clearly.\nFigure 20.5: Homoskedasticity (left) Vs. Heteroskedasticity (right)\nSample data created author demonstration \nIndependence residuals error terms. assumption also followed original assumption dependent variable independent \\(y\\) value dependent upon another set \\(y\\) values. example, can understand like Sepal width one sample affecting width another.","code":""},{"path":"linear-regression.html","id":"interpreting-the-output-of-lm","chapter":"20 Linear Regression","heading":"20.4 Interpreting the output of lm","text":"Let us discuss component section lm output, hereinafter referred model.","code":""},{"path":"linear-regression.html","id":"call","chapter":"20 Linear Regression","heading":"20.4.1 call","text":"call section shows us formula used fit regression model. Sepal.Width dependent response variable, Sepal.Length predictor independent variable. variables refer data-set , first 50 rows iris iris[1:50,].","code":""},{"path":"linear-regression.html","id":"residuals","chapter":"20 Linear Regression","heading":"20.4.2 Residuals","text":"depicts quantile five point summary error terms residuals, also discussed, difference actual values predicted values. can generate values taking actual values \\(y\\) variable subtracting predicted values model.Ideally, median error values/residuals centered around 0 thus telling us somewhat symmetrical model predicting fairly positive/higher negative/lower side. skewness thus show errors normally distributed model may biased towards side.\nFigure 20.6: Histogram Resuduals\n","code":"\nsummary(iris$Sepal.Width[1:50] - lin_reg1$fitted.values)##      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n## -0.723945 -0.182730 -0.003062  0.000000  0.157380  0.517085"},{"path":"linear-regression.html","id":"coefficients","chapter":"20 Linear Regression","heading":"20.4.3 Coefficients","text":"Remember goals exercise. coefficients written coefficient predictor intercept term, .e. \\(\\beta_1\\) \\(\\beta_0\\) respectively. can easily interpret positive coefficients means positive relation negative coefficient means value outcome variable decrease corresponding independent variable increase. , example, can deduce -0 sepal length, sepal width -0.5694 (Though mathematically physically 0 negative width lengths possible).every unit .e. 1 .e. unit increase sepal length, width increases 0.7985Thus regression line equation -\\[\\begin{equation}\nSepal.Width = -0.5694 + 0.7985 * Sepal.Length\n\\tag{20.4}\n\\end{equation}\\]Now one thing note , since adopting OLS approach find (estimated) equation best line, coefficients arrived , estimated values mean coefficients. Actually, started (behind scenes) null hypothesis linear relationship , words, coefficients zero. Alternate hypothesis, case, may guessed now, coefficients zero. coefficients may follow statistical/ probabilistic distribution thus, may infer estimated (mean) value.can see confidence intervals coefficient using function confint. default, 95% confidence intervals may generated. See following.Now, estimated distribution must also standard error, probability statistic p-value.standard error coefficient estimate standard deviation coefficient. tells us much uncertainty coefficient. can build confidence intervals coefficients using statistic, shown .t-statistic simply estimated coefficient divided standard error. now may understood applying student’s t-distribution estimating parameters.Finally p value .e. Pr(>|t|) gives us probability value tells us significant coefficient value.","code":"\nconfint(lin_reg1)##                   2.5 %    97.5 %\n## (Intercept)  -1.6184048 0.4795395\n## Sepal.Length  0.5894925 1.0075641"},{"path":"linear-regression.html","id":"signif.-codes","chapter":"20 Linear Regression","heading":"20.4.4 Signif. codes","text":"nothing code legends simply telling us significant p-value may case. Notice three asterisks coefficient estimate Sepal.Length indicate coefficient extremely significant can reject null hypothesis \\(\\beta_1\\) \\(0\\).codes give us quick way visually see coefficients significant model.","code":""},{"path":"linear-regression.html","id":"residual-standard-error","chapter":"20 Linear Regression","heading":"20.4.5 Residual standard error","text":"residual standard error measure, one metrics, telling us well model fits data. actually standard deviation error terms difference instead taking n terms taking degrees freedom.\\[\\begin{equation}\nRSE = \\sqrt{\\frac{1}{(n-2)} \\sum_{=1}^n (y_i - \\hat{y_i})^2}\n\\tag{20.5}\n\\end{equation}\\]Obviously \\(df\\) \\(n-2\\), one regressor one intercept. can verify equation (20.5) calculation.","code":"\nsqrt(sum((iris[1:50, \"Sepal.Width\"] - lin_reg1$fitted.values)^2)/48)## [1] 0.2565263"},{"path":"linear-regression.html","id":"r-squared-both-multiple-and-adjusted","chapter":"20 Linear Regression","heading":"20.4.6 R-Squared both Multiple and Adjusted","text":"Multiple R-Squared also called coefficient determination. Often cited measurement well model fitting data. tells us percentage variation within dependent variable independent variable explaining model. looking output can say 55% variation explained model. discuss detail, next section.Adjusted R squared hand, shows us percentage variation within dependent variable predictors explaining. Thus, helpful multiple regressors .e. Multiple Linear Regression. difference two metrics might subtle adjust variance attributed adding multiple variables.","code":""},{"path":"linear-regression.html","id":"f-statistic-and-p-value","chapter":"20 Linear Regression","heading":"20.4.7 F-Statistic and p-value","text":"p-value ? hypothesis ? Yes, running regression model, hypothesis test run global model, relationship dependent variable independent variable(s). alternative hypothesis relationship. words, alternate hypothesis means least one coefficient regression non-zero. hypothesis tested F-statistic hence two values. p-value example small lead us reject null hypothesis conclude strong evidence relationship exist Sepal.Length Sepal.Width.reason test based fact run multiple hypothesis tests coefficients, likely variable included isn’t actually significant.","code":""},{"path":"linear-regression.html","id":"model-evaluation-metrics","chapter":"20 Linear Regression","heading":"20.5 Model Evaluation Metrics","text":"evaluate model’s performance accuracy, evaluation metrics needed. several types metrics used evaluate performance model built. discuss -","code":""},{"path":"linear-regression.html","id":"mae---mean-absolute-error","chapter":"20 Linear Regression","heading":"20.5.1 MAE - Mean Absolute Error","text":"name suggests mean absolute values errors residuals. formula thus, can written equation (20.6).\\[\\begin{equation}\n{MAE} = \\frac{1}{N}\\sum_{= 1}^{N}{\\lvert}{y_i - \\hat{y_i}}{\\rvert}\n\\tag{20.6}\n\\end{equation}\\]Clearly, average value residuals larger value denotes lesser accurate model. isolation, MAE useful, however, compare performance several models fitting best regression model, obviously can use metric choose better model.Moreover, extract $residuals model, calculating metric easy. example-","code":"\n# Mean Absolute Error\nlin_reg1$residuals |> abs() |> mean()## [1] 0.199243"},{"path":"linear-regression.html","id":"mse---mean-square-error-and-rmse---root-mean-square-error","chapter":"20 Linear Regression","heading":"20.5.2 MSE - Mean Square Error and RMSE - Root Mean Square Error","text":"name suggests, mean square residuals mean square error MSE. formula may written equation (20.7).\\[\\begin{equation}\n{MSE} = \\frac{1}{N}\\sum_{= 1}^{N}({y_i - \\hat{y_i}})^2\n\\tag{20.7}\n\\end{equation}\\]MSE penalizes higher residuals squaring . may thus thought weighted average weight allocated residual value rises. Similar, MAE, can use metric choose better model several validating models.Interestingly, definition also cost function regression, finding parameters, actually minimising MSE . Similar MAE, calculating require special skills.RMSE root mean square error square root MSE.\\[\\begin{equation}\n{RMSE} = \\sqrt{\\frac{1}{N}\\sum_{= 1}^{N}({y_i - \\hat{y_i}})^2}\n\\tag{20.8}\n\\end{equation}\\]Example-","code":"\n# Mean Square Error\nlin_reg1$residuals^2 |> mean()## [1] 0.0631735\n# Root Mean Square Error\nlin_reg1$residuals^2 |> mean() |> sqrt()## [1] 0.2513434"},{"path":"linear-regression.html","id":"mape---mean-absolute-percentage-error","chapter":"20 Linear Regression","heading":"20.5.3 MAPE - Mean Absolute Percentage Error","text":"metric instead taking residual value isolation, takes residual value percentage actual values. formula thus,\\[\\begin{equation}\n{MAPE} = \\frac{1}{N}\\sum_{= 1}^{N}{\\lvert}\\frac{({y_i - \\hat{y_i}})}{y_i}\\cdot{100}\\%{\\rvert}\n\\tag{20.9}\n\\end{equation}\\]Clearly, MAPE independent scale variables since error estimates terms percentage. example, can calculate MAPE-","code":"\n# Mean Absolute Percentage Error\n{lin_reg1$residuals/iris$Sepal.Width[1:50]} %>% \n  {abs(.)*100} %>% \n  mean(.) %>% \n  sprintf(\"MAPE is %1.2f%%\", .)## [1] \"MAPE is 5.95%\""},{"path":"linear-regression.html","id":"r---squared-and-adjusted-r-squared","chapter":"20 Linear Regression","heading":"20.5.4 R - Squared and adjusted R-squared","text":"already discussed, coefficient determination goodness fit regression. can written equation (20.10).\\[\\begin{equation}\nR^2 = 1 - \\frac{\\sum_{=1}^{N}(y_i - \\hat{y_i})^2}{\\sum_{=1}^{N}(y_i - \\bar{y_i})^2}\n\\tag{20.10}\n\\end{equation}\\]numerator fraction , also called \\(SSE\\) Sum Squares Errors; denominator also called \\(TSS\\) Total Sum Squares. Actually ratio (fraction formula) .e. \\(\\frac{SSE}{TSS}\\) denotes ratio variance errors variance (mean) actual values. Thus \\(R^2\\) actually denotes much variance explained model; clearly variance errors \\(SSE\\) minimises approaches \\(0\\), \\(R^2\\) increases approaches \\(1\\) .e. perfect model.can easily verify formula results obtained.Now, can think \\(R^2\\) one way, simply square correlation actual predicted values. can verify againUsually, keep adding independent variables regression model \\(R^2\\) increases can easily incorporate -fitting . reason, sometimes adjusted r squared used, penalizes R squared number predictors used model.\\[\\begin{equation}\n{Adjusted}\\;{ R^2} = 1 - \\frac{(1-R^2)(N - 1)}{(N - p - 1)}\n\\tag{20.11}\n\\end{equation}\\]\\(p\\) number predictors included model. summary function returns metrics. can verify calculation-","code":"\n## Sum of Squares of Errors\ny_bar <- mean(iris[1:50,\"Sepal.Width\"])\n(SSE <- sum(lin_reg1$residuals^2))## [1] 3.158675\n(TSS <- sum((iris[1:50,\"Sepal.Width\"] - y_bar)^2))## [1] 7.0408\n(r_sq <- 1 - SSE/TSS)## [1] 0.5513756\n(cor(iris[1:50, 'Sepal.Width'], lin_reg1$fitted.values))^2## [1] 0.5513756\n1 - ((1-r_sq)*(50-1)/(50-1-1))## [1] 0.5420292"},{"path":"linear-regression.html","id":"plotting-the-results-and-their-interpretion","chapter":"20 Linear Regression","heading":"20.6 Plotting the results and their interpretion","text":"output lm can plotted plot command see six diagnostics plots, one one, can chosen using argument. six plots -Residuals Vs. Fitted ValuesNormal Q-QScale-LocationCook’s distanceResiduals vs. leverageCook’s distance vs. leverageLet us see , example .\nFigure 20.7: First two diagnostic plots\nResiduals Vs. Fitted Values: plot used determine residuals exhibit non-linear patterns. red line across center plot roughly horizontal can assume residuals follow linear pattern. example can see red line deviates perfect horizontal line severely. likely declare residuals follow roughly linear pattern linear regression model appropriate data-set. plot useful check first assumption linear regression .e. linearity data.Normal Q-Q: plot used determine residuals regression model normally distributed, another assumption. points plot fall roughly along straight diagonal line, can assume residuals normally distributed.Moreover, notice extreme outlier values impacting modelling labeled. can see values rows, 23, 33 42 labeled.\nFigure 20.8: Next two diagnostic plots\nScale-Location: plot used check assumption equal variance, .e. “homoskedasticity” among residuals regression model. red line roughly horizontal across plot, assumption equal variance likely met.Cook’s Distance: influential value value, inclusion exclusion can alter results regression analysis. value associated large residual. outliers (extreme data points) influential linear regression analysis. metric called Cook’s distance, used determine influence value. metric defines influence combination leverage residual size.\nFigure 20.9: Last two diagnostic plots\nResiduals vs. leverage: plot used identify influential observations. points plot fall outside Cook’s distance (dashed lines) influential observation. Actually, data point high leverage, extreme predictor x values.Sixth plot .e. Cooks distance vs. leverage also used identify extreme values may impacting model.Though base R’s command plot can generate plots, may make use library performance generate relevant diagnostic plots beautifully small interpretation. See\nFigure 20.10: Output package-performance\nwarning obvious, multi-collinearity problem one regressor. Actually multi-collinearity another assumption, made case one independent variables. assumption independent variables mutually collinear. see detailed explanation case multiple linear regression subsequent section.","code":"\npar(mfrow = c(1, 2), oma = c(0, 0, 2, 0))\nplot(lin_reg1, which = 1:2, sub.caption = \"\")\npar(mfrow = c(1, 2), oma = c(0, 0, 2, 0))\nplot(lin_reg1, which = 3:4, sub.caption = \"\")\npar(mfrow = c(1, 2), oma = c(0, 0, 2, 0))\nplot(lin_reg1, which = 5:6, sub.caption = \"\")\nlibrary(performance)\ncheck_model(lin_reg1)"},{"path":"linear-regression.html","id":"using-lm-for-predictions","chapter":"20 Linear Regression","heading":"20.7 Using lm for predictions","text":"output lm actually list contains much information saw . See info contained -may extract per requirement. E.g.package broom uses tidy fundamentals returns useful information single function augment.Let’s also visualise predicted values vis--vis actual values/residuals Figure 20.11.\nFigure 20.11: Predicted Vs. Actual Values (Left) Residuals (Right)\npredict output new data, just ensure data exactly format regressor can use predict base R directly. See example.","code":"\nnames(lin_reg1) |>\n  as.data.frame() |>\n  setNames('Objects')##          Objects\n## 1   coefficients\n## 2      residuals\n## 3        effects\n## 4           rank\n## 5  fitted.values\n## 6         assign\n## 7             qr\n## 8    df.residual\n## 9        xlevels\n## 10          call\n## 11         terms\n## 12         model\nlin_reg1$coefficients##  (Intercept) Sepal.Length \n##   -0.5694327    0.7985283\nlibrary(broom)\naugment(lin_reg1)## # A tibble: 50 × 8\n##    Sepal.Width Sepal.Length .fitted   .resid   .hat .sigma    .cooksd .std.resid\n##          <dbl>        <dbl>   <dbl>    <dbl>  <dbl>  <dbl>      <dbl>      <dbl>\n##  1         3.5          5.1    3.50 -0.00306 0.0215  0.259 0.00000160    -0.0121\n##  2         3            4.9    3.34 -0.343   0.0218  0.254 0.0205        -1.35  \n##  3         3.2          4.7    3.18  0.0163  0.0354  0.259 0.0000772      0.0649\n##  4         3.1          4.6    3.10 -0.00380 0.0471  0.259 0.00000568    -0.0152\n##  5         3.6          5      3.42  0.177   0.0200  0.258 0.00495        0.696 \n##  6         3.9          5.4    3.74  0.157   0.0455  0.258 0.00940        0.628 \n##  7         3.4          4.6    3.10  0.296   0.0471  0.255 0.0346         1.18  \n##  8         3.4          5      3.42 -0.0232  0.0200  0.259 0.0000853     -0.0914\n##  9         2.9          4.4    2.94 -0.0441  0.0803  0.259 0.00140       -0.179 \n## 10         3.1          4.9    3.34 -0.243   0.0218  0.257 0.0103        -0.959 \n## # ℹ 40 more rows\nnew_vals <- rnorm(10, 5, 1) |> \n  as.data.frame() |>\n  setNames('Sepal.Length')\npredict(lin_reg1, new_vals)##        1        2        3        4        5        6        7        8 \n## 3.416249 3.552228 2.691731 5.185508 2.801470 3.523949 4.037588 2.183525 \n##        9       10 \n## 4.190880 3.667629"},{"path":"linear-regression.html","id":"multiple-linear-regression","chapter":"20 Linear Regression","heading":"20.8 Multiple Linear Regression","text":"name suggests, multiple linear regression model multiple independent variables may linear relationship dependent variable. case, regression equation -\\[\\begin{equation}\ny = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_3x_3 + ... + \\epsilon\n\\tag{20.12}\n\\end{equation}\\]\\(x_1\\), \\(x_2\\), \\(x_3\\) …, \\(x_n\\) \\(n\\) independent variables \\(y\\) dependent variable usual.may clear equation represents equation plane two regressors. \\(n\\) regressors, equation represent equation hyperplane \\(n-1\\) dimensions. However, visualizing variables relation can bit tricky multiple variables. may decide type visualization suit requirement case.Now, already stated, additional assumption, independent variables mutually independent .e. multi-collinearity . Let’s build example model . just add predictors (independent variables) using + operator formula call.Problem Statement: Example-2. Let’s try establish relationship cars’ mileage (mpg variable mtcars data-set) engine displacement disp, horse power hp weight wt. First let’s visualise individual relationships variables three plots Figure 20.12.\nFigure 20.12: Relationship regressors outcome variables\nLet’s also visualise correlation variables. See Figure 20.13.\nFigure 20.13: Correlation variables Example-2\nNow let’s build model.formula call, please note used . instead naming variables. fact . shorthand style mentioning variables except mentioned y variable independent variables/predictors. results, one coefficient slope predictors one intercept term output. output equation can written equation (20.13).\\[\\begin{equation}\n{mpg} = -0.000937\\cdot{disp} - 0.031157\\cdot{hp} - 3.800891\\cdot{wt} + 37.105505\n\\tag{20.13}\n\\end{equation}\\]Interpretation: Notice slopes negative meaning mileage drops increase weight, displacement horsepower. Interpreting equation similar lines. can now say keeping variables constant, effect change (increase) 1 unit disp result decrease mileage 0.000937 miles per gallon. Keeping variables constant important . course, equation may deduce factors change, effect response variable different.Results: Analysing results, may notice model explains 83% variance data. course, adjusted r-squared lower means adding extra variables may increased r-squared. Global p-value highly significant means least coefficients non-zero. course, least significant coefficient disp can remove re-run model check parameters .","code":"\ndata <- mtcars[, c(\"mpg\", \"disp\", \"hp\", \"wt\")]\nlin_reg2 <- lm(mpg ~ ., \n               data = data)\nsummary(lin_reg2)## \n## Call:\n## lm(formula = mpg ~ ., data = data)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -3.891 -1.640 -0.172  1.061  5.861 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 37.105505   2.110815  17.579  < 2e-16 ***\n## disp        -0.000937   0.010350  -0.091  0.92851    \n## hp          -0.031157   0.011436  -2.724  0.01097 *  \n## wt          -3.800891   1.066191  -3.565  0.00133 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.639 on 28 degrees of freedom\n## Multiple R-squared:  0.8268, Adjusted R-squared:  0.8083 \n## F-statistic: 44.57 on 3 and 28 DF,  p-value: 8.65e-11"},{"path":"linear-regression.html","id":"including-categorical-or-factor-variables-in-lm","chapter":"20 Linear Regression","heading":"20.9 Including categorical or factor variables in lm","text":"now seen linear regression useful predicting numerical output examples seen regressors numerical . ’s input variable categorical nominal?case, ensure categorical variable type factor proceeding build model.Problem Statement: Example-3. Let’s try predict \\({Sepal.Width}\\) \\({Species}\\) iris data-set. time take complete data-set. Since, know Species already factor type need convert one. moving let’s visualize relation two variables using ggplot2. Box-plots best suited .\nFigure 20.14: Species Vs. Sepal Width\nNow let’s build model.results now got one intercept two slopes single regressor Species. happened now?Interpretation: Actually, factor data type requirement categorical regressor due fact factor variable encoded dummy variable category available . Dummy variable numeric can now run linear regression earlier. first category available baseline level. Since three levels included Species encoded two dummy variables. see happened behind scenes, may use contrasts function.model.matrixIt clear versicolor 1 variable 0 vice versa. Obviously 0 means Species setosa ’s separate slope present output. Now can write regression line equation (20.14)\\[\\begin{equation}\n{Sepal.Width} = 3.428 + (-0.658)\\cdot{Speciesversicolor} + (-0.454)\\cdot{Speciesvirginica}\n\\tag{20.14}\n\\end{equation}\\]Interpreting equation now easy. versicolor Sepal.Width may 3.428 - 0.658 2.77. Obviously two dummy variables zero, Sepal.Width equal intercept; thus, can conclude intercept nothing prediction base-line level. fact can subtract 1 obtain interpreted results.Notice two coefficients now adjusted automatically.Results: Observing figure 20.14, may notice coefficients nothing mean Sepal Widths Species, obvious logical . , interested seeing equation (without intercept) can also make one, equation (20.15).\\[\\begin{equation}\n{Sepal.Width} = 3.428\\cdot{Speciessetosa} + (2.77)\\cdot{Speciesversicolor} + (2.974)\\cdot{Speciesvirginica}\n\\tag{20.15}\n\\end{equation}\\]Since, coefficients slopes true sense, refer intercepts, next examples/sections.","code":"\nlm_fact <- lm(Sepal.Width ~ Species, data = iris)\nsummary(lm_fact)## \n## Call:\n## lm(formula = Sepal.Width ~ Species, data = iris)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -1.128 -0.228  0.026  0.226  0.972 \n## \n## Coefficients:\n##                   Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)        3.42800    0.04804  71.359  < 2e-16 ***\n## Speciesversicolor -0.65800    0.06794  -9.685  < 2e-16 ***\n## Speciesvirginica  -0.45400    0.06794  -6.683 4.54e-10 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.3397 on 147 degrees of freedom\n## Multiple R-squared:  0.4008, Adjusted R-squared:  0.3926 \n## F-statistic: 49.16 on 2 and 147 DF,  p-value: < 2.2e-16\ncontrasts(iris$Species)##            versicolor virginica\n## setosa              0         0\n## versicolor          1         0\n## virginica           0         1\nlm_fact <- lm(Sepal.Width ~ Species -1, data = iris)\nsummary(lm_fact)## \n## Call:\n## lm(formula = Sepal.Width ~ Species - 1, data = iris)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -1.128 -0.228  0.026  0.226  0.972 \n## \n## Coefficients:\n##                   Estimate Std. Error t value Pr(>|t|)    \n## Speciessetosa      3.42800    0.04804   71.36   <2e-16 ***\n## Speciesversicolor  2.77000    0.04804   57.66   <2e-16 ***\n## Speciesvirginica   2.97400    0.04804   61.91   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.3397 on 147 degrees of freedom\n## Multiple R-squared:  0.9881, Adjusted R-squared:  0.9879 \n## F-statistic:  4083 on 3 and 147 DF,  p-value: < 2.2e-16"},{"path":"linear-regression.html","id":"parallel-slopes-regression","chapter":"20 Linear Regression","heading":"20.9.1 Parallel slopes regression","text":"understand categorical response variable acts, numerical variables regression model, let us build model step step.Problem statement: Example-4. taking mpg data-set included default ggplot2 package. data-set shows Fuel economy data 1999 2008 38 popular models cars. Let us predict highway mileage hwy engine displacement displ year model year. Let us visualize variables. Figure 20.15 clear highway mileage linearly associated displacement.\nFigure 20.15: Highway Mileage Vs. Displacement cars manufactured 1998 Vs. 2008\nStep-1: Let us first include single numerical variable displ predict highway mileage hwy; examine coefficients.got one intercept one slope coefficient. can even see related visualisation figure 20.16 (left).Step-2: Now let us try predict mileage basis year manufacture . already stated, convert factor. can directly formula. Also note subtracting \\(1\\) response variables, actually replaces intercept baseline level explicitly. Now see coefficients-Notice got intercept category available factor variable. seeing plot Figure 20.16-(Right) intercepts nothing means category.\nFigure 20.16: Mileage vs. Displacement (Left) Year (right)\nStep-3: Now build complete model including variables together; examine coefficients.can see one slope coefficient numerical variable two different intercepts Years. Try visualising . fact two different parallel lines (slope). Refer figure 20.17.\nFigure 20.17: Parallel Slopes\nalso evident, write equation (20.16).\\[\\begin{equation}\n{hwy} = -3.610986\\cdot{displ} + 35.275706\\cdot{year1999} + 36.677842\\cdot{year2008}\n\\tag{20.16}\n\\end{equation}\\]Either one dummy variables 0 another 1, variable 1 act intercept term, slope term remain . words, whatever year, mileage vary displacement rate. actual circumstances, rare. Rate change response variable change per factor variables (regressor) change values. incorporate changes model? answer interaction discussed next section.","code":"\nggplot(mpg, aes(hwy, displ)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, formula = \"y ~ x\") +\n  facet_wrap(.~ year) +\n  theme_bw()\npar_slop1 <- lm(hwy ~ displ, data = mpg)\ncoef(par_slop1)## (Intercept)       displ \n##   35.697651   -3.530589\nlibrary(ggplot2)\npar_slop2 <- lm(hwy ~ factor(year) - 1, data = mpg)\ncoef(par_slop2)## factor(year)1999 factor(year)2008 \n##         23.42735         23.45299\npar_slop <- lm(hwy ~ displ + factor(year) - 1, data = mpg)\ncoef(par_slop)##            displ factor(year)1999 factor(year)2008 \n##        -3.610986        35.275706        36.677842"},{"path":"linear-regression.html","id":"extending-multiple-linear-regression-by-including-interactions","chapter":"20 Linear Regression","heading":"20.9.2 Extending multiple linear regression by including interactions","text":"parallel slopes model, saw previous section enforced common slope category. ’s always best option.Problem Statement: example-4 (earlier section) can introduce interaction two predictors using special operator shorthand notation : formula call. See following example-Now notice extra coefficient, though small change slope moving baseline category category year- 2008. , fact represents model different slope category; refer Figure 20.18.\nFigure 20.18: Changing Slopes interaction\nInterpreting models now, difficult seem earlier. Let’s generate summary first.may write equation now (equation (20.17).\\[\\begin{equation}\n{hwy} = -3.7684\\cdot{displ} + 35.7922\\cdot{year1999} + 36.1367\\cdot{year2008} + 0.3052\\cdot{displ}\\cdot{year2008}\n\\tag{20.17}\n\\end{equation}\\]Clearly, year 2008, slope displ changes 0.3052.can add many interactions like , using shorthand :; however, many interactions may make use another shorthand operator *. x*z mean x + z + x:z x*z*w mean x + z + w + x:z + x:w + z:w. obviously wouldn’t make difference save us lot typing.","code":"\nnew_model <- lm(hwy ~ displ + factor(year) + displ:factor(year) -1 , data = mpg)\ncoef(new_model)##                  displ       factor(year)1999       factor(year)2008 \n##              -3.768406              35.792231              36.136730 \n## displ:factor(year)2008 \n##               0.305168\nsummary(new_model)## \n## Call:\n## lm(formula = hwy ~ displ + factor(year) + displ:factor(year) - \n##     1, data = mpg)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -7.8595 -2.4360 -0.2103  1.6037 15.3677 \n## \n## Coefficients:\n##                        Estimate Std. Error t value Pr(>|t|)    \n## displ                   -3.7684     0.2788 -13.517   <2e-16 ***\n## factor(year)1999        35.7922     0.9794  36.546   <2e-16 ***\n## factor(year)2008        36.1367     1.0492  34.442   <2e-16 ***\n## displ:factor(year)2008   0.3052     0.3882   0.786    0.433    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.784 on 230 degrees of freedom\n## Multiple R-squared:  0.9759, Adjusted R-squared:  0.9755 \n## F-statistic:  2332 on 4 and 230 DF,  p-value: < 2.2e-16"},{"path":"linear-regression.html","id":"multi-collinearity-and-variance-inflation-factor-vif","chapter":"20 Linear Regression","heading":"20.10 Multi-collinearity and Variance Inflation Factor (VIF)","text":"Multi-collinearity indicates strong linear relationship among predictor variables. can create challenges regression analysis becomes difficult determine individual effects independent variable dependent variable accurately. Multi-collinearity can lead unstable unreliable coefficient estimates, making harder interpret results draw meaningful conclusions model. essential detect address multi-collinearity ensure validity robustness regression models.poses problem regression analysis. Actually, multi-collinearity means one independent variable can predicted another turn means independent variables longer independent.Multi-collinearity can detected using many different methods. One method can use correlation plots, explained next section. Another method use Variance Inflation Factor VIF. VIF determines strength correlation independent variables. predicted taking variable regressing every variable. words, VIF score independent variable represents well variable explained independent variables.\\[\\begin{equation}\n{VIF} = \\frac{1}{1-R^2}\n\\tag{20.18}\n\\end{equation}\\]closer \\(R^2\\) value \\(1\\) higher \\({VIF}\\).\\(VIF\\) starts \\(1\\) upper limit\\({VIF} = 1\\) means correlation independent variable variables\\({VIF}\\) exceeding \\(5\\) indicates high multi-collinearity independent variable others.R, manually calculate \\({VIF}\\) variable. Using vif() function, library car can calculate . Let’s use another model built mtcars data mpg variable predicted using variables. variables, instead using names, use another shorthand operator ..may notice high collinearity among predictors also indicated output.","code":"\nmod <- lm(mpg ~ . , data = mtcars)\nlibrary(car)## Loading required package: carData## \n## Attaching package: 'car'## The following object is masked from 'package:psych':\n## \n##     logit## The following object is masked from 'package:dplyr':\n## \n##     recode## The following object is masked from 'package:purrr':\n## \n##     some\ncar::vif(mod)##       cyl      disp        hp      drat        wt      qsec        vs        am \n## 15.373833 21.620241  9.832037  3.374620 15.164887  7.527958  4.965873  4.648487 \n##      gear      carb \n##  5.357452  7.908747"},{"path":"linear-regression.html","id":"one-complete-example","chapter":"20 Linear Regression","heading":"20.11 One Complete Example","text":"Problem Statement: Example-5. data pertains inter-country Life-Cycle Savings 1960-1970. life-cycle savings hypothesis developed Franco Modigliani, savings ratio (aggregate personal saving divided disposable income) sr explained per-capita disposable income dpi, percentage rate change per-capita disposable income ddpi, two demographic variables: percentage population less 15 years old pop15 percentage population 75 years old pop75. data averaged decade 1960–1970 remove business cycle short-term fluctuations.Let’s try linear regression LifeCycleSavings data.call formula , notice shorthand . operator means variable y variable treated input variables. See output-Multiple R squared 34% means nearly 34% variability explained linear model. Let us see diagnostics plots (figure 20.19)\nFigure 20.19: Diagnostic Plots\nmay notice multi-collinearity, two population variables. See figures 20.19 20.20.\nFigure 20.20: x y correlated?\ncan also verified corrplots figure 20.21.\nFigure 20.21: Correlation Plots\nLet us see VIF among predictors.Thus, model tuned better removing multi-collinear variable. Let us try remove pop75 re-run model.Notice multiple R-squared now reduced 30%. Let us try remove variable dpi coefficient significant.Multiple R squared increased slightly .e. now reached around 29%. Let us try visualise relationship scatter plot.\nFigure 20.22: Ascertaining linear relationship\nClearly relationship predictor response variable strong hence results.","code":"\n# Visualise first 6 rows\nhead(LifeCycleSavings)##              sr pop15 pop75     dpi ddpi\n## Australia 11.43 29.35  2.87 2329.68 2.87\n## Austria   12.07 23.32  4.41 1507.99 3.93\n## Belgium   13.17 23.80  4.43 2108.47 3.82\n## Bolivia    5.75 41.89  1.67  189.13 0.22\n## Brazil    12.88 42.19  0.83  728.47 4.56\n## Canada     8.79 31.72  2.85 2982.88 2.43\n# Build a model\nex1 <- lm(sr ~ . , data = LifeCycleSavings)\nsummary(ex1)## \n## Call:\n## lm(formula = sr ~ ., data = LifeCycleSavings)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -8.2422 -2.6857 -0.2488  2.4280  9.7509 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 28.5660865  7.3545161   3.884 0.000334 ***\n## pop15       -0.4611931  0.1446422  -3.189 0.002603 ** \n## pop75       -1.6914977  1.0835989  -1.561 0.125530    \n## dpi         -0.0003369  0.0009311  -0.362 0.719173    \n## ddpi         0.4096949  0.1961971   2.088 0.042471 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.803 on 45 degrees of freedom\n## Multiple R-squared:  0.3385, Adjusted R-squared:  0.2797 \n## F-statistic: 5.756 on 4 and 45 DF,  p-value: 0.0007904\nperformance::check_model(ex1)\nLifeCycleSavings %>% \n  ggplot(aes(pop15, pop75)) +\n  geom_point()+\n  geom_smooth(method = 'lm', formula = 'y~x') +\n  theme_bw()\ncar::vif(ex1)##    pop15    pop75      dpi     ddpi \n## 5.937661 6.629105 2.884369 1.074309\nex2 <- lm(sr ~ pop15 + dpi + ddpi, data = LifeCycleSavings)\nsummary(ex2)## \n## Call:\n## lm(formula = sr ~ pop15 + dpi + ddpi, data = LifeCycleSavings)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -7.6889 -2.8813  0.0296  1.7989 10.4330 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 19.2771687  4.3888974   4.392 6.53e-05 ***\n## pop15       -0.2883861  0.0945354  -3.051  0.00378 ** \n## dpi         -0.0008704  0.0008795  -0.990  0.32755    \n## ddpi         0.3929355  0.1989390   1.975  0.05427 .  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.862 on 46 degrees of freedom\n## Multiple R-squared:  0.3026, Adjusted R-squared:  0.2572 \n## F-statistic: 6.654 on 3 and 46 DF,  p-value: 0.0007941\nex3 <- lm(sr ~ pop15 + ddpi, data = LifeCycleSavings)\nsummary(ex3)## \n## Call:\n## lm(formula = sr ~ pop15 + ddpi, data = LifeCycleSavings)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -7.5831 -2.8632  0.0453  2.2273 10.4753 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 15.59958    2.33439   6.682 2.48e-08 ***\n## pop15       -0.21638    0.06033  -3.586 0.000796 ***\n## ddpi         0.44283    0.19240   2.302 0.025837 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.861 on 47 degrees of freedom\n## Multiple R-squared:  0.2878, Adjusted R-squared:  0.2575 \n## F-statistic: 9.496 on 2 and 47 DF,  p-value: 0.0003438\nLifeCycleSavings %>% \n  ggplot(aes(pop15, sr)) +\n  geom_point() +\n  geom_smooth(method = 'lm', formula = 'y ~ x', se = FALSE) +\n  theme_bw()"},{"path":"linear-regression.html","id":"simpsons-paradox","chapter":"20 Linear Regression","heading":"20.12 Simpson’s paradox","text":"Edward Hugh Simpson, statistician former cryptanalyst Bletchley Park, described statistical phenomenon paper 1951. classic example regressions, without including necessary terms, can misleading. core, paradox arises trend appears different subgroups data either reversed disappears subgroups combined. seemingly counter-intuitive occurrence can lead misleading conclusions thus underscores importance careful analysis interpretation data.Problem Statement: Example-6. data palmerpenguins let’s try establish relationship penguins bills’ depth lengths .e. bill_depth_mm bill_length_mm. See plot fig 20.23.\nFigure 20.23: Regression variable hidden (left) exposed (right)\nplot (left) negative relationship seen, lurking variable exposed (right), can see positive relationship species . Thus, species significant confounding variable assess linear relationship . Thus finalizing model, sure missing important variable. avoid incorrect results due underlying Simpson’s Paradox, must ensure :Identify Confounding Variables: vigilant identifying potential confounding variables affect relationship variables study.Consider Levels: Analyze data different levels, including subgroup aggregate levels, gain comprehensive understanding relationship.Utilise Statistical Techniques: regression analysis propensity score matching, control confounding variables obtain accurate insights.Transparent Reporting: Clearly report methodology, assumptions, limitations analysis ensure others can critically evaluate findings.Simpson’s Paradox, thus, serves powerful reminder data analysis intricate process requires careful consideration underlying factors.","code":""},{"path":"linear-regression.html","id":"conclusion-and-final-thoughts","chapter":"20 Linear Regression","heading":"20.13 Conclusion and Final thoughts","text":"sections, learned techniques regression analysis, powerful useful tool data analytics auditing. may use regression analysis, inter alia, -Detection Anomalies Outliers: Regression analysis can help auditors identifying anomalies, outliers, unexpected patterns financial data. Unusual relationships variables can signal potential errors, fraud, irregularities require investigation.Risk Assessment: analyzing relationships various financial operational variables, regression analysis can assist auditors assessing level risk associated different aspects organization’s operations. helps auditors prioritize efforts allocate resources effectively.Control Testing: Regression analysis can aid us testing effectiveness internal controls within organization. examining relationship control variables outcomes, auditors can assess whether controls functioning intended. can also use regression analysis compare organization’s financial performance industry benchmarks similar companies. Deviations expected relationships can highlight areas warrant closer examination.","code":""},{"path":"principal-component-analysis-in-r.html","id":"principal-component-analysis-in-r","chapter":"21 Principal Component Analysis in R","heading":"21 Principal Component Analysis in R","text":"content development","code":""},{"path":"clustering-in-r-using-kmeans-algorithm.html","id":"clustering-in-r-using-kmeans-algorithm","chapter":"22 Clustering in R (Using Kmeans algorithm)","heading":"22 Clustering in R (Using Kmeans algorithm)","text":"content development","code":""},{"path":"association-rule-mining-in-r-apriori.html","id":"association-rule-mining-in-r-apriori","chapter":"23 Association Rule Mining in R (Apriori)","heading":"23 Association Rule Mining in R (Apriori)","text":"content development","code":""},{"path":"part-v-time-series.html","id":"part-v-time-series","chapter":"Part-V: Time Series","heading":"Part-V: Time Series","text":"","code":""},{"path":"lubridate.html","id":"lubridate","chapter":"24 Date and Time calculations","heading":"24 Date and Time calculations","text":"","code":""},{"path":"lubridate.html","id":"base-r-classes-to-deal-with-date-and-time-variables","chapter":"24 Date and Time calculations","heading":"24.1 Base R classes to deal with date and time variables","text":"may hardly data analytics activity, wherein deal temporal information thus need manipulate dates date-time objects/variables. deal variable types, R four core data types/classes,Date deal date objects (note: D UPPER case)POSIXct deal timesPOSIXlt deal times (difference two refer section 24.1.2.)difftime deal time-spans","code":""},{"path":"lubridate.html","id":"dates","chapter":"24 Date and Time calculations","heading":"24.1.1 Dates","text":"Date objects can created string/character type objects (just see date objects can created numeric objects ), using base R’s function .Date() accepts character vector format parse date given string characters. E.g.example, can see custom format given parse character string date type object. can check class object created.format used function accepts special codes, can seen running ?strptime R console. ready reference used codes however, given table 24.1 section 24.1.9. date objects R, actually based numeric classes actually store number days since epoch. Check following code-, R versions 4.3.0 later, also accepts number convert date using number days elapsed since epoch. ,date since one day default origin date 1970-01-01 returned. However, default origin can changed using argument origin. Example,","code":"\nas.Date(\"2000-12-12\") # default format## [1] \"2000-12-12\"\nas.Date(\"12-12-2000\", format = \"%d-%m-%Y\") # custom format## [1] \"2000-12-12\"\na_date <- as.Date(\"20-12-2022\", format = \"%d-%m-%Y\")\nclass(a_date)## [1] \"Date\"\nclass(Sys.Date())## [1] \"Date\"\ntypeof(Sys.Date())## [1] \"double\"\nunclass(Sys.Date())## [1] 19964\nas.Date(1)## [1] \"1970-01-02\"\n# One day since launch of R version 1.0\nas.Date(1, origin = \"2000-02-29\")## [1] \"2000-03-01\""},{"path":"lubridate.html","id":"timm","chapter":"24 Date and Time calculations","heading":"24.1.2 Times","text":"Now, create date-time objects can use either POSIXct POSIXlt classes, .e. using .POSIXct() /.POSIXlt() functions, shown .regards, difference two classes, POSIXct stores seconds since UNIX epoch (information), POSIXlt, stores list day, month, year, hour, minute, second, etc., can understood following codes.ct POSIXct stands calendar time whereas, lt POSIXlt stands local time, explaining conceptual difference.Similar .Date(), .POSIXct() also accepts number converts date-time object using much number seconds since epoch.","code":"\nas.POSIXct(\"1990/2/17 12:20:05\") # default format## [1] \"1990-02-17 12:20:05 IST\"\nas.POSIXct(\"17-2-1990 15:30:00\", format = \"%d-%m-%Y %H:%M:%S\")## [1] \"1990-02-17 15:30:00 IST\"\nas.POSIXlt(\"17-2-1990 15:30:00\", format = \"%d-%m-%Y %H:%M:%S\")## [1] \"1990-02-17 15:30:00 IST\"\ntime_ct <- as.POSIXct(\"17-2-1990 15:30:00\", format = \"%d-%m-%Y %H:%M:%S\")\nclass(time_ct)## [1] \"POSIXct\" \"POSIXt\"\nunclass(time_ct)## [1] 635248800\n## attr(,\"tzone\")\n## [1] \"\"\n## POSIXlt\n# Convert above object into POSIXlt type\ntime_lt <- as.POSIXlt(time_ct)\n# Let us print\ntime_lt## [1] \"1990-02-17 15:30:00 IST\"\nclass(time_lt)## [1] \"POSIXlt\" \"POSIXt\"\nunclass(time_lt)## $sec\n## [1] 0\n## \n## $min\n## [1] 30\n## \n## $hour\n## [1] 15\n## \n## $mday\n## [1] 17\n## \n## $mon\n## [1] 1\n## \n## $year\n## [1] 90\n## \n## $wday\n## [1] 6\n## \n## $yday\n## [1] 47\n## \n## $isdst\n## [1] 0\n## \n## $zone\n## [1] \"IST\"\n## \n## $gmtoff\n## [1] 19800\n## \n## attr(,\"tzone\")\n## [1] \"\"      \"IST\"   \"+0630\"\n## attr(,\"balanced\")\n## [1] TRUE\nas.POSIXct(60)## [1] \"1970-01-01 05:31:00 IST\""},{"path":"lubridate.html","id":"times-without-dates","chapter":"24 Date and Time calculations","heading":"24.1.3 Times (without dates)","text":"base R, specific class deal time objects. However, package hms part tidyverse can used create time objects perform required calculations.","code":"\nhms::as_hms(10)## 00:00:10"},{"path":"lubridate.html","id":"diffftime","chapter":"24 Date and Time calculations","heading":"24.1.4 Timespan","text":"base R, besides date/date-time object classes, special class creates time difference two given temporal objects specified units. class difftime.Function difftime can create difftime objects, per specific requirement due presence argument units can take values one “auto”, “secs”, “mins”, “hours”, “days”, “weeks”.can also create difftime object coercion.","code":"\n(freedom_age <- Sys.Date() - as.Date(\"1947-08-15\"))## Time difference of 28139 days\nclass(freedom_age)## [1] \"difftime\"\nunclass(freedom_age)## [1] 28139\n## attr(,\"units\")\n## [1] \"days\"\ndifftime(as.Date(\"2020-02-29\"), as.Date(\"2019-02-28\"), units = \"weeks\")## Time difference of 52.28571 weeks\na_difftime <- as.difftime(15, units = \"days\")\na_difftime## Time difference of 15 days\nclass(a_difftime)## [1] \"difftime\""},{"path":"lubridate.html","id":"time-zones","chapter":"24 Date and Time calculations","heading":"24.1.5 Time zones","text":"Run function Sys.timezone() console check current timezone. Function OlsonNames() however, display known location time-zones., may use tz argument .POSIXlt() .POSIXct() functions coerce numeric class variables dates /times.","code":"\nSys.timezone()## [1] \"Asia/Calcutta\"\nOlsonNames()[c(253, 284)]## [1] \"Asia/Calcutta\" \"Asia/Kolkata\"\nas.POSIXct(1, tz = \"GMT\")## [1] \"1970-01-01 00:00:01 GMT\""},{"path":"lubridate.html","id":"coercion","chapter":"24 Date and Time calculations","heading":"24.1.6 Coercion","text":"earlier sections already seen coercing functions useful coerce objects one class another..Date().POSIXct().POSIXlt().difftime()","code":""},{"path":"lubridate.html","id":"extracting-parts-from-datestimes","chapter":"24 Date and Time calculations","heading":"24.1.7 Extracting parts from dates/times","text":"Base R provides us useful function extract relevant part objects dates/times classes.weekdays(x, abbreviate = FALSE) extract weekday name. (Output character vector)months(x, abbreviate = FALSE) extract month name. (Output character vector)quarters(x) extract quarter. Output character vector.julian(x) extract days elapsed since origin. Output numeric vector.Examples-","code":"\ndates_vec <- as.Date(15001 + 1:7)\nweekdays(dates_vec)## [1] \"Friday\"    \"Saturday\"  \"Sunday\"    \"Monday\"    \"Tuesday\"   \"Wednesday\"\n## [7] \"Thursday\"\nmonths(dates_vec)## [1] \"January\"  \"January\"  \"January\"  \"January\"  \"February\" \"February\" \"February\"\nquarters(dates_vec)## [1] \"Q1\" \"Q1\" \"Q1\" \"Q1\" \"Q1\" \"Q1\" \"Q1\"\njulian(dates_vec)## [1] 15002 15003 15004 15005 15006 15007 15008\n## attr(,\"origin\")\n## [1] \"1970-01-01\""},{"path":"lubridate.html","id":"other-useful-related-functions","chapter":"24 Date and Time calculations","heading":"24.1.8 Other useful related functions","text":"certain useful functions related classes useful data analytics.seq.Date(, , , length.= NULL, along.= NULL, ...) -start end dates respectivelyby increment sequence can accept either \nnumber taken days,\nobject class difftime\ncharacter string, containing one “day”, “week”, “month”, “quarter” “year”. can optionally preceded (positive negative) integer space, followed “s”.\nnumber taken days,object class difftimea character string, containing one “day”, “week”, “month”, “quarter” “year”. can optionally preceded (positive negative) integer space, followed “s”.length.optional argument accepting integer, desired length sequence.along.take length length argument.Example-","code":"\nseq(as.Date(\"2000/1/1\"), by = \"month\", length.out = 12)##  [1] \"2000-01-01\" \"2000-02-01\" \"2000-03-01\" \"2000-04-01\" \"2000-05-01\"\n##  [6] \"2000-06-01\" \"2000-07-01\" \"2000-08-01\" \"2000-09-01\" \"2000-10-01\"\n## [11] \"2000-11-01\" \"2000-12-01\""},{"path":"lubridate.html","id":"strpp","chapter":"24 Date and Time calculations","heading":"24.1.9 strptime character codes","text":"format codes useful parse date/date-time objects R, listed ready reference. know codes, readers can see output ?strptime() console. useful codes reproduced table 24.1 ready reference.Table 24.1:  Conversion Specifications Date/Date-time","code":""},{"path":"lubridate.html","id":"using-lubridate-package-for-parsingcreating-dates","chapter":"24 Date and Time calculations","heading":"24.2 Using lubridate package for parsing/creating dates","text":"Package lubridate now part core tidyverse extremely helpful package analysing temporal variables data. Creating date/date-time objects converting variables numeric/character types easier base R. Besides, offers us temporal class objects make time-series analysis easier.","code":""},{"path":"lubridate.html","id":"datedate-time-objects-creation","chapter":"24 Date and Time calculations","heading":"24.2.1 Date/date-time objects creation","text":"Date Date-time object classes lubridate defined similarly base R. can create date (date-time) using number days (seconds) elapsed since epoch.","code":"\nlibrary(lubridate, warn.conflicts = FALSE)\n# Creation using numbers\nas_date(2392)## [1] \"1976-07-20\"\nas_datetime(206668800)## [1] \"1976-07-20 UTC\"\n# Converting from one type to another\n(date_today <- as_date(now()))## [1] \"2024-08-29\"\n# Check its class\nclass(date_today)## [1] \"Date\"\n# Or see what's all in there\nunclass(date_today)## [1] 19964"},{"path":"lubridate.html","id":"parsing-datedate-time-objects-from-character","chapter":"24 Date and Time calculations","heading":"24.2.2 Parsing date/date-time objects from character","text":"lubridate parsing date/date-time objects character strings pretty easy number functions useful parse dates/date-times written specific locale/order irrespective delimiter used separate different components date/time therein. functions, orders year (y), quarter (q), month (m) date (d) hour (h) minute (m) second (s) represented first characters. -ymd_hms, ymd_hm, ymd_h, ymddmy_hms, dmy_hm, dmy_h, dmymdy_hms, mdy_hm, mdy_h, mdyydm_hms, ydm_hm, ydm_h, ydmmyd, dymyq, ym, myA examples -parse date-time fraction year passed, can use date_decimal(). Example-","code":"\nymd(\"19760720\")## [1] \"1976-07-20\"\ndmy(\"01.12.2004\")## [1] \"2004-12-01\"\ndmy(\"15th of January, 2006\")## [1] \"2006-01-15\"\nmdy_hm(\"August 15th, 1947 at 10:45 PM\")## [1] \"1947-08-15 22:45:00 UTC\"\nmy(\"04-2006\")## [1] \"2006-04-01\"\nyq(\"2024: Quarter 4\")## [1] \"2024-10-01\"\ndate_decimal(2024.162)## [1] \"2024-02-29 07:00:28 UTC\""},{"path":"lubridate.html","id":"parsing-datesdate-times-from-individual-conponents","chapter":"24 Date and Time calculations","heading":"24.2.3 Parsing dates/date-times from individual conponents","text":"scenario, create date/date-times can make use two functions make_date make_datetime. Syntax isExample-","code":"make_datetime(\n  year = 1970L,\n  month = 1L,\n  day = 1L,\n  hour = 0L,\n  min = 0L,\n  sec = 0,\n  tz = \"UTC\"\n)\n\nmake_date(year = 1970L, month = 1L, day = 1L)\nmake_date(year = 1947, month = 8, day = 15)## [1] \"1947-08-15\""},{"path":"lubridate.html","id":"extracting-and-setting-datedate-time-components","chapter":"24 Date and Time calculations","heading":"24.3 Extracting and setting date/date-time components","text":"","code":""},{"path":"lubridate.html","id":"extraction","chapter":"24 Date and Time calculations","heading":"24.3.1 Extraction","text":"can extract specific components date/date-times using, intuitively named accessor functions, listed . Note functions individual date components singular; plural component functions used different context, discuss Section 24.4.1.year() Year; isoyear() ISO 8601 Yearsemester() Semesterquarter() Quartermonth() Monthweek Week year, isoweek() ISO 8601 Weekday Day month, wday() Day week, qday() Day quarterhour(), minute() second() Hour, Minute Second respectivelytz() Time ZoneSee following examples.Besides functions, functions helpful knowing certain characteristics date/date-time variable. functions return logical vectors, listed -(x), pm(x) know whether x PM, respectively.dst(x) know whether x Daylight savings.leap_year(x) know whether x leap year.Examples-","code":"\n# Make an example date\n(a_time <- as_datetime(999999999))## [1] \"2001-09-09 01:46:39 UTC\"\n# Extract the DAY of the month\nday(a_time)## [1] 9\n# Extract Weekday name in full\nwday(a_time, label = TRUE, abbr = FALSE)## [1] Sunday\n## 7 Levels: Sunday < Monday < Tuesday < Wednesday < Thursday < ... < Saturday\n# Extract second component\nsecond(a_time)## [1] 39\n# Extract Month name (abbreviated)\nmonth(a_time, label = TRUE, abbr = TRUE)## [1] Sep\n## 12 Levels: Jan < Feb < Mar < Apr < May < Jun < Jul < Aug < Sep < ... < Dec\n# Extract week number of the year\nweek(a_time)## [1] 36\n# Is our a_time AM?\nam(a_time)## [1] TRUE\n# Is a_time falling in leap year?\nleap_year(a_time)## [1] FALSE\n# Is it during DST?\ndst(a_time)## [1] FALSE"},{"path":"lubridate.html","id":"setting-components","chapter":"24 Date and Time calculations","heading":"24.3.2 Setting Components","text":"Just like used accessor functions extract date-time components, can use functions set specific component date/date-time object/variable. See Example-Finally, function update can also used return date specified elements updated. Example-","code":"\n# Initial variable\na_time## [1] \"2001-09-09 01:46:39 UTC\"\n# Setting component - Year\nyear(a_time) <- 2024\n# Print modified variable\na_time## [1] \"2024-09-09 01:46:39 UTC\"\n# Modify Time zone\ntz(a_time) <- \"Asia/Kolkata\"\n# print modified variable\na_time## [1] \"2024-09-09 01:46:39 IST\"\nEOD <- dmy(\"31-01-2024\")\nEOD <- update(EOD, year = 2020)\nEOD## [1] \"2020-01-31\"\n# If values are too big, they will roll-over:\nupdate(EOD, month = 2)## [1] \"2020-03-02\""},{"path":"lubridate.html","id":"time-spans","chapter":"24 Date and Time calculations","heading":"24.4 Time spans","text":"subtract date-time object another base R, get difftime object, already seen section 24.1.4.know every time/date unit strict sense. Like leap years may 366 days whereas others 365 days. Now concept leap seconds . Moreover, certain countries daylight saving time thus, every day equal length . order deal specific clear requirements date calculations, three different classes lubridate, discussed next.Lubridate three classes, may sound similar first, different working. -perioddurationintervalLet us discuss one separately.","code":""},{"path":"lubridate.html","id":"perriod","chapter":"24 Date and Time calculations","heading":"24.4.1 Periods","text":"period objects track changes clock times ignore time line irregularities. every time object standard length like clocks.Period objects can created lubridate using pluralized date component functions. Example-","code":"\n(one_year <- years(1))## [1] \"1y 0m 0d 0H 0M 0S\"\nclass(one_year)## [1] \"Period\"\n## attr(,\"package\")\n## [1] \"lubridate\"\n# Add one year PERIOD to get a leap year?\na_date <- dmy(\"01-03-2019\")\na_date + years(1)## [1] \"2020-03-01\""},{"path":"lubridate.html","id":"durations","chapter":"24 Date and Time calculations","heading":"24.4.2 Durations","text":"Durations, hand track changes physical time, .e. taking account timeline adjusting irregularities. duration objects can created lubridate, adding d prefix pluralized period objects.","code":"\n(one_year_duration <- dyears(1))## [1] \"31557600s (~1 years)\"\nclass(one_year_duration)## [1] \"Duration\"\n## attr(,\"package\")\n## [1] \"lubridate\"\n# Add one year DURATION to get a leap year date?\na_date + dyears(1)## [1] \"2020-02-29 06:00:00 UTC\""},{"path":"lubridate.html","id":"intervals","chapter":"24 Date and Time calculations","heading":"24.4.3 Intervals","text":"objects class interval represent specific interval timeline. words, specific start end date/datetimes. can created lubridate using function interval, syntax like -clear following examples.Intervals can also created using special operator %--%. E.g.","code":"interval(start = NULL, end = NULL, tzone = tz(start))\n(a_interval <- interval(dmy(\"15-08-1947\"), today()))## [1] 1947-08-15 UTC--2024-08-29 UTC\nclass(a_interval)## [1] \"Interval\"\n## attr(,\"package\")\n## [1] \"lubridate\"\n(interval1 <- dmy(\"15-08-1947\") %--% dmy(\"26-01-1950\"))## [1] 1947-08-15 UTC--1950-01-26 UTC"},{"path":"lubridate.html","id":"performing-calculations-on-intervalsdates","chapter":"24 Date and Time calculations","heading":"24.5 Performing calculations on intervals/dates","text":"functions make life easier performing data analysis temporal fields. -","code":""},{"path":"lubridate.html","id":"within-operator","chapter":"24 Date and Time calculations","heading":"24.5.1 %within% operator","text":"Operator %within% b checks whether interval/date-time falls interval b. Returns boolean value(s).","code":"\ninterval1 %within% a_interval## [1] TRUE"},{"path":"lubridate.html","id":"backward-intervals","chapter":"24 Date and Time calculations","heading":"24.5.2 Backward intervals","text":"Intervals lubridate can backwards . E.g.","code":"\n(back_interval <- dmy(\"26-01-2024\") %--% dmy(\"15-01-2024\"))## [1] 2024-01-26 UTC--2024-01-15 UTC"},{"path":"lubridate.html","id":"flipping-intervals","chapter":"24 Date and Time calculations","heading":"24.5.3 Flipping intervals","text":"Function int_flip() can flip interval. E.g.","code":"\nint_flip(back_interval)## [1] 2024-01-15 UTC--2024-01-26 UTC"},{"path":"lubridate.html","id":"checking-alignment-of-two-intervals","chapter":"24 Date and Time calculations","heading":"24.5.4 Checking alignment of two intervals","text":"Function int_aligns tests two intervals share endpoint. direction interval ignored. words, actually tests whether earliest latest moments interval occur time. E.g.","code":"\nint1 <- interval(ymd(\"2001-01-01\"), ymd(\"2002-01-01\"))\nint2 <- interval(ymd(\"2001-06-01\"), ymd(\"2002-01-01\"))\nint3 <- interval(ymd(\"2003-01-01\"), ymd(\"2004-01-01\"))\n\nint_aligns(int1, int2)## [1] TRUE\nint_aligns(int1, int3)## [1] FALSE"},{"path":"lubridate.html","id":"checking-overlap-in-two-intervals","chapter":"24 Date and Time calculations","heading":"24.5.5 Checking Overlap in two intervals","text":"Function int_overlaps can test two intervals overlap .","code":"\nint_overlaps(int1, int2)## [1] TRUE"},{"path":"lubridate.html","id":"length-of-the-interval","chapter":"24 Date and Time calculations","heading":"24.5.6 Length of the interval","text":"Function int_length() can calculate length interval returns numeric variable equal seconds interval. E.g.","code":"\nint_length(back_interval)## [1] -950400"},{"path":"lubridate.html","id":"adding-months-without-exceeding-last-day-of-the-month.","chapter":"24 Date and Time calculations","heading":"24.5.7 Adding Months without exceeding last day of the month.","text":"Operators %m+% %m-% add (subtract) months date without exceeding last day new month. E.g.Another Example-","code":"\n(leap <- ymd(\"2012-02-29\"))## [1] \"2012-02-29\"\nleap %m+% years(1)## [1] \"2013-02-28\"\nleap %m+% years(-1)## [1] \"2011-02-28\"\nleap %m-% years(1)## [1] \"2011-02-28\"\njan <- ymd_hms(\"2010-01-31 03:04:05\")\njan + months(1:3) # Feb 31 and April 31 returned as NA## [1] NA                        \"2010-03-31 03:04:05 UTC\"\n## [3] NA\n# NA \"2010-03-31 03:04:05 UTC\" NA\njan %m+% months(1:3) # No rollover## [1] \"2010-02-28 03:04:05 UTC\" \"2010-03-31 03:04:05 UTC\"\n## [3] \"2010-04-30 03:04:05 UTC\""},{"path":"lubridate.html","id":"adding-with-rollback","chapter":"24 Date and Time calculations","heading":"24.5.8 Adding with Rollback","text":"One function add_with_rollback() performs similarly, control due specific syntax-Example-","code":"add_with_rollback(e1, e2, roll_to_first = FALSE, preserve_hms = TRUE)\nx <- ymd_hms(\"2019-01-29 01:02:03\")\nadd_with_rollback(x, months(1))## [1] \"2019-02-28 01:02:03 UTC\"\nadd_with_rollback(x, months(1), preserve_hms = FALSE)## [1] \"2019-02-28 UTC\"\nadd_with_rollback(x, months(1), roll_to_first = TRUE)## [1] \"2019-03-01 01:02:03 UTC\"\nadd_with_rollback(x, months(1), roll_to_first = TRUE, preserve_hms = FALSE)## [1] \"2019-03-01 UTC\""},{"path":"lubridate.html","id":"coercing-one-time-span-unit-to-another","chapter":"24 Date and Time calculations","heading":"24.5.9 Coercing one time span unit to another","text":"Time-span objects lubridate can coerced one another using coercing functions,.period(x, unit).duration(x).interval(x, start)make_difftime(x)Examples-","code":"\n# With Period - clock time\n(per <- days(31))## [1] \"31d 0H 0M 0S\"\n(int1 <- as.interval(per, dmy(\"01022020\")))## [1] 2020-02-01 UTC--2020-03-03 UTC\n# With Duration - physical time\n(dur <- ddays(31))## [1] \"2678400s (~4.43 weeks)\"\n(int2 <- as.interval(dur, dmy(\"01022020\")))## [1] 2020-02-01 UTC--2020-03-03 UTC"},{"path":"lubridate.html","id":"rounding-date-time-variables","chapter":"24 Date and Time calculations","heading":"24.6 Rounding date-time variables","text":"dedicated functions round dates per specific requirements.floor_date() takes date-time object rounds nearest boundary specified time unit.ceiling_date() takes date-time object rounds nearest boundary specified time unit.round_date() takes date-time object time unit, rounds nearest value specified time unit.Examples-Three functions helps rolling date forward backwards, asrollbackward() changes date last day previous month first day month.rollforward() rolls last day current month first day next month. Optionally, new date can retain hour, minute, second information.rollback() synonym rollbackward().See examples-","code":"\nx <- ymd_hms(\"2009-08-03 12:01:59.23\")\nround_date(x, \"month\")## [1] \"2009-08-01 UTC\"\nround_date(x, \"week\")## [1] \"2009-08-02 UTC\"\nfloor_date(x, \"day\")## [1] \"2009-08-03 UTC\"\nceiling_date(x, \"month\")## [1] \"2009-09-01 UTC\"\ndate <- ymd_hms(\"2010-03-03 12:44:22\")\n\nrollbackward(date)## [1] \"2010-02-28 12:44:22 UTC\"\nrollbackward(date, roll_to_first = TRUE)## [1] \"2010-03-01 12:44:22 UTC\"\nrollbackward(date, preserve_hms = FALSE)## [1] \"2010-02-28 UTC\"\nrollbackward(date, roll_to_first = TRUE, preserve_hms = FALSE)## [1] \"2010-03-01 UTC\"\nrollforward(date)## [1] \"2010-03-31 12:44:22 UTC\"\nrollforward(date, roll_to_first = TRUE)## [1] \"2010-04-01 12:44:22 UTC\""},{"path":"lubridate.html","id":"representing-date-and-date-times-in-cutomised-formats","chapter":"24 Date and Time calculations","heading":"24.7 Representing date and date-times in cutomised formats","text":"sections, saw R recognises temporal object, depicts objects uniform format. SeeIn output/console dates print like character/string. However, sometimes requirement print dates specific customised format. scenario, function strftime() comes rescue converts objects classes “POSIXlt” “POSIXct” representing calendar dates times specific character representation. Character codes can used table 24.1 section 24.1.9.See following examples-context, let’s also discuss special date stamping function lubridate can format date/time outputs based human friendly formats. Functions stamp(), stamp_date stamp_time() create function given template, can applied date/time objects re-format . Example-","code":"\n(date1 <- dmy(\"01-01-2020\"))## [1] \"2020-01-01\"\n(date2 <- ymd(\"2020/01/01\"))## [1] \"2020-01-01\"\nas.character(date1)## [1] \"2020-01-01\"\nstrftime(Sys.Date(), format = \"%d %B %Y\")## [1] \"29 August 2024\"\neclipse_dates <- dmy(c(\"11-7-2010\", \"13-11-2012\", \"3-11-2013\"))\neclipse_stamp <- stamp_date(\"There was a solar eclipse on January 13th, 1999\")\neclipse_stamp(eclipse_dates)## [1] \"There was a solar eclipse on July 11th, 2010\"    \n## [2] \"There was a solar eclipse on November 13th, 2012\"\n## [3] \"There was a solar eclipse on November 03th, 2013\""},{"path":"lubridate.html","id":"dates-in-ggplot2-visualisations","chapter":"24 Date and Time calculations","heading":"24.8 Dates in GGPLOT2 visualisations","text":"concluding chapter date time variables, important learn related functions deal impact date time formats visualisations. Since already seen , date date-time variables actually continuous variables labels depicted specific formats, two scale functions ggplot2 deal -scale_x_date()scale_x_datetime()functions, date_breaks date_minor_breaks arguments position breaks date units. E.g. date_breaks = \"6 months\" place major tick mark every six months.Using another argument date_labels can control display labels plots. values argument may strptime formats already seen 24.1 .Example-\nFigure 24.1: Use Date Time Scale ggplot2\n","code":"\nlibrary(tidyverse, warn.conflicts = FALSE)\n\neconomics %>% \n  ggplot(aes(date, uempmed)) +\n  geom_line() +\n  scale_x_date(date_labels = \"%Y\", \n               date_breaks = \"5 years\", \n               date_minor_breaks = \"1 year\") +\n  labs(x = \"\", y = \"\")"},{"path":"time-series-analysis.html","id":"time-series-analysis","chapter":"25 Time Series Analysis","heading":"25 Time Series Analysis","text":"Time series analysis essential technique forecasting analyzing trends data time. commonly used various fields like finance, economics, engineering.question arises, time series? time series sequence data points collected time, observations recorded chronological order. Time series analysis involves analyzing modeling data points understand patterns, trends, make predictions future values. therefore used predictive well descriptive analysis.Time Series Analysis plays important role audit analytics well. use cases can -Revenue Analysis: auditor may analyze revenue data time identify irregularities suspicious patterns indicate potential fraud misstatement. examining revenue time series, auditor can look unexpected fluctuations, unusual growth decline trends, abnormal seasonality. Deviations historical patterns industry benchmarks may indicate fraudulent activities, revenue manipulation, fictitious transactions, irregular recognition practices.Inventory Analysis: Auditors often analyze inventory data assess adequacy inventory levels, identify potential inventory obsolescence shrinkage, evaluate efficiency inventory management. Time series analysis can valuable understanding inventory patterns identifying potential risks anomalies.several time series data loaded default R; listed . use datasets understand various concepts related time series analysis.AirPassengers containing monthly airline passenger numbers 1949-1960. See figure 25.1 ()Nile contains flow Nile river data. See figure 25.1 (b)sunspots containing monthly sunspot numbers 1749-1983. See figure 25.1 (c)JohnsonJohnson contains quarterly earnings per Johnson & Johnson share. See figure 25.1 (d)\nFigure 25.1: time series data sets R\nanalyse time series, understand different components.","code":""},{"path":"time-series-analysis.html","id":"components-of-time-series","chapter":"25 Time Series Analysis","heading":"25.1 Components of Time series","text":"time series can decomposed several components:Level: baseline value average series time. represents long-term behavior series. basic component, always present time series object. E.g. straight horizontal line level equal value y intercept.Trend: overall direction series. indicates whether series increasing, decreasing, staying relatively constant time. E.g. clear increasing trend can seen Johnson & Johnson earnings, Figure 25.1 (d).Seasonality: repetitive predictable patterns within series occur regular intervals. Seasonality can daily, weekly, monthly, quarterly, yearly, etc. E.g. figure 25.1 (d) may see seasonal patterns earnings Johnson&Johnson share.Errors (Residuals): random fluctuations noise series explained level, trend, seasonality. E.g. See figure 25.1 (c). error component important aspect time series analysis provides information uncertainty variability data.","code":""},{"path":"time-series-analysis.html","id":"additive-or-multiplicative-components","chapter":"25 Time Series Analysis","heading":"25.2 Additive or multiplicative components","text":"time series analysis, trend, seasonal, residual components can modeled either additive multiplicative. choice model depends components combine create observed values series.additive model assumes components time series added together create observed values. words, value time series point time equal sum trend, seasonal, residual components point time. expressed mathematically :\\[\ny_t = T_t + S_t + e_t\n\\]\\(y_t\\), \\(T_t\\), \\(S_t\\) \\(e_t\\) values series, trend component, seasonal component residuals, respectively time \\(t\\).multiplicative model, hand, assumes components time series multiplied together create observed values. words, value time series point time equal product trend, seasonal, residual components point time. expressed mathematically :\\[\ny_t = T_t \\times S_t \\times e_t\n\\]\\(y_t\\), \\(T_t\\), \\(S_t\\) \\(e_t\\) defined . can convert multiplicative time series additive time series taking \\(log\\). Note seasonal component AirPassengers time series depicted 25.1 () multiplicative. See Figure 25.2.\nFigure 25.2: Transforming Time Series\nlearn decomposing time series components next sections.Damped Trends: Gardner Mckenzie, 198531 observed time series methods assume trend continue unabated, regardless forecast lead time. , based empirical findings, suggested forecast accuracy can improved either damping ignoring altogether trends low probability persistence. learn concept, well next sections.","code":""},{"path":"time-series-analysis.html","id":"practical-examples-in-r","chapter":"25 Time Series Analysis","heading":"25.3 Practical examples in R","text":"analysing time series, R let’s see time series objects dealt R.","code":""},{"path":"time-series-analysis.html","id":"pre-requisites","chapter":"25 Time Series Analysis","heading":"Pre-requisites","text":"Though time using base R, yet forecast fabulous package R, developed Rob Hyndman, using predicting. Predicting values help us understand well captured hidden trends patterns time series.","code":"\nlibrary(forecast)\nlibrary(ggplot2)"},{"path":"time-series-analysis.html","id":"creating-a-time-series-object-in-r","chapter":"25 Time Series Analysis","heading":"25.3.1 Creating a time series object in R","text":"R provides us simple function ts() convert series observations (basically vector) specific time series object. syntax -:data: numeric vector matrix containing data time seriesstart: start time time series, represented either numeric Date POSIXct objectend: end time time series, represented either numeric Date POSIXct objectfrequency: number observations per unit time time series. example, data recorded monthly, frequency 12...: additional arguments can passed function, names, delim, tsp (time series start end points)Example-1: Let’s create time series.can check class.Let’s also check Nile, already available base R.","code":"ts(data, \n  start = 1, \n  end = numeric(length(data)), \n  frequency = 1, \n  ...)\n# Create a numeric vector representing monthly sales data for a year\nsales <- c(10, 20, 30, 25, 35, 40, 45, 50, 55, 60, 65, 70)\n\n# Create a time series object with monthly frequency starting from January\nsales_ts <- ts(sales, start = c(2022, 4), frequency = 12)\n\n# Print the time series object\nsales_ts##      Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n## 2022              10  20  30  25  35  40  45  50  55\n## 2023  60  65  70\nclass(sales_ts)## [1] \"ts\"\nNile## Time Series:\n## Start = 1871 \n## End = 1970 \n## Frequency = 1 \n##   [1] 1120 1160  963 1210 1160 1160  813 1230 1370 1140  995  935 1110  994 1020\n##  [16]  960 1180  799  958 1140 1100 1210 1150 1250 1260 1220 1030 1100  774  840\n##  [31]  874  694  940  833  701  916  692 1020 1050  969  831  726  456  824  702\n##  [46] 1120 1100  832  764  821  768  845  864  862  698  845  744  796 1040  759\n##  [61]  781  865  845  944  984  897  822 1010  771  676  649  846  812  742  801\n##  [76] 1040  860  874  848  890  744  749  838 1050  918  986  797  923  975  815\n##  [91] 1020  906  901 1170  912  746  919  718  714  740"},{"path":"time-series-analysis.html","id":"plotting-time-series-objects","chapter":"25 Time Series Analysis","heading":"25.3.2 Plotting Time Series Objects","text":"plot ts object can simply use plot() command alternatively can use ggplot2 well.\nFigure 25.3: Example time series plotting R; Base R (Left) ggplot2 (Right)\nNote: Library forecast R also provides us function autoplot() plots time series object, similar plot(), plot returned ggplot2 object can modified/fine-tuned using ggplot2 functions.","code":"\n# Plotting in base R\nplot(sales_ts, main = \"Sales during FY 2022-23\\nDummy Data by author\")\n\nforecast::autoplot(AirPassengers) +\n  scale_y_log10('Logrithmic values') +\n  ggtitle('AirPassengers converted to additive') +\n  theme_bw()"},{"path":"time-series-analysis.html","id":"time-series-modelling-and-forecasting","chapter":"25 Time Series Analysis","heading":"25.4 Time series modelling and forecasting","text":"already discussed, forecasting future plays important part time series analysis; important prerequisite model data. fact, forecasting/modelling techniques can broadly classified two categories, data based techniques model-based techniques.","code":""},{"path":"time-series-analysis.html","id":"data-based-techniques","chapter":"25 Time Series Analysis","heading":"Data-based techniques","text":"Data-based techniques, also known statistical empirical techniques, focus analyzing patterns characteristics observed time series data directly. techniques explicitly assume specific underlying model structure. Instead, rely statistical properties patterns present data. Examples data-based techniques include:","code":""},{"path":"time-series-analysis.html","id":"naive-method","chapter":"25 Time Series Analysis","heading":"25.4.1 Naive method","text":"method assumes future value last observed value. simplest basic forecasting method hence named naive. formula naive method :\\[\n\\text{Naive method: }\\hat{Y}_{T+1} = Y_T\n\\], \\(\\hat{Y}_{T+1}\\) forecast value next time period \\(Y_T\\) last observed value .e. series \\(T\\) observations. Thus, predicting future values using naive method require special skill.Problem Statement-1: Let’s predict say 5, future values Nile flow data. forecast function naive job, give value parameter h short forecasting horizon. Figure 25.4, can see forecast values, red, exactly previous/last value.\nFigure 25.4: Naive forecasting\naren’t interested visualising confidence values (default 80% 95% confidence values shown bands), can tune parameter level accordingly.","code":"\nforecast::naive(Nile, h = 5) %>% \n  autoplot() +\n  theme_bw() +\n  labs(title = \"Nile flow - forecast by Naive method\",\n       x = \"Year\")"},{"path":"time-series-analysis.html","id":"moving-averages","chapter":"25 Time Series Analysis","heading":"25.4.2 Moving averages","text":"technique, forecast value computed average recent observations within sliding window fixed length, say \\(n\\).. technique smooth time series averaging neighboring values. helps reducing noise revealing underlying trends.formula moving average method :\\[\n\\hat{Y}_{t+1} = \\frac{1}{n}(Y_t + Y_{t-1} + ... + Y_{t-n+1})\n\\], \\(\\hat{Y}_{t+1}\\) forecast value next time period \\(Y_t\\), \\(Y_{t-1}\\), …, \\(Y_{t-n+1}\\) past \\(n\\) observations.Smoothing moving average can either centre-weighted tailed-weighted. former, \\(k\\) .e. order moving average, odd number data point replaced mean observation \\((k-1)/2\\) observations \\((k-1)/2\\) observations . Smoothed time series Nile different k can seen Figure 25.5.\nFigure 25.5: Simple Moving Averages\nProblem Statement-2: Let’s try visualise moving average trend Nile data. use ma function forecast . left side Figure 25.6 smoother (k = 7) time series 10 forecast values using last value moving average.can also use rollmean function zoo library, another package analyse time series objects. Refer right-side figure 25.6. example, used wineind data/time series shows Australian total wine sales wine makers bottles <= 1 litre. Jan 1980 – Aug 1994. One advantage using method, can see original smoothed time series.\nFigure 25.6: Simple Moving averages\n","code":""},{"path":"time-series-analysis.html","id":"exponential-smoothing","chapter":"25 Time Series Analysis","heading":"25.4.3 Exponential smoothing","text":"Exponential smoothing techniques update forecasts based weighted averages past observations, giving weight recent observations. Examples include simple exponential smoothing, Holt’s method, Holt-Winters’ method.name suggests simple exponential smoothing SES simplest . Simple exponential smoothing (SES) actually moving weighted average recent observations given weights calculating averages. method suitable forecasting data clear trend seasonal pattern.\\[\n\\hat{y}_{T+1} = \\alpha{y_T} + \\alpha(1- \\alpha){y_{T-1}} + \\alpha(1 - \\alpha)^2{y_{T-2}} + ...\n\\]value \\(\\alpha\\), smoothing parameter, equation follow \\(0 \\le \\alpha \\le 1\\). Clearly different values \\(\\alpha\\) give different smoothing adopt suitable one, one example \\(\\alpha = 0.1\\) shown Figure 25.7 (Left).Holt extended SES introducing trend component thus new smoothing parameter trend \\(\\beta\\). method also named Holt’s Linear Method exponential smoothing. Now two equations, mathematically.\\[\n\\begin{aligned}\n\\hat{y}_{t+h|t} &= l_t + hb_t \\\\\nl_t &= \\alpha y_t + (1-\\alpha)(l_{t-1} + b_{t-1}) \\\\\nb_t &= \\beta(l_t - l_{t-1}) + (1-\\beta)b_{t-1}\n\\end{aligned}\n\\]\\(\\alpha\\) \\(\\beta\\) (\\(0 \\le \\beta \\le 1\\)) equations smoothing parameters level trend components, respectively; \\(h\\) forecast horizon. Also, \\(y_t\\) actual value, \\(l_t\\) level (intercept), \\(b_t\\) trend (slope) time series time \\(t\\). Refer 25.7 (Right).apply SES, R, can use function ses applying Holt’s smoothing, can use holt; forecast library. Example usages follows.\nFigure 25.7: Simple Exponential Smoothing (Left) Vs. Holt’s Linear Method (Right)\ndiscussed earlier, damped trends work better many times. based Mckenzie gardner work, additional dampening paramater \\(\\phi\\) introduced. equations modified -\n\\[\n\\begin{aligned}\n\\hat{y}_{t+h|t} &= l_t + (\\phi + \\phi^2 + ... + \\phi^h)b_t \\\\\nl_t &= \\alpha y_t + (1-\\alpha)(l_{t-1} + \\phi b_{t-1}) \\\\\nb_t &= \\beta(l_t - l_{t-1}) + (1-\\beta)\\phi b_{t-1}\n\\end{aligned}\n\\]Clearly, \\(\\phi = 1\\), equations equivalent Holt’s Linear Method. \\(\\phi\\) values \\(0 < \\phi < 1\\), dampens trend, shown figure 25.8 (Left). comparison can seen Figure 25.8 (Right).extend holt’s linear method damped trends R, can set parameter damped = TRUE holt function discussed . Example code can seen .\nFigure 25.8: Linear methos Vs. Damped trends\nNow incorporate seasonality (say \\(m\\) periods) along trend level, Holt Winters suggested incorporate additional parameter \\(\\gamma\\) equations.\\[\n\\text{Holt-Winters method:}\n\\]\n\\[\n\\begin{aligned}\n\\hat{y}_{t+h|t} &= l_t + hb_t + s_{t+m-(m-1 \\bmod m)} \\\\\nl_t &= \\alpha(y_t - s_{t-m}) + (1-\\alpha)(l_{t-1} + b_{t-1}) \\\\\nb_t &= \\beta(l_t - l_{t-1}) + (1-\\beta)b_{t-1} \\\\\ns_t &= \\gamma(y_t - l_{t-1} - b_{t-1}) + (1-\\gamma)s_{t-m}\n\\end{aligned}\n\\]example can seen figure 25.9. apply Holt-Winters method R, can use hw function desired values parameters. example code may follows, tried extend AirPassengers time series method additive multiplicative trends.\nFigure 25.9: Holt-Winter’s Method Forecasting\n","code":"\nlynxses <- ses(lynx, h= 10, alpha = 0.1)\nholtfit <- holt(airmiles, h = 10, alpha = 0.3, beta = 0.1)\n\nautoplot(lynxses) +\n  autolayer(lynxses$fitted) +\n  ggplot2::ggtitle('SES with alpha = 0.1 : Lynx data') +\n  ggplot2::theme_bw() +\n  ggplot2::theme(legend.position = \"bottom\") +\n  ggplot2::labs(color = \"\")\n\nautoplot(holtfit) +\n  autolayer(holtfit$fitted) +\n  ggplot2::ggtitle(\"Holt's Linear Trend method with beta = 0.1 : AirMiles data\") +\n  ggplot2::theme_bw() +\n  ggplot2::theme(legend.position = \"bottom\") +\n  ggplot2::labs(color = \"\")\nlibrary(ggplot2)\n\nholtfit <- holt(airmiles, h = 10, alpha = 0.3, beta = 0.1, damped = TRUE, phi = 0.8)\nautoplot(holtfit) +\n  autolayer(holtfit$fitted) +\n  ggplot2::ggtitle('Damped Trend Forecast with phi=0.8') +\n  ggplot2::theme_bw() +\n  ggplot2::theme(legend.position = \"bottom\") +\n  ggplot2::labs(color = \"\")\n\n\nholtfit <- holt(airmiles,\n                h = 10,\n                alpha = 0.3,\n                beta = 0.1)\n\nholtfit2 <- holt(\n  airmiles,\n  h = 10,\n  alpha = 0.3,\n  beta = 0.1,\n  damped = TRUE,\n  phi = 0.8\n)\n\nautoplot(airmiles)+\n  autolayer(holtfit, series=\"Holt's method\", PI = FALSE) +\n  autolayer(holtfit2, series=\"Holt's method with damped trend\", PI = FALSE)+\n  ggplot2::ggtitle(\"Holt's method\") + xlab(\"Year\") +\n  ylab(\"Revenue passenger miles\") +\n  guides(colour=guide_legend(title=\"Method\")) +\n  ggplot2::theme_bw() +\n  theme(legend.position = \"bottom\")\nhw1 <- hw(AirPassengers, seasonal = \"additive\")\nhw2 <- hw(AirPassengers, seasonal = \"multiplicative\")\n\n\nautoplot(hw1) +\n  ggtitle(\"Additive Model\")\n\nautoplot(hw2) +\n  ggtitle(\"Multiplicative Method\")"},{"path":"time-series-analysis.html","id":"seasonal-decomposition-methods","chapter":"25 Time Series Analysis","heading":"Seasonal decomposition methods","text":"Seasonal decomposition techniques decompose time series different components, trend, seasonality, noise. allows better understanding individual components impact overall series. several methods decompose time series different components. however, discuss two .","code":""},{"path":"time-series-analysis.html","id":"classical-seasonal-decomposition","chapter":"25 Time Series Analysis","heading":"25.4.4 Classical seasonal decomposition","text":"First methods classical decomposition method decomposes time series trend, seasonal, residual components. assumes seasonal component repeats identically year year, trend component changes linearly time. steps involved classical decomposition method follows:Trend Component: trend component estimated using moving average regression method.Seasonal Component: seasonal component estimated averaging values across seasonal periods different years.Residual Component: residual component obtained subtracting trend seasonal components original time series.Problem Statement: Let’s try decompose time series say AirPassengers see components. decompose time series R, use function decompose shown following code. Decomposing time series R gives us four different plots () Observed .e. original values, (ii) trend, (iii) seasonality (iv) random noise available.Case-1: Additive decompositionCase-2: Multiplicative decomposition. case can use argument type.plot decomposed AirPassengers, using classical approach shown Figure 25.10.\nFigure 25.10: Seasonal Decomposition Techniques\n","code":"\ndecomposed_air_passengers <- decompose(AirPassengers)\nsummary(decomposed_air_passengers)##          Length Class  Mode     \n## x        144    ts     numeric  \n## seasonal 144    ts     numeric  \n## trend    144    ts     numeric  \n## random   144    ts     numeric  \n## figure    12    -none- numeric  \n## type       1    -none- character\ndecomposed_air_passengers2 <- decompose(AirPassengers, type = \"multiplicative\")\nsummary(decomposed_air_passengers2)##          Length Class  Mode     \n## x        144    ts     numeric  \n## seasonal 144    ts     numeric  \n## trend    144    ts     numeric  \n## random   144    ts     numeric  \n## figure    12    -none- numeric  \n## type       1    -none- character\nAirPassengers |>\n  decompose() |>\n  plot()"},{"path":"time-series-analysis.html","id":"seasonal-and-trend-decomposition-using-loess.","chapter":"25 Time Series Analysis","heading":"25.4.5 Seasonal and Trend decomposition using Loess32.","text":"STL (Seasonal Trend decomposition using Loess) robust flexible method decomposing time series trend, seasonal, residual components. uses local regression (Loess) estimate trend seasonal components. STL algorithm follows:Seasonal Component: seasonal component estimated using Loess, fits smooth curve seasonal patterns.Trend Component: trend component estimated removing estimated seasonal component original time series.Residual Component: residual component obtained subtracting estimated trend seasonal components original time series.decompose time series R, using STL, use function stl shown . plot STL decomposed AirPassengers shown figure 25.11.\nFigure 25.11: STL Decomposition\n","code":"\nstl(AirPassengers, s.window = \"periodic\") %>% \n  plot()"},{"path":"time-series-analysis.html","id":"model-based-techniques","chapter":"25 Time Series Analysis","heading":"Model-based techniques","text":"Model-based techniques involve fitting specific mathematical statistical model observed time series data. techniques assume particular structure data estimate model parameters based . Model-based techniques typically require assumptions can provide detailed understanding underlying dynamics. Examples model-based techniques include:Autoregressive Integrated Moving Average (ARIMA): ARIMA models capture linear dependencies lagged observations differences time series. commonly used modeling stationary time series.Autoregressive Integrated Moving Average (ARIMA): ARIMA models capture linear dependencies lagged observations differences time series. commonly used modeling stationary time series.Seasonal ARIMA (SARIMA): SARIMA models extend ARIMA framework incorporate seasonality data. suitable time series exhibiting trend seasonality.Seasonal ARIMA (SARIMA): SARIMA models extend ARIMA framework incorporate seasonality data. suitable time series exhibiting trend seasonality.future version book, discuss methods.","code":""},{"path":"time-series-analysis.html","id":"plotting-different-elements-of-time-series","chapter":"25 Time Series Analysis","heading":"25.5 Plotting different elements of time series","text":"R, can use functions -ggseasonplot(): create seasonal plotggsubseriesplot(): create mini plots season show seasonal meansgglagplot(): Plot time series lags itselfggAcf(): Plot autocorrelation function (ACF)Example-1:\nFigure 25.12: Seasonalilty\nExample-2: Lag Plot AirPassengers may seen Figure 25.13. figure notice plot lag=12 suggest series seasonality 12 periods.\nFigure 25.13: Seasonalilty Lagplots\n","code":"\ntheme_set(theme(axis.text.x = element_text(angle = 90, \n                                           vjust = 0.5, \n                                           hjust=1)))\nlibrary(patchwork)\ng1 <- ggseasonplot(AirPassengers) +\n  theme_bw()\ng2 <- ggsubseriesplot(AirPassengers) +\n  ggtitle('Sub-Series Plot') +\n  theme_bw()\n(g1 / g2)\ngglagplot(AirPassengers) +\n  ggtitle('Lag Plots') +\n  theme_bw() +\n  theme(legend.position = \"bottom\") +\n  labs(color = NULL) +\n  guides(color = guide_legend(nrow = 1))"},{"path":"part-vi-network-analytics.html","id":"part-vi-network-analytics","chapter":"Part VI: Network Analytics","heading":"Part VI: Network Analytics","text":"","code":""},{"path":"network-analyticsgraph-theory-in-r.html","id":"network-analyticsgraph-theory-in-r","chapter":"26 Network Analytics/Graph theory in R","heading":"26 Network Analytics/Graph theory in R","text":"content development/Finalisation.Network analysis emerged pivotal tool realm audit analytics, offering auditors powerful method uncover intricate relationships patterns within vast datasets. Leveraging rich ecosystem packages R, auditors can construct, visualize, analyze networks ease, enabling detect anomalies, identify key players, assess robustness interconnected systems. detecting fraud optimizing supply chains, application network analysis auditing enhances risk assessment also empowers auditors make data-driven decisions greater precision confidence. let’s dive .","code":""},{"path":"network-analyticsgraph-theory-in-r.html","id":"an-introduction-to-graph-theory","chapter":"26 Network Analytics/Graph theory in R","heading":"26.1 An introduction to Graph theory","text":"Network analysis applies concepts graph theory, branch mathematics, analyze interpret complex systems, social networks, transportation networks, biological networks, uncover patterns, structures, dynamics within systems. Graph theory focuses properties characteristics graphs, paths, cycles, connectivity, graph coloring.","code":""},{"path":"network-analyticsgraph-theory-in-r.html","id":"definition-of-a-graph","chapter":"26 Network Analytics/Graph theory in R","heading":"26.1.1 Definition of a graph","text":"\nFigure 26.1: Example Graph\n","code":""},{"path":"network-analyticsgraph-theory-in-r.html","id":"types-of-graph","chapter":"26 Network Analytics/Graph theory in R","heading":"Types of graph","text":"Example graph shown figure 26.1 basically Undirected Graph, edges nodes aren’t directed. E.g. connection two computers. hand, edge(s) nodes following direction specific order, graph known Directed Graph. Example twitter (now X) user may follow users without followed turn, shown figure 26.2.\nFigure 26.2: Example Directed Graph\nMoreover, sometimes edge may connected . case, edge called self-edge graphs (allowing vertices join ) called Pseudographs. real-world example pseudographs think transactions different firms. firm usually transact firms sometimes may transact within account maintained separately specific purpose. Example figure 26.3 (left) node E connected .sometimes may one edge connecting pair nodes (order). edges called multi-edges; e.g. figure 26.3 (right) two edges connecting B E. real-world example can think multiple flights operating pair airports different carriers.\nFigure 26.3: Examples Self Edge Multi Edge\nComplete Graphs graphs pairs vertices connected edge. Example complete graphs shown figure 26.4. real-world instances complete graphs may rare, maybe possible nodes/vertices bigger graph show complete mutual relationship. words, graph vertices taken sub-graph complete, complete subgraph called clique. example think subgraph consisting nodes B, D, E G graph shown 26.1 clique.\nFigure 26.4: Examples Complete Graphs\nmoving , let us learn two specific types graph. One known Bipartite Graphs. actually graphs two mutually disjoint set vertices condition pair within set connected. E.g. graph figure 26.5 (Left) bipartite graph. can think two departments edges represent correspondence officials two departments. Actually idea can extended form k-partite graphs k disjoint sets.Another category Trees, representing hierarchical relationships among entities/individuals. tree, vertices connected edges links, making type graph. graph qualify tree, must precisely one path pair vertices considered undirected. bipartite graph illustrated example figure 26.5 (left) can also interpreted tree meets criterion. Refer figure 26.5 (Right) wherein graph redrawn tree.\nFigure 26.5: Bipartite Tree Graph example\n","code":""},{"path":"network-analyticsgraph-theory-in-r.html","id":"vertex-and-edge-attributes","chapter":"26 Network Analytics/Graph theory in R","heading":"26.1.2 Vertex and Edge attributes","text":"Recall Graph basically consists two sets \\((V, E)\\) wherein \\(V\\) set vertices \\(E\\) set edges. simplest form, sets can conceived lists, can enriched incorporating additional attributes. E.g. graph consists airports flights operating , set \\(V\\) representing airports include attributes , () names, (ii) types whether domestic international, (iii) geographical coordinates, (iv) State/Country located, etc. . Similarly, set edges \\(E\\) encompass additional details like () count flights operating two airports, (ii) airlines servicing flights, (iii) aerial distance airports, . example may seen figure 26.6.\nFigure 26.6: Examples Vertex Edge properties\n","code":""},{"path":"network-analyticsgraph-theory-in-r.html","id":"practical-approach-for-creating-graphs-in-r","chapter":"26 Network Analytics/Graph theory in R","heading":"26.2 Practical approach for creating graphs in R","text":"","code":""},{"path":"network-analyticsgraph-theory-in-r.html","id":"packages-to-use","chapter":"26 Network Analytics/Graph theory in R","heading":"26.2.1 Packages to use","text":"One best packages creating analysing graphs R igraph. Package igraph originally developed Gábor Csárdi Tamás Nepusz, written C programming language order achieve good performance.\nLet’s load .Apart igraph complete package network analysis, also using visNetwork ggraph packages. Former used create interactive network charts latter used create ggplot2 compatible plots can customised familiar environment.Let’s now proceed learn creating graph objects R. Actually graph objects can created many ways, learn three methods. methods serve purpose time.","code":"library(igraph)"},{"path":"network-analyticsgraph-theory-in-r.html","id":"creating-a-graph-from-data-frame","chapter":"26 Network Analytics/Graph theory in R","heading":"26.2.2 Creating a graph from data frame","text":"working data analysis, data frames often primary tool. Consequently, common practical approach creating graphs igraph involves utilizing data.frame objects. Essentially, data.frame objects use generate graphs consist least two columns, row represents edge within intended graph. first column interpreted '' node, second column considered '' node, regardless respective column names. achieve , employ graph_from_data_frame() function igraph library. syntax follows:Example-1: Let’s construct graph object data frame containing four edges., observe graph object named gr1 successfully generated printed console, providing us pertinent information .first line always begins IGRAPH, followed seven characters, represent initial characters unique graph ID. Interested users can employ graph_id() function retrieve full ID needed.Subsequently, four-letter character string displayed. example, two UN, followed two blanks --. characters signify following:\nfirst letter distinguishes directed (D) undirected (U) graphs.\nsecond letter, N, denotes named graphs, .e., graphs name vertex attribute set.\nthird letter, W, indicates weighted graphs, .e., graphs weight edge attribute set.\nfourth letter, B, signifies bipartite graphs, .e., graphs type vertex attribute set.\nfirst letter distinguishes directed (D) undirected (U) graphs.second letter, N, denotes named graphs, .e., graphs name vertex attribute set.third letter, W, indicates weighted graphs, .e., graphs weight edge attribute set.fourth letter, B, signifies bipartite graphs, .e., graphs type vertex attribute set.Following count vertices edges, separated two dashes.Starting second line, graph’s attributes listed, separated commas. attribute’s type – graph (g), vertex (v), edge (e) – data type – character (c), numeric (n), logical (l), (x) – specified.last line edges printed.Readers may notice four edges df imported gr1 graph displayed accordingly.Now directed argument default TRUE default graph created directed also confirmed first alphabet D four character string. let’s check undirected graph created printed.good. First letter now U representing undirected graph. Readers may also notice another change printing edges now printed without arrow mark. created another graph object just show directed argument used. However, existing directed graph can converted undirected graph using function .directed(). Check-Now last argument vertices = function graph_from_data_frame? cases, d argument data edges may sufficient, yet sometimes graph may contain isolates (isolated nodes connected edge). include vertices, may use vertices argument. Additionally include vertex property graph created, may use additional columns data frame. Similar analogy, additional columns edges dataset (first two columns) used set edge properties.Example-2: Let’s add isolated edge \"6\" graph.Notice change number edges now. Also notice edges printed different names due presence column named name vertices data frame.Readers may try add additional columns edge dataset/data frame create respective graph objects.","code":"graph_from_data_frame(\n  d =         ,  # data.frame\n  directed = TRUE,\n  vertices = NULL # optional data.frame of vertices\n)\n# Example data frame of edges\ndf <- data.frame(\n  from = c(1, 1, 3, 3),\n  to = c(2, 3, 5, 4)\n)\n# Creating graph object\ngr1 <- graph_from_data_frame(df)\n# Let's print it\ngr1## IGRAPH 8eefd91 DN-- 5 4 -- \n## + attr: name (v/c)\n## + edges from 8eefd91 (vertex names):\n## [1] 1->2 1->3 3->5 3->4\n# Creating graph object\ngr2 <- graph_from_data_frame(df, directed = FALSE)\n# Let's print it\ngr2## IGRAPH 8ef36f7 UN-- 5 4 -- \n## + attr: name (v/c)\n## + edges from 8ef36f7 (vertex names):\n## [1] 1--2 1--3 3--5 3--4\nas.undirected(gr1)## IGRAPH 8ef69c7 UN-- 5 4 -- \n## + attr: name (v/c)\n## + edges from 8ef69c7 (vertex names):\n## [1] 1--3 1--2 3--5 3--4\ndf_v <- data.frame(\n  id = c(1, 2, 3, 4, 5, 6),\n  name = c(\"Ram\", \"Shyam\", \"Alex\", \"Bob\", \"Charlie\", \"Kumar\")\n)\n# Creating graph object\ngr3 <- graph_from_data_frame(df, directed = FALSE, vertices = df_v)\n# Let's print it\ngr3## IGRAPH 8efb016 UN-- 6 4 -- \n## + attr: name (v/c)\n## + edges from 8efb016 (vertex names):\n## [1] Ram --Shyam   Ram --Alex    Alex--Charlie Alex--Bob"},{"path":"network-analyticsgraph-theory-in-r.html","id":"creating-a-graph-from-edge-list","chapter":"26 Network Analytics/Graph theory in R","heading":"26.2.3 Creating a graph from Edge list","text":"Creating graphs edge-lists nearly approach creating graphs data frames. difference absence vertices argument . syntax isHere el two column matrix, character numeric, representing edges; thus ’s another difference .Fine enough. successfully created undirected graph 5 nodes 4 edges.","code":"graph_from_edgelist(el, directed = TRUE)\nedges <- data.frame(\n  origin = c(\"Ram\", \"Ram\", \"Alex\", \"Alex\"), \n  dest = c(\"Shyam\", \"Alex\", \"Charlie\", \"Bob\")\n)\nedges <- as.matrix(edges)\n# Now let's create a graph\ngr4 <- graph_from_edgelist(edges, directed = FALSE)\n# And print it\ngr4## IGRAPH 8efeb9e UN-- 5 4 -- \n## + attr: name (v/c)\n## + edges from 8efeb9e (vertex names):\n## [1] Ram --Shyam   Ram --Alex    Alex--Charlie Alex--Bob"},{"path":"network-analyticsgraph-theory-in-r.html","id":"creating-a-graph-from-adjacency-matrix","chapter":"26 Network Analytics/Graph theory in R","heading":"26.2.4 Creating a graph from adjacency matrix","text":"exactly adjacency matrix? ’s square matrix sized \\(n \\times n\\), rows columns indexed \\(n\\) vertices. \\((, j)\\)-th entry matrix holds significance:graph devoid edge attribute weight, value 1 signifies existence edge vertex \\(v_{}\\) \\(v_j\\). Conversely, value 0 indicates absence edge two vertices.However, graphs equipped edge attributes like weight, corresponding numerical value represents weight edge vertex \\(v_{}\\) \\(v_j\\).Example: Let’s try create graph shown 26.6 corresponding adjacency matrix. First, without weights.far good. Directed graph (default) created edges airports separately. Now let us try create weighted graph data weights now distance airports.time notice edge property named weight numeric type added graph.","code":"\n# Let's create the matrix\nadj_mat <- structure(c(0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0), dim = c(4L, \n4L), dimnames = list(c(\"DEL\", \"BNG\", \"BOM\", \"PNQ\"), c(\"DEL\", \n\"BNG\", \"BOM\", \"PNQ\")))\nadj_mat##     DEL BNG BOM PNQ\n## DEL   0   1   1   1\n## BNG   1   0   1   0\n## BOM   1   1   0   1\n## PNQ   1   0   1   0\n# Let's create the graph\ngr5 <- graph_from_adjacency_matrix(adj_mat)\n# Let's print it\ngr5## IGRAPH 8f024d1 DN-- 4 10 -- \n## + attr: name (v/c)\n## + edges from 8f024d1 (vertex names):\n##  [1] DEL->BNG DEL->BOM DEL->PNQ BNG->DEL BNG->BOM BOM->DEL BOM->BNG BOM->PNQ\n##  [9] PNQ->DEL PNQ->BOM\n# Let's create the matrix\nadj_mat2 <- structure(c(0, 1750, 1150, 1200, 1750, 0, 850, 0, 1150, 850, \n0, 120, 1200, 0, 120, 0), dim = c(4L, 4L), dimnames = list(c(\"DEL\", \n\"BNG\", \"BOM\", \"PNQ\"), c(\"DEL\", \"BNG\", \"BOM\", \"PNQ\")))\nadj_mat2##      DEL  BNG  BOM  PNQ\n## DEL    0 1750 1150 1200\n## BNG 1750    0  850    0\n## BOM 1150  850    0  120\n## PNQ 1200    0  120    0\n# Let's create the graph\ngr6 <- graph_from_adjacency_matrix(adj_mat2, weighted = TRUE)\n# Let's print it\ngr6## IGRAPH 8f06e35 DNW- 4 10 -- \n## + attr: name (v/c), weight (e/n)\n## + edges from 8f06e35 (vertex names):\n##  [1] DEL->BNG DEL->BOM DEL->PNQ BNG->DEL BNG->BOM BOM->DEL BOM->BNG BOM->PNQ\n##  [9] PNQ->DEL PNQ->BOM"},{"path":"network-analyticsgraph-theory-in-r.html","id":"getting-data-back-from-graph-objects","chapter":"26 Network Analytics/Graph theory in R","heading":"26.2.5 Getting data back from graph objects","text":"graph objects created getting data back data.frame adjacency matrix pretty easy. can use either igraph functions as_data_frame() as_adjacency_matrix(). Let check functions.as_data_frame() : Since package tibble’s earlier versions also named function, safe use igraph::as_data_frame() avoid conflict bug code. takes igraph object outputs data.frame. example let’s get data back gr6 object created code. Since gr6 created adjacency matrix amusing see function working correctly.output , clear function worked correctly. Since graph directed edges created sides. let’s also check function’s output case undirected graph.Absolutely fine. notice weights edges combined converting directed graph undirected graph. also can tackled using .undirected() tweaking argument edge.attr.comb basically takes function combine separate edge attributes. ,place mention function additional argument , case requirement export \"vertex\" /\"edges\" data. argument set \"\" two data-sets returned list.as_adjacency_matrix() : Similar function, function take igraph object returns adjacency matrix instead. let’s check also.may notice adjacency matrix returned without weights. map weights may use attr argument default NULL.","code":"\nigraph::as_data_frame(gr6)##    from  to weight\n## 1   DEL BNG   1750\n## 2   DEL BOM   1150\n## 3   DEL PNQ   1200\n## 4   BNG DEL   1750\n## 5   BNG BOM    850\n## 6   BOM DEL   1150\n## 7   BOM BNG    850\n## 8   BOM PNQ    120\n## 9   PNQ DEL   1200\n## 10  PNQ BOM    120\nigraph::as_data_frame(as.undirected(gr6))##   from  to weight\n## 1  DEL BNG   3500\n## 2  DEL BOM   2300\n## 3  BNG BOM   1700\n## 4  DEL PNQ   2400\n## 5  BOM PNQ    240\nas.undirected(gr6, edge.attr.comb = list(weight = mean)) %>%\n  igraph::as_data_frame()##   from  to weight\n## 1  DEL BNG   1750\n## 2  DEL BOM   1150\n## 3  BNG BOM    850\n## 4  DEL PNQ   1200\n## 5  BOM PNQ    120\n# Vertices data only\nigraph::as_data_frame(gr6, what = \"vertices\")##     name\n## DEL  DEL\n## BNG  BNG\n## BOM  BOM\n## PNQ  PNQ\nas.undirected(gr6, edge.attr.comb = list(weight = mean)) %>%\n  igraph::as_adjacency_matrix()## 4 x 4 sparse Matrix of class \"dgCMatrix\"\n##     DEL BNG BOM PNQ\n## DEL   .   1   1   1\n## BNG   1   .   1   .\n## BOM   1   1   .   1\n## PNQ   1   .   1   .\nas.undirected(gr6, edge.attr.comb = list(weight = mean)) %>%\n  igraph::as_adjacency_matrix(attr = \"weight\")## 4 x 4 sparse Matrix of class \"dgCMatrix\"\n##      DEL  BNG  BOM  PNQ\n## DEL    . 1750 1150 1200\n## BNG 1750    .  850    .\n## BOM 1150  850    .  120\n## PNQ 1200    .  120    ."},{"path":"network-analyticsgraph-theory-in-r.html","id":"adding-vertex-andor-edge-attributes-to-an-existing-igraph-object","chapter":"26 Network Analytics/Graph theory in R","heading":"26.3 Adding vertex and/or edge attributes to an existing igraph object","text":"’ve previously explored vertex edge properties can incorporated igraph creation. However, may instances need arises add vertex /edge properties igraph object creation existing igraph object. delving , let’s first understand can retrieve existing edges, nodes, attributes existing igraph.Getting vertices igraph using V()Getting edges igraph using E()Extracting edge vertex attribute using $Adding vertex edge property similarly using $Let’s also add edge property .","code":"\nV(gr6)## + 4/4 vertices, named, from 8f06e35:\n## [1] DEL BNG BOM PNQ\nE(gr6)## + 10/10 edges from 8f06e35 (vertex names):\n##  [1] DEL->BNG DEL->BOM DEL->PNQ BNG->DEL BNG->BOM BOM->DEL BOM->BNG BOM->PNQ\n##  [9] PNQ->DEL PNQ->BOM\n# Vertex Property \"name\"\nV(gr6)$name## [1] \"DEL\" \"BNG\" \"BOM\" \"PNQ\"\n# Edge property \"weight\"\nE(gr6)$weight##  [1] 1750 1150 1200 1750  850 1150  850  120 1200  120\n# adding vertex property say \"airport_name\"\nV(gr6)$airport_name <- c(\"New Delhi\", \"Bengaluru\", \"Mumbai\", \"Pune\")\n# Single value will be replicated across all values\nV(gr6)$country <- \"India\"\n\n## Let's check it\nigraph::as_data_frame(gr6, what = \"vertices\")##     name airport_name country\n## DEL  DEL    New Delhi   India\n## BNG  BNG    Bengaluru   India\n## BOM  BOM       Mumbai   India\n## PNQ  PNQ         Pune   India\n# Adding edge property \"carrier\"\nE(gr6)$carrier <- c(rep(\"Vistara\",6), rep(\"Indigo\", 2), rep(\"Air India\", 2))\n# Adding another edge property on the basis of condition\nE(gr6)$route <- case_when(\n  E(gr6)$weight <= 400 ~ \"Short\",\n  E(gr6)$weight <= 1000 ~ \"Medium\",\n  TRUE ~ \"Long\"\n)\n\n## Let's check it\nigraph::as_data_frame(gr6)##    from  to weight   carrier  route\n## 1   DEL BNG   1750   Vistara   Long\n## 2   DEL BOM   1150   Vistara   Long\n## 3   DEL PNQ   1200   Vistara   Long\n## 4   BNG DEL   1750   Vistara   Long\n## 5   BNG BOM    850   Vistara Medium\n## 6   BOM DEL   1150   Vistara   Long\n## 7   BOM BNG    850    Indigo Medium\n## 8   BOM PNQ    120    Indigo  Short\n## 9   PNQ DEL   1200 Air India   Long\n## 10  PNQ BOM    120 Air India  Short"},{"path":"network-analyticsgraph-theory-in-r.html","id":"visualising-graphs","chapter":"26 Network Analytics/Graph theory in R","heading":"26.4 Visualising graphs","text":"covering graph definition storage methods, let’s explore techniques visualizing . Visualization crucial conveying essence graphs networks, arrangement style playing significant roles communication. Apart factors related visual appeal, layout becomes crucial. relative positioning vertices greatly impacts visualization effectiveness. evident comparison two graphs Figure 26.7, representing graph depicted earlier Figure 26.2.\nFigure 26.7: Two layouts graph\nNowadays, many packages available R plot visualise graph objects analysing network data. , learn two , () one plotting igraph though plots created static; (ii) visNetwork used create interactive visualizations. Visualizing geographical networks discussed separately another Chapter.","code":""},{"path":"network-analyticsgraph-theory-in-r.html","id":"plotting-using-plot-in-igraph","chapter":"26 Network Analytics/Graph theory in R","heading":"26.5 Plotting using plot() in igraph","text":"Plotting igraph objects R simple. Just use plot command basically uses plot.igraph method, plot igraph object using default values arguments. already seen layouts important plotting graph objects, can set layout using argument layout plot.following example (Figure 26.8), can see two random layouts graph seen Figure 26.7.\nFigure 26.8: Two layouts graph\nexample, seen layout may generate different coordinates plotting iteration. However, using specific random number seed, can fix random layout graph purpose reproducibility.moving let’s create famous network/graph Zachary karate club learn plot network graph effectively. description available Wikipedia network, reproducing . social network karate club studied Wayne W. Zachary period three years 1970 1972.[2] network captures 34 members karate club, documenting links pairs members interacted outside club. study conflict arose administrator “John ” instructor “Mr. Hi” (pseudonyms), led split club two. Half members formed new club around Mr. Hi; members part found new instructor gave karate. Based collected data Zachary correctly assigned one member club groups actually joined split.graph object available package igraphdata can load .","code":"\nmy_gr <- igraph::as_data_frame(kite) %>% \n  slice(1:9) %>% \n  graph_from_data_frame()\n\nset.seed(12345)\nplot(my_gr, vertex.size = 25, layout = layout_randomly)\n\nset.seed(54321)\nplot(my_gr, vertex.size = 25, layout = layout_randomly)\nlibrary(igraph)\nlibrary(igraphdata)\ndata(\"karate\")\nkarate## IGRAPH 4b458a1 UNW- 34 78 -- Zachary's karate club network\n## + attr: name (g/c), Citation (g/c), Author (g/c), Faction (v/n), name\n## | (v/c), label (v/c), color (v/n), weight (e/n)\n## + edges from 4b458a1 (vertex names):\n##  [1] Mr Hi  --Actor 2  Mr Hi  --Actor 3  Mr Hi  --Actor 4  Mr Hi  --Actor 5 \n##  [5] Mr Hi  --Actor 6  Mr Hi  --Actor 7  Mr Hi  --Actor 8  Mr Hi  --Actor 9 \n##  [9] Mr Hi  --Actor 11 Mr Hi  --Actor 12 Mr Hi  --Actor 13 Mr Hi  --Actor 14\n## [13] Mr Hi  --Actor 18 Mr Hi  --Actor 20 Mr Hi  --Actor 22 Mr Hi  --Actor 32\n## [17] Actor 2--Actor 3  Actor 2--Actor 4  Actor 2--Actor 8  Actor 2--Actor 14\n## [21] Actor 2--Actor 18 Actor 2--Actor 20 Actor 2--Actor 22 Actor 2--Actor 31\n## [25] Actor 3--Actor 4  Actor 3--Actor 8  Actor 3--Actor 9  Actor 3--Actor 10\n## + ... omitted several edges\nkarate_data <- igraph::as_data_frame(karate)\nhead(karate_data)##    from      to weight\n## 1 Mr Hi Actor 2      4\n## 2 Mr Hi Actor 3      5\n## 3 Mr Hi Actor 4      3\n## 4 Mr Hi Actor 5      3\n## 5 Mr Hi Actor 6      3\n## 6 Mr Hi Actor 7      3\nplot(karate, layout = layout_nicely)"},{"path":"network-analyticsgraph-theory-in-r.html","id":"layouts","chapter":"26 Network Analytics/Graph theory in R","heading":"26.5.1 Layouts","text":"Network layouts algorithms return coordinates node network. igraph library offers several built-layouts. Learning algorithm behind layouts outside scope chapter.layout_as_bipartite() Minimize edge-crossings simple two-row (column) layout bipartite graphs.layout_as_star() simple layout generator, places one vertex center circle rest vertices equidistantly perimeter.layout_as_tree() Reingold-Tilford graph layout algorithm tree-like layout, perfect trees, acceptable graphs many cycles.layout_in_circle() Places vertices circle, order vertex ids.layout_nicely() function tries choose appropriate graph layout algorithm graph, automatically, based simple algorithm.layout_on_grid() places vertices rectangular grid, two three dimensionslayout_on_sphere() Places vertices sphere, approximately uniformly, order vertex ids.layout_randomly() function uniformly randomly places vertices graph two three dimensions.layout_with_dh() Places vertices graph plane, according simulated annealing algorithm Davidson Harel.layout_with_fr() Places vertices plane using force-directed layout algorithm Fruchterman Reingold.layout_with_gem() Places vertices plane using GEM force-directed layout algorithm.layout_with_kk() Kamada-Kawai layout algorithm places vertices plane, 3D space, based physical model springs.layout_with_sugiyama() Sugiyama layout algorithm layered directed acyclic graphs. algorithm minimized edge crossings.Discussing every layout scope. Readers advised play different layouts get fair understanding layouts. However, may see useful layouts example karate data.Circular layout: Figure 26.9 Left.Fruchterman Reingold: Figure 26.9 Right.Kamada Kawai: Figure 26.10 Left.Sugiyama: Figure 26.10 Right.Tree layouts (two representations): Figure 26.11.\nFigure 26.9: Two layouts Zachary Karate Club Network\n\nFigure 26.10: Two layouts Zachary Karate Club Network\n\nFigure 26.11: Two Tree layouts Zachary Karate Club Network\n","code":"\nigraph_options(vertex.size = 18)\npar(mfrow = c(1, 2))\n\nplot(karate, layout = layout_in_circle)\ntitle(\"Circular layout\")\n\nplot(karate, layout = layout_with_fr)\ntitle(\"Fruchterman - Reingold\")\npar(mfrow = c(1, 2))\n\nplot(karate, layout = layout_with_kk)\ntitle(\"Kamada Kawai\")\n\nplot(karate, layout = layout_with_sugiyama)\ntitle(\"Sugiyama\")\npar(mfrow = c(1, 2))\n\nplot(karate, layout = layout_as_tree)\ntitle(\"Default Tree layout\")\n\nplot(karate, layout = layout_as_tree(karate, circular = TRUE))\ntitle(\"circular Tree\")"},{"path":"network-analyticsgraph-theory-in-r.html","id":"displaying-vertexedge-properties","chapter":"26 Network Analytics/Graph theory in R","heading":"26.5.2 Displaying Vertex/Edge properties","text":"layouts may important displaying networks, additional information displaying vertex edge properties categories, etc. may also play important role. Edge/Vertex properties can continuous /discrete visualising properties network shall usual properties like color, size, width, shape, etc. karate graph object already colored, readers may wondering vertices colored representation.Actually vertices, igraph object can colored using property attribute color. Let’s retrieve understand.may see discrete integer values stored therein. Let’s reallocate specific colors want.\nFigure 26.12: Vertex coloring Network\nSimilarly, can use following attributes display certain properties vertices/edges igraph plot.Vertex Properties\nsize - Size vertex. Default 15\ncolor - Fill color vertex\nframe.color - Border color vertex\nshape - shape vertex. Can allocate one following c(\"circle\", \"square\", \"rectangle\", \"none\").\nlabel - character vector used label nodes\nlabel.family Font family label. Default serif\nlabel.font Font label. 1 means plain (default), 2: bold, 3 italic, 4 bold italic 5 symbol\nlabel.cex font size label\nsize - Size vertex. Default 15color - Fill color vertexframe.color - Border color vertexshape - shape vertex. Can allocate one following c(\"circle\", \"square\", \"rectangle\", \"none\").label - character vector used label nodeslabel.family Font family label. Default seriflabel.font Font label. 1 means plain (default), 2: bold, 3 italic, 4 bold italic 5 symbollabel.cex font size labelEdge properties\ncolor color edge\nwidth edge width\nlty line type edges (0 \"blank\"; 1 \"solid\"; 2 \"dashed\"; 3 \"dotted\"; 4 \"dotdash\", 6 \"twodash\")\nlabel label edges (label.family, lable.font label.cex similarly font family, font type font size respectively)\ncurved: Edge curvature; range 0-1\narrow.size Arrow size (default 1) (directed graphs)\narrow.width arrow width, default 1.\narrow.mode: arrow mode (0 means arrow; 1 back, 2 forward arrow, 3 )\ncolor color edgewidth edge widthlty line type edges (0 \"blank\"; 1 \"solid\"; 2 \"dashed\"; 3 \"dotted\"; 4 \"dotdash\", 6 \"twodash\")label label edges (label.family, lable.font label.cex similarly font family, font type font size respectively)curved: Edge curvature; range 0-1arrow.size Arrow size (default 1) (directed graphs)arrow.width arrow width, default 1.arrow.mode: arrow mode (0 means arrow; 1 back, 2 forward arrow, 3 )Apart adding properties igraph object can set properties arguments plot function, just adding edge. vertex. property/attribute name.Let’s see following example.\nFigure 26.13: Edge Vertex properties Network\n","code":"\nV(karate)$color##  [1] 1 1 1 1 1 1 1 1 2 2 1 1 1 1 2 2 1 1 2 1 2 1 2 2 2 2 2 2 2 2 2 2 2 2\nV(karate)$color <- c(\"red\", \"dodgerblue\")[V(karate)$color]\nhead(V(karate)$color)## [1] \"red\" \"red\" \"red\" \"red\" \"red\" \"red\"\nplot(karate, layout = layout_nicely)\n# set reproducible layout\nset.seed(123)\nl_s <- layout_nicely(karate)\n\n# Add vertex labels\nV(karate)$label <- stringr::str_remove(V(karate)$name, \"Actor \")\n\n# Change shapes of two prominent actors\nV(karate)$shape <- ifelse(V(karate)$name %in% c(\"Mr Hi\", \"John A\"), \"rectangle\", \"circle\")\n\n# Change Edge width as per \"weight\"\nE(karate)$width <- E(karate)$weight\n\nplot(karate, layout = l_s, edge.label.cex = 0.7)"},{"path":"network-analyticsgraph-theory-in-r.html","id":"using-visnetwork-for-interactive-visualizations","chapter":"26 Network Analytics/Graph theory in R","heading":"26.6 Using visNetwork for interactive visualizations","text":"","code":""},{"path":"network-analyticsgraph-theory-in-r.html","id":"subgraphs-adding-or-deleting-edgesvertices","chapter":"26 Network Analytics/Graph theory in R","heading":"26.7 Subgraphs, adding or deleting edges/vertices","text":"","code":""},{"path":"network-analyticsgraph-theory-in-r.html","id":"describing-networkgraph-for-edges-and-nodes-therein","chapter":"26 Network Analytics/Graph theory in R","heading":"26.8 Describing network/graph for edges and nodes therein","text":"","code":""},{"path":"applying-network-analysis-in-auditfraud-detection.html","id":"applying-network-analysis-in-auditfraud-detection","chapter":"27 Applying network analysis in audit/fraud detection","heading":"27 Applying network analysis in audit/fraud detection","text":"","code":""},{"path":"applying-network-analysis-in-auditfraud-detection.html","id":"dup_net","chapter":"27 Applying network analysis in audit/fraud detection","heading":"27.1 Use case-1: Finding network of related entities/persons - Identity Theft","text":"Following Content Development.Imagine scenario users may multiple IDs mobile numbers, email ids, say ID issued Government Department say Income Tax Department (e.g. PAN number Indian Scenario). Using techniques mentioned section 42, may easily find duplicate users, .e. duplicates basis one ID. Sometimes need arise find network duplicate users changed one two IDs retained another. E.g. may social sector scheme beneficiary expected registered getting scheme benefits. Scheme audit(s) may require auditors check duplicate beneficiaries using multiple IDs.Understand table 27.1.Table 27.1: Dummy DataIt may seen ten persons, two IDs 6 10 respectively share none IDs Email, PAN Telephone number. see closely, ID-6 shares mobile number ID-1 turn share PAN number ID-2. ID-2 shares Email Mobile number ID-6 thus establishing relation network ID-6 ID-10. clear figure 27.1. Note considering names finding duplicates.\nFigure 27.1: Network diagram connected entities\nmay find duplicates using branch mathematics called Graph Theory.33 won’t discussing core concepts graph theory . packages work graph theory concepts R, using igraph34 analysis . Let’s load library.Now complete algorithm -may see got unique ID users based three IDs. Let us understand algorithm used step step.Step-1: First ensure ID columns (Store names columns one vector say id_cols) must type. Since mix character (Alphanumeric) numeric IDs, using dplyr::across dplyr::mutate can convert three ID columns character type. Readers may refer section 1.4.1 type change, section 14.14 changing data type multiple columns simultaneously using dplyr::across.Thus, first two lines code correspond step .Step-2: Pivot id columns longer format Ids linked one main ID. Now two things kept mind. One main_Id column data frame. create one using dplyr::row_number() pivoting. Secondly, NAs IDs removed pivoting. Use argument values_drop_na = TRUE inside tidyr::pivot_longer. Thus, step correspond line-- first argument data invisibly passed dplyr pipe .e. %>%. Upto step, data frame look like -Step-3: Now need two columns, one mainID another value created pivoting ID columns. use select(MainID, value) .Step-4: Thereafter create graph object data (output step-3), using igraph package. Interested readers may see graph object look like, using plot() function. output shown figure 27.2. However, step entirely optional may also kept mind graph output large data highly cluttered may comprehensible .\nFigure 27.2: Plot graph object\nStep-5: step combination three lines codes number ID based connectivity components graph objects. Actually components give us object $membership give us unique_ids component graph.Next purrr::pluck, $membership object, return named vector.can stack named vector data frame using stack set_namesI suggest purposefully name second column output data MainID can joined original data frame last step. UNIQUE_ID data give us new column allocate ID possible duplicates network three IDs.Step-6: last step join data frame back original data frame. Since type MainID now factor type, can convert type column original data frame right_join . Hence final step, right_join(dat %>% mutate(MainID = .factor(MainID)), = c('MainID')).","code":"\nknitr::include_graphics(\"images/canvas.png\")\nlibrary(igraph)\ndat <- data.frame(\n  MainID = 1:9,\n  Name = c(\"A\", \"B\", \"C\", \"B\", \"E\", \"A\", \"F\", \"G\", \"H\"),\n  ID1 = c(11,12,13,13,14,15,16,17,17),\n  ID2 = c(\"1a\", \"1b\",\"1b\", \"2a\", \"2b\", \"2c\", \"2c\", \"2e\", \"3a\"),\n  ID3 = c(\"AB\", \"AB\", \"BC\", \"CD\", \"EF\", \"GH\", \"HI\", \"HI\", \"JK\")\n)\n# A preview of our sample data\ndat##   MainID Name ID1 ID2 ID3\n## 1      1    A  11  1a  AB\n## 2      2    B  12  1b  AB\n## 3      3    C  13  1b  BC\n## 4      4    B  13  2a  CD\n## 5      5    E  14  2b  EF\n## 6      6    A  15  2c  GH\n## 7      7    F  16  2c  HI\n## 8      8    G  17  2e  HI\n## 9      9    H  17  3a  JK\nid_cols <- c(\"ID1\", \"ID2\", \"ID3\")\ndat %>% \n  mutate(across(.cols = all_of(id_cols), as.character)) %>% \n  pivot_longer(cols = all_of(id_cols), \n               values_drop_na = TRUE) %>% \n  select(MainID, value) %>% \n  graph_from_data_frame() %>%\n  components() %>%\n  pluck(membership) %>%\n  stack() %>%\n  set_names(c('UNIQUE_ID', 'MainID')) %>%\n  right_join(dat %>% \n               mutate(MainID = as.factor(MainID)), \n             by = c('MainID'))##   UNIQUE_ID MainID Name ID1 ID2 ID3\n## 1         1      1    A  11  1a  AB\n## 2         1      2    B  12  1b  AB\n## 3         1      3    C  13  1b  BC\n## 4         1      4    B  13  2a  CD\n## 5         2      5    E  14  2b  EF\n## 6         3      6    A  15  2c  GH\n## 7         3      7    F  16  2c  HI\n## 8         3      8    G  17  2e  HI\n## 9         3      9    H  17  3a  JKid_cols <- c(\"ID1\", \"ID2\", \"ID3\")\ndat %>%\n  mutate(across(.cols = id_cols, as.character))pivot_longer(cols = all_of(id_cols), values_drop_na = TRUE)## # A tibble: 27 × 4\n##    MainID Name  name  value\n##     <int> <chr> <chr> <chr>\n##  1      1 A     ID1   11   \n##  2      1 A     ID2   1a   \n##  3      1 A     ID3   AB   \n##  4      2 B     ID1   12   \n##  5      2 B     ID2   1b   \n##  6      2 B     ID3   AB   \n##  7      3 C     ID1   13   \n##  8      3 C     ID2   1b   \n##  9      3 C     ID3   BC   \n## 10      4 B     ID1   13   \n## # ℹ 17 more rows\ndat %>% \n  mutate(across(.cols = all_of(id_cols), as.character)) %>% \n  pivot_longer(cols = all_of(id_cols), \n               values_drop_na = TRUE) %>% \n  select(MainID, value) %>% \n  graph_from_data_frame() %>%\n  plot()## $membership\n##  1  2  3  4  5  6  7  8  9 11 1a AB 12 1b 13 BC 2a CD 14 2b EF 15 2c GH 16 HI \n##  1  1  1  1  2  3  3  3  3  1  1  1  1  1  1  1  1  1  2  2  2  3  3  3  3  3 \n## 17 2e 3a JK \n##  3  3  3  3 \n## \n## $csize\n## [1] 13  4 13\n## \n## $no\n## [1] 3##  1  2  3  4  5  6  7  8  9 11 1a AB 12 1b 13 BC 2a CD 14 2b EF 15 2c GH 16 HI \n##  1  1  1  1  2  3  3  3  3  1  1  1  1  1  1  1  1  1  2  2  2  3  3  3  3  3 \n## 17 2e 3a JK \n##  3  3  3  3##    UNIQUE_ID MainID\n## 1          1      1\n## 2          1      2\n## 3          1      3\n## 4          1      4\n## 5          2      5\n## 6          3      6\n## 7          3      7\n## 8          3      8\n## 9          3      9\n## 10         1     11\n## 11         1     1a\n## 12         1     AB\n## 13         1     12\n## 14         1     1b\n## 15         1     13\n## 16         1     BC\n## 17         1     2a\n## 18         1     CD\n## 19         2     14\n## 20         2     2b\n## 21         2     EF\n## 22         3     15\n## 23         3     2c\n## 24         3     GH\n## 25         3     16\n## 26         3     HI\n## 27         3     17\n## 28         3     2e\n## 29         3     3a\n## 30         3     JK"},{"path":"applying-network-analysis-in-auditfraud-detection.html","id":"use-case-2-classification-using-graph-theory","chapter":"27 Applying network analysis in audit/fraud detection","heading":"27.2 Use case-2: Classification using graph theory","text":"Content Development.","code":""},{"path":"applying-network-analysis-in-auditfraud-detection.html","id":"use-case-3-finding-circular-transactions","chapter":"27 Applying network analysis in audit/fraud detection","heading":"27.3 Use case-3: Finding circular transactions","text":"Content Development.","code":""},{"path":"part-vii-text-analytics-in-r.html","id":"part-vii-text-analytics-in-r","chapter":"Part-VII: Text Analytics in R","heading":"Part-VII: Text Analytics in R","text":"","code":""},{"path":"string-manipulation-in-stringr.html","id":"string-manipulation-in-stringr","chapter":"28 String manipulation in stringr","heading":"28 String manipulation in stringr","text":"earlier sections covered essential tools data mining help us reading data, data cleaning, reshaping data per requirements, deriving insights getting inferences . However, analyzing text bit different usually text data unstructured. data science projects, often find data-sets text form strings. strings often important information, can get effectively working analyzing . String manipulation techniques essential preparing data, creating features, text mining, tasks natural language processing (NLP).Chapter related functions, saw functions base R string manipulation. However, stringr part tidyverse plethora functions designed make working strings easy possible. learn chapter.First , let’s load . Readers may note use special function namely str_view() used print underlying representation string see pattern matches. actual code code may rarely used.Let us also create example strings.","code":"\nlibrary(stringr)\nline1 <- \"I'm gonna make him an offer he can't refuse.\"\nline2 <- \"Carpe diem.\\nSeize the day, boys.\"\nline3 <- \"You've got to ask yourself one question: \\\"Do I feel lucky?\\\"\""},{"path":"string-manipulation-in-stringr.html","id":"printing-strings-the-way-we-want.","chapter":"28 String manipulation in stringr","heading":"28.1 Printing strings the way we want.","text":"Let us try printing stringsNot pretty! earlier chapter learnt function cat helps us printing strings way want .e. avoiding escape characters unwanted things. let’s use .Prettier! Still ’s problem. Actually, cat() accepts sep argument lines/strings separated. let’s use .Base R another function writeLines() also designed print strings way usually want, cat() general purpose designed concatenating objects printing .reference, let’s also discuss bit str_view stringr designed view strings matching, see next sub-sections.","code":"\nex_lines <- c(line1, line2, line3)\nprint(ex_lines)## [1] \"I'm gonna make him an offer he can't refuse.\"                 \n## [2] \"Carpe diem.\\nSeize the day, boys.\"                            \n## [3] \"You've got to ask yourself one question: \\\"Do I feel lucky?\\\"\"\ncat(line1, line2, line3)## I'm gonna make him an offer he can't refuse. Carpe diem.\n## Seize the day, boys. You've got to ask yourself one question: \"Do I feel lucky?\"\ncat(line1, line2, line3, sep = \"\\n\")## I'm gonna make him an offer he can't refuse.\n## Carpe diem.\n## Seize the day, boys.\n## You've got to ask yourself one question: \"Do I feel lucky?\"\nwriteLines(ex_lines)## I'm gonna make him an offer he can't refuse.\n## Carpe diem.\n## Seize the day, boys.\n## You've got to ask yourself one question: \"Do I feel lucky?\"\n# Let's also print some Unicode and special characters.\nwriteLines(\"\\u0928\\u092e\\u0938\\u094d\\u0924\\u0947 \n         \\u0926\\u0941\\u0928\\u093f\\u092f\\u093e\")## नमस्ते \n##          दुनिया\nwriteLines(\"He owes me \\U20b9 15 lakh.\")## He owes me ₹ 15 lakh.\nstr_view(ex_lines)## [1] │ I'm gonna make him an offer he can't refuse.\n## [2] │ Carpe diem.\n##     │ Seize the day, boys.\n## [3] │ You've got to ask yourself one question: \"Do I feel lucky?\""},{"path":"string-manipulation-in-stringr.html","id":"unicode","chapter":"28 String manipulation in stringr","heading":"28.2 Unicode","text":"Unicode R, precedes \\U. examples emoticons.","code":"\nwriteLines(\"\\U1f600\")## 😀\nwriteLines(\"\\U1f634\")## 😴"},{"path":"string-manipulation-in-stringr.html","id":"cleaning-whitespaces","chapter":"28 String manipulation in stringr","heading":"28.3 Cleaning whitespaces","text":"may often encounter text strings extra whitespaces either end strings may make comparision two strings difficult. ExampleWe may also encounter extra whitespaces two different words ideally separated single white-space. deal situations remove extra white-spaces programatically, stringr provides us two functions -str_trim(string, side = c(\"\", \"left\", \"right\")) remove whitespaces start end string respectively (using side argument default).str_squish(string) remove internal whitespaces single white-space.Examples-","code":"\n\"anil goyal\" == \"anil goyal \"## [1] FALSE\nstr_squish(\"anil     goyal \")## [1] \"anil goyal\"\nstr_trim(\"anil    goyal  \")## [1] \"anil    goyal\""},{"path":"string-manipulation-in-stringr.html","id":"string-concatenation-with-str_c","chapter":"28 String manipulation in stringr","heading":"28.4 String concatenation with str_c()","text":"already seen two functions paste paste0 base R earlier chapter. However stringr package function str_c (c short concatenation) similar purposes. couple differences.default sep \"\" opposed \" \" paste() absence sep argument paste0() altogether.Function paste() turns missing values string “NA”, whereas str_c() propagates missing values. means combining strings missing value result another missing value.also ensures returning length output given vectors making especially useful working dplyr::mutate. However, want flatten given vector strings using separator, use collapse argument paste paste0. Stringr function str_flatten() designed specifically purpose, making useful working dplyr::summarise. , extra argument last extremely useful flattening last piece vector.special variant str_flatten_comma() wherein “comma” default collapse argument. type bit lesser case.context, may also discuss one function str_glue provides us powerful elegant syntax interpolating strings {}. See following example.","code":"\ncompany <- c(\"Microsoft\", \"Salesforce\", NA)\nproduct <- c(\"Excel\", \"Tableau\", \"R\")\npaste(company, product)## [1] \"Microsoft Excel\"    \"Salesforce Tableau\" \"NA R\"\nstr_c(company, product, sep = \" \")## [1] \"Microsoft Excel\"    \"Salesforce Tableau\" NA\nfruits <- c(\"apple\", \"banana\", \"pineapple\")\n\nstr_flatten(fruits, collapse = \", \")## [1] \"apple, banana, pineapple\"\nstr_flatten(fruits, collapse = \", \", last = \" and \")## [1] \"apple, banana and pineapple\"\nstr_flatten_comma(fruits)## [1] \"apple, banana, pineapple\"\n# Note that output will be of same length as given variable/string vector.\nstr_glue(\"I like {fruits}\")## I like apple\n## I like banana\n## I like pineapple\nmy_fruits <- str_flatten_comma(fruits, last = \" and \")\nstr_glue(\"I like {my_fruits} in fruits.\")## I like apple, banana and pineapple in fruits."},{"path":"string-manipulation-in-stringr.html","id":"string-length-with-str_length","chapter":"28 String manipulation in stringr","heading":"28.5 String length with str_length()","text":"counting number characters string use nchar() base R. However, str_length() designed similar purpose.However, designed handle factors better sense nchar().","code":"\nstr_length(ex_lines)## [1] 44 32 59\n# nchar(unique(iris$Species))\n# Returns an error\n\n# This will work\nstr_length(unique(iris$Species))## [1]  6 10  9"},{"path":"string-manipulation-in-stringr.html","id":"string-extraction-with-str_sub","chapter":"28 String manipulation in stringr","heading":"28.6 String extraction with str_sub()","text":"Function str_sub() extracts parts strings based location. takes three arguments, first argument, string, vector strings. arguments start end specify boundaries piece extract characters.wondering works similarly substr worthwhile mention unlike substr base R, can accept negative position integers wherein counting done backwards.won’t fail string falls short given positions.","code":"\n# Extracting first two characters\nstr_sub(fruits, 1, 2)## [1] \"ap\" \"ba\" \"pi\"\n## Note the difference\nsubstr(fruits, -2, -1)## [1] \"\" \"\" \"\"\nstr_sub(fruits, -2, -1)## [1] \"le\" \"na\" \"le\"\nstr_sub(fruits, 5, 6)## [1] \"e\"  \"na\" \"ap\"\nstr_sub(fruits, -6, -5)## [1] \"a\"  \"ba\" \"ea\""},{"path":"string-manipulation-in-stringr.html","id":"string-matching-based-on-regex-with-str_detect-str_subset-and-str_count","chapter":"28 String manipulation in stringr","heading":"28.7 String matching based on regex with str_detect(), str_subset() and str_count()","text":"Let’s search \"apple\" three fruits strings.three functions stringr job.str_detect() works like grepl returns logical vector.str_subset() works like grep value = TRUE argument.str_count() return count matches element given string.","code":"\nstr_view(fruits, \"apple\", match = NA)## [1] │ <apple>\n## [2] │ banana\n## [3] │ pine<apple>\nstr_detect(fruits, \"apple\")## [1]  TRUE FALSE  TRUE\nstr_subset(fruits, \"apple\")## [1] \"apple\"     \"pineapple\"\nstr_count(fruits, \"apple\")## [1] 1 0 1\n# Let's count character \"a\" in each of `fruits`\nstr_count(fruits, \"a\")## [1] 1 3 1"},{"path":"string-manipulation-in-stringr.html","id":"changing-case-in-stringr","chapter":"28 String manipulation in stringr","heading":"28.8 Changing case in stringr","text":"four functions stringr make life easier changing case given strings.str_to_lower() converts string lower case.str_to_upper() converts string UPPER CASE.str_to_title() make given string Title Case, wherein first alphabet characters upper case.str_to_sentence() convert sentence case, first letter sentence capitalized.Examples.","code":"\n# lower case\nstr_view(str_to_lower(ex_lines))## [1] │ i'm gonna make him an offer he can't refuse.\n## [2] │ carpe diem.\n##     │ seize the day, boys.\n## [3] │ you've got to ask yourself one question: \"do i feel lucky?\"\n# UPPER CASE\nstr_view(str_to_upper(ex_lines))## [1] │ I'M GONNA MAKE HIM AN OFFER HE CAN'T REFUSE.\n## [2] │ CARPE DIEM.\n##     │ SEIZE THE DAY, BOYS.\n## [3] │ YOU'VE GOT TO ASK YOURSELF ONE QUESTION: \"DO I FEEL LUCKY?\"\n# Title Case\nstr_view(str_to_title(ex_lines))## [1] │ I'm Gonna Make Him An Offer He Can't Refuse.\n## [2] │ Carpe Diem.\n##     │ Seize The Day, Boys.\n## [3] │ You've Got To Ask Yourself One Question: \"Do I Feel Lucky?\"\n# Sentence case\nstr_view(str_to_sentence(ex_lines))## [1] │ I'm gonna make him an offer he can't refuse.\n## [2] │ Carpe diem.\n##     │ Seize the day, boys.\n## [3] │ You've got to ask yourself one question: \"do i feel lucky?\""},{"path":"string-manipulation-in-stringr.html","id":"controlling-matching-behaviour-with-modifier-functions-in-stringr","chapter":"28 String manipulation in stringr","heading":"28.9 Controlling matching behaviour with modifier functions in stringr","text":"Usually ans specifically working English language text, may require two type modifier functions detecting/extracting matches.One fixed(), compares literal bytes. extra argument ignore_case can used ignore/ignore cases matching/extracting pattern string vectors.Second regex several arguments apart ignore_case.See examples.one control function boundary() matches boundary strings. argument type accepts one values c(\"character\", \"line_break\", \"sentence\", \"word\").","code":"\nex_str <- \"This is an example string.\"\nstr_view(ex_str, \"t\")## [1] │ This is an example s<t>ring.\nstr_view(ex_str, fixed(\".\"))## [1] │ This is an example string<.>\nstr_view(ex_str, regex(\".\"))## [1] │ <T><h><i><s>< ><i><s>< ><a><n>< ><e><x><a><m><p><l><e>< ><s><t><r><i><n><g><.>\nstr_view(ex_str, regex(\"\\\\b.{2}\\\\b\"))## [1] │ This <is> <an> example string.\nstr_view(ex_str, boundary(\"word\"))## [1] │ <This> <is> <an> <example> <string>.\nstr_view(ex_lines, boundary(\"sentence\"))## [1] │ <I'm gonna make him an offer he can't refuse.>\n## [2] │ <Carpe diem.\n##     │ ><Seize the day, boys.>\n## [3] │ <You've got to ask yourself one question: \"Do I feel lucky?\">"},{"path":"string-manipulation-in-stringr.html","id":"extracting-text-from-strings","chapter":"28 String manipulation in stringr","heading":"28.10 Extracting text from strings","text":"parts, learnt function str_subset() returns strings matching text/pattern found. cases want specific matching text/patterns returned. cases, stringr str_extract str_extract_all() powerhouse. clear following example, wherein extract PAN numbers given text string(s).function return first match found. variant str_extract_all() return matches, expected, list.latter function additional argument simplify output form matrix, TRUE., find many PAN numbers stored text_2 .","code":"\nex_text <- c(\"My PAN number is TEMPZ9999Z.\",\n             \"He has mentioned TEMP9999Z as his PAN number, incorrectly.\",\n             \"Is your PAN ABCTY1234D?\")\n# Let's define simple regex for PAN\npan <- \"[A-Z]{5}[0-9]{4}[A-Z]\"\n# str_subset will return strings which contain PAN numbers\nstr_subset(ex_text, pattern = regex(pan))## [1] \"My PAN number is TEMPZ9999Z.\" \"Is your PAN ABCTY1234D?\"\n# str_extract will however, extract those.\nstr_extract(ex_text, pattern = regex(pan))## [1] \"TEMPZ9999Z\" NA           \"ABCTY1234D\"\ntext_2 <- str_flatten(ex_text, collapse = \"\\n\")\nstr_extract(text_2, regex(pan))## [1] \"TEMPZ9999Z\"\nstr_extract_all(text_2, regex(pan))## [[1]]\n## [1] \"TEMPZ9999Z\" \"ABCTY1234D\"\nstr_extract_all(text_2, regex(pan), simplify = TRUE)##      [,1]         [,2]        \n## [1,] \"TEMPZ9999Z\" \"ABCTY1234D\"\nstr_count(text_2, regex(pan))## [1] 2"},{"path":"string-manipulation-in-stringr.html","id":"splitting-strings","chapter":"28 String manipulation in stringr","heading":"28.11 Splitting strings","text":"kitty, stringr another powerful function str_split() used split strings meaningful fragments using pattern. output format, expected list.Example-argument n used specify maximum pieces return. Default Inf. Extra results however flattened.function three variants. First str_split_fixed() splits string character vector fixed number pieces, returning character matrix. Example -Another variant str_split_1() takes single string splits pieces, returning single character vector.Last one str_split_i() splits string character vector pieces extracts ith value, returning character vector.","code":"\nstr_split(ex_text, boundary(\"word\"))## [[1]]\n## [1] \"My\"         \"PAN\"        \"number\"     \"is\"         \"TEMPZ9999Z\"\n## \n## [[2]]\n## [1] \"He\"          \"has\"         \"mentioned\"   \"TEMP9999Z\"   \"as\"         \n## [6] \"his\"         \"PAN\"         \"number\"      \"incorrectly\"\n## \n## [[3]]\n## [1] \"Is\"         \"your\"       \"PAN\"        \"ABCTY1234D\"\n# Extract first two words from each string.\nstr_split(ex_text, boundary(\"word\"), n = 2)## [[1]]\n## [1] \"My\"                        \"PAN number is TEMPZ9999Z.\"\n## \n## [[2]]\n## [1] \"He\"                                                     \n## [2] \"has mentioned TEMP9999Z as his PAN number, incorrectly.\"\n## \n## [[3]]\n## [1] \"Is\"                   \"your PAN ABCTY1234D?\"\n# Here value of `n` is required\nstr_split_fixed(ex_text, boundary(\"word\"), n = 3)##      [,1] [,2]   [,3]                                                 \n## [1,] \"My\" \"PAN\"  \"number is TEMPZ9999Z.\"                              \n## [2,] \"He\" \"has\"  \"mentioned TEMP9999Z as his PAN number, incorrectly.\"\n## [3,] \"Is\" \"your\" \"PAN ABCTY1234D?\"\n# Note that vector with one element should be passed.\nstr_split_1(ex_text[1], boundary(\"word\"))## [1] \"My\"         \"PAN\"        \"number\"     \"is\"         \"TEMPZ9999Z\"\nstr_split_i(ex_text, boundary(\"word\"), i = 1)## [1] \"My\" \"He\" \"Is\""},{"path":"string-manipulation-in-stringr.html","id":"replacing-values-with-str_replace-str_replace_all","chapter":"28 String manipulation in stringr","heading":"28.12 Replacing values with str_replace(), str_replace_all()","text":"matched text strings/values required replaced values, can use str_replace() /str_replace_all().expected functions require additional argument replacement.replacement multiple matches, vectors length pattern replacement can provided. may understood following example.Alternatively, named vector (c(pattern1 = replacement1, ...)), may supplied pattern argument, order perform multiple replacements element string effectively.Note: named vector, names need quoted.Back-references: References form ⁠\\1⁠,⁠\\2⁠, etc replaced contents respective matched group (created ⁠(..)replacement argument functions, may also supply function, called match (right left) return value used replace match.Another example.","code":"\n# Example Task: mask all PAN numbers from `text_2`\n# Let's view the string\nstr_view(text_2)## [1] │ My PAN number is TEMPZ9999Z.\n##     │ He has mentioned TEMP9999Z as his PAN number, incorrectly.\n##     │ Is your PAN ABCTY1234D?\n# Replace first match only\nstr_replace(text_2, regex(pan), replacement = \"XXXXX0000X\") %>% \n  str_view()## [1] │ My PAN number is XXXXX0000X.\n##     │ He has mentioned TEMP9999Z as his PAN number, incorrectly.\n##     │ Is your PAN ABCTY1234D?\n# Replace all matches\nstr_replace_all(text_2, regex(pan), replacement = \"XXXXX0000X\") %>% \n  str_view()## [1] │ My PAN number is XXXXX0000X.\n##     │ He has mentioned TEMP9999Z as his PAN number, incorrectly.\n##     │ Is your PAN XXXXX0000X?\n# Create a new string vector\nfruits <- c(\"one apple\",\n            \"two bananas\",\n            \"three pineapples\")\n# See what's there in `fruits`\nstr_view(fruits)## [1] │ one apple\n## [2] │ two bananas\n## [3] │ three pineapples\n# Let's replace each number word to numeral\nstr_replace_all(\n  fruits,\n  pattern = c(\"one\", \"two\", \"three\"),\n  replacement = c(\"1\", \"2\", \"3\")\n)## [1] \"1 apple\"      \"2 bananas\"    \"3 pineapples\"\nstr_replace_all(\n  fruits,\n  pattern = c(one = \"1\", two = \"2\", three = \"3\")\n)## [1] \"1 apple\"      \"2 bananas\"    \"3 pineapples\"\n# If any consonant is repeated, make it single\nstr_replace_all(fruits, \n                pattern = regex(\"([^aeiou])\\\\1\", ignore_case = TRUE),\n                replacement = \"\\\\1\")## [1] \"one aple\"        \"two bananas\"     \"three pineaples\"\n# Change case of all PAN numbers which are in lower case.\ntext_3 <- str_to_lower(text_2)\n# Let's view the string\nstr_view(text_3)## [1] │ my pan number is tempz9999z.\n##     │ he has mentioned temp9999z as his pan number, incorrectly.\n##     │ is your pan abcty1234d?\n# Change case of all lower case PAN numbers\nstr_replace_all(text_3, \n                pattern = regex(pan, ignore_case = TRUE), \n                replacement = str_to_upper) %>% \n  str_view()## [1] │ my pan number is TEMPZ9999Z.\n##     │ he has mentioned temp9999z as his pan number, incorrectly.\n##     │ is your pan ABCTY1234D?"},{"path":"string-manipulation-in-stringr.html","id":"removing-textpattern-using-str_remove-and-str_remove_all","chapter":"28 String manipulation in stringr","heading":"28.13 Removing text/pattern using str_remove and str_remove_all","text":"Removing text pattern strings similar replacing matches empty text \"\". See example removing numbers(digits) valid PAN number, , given text.","code":"\nstr_remove_all(ex_text,\n               pattern = regex(\"(?<=[A-Z]{5})(\\\\d{4})(?=[A-Z])\", ignore_case = TRUE)) %>% \n  str_view()## [1] │ My PAN number is TEMPZZ.\n## [2] │ He has mentioned TEMP9999Z as his PAN number, incorrectly.\n## [3] │ Is your PAN ABCTYD?"},{"path":"string-manipulation-in-stringr.html","id":"formatting-numbers-with-format-and-formatc","chapter":"28 String manipulation in stringr","heading":"28.14 Formatting numbers with format and formatC","text":"Sometimes numbers may required format special types like preceding currency symbol, thousand separator scientific format fixed format (vice versa). case format function base R comes handy. scientific argument format() controls whether numbers displayed fixed (scientific = FALSE) scientific (scientific = TRUE) format. representation scientific, digits argument number digits exponent. Whereas, representation fixed, digits controls significant digits used smallest (magnitude) number.number formatted match number decimal places smallest number. means number decimal places get output depends values formatting.Explanation : smallest number 0.00123 controlling number decimals numbers. Significant digit number 1 require three decimal places.may also note output nicely aligned decimal. stop behavior may set trim = TRUE .function formatC() provides alternative way format numbers based C style syntax.Rather scientific argument, formatC() format argument takes code representing required format. useful :\"f\" fixed format. case, digits number digits decimal point. predictable format(), number places decimal fixed regardless values formatted.\"e\" scientific. , digits argument behaves like format(); specifies number significant digits.\"g\" fixed unless scientific saves space.Function formatC() also formats numbers individually, means always get output regardless numbers vector.Lastly one package scales also pretty job formatting numbers.scales::percent(): forces decimal display numbers (.e. don’t use scientific notation)scales::comma() : inserts comma every three digit.scales::dollar : Used format numbers currency symbol.","code":"\n# Some example numbers\nnumbers <- c(0.00123, 123, 1.2356)\n# Scientific (default)\nformat(numbers, digits = 1) %>% \n  writeLines()## 1e-03\n## 1e+02\n## 1e+00\n# Fixed format\nformat(numbers, digits = 1, scientific = FALSE) %>% \n  writeLines()##   0.001\n## 123.000\n##   1.236\nformat(numbers, \n       digits = 1, \n       scientific = FALSE,\n       trim = TRUE) %>% \n  writeLines()## 0.001\n## 123.000\n## 1.236\nformatC(numbers,\n        format = \"f\",\n        digits = 2) %>% \n  writeLines()## 0.00\n## 123.00\n## 1.24\nformatC(numbers,\n        format = \"g\",\n        digits = 2) %>% \n  writeLines()## 0.0012\n## 1.2e+02\n## 1.2\nlibrary(scales)\n# In per cent up to two digits after decimal\npercent(c(0.001, 0.1234, 0.002), accuracy = 0.01) %>% \n  writeLines()## 0.10%\n## 12.34%\n## 0.20%\n# With thousand separator\ncomma(numbers*1000) %>% \n  writeLines()## 1\n## 123,000\n## 1,236\n# With rupee symbol\nset.seed(123)\nrunif(3, 1000, 90000) %>% \n  dollar(prefix = \"\\U20b9\") %>% \n  writeLines()## ₹26,594.40\n## ₹71,159.16\n## ₹37,398.95"},{"path":"string-manipulation-in-stringr.html","id":"padding-strings","chapter":"28 String manipulation in stringr","heading":"28.15 Padding strings","text":"dealt removing extra white-spaces strings using str_trim. Sometimes requirements contrary .e. add white-space character left right sides string (vector usually) length can made uniform. may use str_pad() scenarios. syntax -Example -","code":"str_pad(\n  string, \n  width,\n  side = c(\"left\", \"right\", \"both\"),\n  pad = \" \",\n  use_width = TRUE\n)\nstr_view(\n  c(str_pad(\"anil\", 30, \"left\"),\n  str_pad(\"anil\", 30, \"right\"),\n  str_pad(\"anil\", 30, \"both\"))\n)## [1] │                           anil\n## [2] │ anil                          \n## [3] │              anil"},{"path":"string-manipulation-in-stringr.html","id":"sorting-strings","chapter":"28 String manipulation in stringr","heading":"28.16 Sorting strings","text":"sort strings, three powerful functions kitty stringr.str_sort() returns sorted vector.str_order() returns integer vector returns desired order used sub-setting, .e. x[str_order(x)] str_sort()str_rank() returns ranks values, .e. arrange(df, str_rank(x)) str_sort(df$x)Besides sorting us, functions argument numeric set TRUE sort digits numerically, instead strings. following example clarify purpose.","code":"\nstr_view(fruits)## [1] │ one apple\n## [2] │ two bananas\n## [3] │ three pineapples\n# Let's sort these alphabetically\nstr_sort(fruits)## [1] \"one apple\"        \"three pineapples\" \"two bananas\"\n# Let's find the alphabetic order\nstr_order(fruits)## [1] 1 3 2\n## Example-2\nex_text <- c(\"₹100\", \"₹200\", \"₹1000\", \"₹500\", \"₹5000\", \"₹10000\")\n\n# default sorting\nstr_sort(ex_text)## [1] \"₹100\"   \"₹1000\"  \"₹10000\" \"₹200\"   \"₹500\"   \"₹5000\"\n# Order\nstr_order(ex_text)## [1] 1 3 6 2 4 5\n# Rank\nstr_rank(ex_text)## [1] 1 4 2 5 6 3\n# sorting based on numbers\nstr_sort(ex_text, numeric = TRUE)## [1] \"₹100\"   \"₹200\"   \"₹500\"   \"₹1000\"  \"₹5000\"  \"₹10000\""},{"path":"regex---a-quick-introduction.html","id":"regex---a-quick-introduction","chapter":"29 Regex - A quick introduction","heading":"29 Regex - A quick introduction","text":"Regular Expression, regex short, powerful tool, helps us writing code pattern matching texts. Regex, pattern describes set strings. sequence characters define search pattern. used search manipulate text. Regex can used many programming languages, including R.Regex patterns made combination regular characters special characters. Regular characters include letters, digits, punctuation marks. Special characters specific meaning regex used represent patterns characters.Regex patterns can used variety purposes, including:Searching specific strings textExtracting specific parts stringReplacing parts string textValidating input usersIn R, can use grep gsub functions search manipulate text using regex.","code":""},{"path":"regex---a-quick-introduction.html","id":"basic-regex---literal-characters","chapter":"29 Regex - A quick introduction","heading":"29.1 Basic Regex - Literal Characters","text":"Every literal character, regex matches . Thus, matches third character text Charles. literal characters case sensitive.Example-1","code":"\nex_text <- \"This is an example text\"\n# Match literal `x`\nstr_view(ex_text, \"x\")## [1] │ This is an e<x>ample te<x>t\n# Match Upper case literal \"X\"\nstr_view(ex_text, \"X\", match = NA)## [1] │ This is an example text"},{"path":"regex---a-quick-introduction.html","id":"case-sensitivity","chapter":"29 Regex - A quick introduction","heading":"29.1.1 Case sensitivity","text":"literals case_sensitive sometimes aware exact case, match case insensitive literals, can make use stringr function regex case, wherein argument ignore_case (note snake case) . Actually, behind scenes, regex expressions stringr wrapped function argument defaults FALSE. Thus, code example actually equivalent following-Thus, match case insensitive literals (regex expressions) may make use argument ignore_case like -","code":"\n# Match literal `x`\nstr_view(ex_text, regex(\"x\"))## [1] │ This is an e<x>ample te<x>t\n# Match Upper case literal \"X\"\nstr_view(ex_text, regex(\"X\"), match = NA)## [1] │ This is an example text\n# Match literal `x`\nstr_view(ex_text, regex(\"X\", ignore_case = TRUE))## [1] │ This is an e<x>ample te<x>t"},{"path":"regex---a-quick-introduction.html","id":"metacharacters","chapter":"29 Regex - A quick introduction","heading":"29.2 Metacharacters","text":"","code":""},{"path":"regex---a-quick-introduction.html","id":"character-sets","chapter":"29 Regex - A quick introduction","heading":"29.2.1 Character sets","text":"always feasible put every literal characters. may also match literal characters given set options. match group characters put square brackets. , [abc] matches either , b, c.Example-match range characters/numbers can separate hyphen square brackets. , [-n] match character range [abcdefghijklmn].Example-Example-2We can also use pre-built character classes listed .[:punct:] punctuation.[:alpha:] letters.[:lower:] lowercase letters.[:upper:] uppercase letters.[:digit:] digits.[:xdigit:] hex digits.[:alnum:] letters numbers.[:cntrl:] control characters.[:graph:] letters, numbers, punctuation.[:print:] letters, numbers, punctuation, white-space.[:space:] space characters (basically equivalent \\\\s).[:blank:] space tab.Example-","code":"\nex_vec <- c(\"Apple\", \"Orange\", \"Myrrh\")\n# matches a vowel\nstr_view(ex_vec, \"[aeiou]\")## [1] │ Appl<e>\n## [2] │ Or<a>ng<e>\n# matches a vowel irrespective of case\nstr_view(ex_vec, regex(\"[aeiou]\", ignore_case = TRUE))## [1] │ <A>ppl<e>\n## [2] │ <O>r<a>ng<e>\nex_text <- \"The quick brown fox jumps over the lazy dog\"\n# Match a, b or c in lower case\nstr_view(ex_text, regex(\"[a-c]\"))## [1] │ The qui<c>k <b>rown fox jumps over the l<a>zy dog\nex_colors <- c(\"grey\", \"black\", \"gray\")\nstr_view(ex_colors, \"gr[ae]y\")## [1] │ <grey>\n## [3] │ <gray>\nex_vec2 <- c(\"One apple\", \"2 Oranges\")\nstr_view(ex_vec2, \"[:digit:]\", match = NA)## [1] │ One apple\n## [2] │ <2> Oranges"},{"path":"regex---a-quick-introduction.html","id":"non-printable-characters-meta-characters-short-hand-character-classes","chapter":"29 Regex - A quick introduction","heading":"29.2.2 Non-printable characters/ Meta characters (short-hand character classes)","text":"can use special character sequences put non-printable characters regular expression(s). E.g. \\t matches tab character. since \\ escape character R, need escape . match tab character put \\\\t regex sequence. Regex matches new line (line feed) \\\\n. Regex meta characters listed -\\\\s matches white-space character. Moreover, complement \\\\S matches character except white-space.\\\\w matches alphanumeric character. Similarly, complement \\\\W matches character except alphanumeric characters.\\\\d matches digit. Similarly, complement \\\\D matches character except digits.\\\\b matches word boundary. Thus, \\\\B matches character except word boundary.. matches character. match literal dot . escape ; thus \\\\. matches dot character.See examples-","code":"\nex_vec3 <- c(\"One apple\", \"2 oranges & 3 bananas.\")\n# match word character\nstr_view(ex_vec3, \"\\\\w\", match = NA)## [1] │ <O><n><e> <a><p><p><l><e>\n## [2] │ <2> <o><r><a><n><g><e><s> & <3> <b><a><n><a><n><a><s>.\n# match any character followed by a dot character\nstr_view(ex_vec3, \".\\\\.\", match = NA)## [1] │ One apple\n## [2] │ 2 oranges & 3 banana<s.>\n# Note both character and dot will be matched"},{"path":"regex---a-quick-introduction.html","id":"quantifiers","chapter":"29 Regex - A quick introduction","heading":"29.3 Quantifiers","text":"want match one literal/character regex? Let’s say want check whether given string string vector contain two consecutive vowels. One method may use character classes two times .e. using [aeiou][aeiou]. method principles DRY35 one common principle programming. solve issues, quantifiers.+ 1 occurrences* 0 ? 0 1{} specified numbers\n{n} exactly n\n{n,} n \n{n,m} n m\n{n} exactly n{n,} n {n,m} n mThus, may match two consecutive vowels using [aeiou]{2}. See example","code":"\nex_vec <- c(\"Apple\", \"Banana\", \"pineapple\")\nstr_view(ex_vec, \"[aeiou]{2}\", match = NA)## [1] │ Apple\n## [2] │ Banana\n## [3] │ pin<ea>pple"},{"path":"regex---a-quick-introduction.html","id":"alternation","chapter":"29 Regex - A quick introduction","heading":"29.4 Alternation","text":"Alternation regular expressions allows match one pattern another, depending one appears first input string. pipe symbol | used separate alternative patterns.","code":""},{"path":"regex---a-quick-introduction.html","id":"basic-alternation","chapter":"29 Regex - A quick introduction","heading":"29.4.0.0.1 Basic Alternation","text":"Let’s start basic example illustrate alternation works:","code":"\nstring <- \"I have an apple and a banana\"\npattern <- \"apple|banana\"\n\nstr_extract(string, pattern)## [1] \"apple\""},{"path":"regex---a-quick-introduction.html","id":"order-of-precedence","chapter":"29 Regex - A quick introduction","heading":"29.4.0.0.2 Order of Precedence","text":"using alternation, ’s important keep mind order precedence rules. general, first pattern matches input string selected, subsequent patterns considered. ’s example illustrate :example, string string contains words “apple” “pineapple”. want extract first occurrence either “apple” “pineapple” text using regular expression pattern utilizes alternation. pattern apple|pineapple means “match ‘apple’ ‘pineapple’”. However, since input string contains “pineapple” “apple”, str_extract() function selects first matching string “pineapple”.","code":"\nstring <- \"I have a pineapple and an apple\"\nstr_extract(string, pattern = \"apple|pineapple\")## [1] \"pineapple\""},{"path":"regex---a-quick-introduction.html","id":"grouping-alternatives","chapter":"29 Regex - A quick introduction","heading":"29.4.0.0.3 Grouping Alternatives","text":"can also use parentheses group alternative patterns together. can useful specifying complex patterns. Example:examples, used stringr::regex() modify regex flag ignore cases matching.","code":"\nstring <- \"Apple and pineapples are good for health\"\npattern <- \"(apple|banana|cherry) (and|or) (pineapple|kiwi|mango)\"\n\nstr_view(string, regex(pattern, ignore_case = TRUE))## [1] │ <Apple and pineapple>s are good for health"},{"path":"regex---a-quick-introduction.html","id":"anchors","chapter":"29 Regex - A quick introduction","heading":"29.5 Anchors","text":"Anchors regular expressions allow match patterns specific positions within input string. R, can use various anchors regular expressions match beginning, end, specific positions within input text.","code":""},{"path":"regex---a-quick-introduction.html","id":"beginning-and-end-anchors","chapter":"29 Regex - A quick introduction","heading":"29.5.1 Beginning and End Anchors","text":"beginning anchor ^ end anchor $ used match patterns beginning end input string, respectively. ExampleIn example, matching word beginning sentence .","code":"\nstring <- \"The quick brown fox jumps over the lazy dog. The fox is brown.\"\npattern <- \"^the\"\nstr_view(string, regex(pattern, ignore_case = TRUE))## [1] │ <The> quick brown fox jumps over the lazy dog. The fox is brown."},{"path":"regex---a-quick-introduction.html","id":"word-boundary-anchors","chapter":"29 Regex - A quick introduction","heading":"29.5.2 Word Boundary Anchors","text":"word boundary anchor \\\\b used match patterns beginning end word within input string. ExampleIn example, though apple string contained another word pineapple limiting search whole words .","code":"\nstring <- 'Apple and pineapple, both are good for health'\npattern <- '\\\\bapple\\\\b'\nstr_view(string, regex(pattern, ignore_case = TRUE))## [1] │ <Apple> and pineapple, both are good for health"},{"path":"regex---a-quick-introduction.html","id":"capture-groups","chapter":"29 Regex - A quick introduction","heading":"29.6 Capture Groups","text":"capture group way group part regular expression capture separate sub-string. can useful want extract replace specific part string. R, capture groups denoted parentheses (). Anything inside parentheses captured can referenced later regular expression replacement string.One use capturing group refer back within match back reference: \\1 refers match contained first parenthesis, \\2 second parenthesis, .Example-1Example-2Another way use capturing group , want replace pattern something else. better understand following example-","code":"\nmy_fruits <- c('apple', 'banana', 'coconut', 'berry', 'cucumber', 'date')\n# search for repeated alphabet\npattern <- '(.)\\\\1'\nstr_view(my_fruits, regex(pattern), match = NA)## [1] │ a<pp>le\n## [2] │ banana\n## [3] │ coconut\n## [4] │ be<rr>y\n## [5] │ cucumber\n## [6] │ date\n# search for repeated pair of alphabets\npattern <- '(..)\\\\1'\nstr_view(my_fruits, regex(pattern), match = NA)## [1] │ apple\n## [2] │ b<anan>a\n## [3] │ <coco>nut\n## [4] │ berry\n## [5] │ <cucu>mber\n## [6] │ date\n# We have names in last_name, first_name format\nnames <- c('Hanks, Tom', 'Affleck, Ben', 'Damon, Matt')\nstr_view(names)## [1] │ Hanks, Tom\n## [2] │ Affleck, Ben\n## [3] │ Damon, Matt\n# Using this regex, we can convert these to first_name last_name format\nstr_replace_all(names, '(\\\\w+),\\\\s+(\\\\w+)', '\\\\2 \\\\1')## [1] \"Tom Hanks\"   \"Ben Affleck\" \"Matt Damon\""},{"path":"regex---a-quick-introduction.html","id":"lookarounds","chapter":"29 Regex - A quick introduction","heading":"29.7 Lookarounds","text":"Look-ahead look-behinds zero-width assertions regex. used match pattern followed preceded another pattern, respectively. pattern look-ahead look-behind included match.","code":""},{"path":"regex---a-quick-introduction.html","id":"lookahead","chapter":"29 Regex - A quick introduction","heading":"29.7.1 Lookahead","text":"look-ahead used match pattern followed another pattern. Positive Lookaheads written (?=...), ... pattern must follow match.example, regex pattern hello(?= world) matches “hello” followed ” world”. matches “hello world” “hello world” “hello”.Example","code":"\nstring <- c(\"hello world\", \"hello there world\", \"hello\")\nstr_view(string, \"hello(?= world)\", match = NA)## [1] │ <hello> world\n## [2] │ hello there world\n## [3] │ hello\n# Note that \"world\" is not included in the match"},{"path":"regex---a-quick-introduction.html","id":"lookbehind","chapter":"29 Regex - A quick introduction","heading":"29.7.2 Lookbehind","text":"look-behind used match pattern preceded another pattern. Look-behinds written (?<=...), ... pattern must precede match.example, regex pattern (?<=hello )world matches “world” preceded “hello”. matches “hello world” “world hello” “hello world”.Example","code":"\nstring <- c(\"hello world\", \"world hello\", \"hello there world\")\nstr_view(string, \"(?<=hello )world\", match = NA)## [1] │ hello <world>\n## [2] │ world hello\n## [3] │ hello there world"},{"path":"regex---a-quick-introduction.html","id":"negative-lookahead-and-lookbehinds","chapter":"29 Regex - A quick introduction","heading":"29.7.3 Negative Lookahead and Lookbehinds","text":"Negative look-ahead negative look-behinds used match pattern followed preceded another pattern, respectively. Negative look-ahead look-behinds written (?!...) (?<!...), respectively.example, regex pattern hello(?! world) matches “hello” followed ” world”. matches “hello ” “hello world” “hello world ”.Example-regex pattern (?<!hello )world matches “world” preceded “hello”. matches “world hello” “hello world” “hello world”.difference look-ahead look-behind may subtle, yet become clear string/pattern replacement extraction required.Examples-","code":"\nstring <- c(\"hello there\", \"hello world\", \"hello world there\")\nstr_view(string, \"hello(?! world)\", match = NA)## [1] │ <hello> there\n## [2] │ hello world\n## [3] │ hello world there\nstring <- c(\"hello world\", \"world hello\", \"hello there world\")\nstr_view(string, \"(?<!hello )world\", match = NA)## [1] │ hello world\n## [2] │ <world> hello\n## [3] │ hello there <world>\nstring <- \"I have 10 apples, 6 pineapples and 5 bananas\"\n\n# look-behind to match \"apples\" preceded by a digit and a space\npattern1 <- \"(?<=\\\\d\\\\s)apples\"  \n\n# look-ahead to match count of apples\npattern2 <- \"\\\\d+(?=\\\\sapple)\"  \n\nstr_view(string = string, pattern = pattern1, match = NA)## [1] │ I have 10 <apples>, 6 pineapples and 5 bananas\nstr_view(string = string, pattern = pattern2, match = NA)## [1] │ I have <10> apples, 6 pineapples and 5 bananas"},{"path":"regex---a-quick-introduction.html","id":"comments","chapter":"29 Regex - A quick introduction","heading":"29.8 Comments","text":"","code":""},{"path":"regex---a-quick-introduction.html","id":"comments-within-regex","chapter":"29 Regex - A quick introduction","heading":"29.8.1 Comments within regex","text":"can use # character add comments within regex pattern. text following # symbol line ignored regex engine treated comment. can useful documenting regex patterns temporarily disabling parts pattern testing debugging. Example-","code":"\nstr_view(c(\"xyz\",\"abc\"), \"x(?#this is a comment)\", match = NA)## [1] │ <x>yz\n## [2] │ abc"},{"path":"regex---a-quick-introduction.html","id":"verbose-mode-multi-line-comments","chapter":"29 Regex - A quick introduction","heading":"29.8.2 Verbose Mode (multi-line comments)","text":"regular expressions, verbose mode feature allows write readable maintainable regex patterns adding comments white-space without affecting behavior. enable verbose mode, can use (?x) (?verbose) modifier beginning regex pattern.Example - Using regex can extract words contain vowel third place.","code":"\nstring <- \"The quick brown fox jumps over the lazy dog\"\npattern <- \"(?x)      # Enable verbose mode\n            \\\\b       # Match word boundary\n            \\\\w{2}    # matches first two alphabets\n            [aeiou]   # Match a vowel\n            \\\\w*      # Match optional word characters\n            \\\\b       # Match word boundary\"\nstr_view(string, pattern, match = NA)## [1] │ <The> <quick> <brown> fox jumps <over> <the> lazy dog"},{"path":"regex-in-human-readble-format-using-rebus.html","id":"regex-in-human-readble-format-using-rebus","chapter":"30 Regex in human readble format using rebus","heading":"30 Regex in human readble format using rebus","text":"Regular expressions, explained earlier chapter, powerful. However often difficult interpret. package called rebus R, allows us build complex regular expressions human readable expressions. instead writing [-zA-Z0-9._%+-]+@[-zA-Z0-9.-]+\\.[-zA-Z]{2,4} later trying decipher expression actually meant, can use human-readble format -Many us now correctly guessed regular expressions detect email addresses given text strings. Rebus actually contains functions like char_class() one_or_more() make building regular expressions easier. let’s dive learn package.First let’s load library. Alongside let’s also load stringr can use function str_view understand examples. Readers may note library rebus also contains function regex creates conflict stringr::regex. avoid conflict may use library conflicted may set preferential library case conflicts.","code":"one_or_more(char_class(ASCII_ALNUM %R% \"._%+-\")) %R%\n  \"@\" %R%\n  one_or_more(char_class(ASCII_ALNUM %R% \".-\")) %R%\n  DOT %R%\n  ascii_alpha(2, 4)\nlibrary(rebus)\nlibrary(stringr)\nlibrary(conflicted)\nconflict_prefer(\"regex\", \"stringr\")\nconflicts_prefer(rebus::or)"},{"path":"regex-in-human-readble-format-using-rebus.html","id":"operators-for-concatenation-and-alternation","chapter":"30 Regex in human readble format using rebus","heading":"30.1 Operators for concatenation and alternation","text":"concatenating two regular expressions may use either operators %R% %c% like pipe operator concatenates LHS RHS together one regular expression.alternation may use either operator %|% function () package separates two regular expressions pipe alternation.\nExample-1:","code":"\nmy_colors <- c(\"red\", \"grey\", \"blue\", \"gray\")\nstr_view(my_colors, \"gr\" %R% or(\"a\" , \"e\") %R% \"y\")## [2] │ <grey>\n## [4] │ <gray>\n# Note operator precedence\n# This actually means either \"gra\" or \"ey\"\nstr_view(my_colors, \"gr\" %R% \"a\" %|% \"e\" %R% \"y\")## [2] │ gr<ey>\n## [4] │ <gra>y"},{"path":"regex-in-human-readble-format-using-rebus.html","id":"literal-characters-specvial-characters-and-case-sensitivity","chapter":"30 Regex in human readble format using rebus","heading":"30.2 Literal Characters, specvial characters and case sensitivity","text":"literal characters may, learnt earlier, may given string mean. Literals case sensitive . want case insensitive match, function case_insensitive take care requirement.Example-2:match special characters may special meaning regular expressions, constants . rebus, constants usually available UPPER CASE equivalent functions available lower case. following special character constants available -Example-3:","code":"\nex_text <- \"This is an example text.\"\n# Match literal `x`\nstr_view(ex_text, \"X\", match = NA)## [1] │ This is an example text.\nstr_view(ex_text, case_insensitive(\"X\"), match = NA)## [1] │ This is an e<x>ample te<x>t.\nBACKSLASH## <regex> \\\\\nCARET## <regex> \\^\nDOLLAR## <regex> \\$\nDOT## <regex> \\.\nPIPE## <regex> \\|\nQUESTION## <regex> \\?\nSTAR## <regex> \\*\nPLUS## <regex> \\+\nOPEN_PAREN## <regex> \\(\nCLOSE_PAREN## <regex> \\)\nOPEN_BRACKET## <regex> \\[\nCLOSE_BRACKET## <regex> \\]\nOPEN_BRACE## <regex> \\{\nstr_view(ex_text, DOT)## [1] │ This is an example text<.>"},{"path":"regex-in-human-readble-format-using-rebus.html","id":"metacharacters-1","chapter":"30 Regex in human readble format using rebus","heading":"30.3 Metacharacters","text":"","code":""},{"path":"regex-in-human-readble-format-using-rebus.html","id":"character-classes","chapter":"30 Regex in human readble format using rebus","heading":"30.3.1 Character classes","text":"group characters together class match , may use function char_class() rebus package. See example-4:match range characters/numbers separate hyphen square brackets (normal regex building). , char_class(\"-d\") match character range [abcd].Example-5:negated character classes intuitively named function negated_char_class() R, can use per requirement.Example-6:","code":"\nex_vec <- c(\"Apple\", \"Orange\", \"Myrrh\")\n# matches a vowel\nstr_view(ex_vec, char_class(\"aeiou\"))## [1] │ Appl<e>\n## [2] │ Or<a>ng<e>\n# matches a vowel irrespective of case\nstr_view(ex_vec, case_insensitive(char_class(\"aeiou\")))## [1] │ <A>ppl<e>\n## [2] │ <O>r<a>ng<e>\nex_text <- \"The quick brown fox jumps over the lazy dog.\"\n# Match a, b or c in lower case\nstr_view(ex_text, char_class(\"a-d\"))## [1] │ The qui<c>k <b>rown fox jumps over the l<a>zy <d>og.\nex_text <- \"The quick brown fox jumps over the lazy dog.\"\n# Match all text except vowels\nstr_view(ex_text, negated_char_class(\"aeiou\"))## [1] │ <T><h>e< ><q>ui<c><k>< ><b><r>o<w><n>< ><f>o<x>< ><j>u<m><p><s>< >o<v>e<r>< ><t><h>e< ><l>a<z><y>< ><d>o<g><.>\n# Note that upper case and dot character have also been matched."},{"path":"regex-in-human-readble-format-using-rebus.html","id":"built-in-character-classes","chapter":"30 Regex in human readble format using rebus","heading":"30.3.2 Built-in Character classes","text":"can also use pre-built character classes available rebus, listed .See another example.Example-7:Besides afore-mentioned class constants, lower case equivalent functions character classes useful regex building.functions, lo hi accept positive integers quantifiers; char_class argument logical value. See examples.Example-8:","code":"\nALPHA## <regex> [:alpha:]\nALNUM## <regex> [:alnum:]\nBLANK## <regex> [:blank:]\nDIGIT## <regex> [:digit:]\nLOWER## <regex> [:lower:]\nPRINT## <regex> [:print:]\nPUNCT## <regex> [:punct:]\nSPACE## <regex> [:space:]\nUPPER## <regex> [:upper:]\nHEX_DIGIT## <regex> [:xdigit:]\nANY_CHAR## <regex> .\nGRAPHEME## <regex> \\X\nNEWLINE## <regex> \\R\nDGT## <regex> \\d\nWRD## <regex> \\w\nSPC## <regex> \\s\nNOT_DGT## <regex> \\D\nNOT_WRD## <regex> \\W\nNOT_SPC  # Equivalent to \"\\\\S\"## <regex> \\S\nASCII_DIGIT## <regex> 0-9\nASCII_LOWER## <regex> a-z\nASCII_UPPER## <regex> A-Z\nASCII_ALPHA## <regex> a-zA-Z\nASCII_ALNUM## <regex> a-zA-Z0-9\nex_text <- \"The quick brown fox jumps over the lazy dog.\"\n# Match TAB or SPACE Characters\nstr_view(ex_text, BLANK)## [1] │ The< >quick< >brown< >fox< >jumps< >over< >the< >lazy< >dog.\n# Match all UPPER CASE characters\nstr_view(ex_text, UPPER)## [1] │ <T>he quick brown fox jumps over the lazy dog.alnum(lo, hi, char_class = TRUE)\nalpha(lo, hi, char_class = TRUE)\nblank(lo, hi, char_class = TRUE)\ncntrl(lo, hi, char_class = TRUE)\ndigit(lo, hi, char_class = TRUE)\ngraph(lo, hi, char_class = TRUE)\nlower(lo, hi, char_class = TRUE)\nprintable(lo, hi, char_class = TRUE)\npunct(lo, hi, char_class = TRUE)\nspace(lo, hi, char_class = TRUE)\nupper(lo, hi, char_class = TRUE)\nhex_digit(lo, hi, char_class = TRUE)\nany_char(lo, hi)\ngrapheme(lo, hi)\nnewline(lo, hi)\ndgt(lo, hi, char_class = FALSE)\nwrd(lo, hi, char_class = FALSE)\nspc(lo, hi, char_class = FALSE)\nnot_dgt(lo, hi, char_class = FALSE)\nnot_wrd(lo, hi, char_class = FALSE)\nnot_spc(lo, hi, char_class = FALSE)\nascii_digit(lo, hi, char_class = TRUE)\nascii_lower(lo, hi, char_class = TRUE)\nascii_upper(lo, hi, char_class = TRUE)\nascii_alpha(lo, hi, char_class = TRUE)\nascii_alnum(lo, hi, char_class = TRUE)\nchar_range(lo, hi, char_class = lo < hi)\nnumber_range(lo, hi, allow_leading_zeroes = FALSE, capture = FALSE)\nip_add <- \"My IP address is 255.1.2.50\"\nstr_view(ip_add, digit(1, 3))## [1] │ My IP address is <255>.<1>.<2>.<50>\nstr_view(ip_add, digit(3))## [1] │ My IP address is <255>.1.2.50\nstr_view(ip_add, not_dgt(5))## [1] │ <My IP>< addr><ess i>s 255.1.2.50\n# Note this will match none\nstr_view(ip_add, space(2))"},{"path":"regex-in-human-readble-format-using-rebus.html","id":"word-boundaries","chapter":"30 Regex in human readble format using rebus","heading":"30.3.3 Word Boundaries","text":"match word boundary (negation) BOUNDARY (NOT_BOUNDARY) rebus. Function whole_word(x), hand, wraps regex word boundary tokens match whole word. See following example.Example-9:","code":"\nex_text <- \"The thermometer, they were searching is placed in a leather box.\"\n# Note three matches\nstr_view(ex_text, \"the\")## [1] │ The <the>rmometer, <the>y were searching is placed in a lea<the>r box.\n# There is no match\nstr_view(ex_text, whole_word(\"the\"), match = NA)## [1] │ The thermometer, they were searching is placed in a leather box.\nstr_view(ex_text, case_insensitive(whole_word(\"the\")), match = NA)## [1] │ <The> thermometer, they were searching is placed in a leather box.\nstr_view(ex_text, BOUNDARY %R% case_insensitive(\"the\"))## [1] │ <The> <the>rmometer, <the>y were searching is placed in a leather box.\nstr_view(ex_text, NOT_BOUNDARY %R% \"the\")## [1] │ The thermometer, they were searching is placed in a lea<the>r box."},{"path":"regex-in-human-readble-format-using-rebus.html","id":"quantifiers-1","chapter":"30 Regex in human readble format using rebus","heading":"30.4 Quantifiers","text":"learnt following quantifiers regular expressions.+ 1 occurrences* 0 ? 0 1{} specified numbers\n{n} exactly n\n{n,} n \n{n,m} n m\n{n} exactly n{n,} n {n,m} n mWe meaningfully named functions quantifiers rebus.one_or_more(x, char_class = NA)zero_or_more(x, char_class = NA)optional(x, char_class = NA)repeated(x, lo, hi, lazy = FALSE, char_class = NA)\nlo hi represent n m equivalently.\nlo hi represent n m equivalently.Additionally, may notice argument char_class = NA accepts logical value case x required wrapped character class.may match two consecutive vowels using repeated(\"aeiou\", 2, char_class = TRUE). See example.Example-10:","code":"\nex_vec <- c(\"1 apple\", \"2 bananas\", \"3 pineapples\")\n# match two consecutive vowels\nstr_view(ex_vec, repeated(\"aeiou\", 2, char_class = TRUE))## [3] │ 3 pin<ea>pples\n# match a number followed by apple with an optional space\nstr_view(ex_vec, one_or_more(DIGIT) %R% optional(BLANK) %R% \"apple\")## [1] │ <1 apple>"},{"path":"regex-in-human-readble-format-using-rebus.html","id":"anchors-1","chapter":"30 Regex in human readble format using rebus","heading":"30.5 Anchors","text":"Anchors regular expressions allow us match patterns specific positions within input string. rebus, constants START END match beginning end positions within input text, respectively.Example-11:example, matching word beginning sentence . one function exactly(x) rebus makes regular expression match whole string, start end, fact shortcut START %R% x %R% END.Example-12:","code":"\nstring <- \"The quick brown fox jumps over the lazy dog. The fox is brown.\"\nstr_view(string, START %R% case_insensitive(\"the\"))## [1] │ <The> quick brown fox jumps over the lazy dog. The fox is brown.\nex_vec <- c(\"apple\", \"banana\", \"cherry\", \"pineapple\")\nstr_view(ex_vec, exactly(\"apple\"))## [1] │ <apple>"},{"path":"regex-in-human-readble-format-using-rebus.html","id":"capture-groups-1","chapter":"30 Regex in human readble format using rebus","heading":"30.6 Capture Groups","text":"seen capture group way group part regular expression capture separate substring. useful want extract replace specific part string. R, capture groups denoted parentheses (). Anything inside parentheses captured can referenced later regular expression replacement string. also learnt capturing groups useful refer back within match back reference: \\1 refers match contained first parenthesis, \\2 second parenthesis, .rebus, two functions capture(x) group(x) capturing regex. Former good match functions latter mostly used alternations.Example-13:Backreferences replacement operations, denoted constants REF# # actually digit 1 9.Example-14:Similar previous chapter, can use capturing group replace pattern something else.Example-15:","code":"\n## capture(x)\nmy_fruits <- c('apple', 'banana', 'coconut', 'berry', 'cucumber', 'date')\nstr_remove_all(my_fruits, capture(char_class(\"aeiou\")))## [1] \"ppl\"   \"bnn\"   \"ccnt\"  \"brry\"  \"ccmbr\" \"dt\"\n## group()\nmy_toppings <- group(\"olive\" %|% \"mushroom\" %|% \"tomato\")\npizza <- \"We have olive, mushroom, chicken and capsicum pizza in our menu.\"\n# Extract my favourite topping from available pizza menu.\nstr_extract_all(pizza, pattern = my_toppings)## [[1]]\n## [1] \"olive\"    \"mushroom\"\nmy_fruits <- c('apple', 'banana', 'coconut', 'berry', 'cucumber', 'date')\n# search for repeated alphabet\nstr_view(my_fruits, capture(ANY_CHAR) %R% REF1 , match = NA)## [1] │ a<pp>le\n## [2] │ banana\n## [3] │ coconut\n## [4] │ be<rr>y\n## [5] │ cucumber\n## [6] │ date\n# We have names in last_name, first_name format\nnames <- c('Hanks, Tom', 'Affleck, Ben', 'Damon, Matt')\nstr_view(names)## [1] │ Hanks, Tom\n## [2] │ Affleck, Ben\n## [3] │ Damon, Matt\n# Pattern to capture first name and last name\npat <- capture(whole_word(one_or_more(ALPHA))) %R% \",\" %R% optional(SPACE) %R% capture(whole_word(one_or_more(ALPHA)))\nrepl <- REF2 %R% \" \" %R% REF1\n# Using this regex, we can convert these to first_name last_name format\nstr_replace_all(names, pat, repl)## [1] \"Tom Hanks\"   \"Ben Affleck\" \"Matt Damon\""},{"path":"regex-in-human-readble-format-using-rebus.html","id":"lookarounds-1","chapter":"30 Regex in human readble format using rebus","heading":"30.7 Lookarounds","text":"learnt, lookaheads lookbehinds zero-width assertions regex. used match pattern followed preceded another pattern, respectively. pattern lookahead lookbehind included match.intuitively named functions deal four lookarounds-Example-16: Find character q followed uExample-17:examples.Example-18:","code":"lookahead(x)\nnegative_lookahead(x)\nlookbehind(x)\nnegative_lookbehind(x)\ncountries <- c(\"mozambique\", \"qatar\", \"iraq\")\n# With lookahead\nstr_view(countries, \"q\" %R% negative_lookahead(\"u\"))## [2] │ <q>atar\n## [3] │ ira<q>\n# Without Lookahaed - Notice the difference\nstr_view(countries, \"q\" %R% negated_char_class(\"u\"))## [2] │ <qa>tar\n# Lookahead\nstring <- c(\"hello world\", \"hello there world\", \"hello\")\nstr_view(string, \"hello\" %R% lookahead(optional(SPACE) %R% \"world\"), match = NA)## [1] │ <hello> world\n## [2] │ hello there world\n## [3] │ hello\n# Note that \"world\" is not included in the match\n\n# Lookbehind\nstr_view(string, lookbehind(\"hello\" %R% optional(SPACE)) %R% \"world\", match = NA)## [1] │ hello <world>\n## [2] │ hello there world\n## [3] │ hello\n# Negative lookahead\nstr_view(string, \"hello\" %R% negative_lookahead(optional(SPACE) %R% \"world\"), match = NA)## [1] │ hello world\n## [2] │ <hello> there world\n## [3] │ <hello>\n# Negative lookbehind\nstr_view(string, negative_lookbehind(\"hello\" %R% optional(SPACE)) %R% \"world\", match = NA)## [1] │ hello world\n## [2] │ hello there <world>\n## [3] │ hello\nstring <- \"I have 10 apples, 6 pineapples and 5 bananas\"\n\n# lookahead to match count of apples\npattern1 <- one_or_more(DIGIT) %R% lookahead(optional(SPACE) %R% \"apple\") \n# How many apples?\nstr_view(string = string, pattern = pattern1, match = NA)## [1] │ I have <10> apples, 6 pineapples and 5 bananas"},{"path":"regex-in-human-readble-format-using-rebus.html","id":"some-useful-regex-functions","chapter":"30 Regex in human readble format using rebus","heading":"30.8 Some useful regex functions","text":"","code":""},{"path":"regex-in-human-readble-format-using-rebus.html","id":"matching-valid-dates","chapter":"30 Regex in human readble format using rebus","heading":"30.8.1 Matching valid dates","text":"Example-19:","code":"\n# Individual date-time components\nDTSEP             # optional selected punctuation or space## <regex> [-/.:,\\ ]?\nCENTURY           # exactly two digits## <regex> [0-9]{2}\nYEAR              # one to four digits## <regex> [0-9]{1,4}\nYEAR2             # exactly two digits## <regex> [0-9]{2}\nYEAR4             # exactly four digits## <regex> [0-9]{4}\nMONTH             # number from 1 to 12, leading zero## <regex> (?:0[1-9]|1[0-2])\nWEEK_OF_YEAR      # number from 0 to 53, leading zero## <regex> (?:[0-4][0-9]|5[0-3])\nDAY               # number from 1 to 31, leading zero## <regex> (?:0[1-9]|[12][0-9]|3[01])\nDAY_SINGLE        # leading space## <regex> (?: [1-9]|[12][0-9]|3[01])\nHOUR24            # 24 hour clock, leading zero## <regex> (?:[01][0-9]|2[0-3])\nHOUR12            # 12 hour clock, leading zero## <regex> (?:0[1-9]|1[0-2])\nHOUR24_SINGLE     # 24 hour clock, leading space## <regex> (?:[ 1][0-9]|2[0-3])\nHOUR12_SINGLE     # 12 hour clock, leading space## <regex> (?: [1-9]|1[0-2])\nMINUTE            # number from 0 to 59, leading zero## <regex> [0-5][0-9]\nSECOND            # number from 0 to 61 (leap seconds), leading zero## <regex> (?:[0-5][0-9]|6[01])\nFRACTIONAL_SECOND # a second optional decimal point and up to 6 digits## <regex> (?:[0-5][0-9]|6[01])(?:[.,][0-9]{1,6})?\nAM_PM             # AM or PM, any case## <regex> (?:am|AM|pm|PM)\nTIMEZONE_OFFSET   # optional plus or minus, then four digits## <regex> [-+]?[0-9]{4}\nTIMEZONE          # Any value returned by OlsonNames()## <regex> (?:Africa/Abidjan|Africa/Accra|Africa/Addis_Ababa|Africa/Algiers|Africa/Asmara|Africa/Asmera|Africa/Bamako|Africa/Bangui|Africa/Banjul|Africa/Bissau|Africa/Blantyre|Africa/Brazzaville|Africa/Bujumbura|Africa/Cairo|Africa/Casablanca|Africa/Ceuta|Africa/Conakry|Africa/Dakar|Africa/Dar_es_Salaam|Africa/Djibouti|Africa/Douala|Africa/El_Aaiun|Africa/Freetown|Africa/Gaborone|Africa/Harare|Africa/Johannesburg|Africa/Juba|Africa/Kampala|Africa/Khartoum|Africa/Kigali|Africa/Kinshasa|Africa/Lagos|Africa/Libreville|Africa/Lome|Africa/Luanda|Africa/Lubumbashi|Africa/Lusaka|Africa/Malabo|Africa/Maputo|Africa/Maseru|Africa/Mbabane|Africa/Mogadishu|Africa/Monrovia|Africa/Nairobi|Africa/Ndjamena|Africa/Niamey|Africa/Nouakchott|Africa/Ouagadougou|Africa/Porto-Novo|Africa/Sao_Tome|Africa/Timbuktu|Africa/Tripoli|Africa/Tunis|Africa/Windhoek|America/Adak|America/Anchorage|America/Anguilla|America/Antigua|America/Araguaina|America/Argentina/Buenos_Aires|America/Argentina/Catamarca|America/Argentina/ComodRivadavia|America/Argentina/Cordoba|America/Argentina/Jujuy|America/Argentina/La_Rioja|America/Argentina/Mendoza|America/Argentina/Rio_Gallegos|America/Argentina/Salta|America/Argentina/San_Juan|America/Argentina/San_Luis|America/Argentina/Tucuman|America/Argentina/Ushuaia|America/Aruba|America/Asuncion|America/Atikokan|America/Atka|America/Bahia|America/Bahia_Banderas|America/Barbados|America/Belem|America/Belize|America/Blanc-Sablon|America/Boa_Vista|America/Bogota|America/Boise|America/Buenos_Aires|America/Cambridge_Bay|America/Campo_Grande|America/Cancun|America/Caracas|America/Catamarca|America/Cayenne|America/Cayman|America/Chicago|America/Chihuahua|America/Ciudad_Juarez|America/Coral_Harbour|America/Cordoba|America/Costa_Rica|America/Creston|America/Cuiaba|America/Curacao|America/Danmarkshavn|America/Dawson|America/Dawson_Creek|America/Denver|America/Detroit|America/Dominica|America/Edmonton|America/Eirunepe|America/El_Salvador|America/Ensenada|America/Fort_Nelson|America/Fort_Wayne|America/Fortaleza|America/Glace_Bay|America/Godthab|America/Goose_Bay|America/Grand_Turk|America/Grenada|America/Guadeloupe|America/Guatemala|America/Guayaquil|America/Guyana|America/Halifax|America/Havana|America/Hermosillo|America/Indiana/Indianapolis|America/Indiana/Knox|America/Indiana/Marengo|America/Indiana/Petersburg|America/Indiana/Tell_City|America/Indiana/Vevay|America/Indiana/Vincennes|America/Indiana/Winamac|America/Indianapolis|America/Inuvik|America/Iqaluit|America/Jamaica|America/Jujuy|America/Juneau|America/Kentucky/Louisville|America/Kentucky/Monticello|America/Knox_IN|America/Kralendijk|America/La_Paz|America/Lima|America/Los_Angeles|America/Louisville|America/Lower_Princes|America/Maceio|America/Managua|America/Manaus|America/Marigot|America/Martinique|America/Matamoros|America/Mazatlan|America/Mendoza|America/Menominee|America/Merida|America/Metlakatla|America/Mexico_City|America/Miquelon|America/Moncton|America/Monterrey|America/Montevideo|America/Montreal|America/Montserrat|America/Nassau|America/New_York|America/Nipigon|America/Nome|America/Noronha|America/North_Dakota/Beulah|America/North_Dakota/Center|America/North_Dakota/New_Salem|America/Nuuk|America/Ojinaga|America/Panama|America/Pangnirtung|America/Paramaribo|America/Phoenix|America/Port-au-Prince|America/Port_of_Spain|America/Porto_Acre|America/Porto_Velho|America/Puerto_Rico|America/Punta_Arenas|America/Rainy_River|America/Rankin_Inlet|America/Recife|America/Regina|America/Resolute|America/Rio_Branco|America/Rosario|America/Santa_Isabel|America/Santarem|America/Santiago|America/Santo_Domingo|America/Sao_Paulo|America/Scoresbysund|America/Shiprock|America/Sitka|America/St_Barthelemy|America/St_Johns|America/St_Kitts|America/St_Lucia|America/St_Thomas|America/St_Vincent|America/Swift_Current|America/Tegucigalpa|America/Thule|America/Thunder_Bay|America/Tijuana|America/Toronto|America/Tortola|America/Vancouver|America/Virgin|America/Whitehorse|America/Winnipeg|America/Yakutat|America/Yellowknife|Antarctica/Casey|Antarctica/Davis|Antarctica/DumontDUrville|Antarctica/Macquarie|Antarctica/Mawson|Antarctica/McMurdo|Antarctica/Palmer|Antarctica/Rothera|Antarctica/South_Pole|Antarctica/Syowa|Antarctica/Troll|Antarctica/Vostok|Arctic/Longyearbyen|Asia/Aden|Asia/Almaty|Asia/Amman|Asia/Anadyr|Asia/Aqtau|Asia/Aqtobe|Asia/Ashgabat|Asia/Ashkhabad|Asia/Atyrau|Asia/Baghdad|Asia/Bahrain|Asia/Baku|Asia/Bangkok|Asia/Barnaul|Asia/Beirut|Asia/Bishkek|Asia/Brunei|Asia/Calcutta|Asia/Chita|Asia/Choibalsan|Asia/Chongqing|Asia/Chungking|Asia/Colombo|Asia/Dacca|Asia/Damascus|Asia/Dhaka|Asia/Dili|Asia/Dubai|Asia/Dushanbe|Asia/Famagusta|Asia/Gaza|Asia/Harbin|Asia/Hebron|Asia/Ho_Chi_Minh|Asia/Hong_Kong|Asia/Hovd|Asia/Irkutsk|Asia/Istanbul|Asia/Jakarta|Asia/Jayapura|Asia/Jerusalem|Asia/Kabul|Asia/Kamchatka|Asia/Karachi|Asia/Kashgar|Asia/Kathmandu|Asia/Katmandu|Asia/Khandyga|Asia/Kolkata|Asia/Krasnoyarsk|Asia/Kuala_Lumpur|Asia/Kuching|Asia/Kuwait|Asia/Macao|Asia/Macau|Asia/Magadan|Asia/Makassar|Asia/Manila|Asia/Muscat|Asia/Nicosia|Asia/Novokuznetsk|Asia/Novosibirsk|Asia/Omsk|Asia/Oral|Asia/Phnom_Penh|Asia/Pontianak|Asia/Pyongyang|Asia/Qatar|Asia/Qostanay|Asia/Qyzylorda|Asia/Rangoon|Asia/Riyadh|Asia/Saigon|Asia/Sakhalin|Asia/Samarkand|Asia/Seoul|Asia/Shanghai|Asia/Singapore|Asia/Srednekolymsk|Asia/Taipei|Asia/Tashkent|Asia/Tbilisi|Asia/Tehran|Asia/Tel_Aviv|Asia/Thimbu|Asia/Thimphu|Asia/Tokyo|Asia/Tomsk|Asia/Ujung_Pandang|Asia/Ulaanbaatar|Asia/Ulan_Bator|Asia/Urumqi|Asia/Ust-Nera|Asia/Vientiane|Asia/Vladivostok|Asia/Yakutsk|Asia/Yangon|Asia/Yekaterinburg|Asia/Yerevan|Atlantic/Azores|Atlantic/Bermuda|Atlantic/Canary|Atlantic/Cape_Verde|Atlantic/Faeroe|Atlantic/Faroe|Atlantic/Jan_Mayen|Atlantic/Madeira|Atlantic/Reykjavik|Atlantic/South_Georgia|Atlantic/St_Helena|Atlantic/Stanley|Australia/ACT|Australia/Adelaide|Australia/Brisbane|Australia/Broken_Hill|Australia/Canberra|Australia/Currie|Australia/Darwin|Australia/Eucla|Australia/Hobart|Australia/LHI|Australia/Lindeman|Australia/Lord_Howe|Australia/Melbourne|Australia/NSW|Australia/North|Australia/Perth|Australia/Queensland|Australia/South|Australia/Sydney|Australia/Tasmania|Australia/Victoria|Australia/West|Australia/Yancowinna|Brazil/Acre|Brazil/DeNoronha|Brazil/East|Brazil/West|CET|CST6CDT|Canada/Atlantic|Canada/Central|Canada/Eastern|Canada/Mountain|Canada/Newfoundland|Canada/Pacific|Canada/Saskatchewan|Canada/Yukon|Chile/Continental|Chile/EasterIsland|Cuba|EET|EST|EST5EDT|Egypt|Eire|Etc/GMT|Etc/GMT\\+0|Etc/GMT\\+1|Etc/GMT\\+10|Etc/GMT\\+11|Etc/GMT\\+12|Etc/GMT\\+2|Etc/GMT\\+3|Etc/GMT\\+4|Etc/GMT\\+5|Etc/GMT\\+6|Etc/GMT\\+7|Etc/GMT\\+8|Etc/GMT\\+9|Etc/GMT-0|Etc/GMT-1|Etc/GMT-10|Etc/GMT-11|Etc/GMT-12|Etc/GMT-13|Etc/GMT-14|Etc/GMT-2|Etc/GMT-3|Etc/GMT-4|Etc/GMT-5|Etc/GMT-6|Etc/GMT-7|Etc/GMT-8|Etc/GMT-9|Etc/GMT0|Etc/Greenwich|Etc/UCT|Etc/UTC|Etc/Universal|Etc/Zulu|Europe/Amsterdam|Europe/Andorra|Europe/Astrakhan|Europe/Athens|Europe/Belfast|Europe/Belgrade|Europe/Berlin|Europe/Bratislava|Europe/Brussels|Europe/Bucharest|Europe/Budapest|Europe/Busingen|Europe/Chisinau|Europe/Copenhagen|Europe/Dublin|Europe/Gibraltar|Europe/Guernsey|Europe/Helsinki|Europe/Isle_of_Man|Europe/Istanbul|Europe/Jersey|Europe/Kaliningrad|Europe/Kiev|Europe/Kirov|Europe/Kyiv|Europe/Lisbon|Europe/Ljubljana|Europe/London|Europe/Luxembourg|Europe/Madrid|Europe/Malta|Europe/Mariehamn|Europe/Minsk|Europe/Monaco|Europe/Moscow|Europe/Nicosia|Europe/Oslo|Europe/Paris|Europe/Podgorica|Europe/Prague|Europe/Riga|Europe/Rome|Europe/Samara|Europe/San_Marino|Europe/Sarajevo|Europe/Saratov|Europe/Simferopol|Europe/Skopje|Europe/Sofia|Europe/Stockholm|Europe/Tallinn|Europe/Tirane|Europe/Tiraspol|Europe/Ulyanovsk|Europe/Uzhgorod|Europe/Vaduz|Europe/Vatican|Europe/Vienna|Europe/Vilnius|Europe/Volgograd|Europe/Warsaw|Europe/Zagreb|Europe/Zaporozhye|Europe/Zurich|GB|GB-Eire|GMT|GMT\\+0|GMT-0|GMT0|Greenwich|HST|Hongkong|Iceland|Indian/Antananarivo|Indian/Chagos|Indian/Christmas|Indian/Cocos|Indian/Comoro|Indian/Kerguelen|Indian/Mahe|Indian/Maldives|Indian/Mauritius|Indian/Mayotte|Indian/Reunion|Iran|Israel|Jamaica|Japan|Kwajalein|Libya|MET|MST|MST7MDT|Mexico/BajaNorte|Mexico/BajaSur|Mexico/General|NZ|NZ-CHAT|Navajo|PRC|PST8PDT|Pacific/Apia|Pacific/Auckland|Pacific/Bougainville|Pacific/Chatham|Pacific/Chuuk|Pacific/Easter|Pacific/Efate|Pacific/Enderbury|Pacific/Fakaofo|Pacific/Fiji|Pacific/Funafuti|Pacific/Galapagos|Pacific/Gambier|Pacific/Guadalcanal|Pacific/Guam|Pacific/Honolulu|Pacific/Johnston|Pacific/Kanton|Pacific/Kiritimati|Pacific/Kosrae|Pacific/Kwajalein|Pacific/Majuro|Pacific/Marquesas|Pacific/Midway|Pacific/Nauru|Pacific/Niue|Pacific/Norfolk|Pacific/Noumea|Pacific/Pago_Pago|Pacific/Palau|Pacific/Pitcairn|Pacific/Pohnpei|Pacific/Ponape|Pacific/Port_Moresby|Pacific/Rarotonga|Pacific/Saipan|Pacific/Samoa|Pacific/Tahiti|Pacific/Tarawa|Pacific/Tongatapu|Pacific/Truk|Pacific/Wake|Pacific/Wallis|Pacific/Yap|Poland|Portugal|ROC|ROK|Singapore|Turkey|UCT|US/Alaska|US/Aleutian|US/Arizona|US/Central|US/East-Indiana|US/Eastern|US/Hawaii|US/Indiana-Starke|US/Michigan|US/Mountain|US/Pacific|US/Samoa|UTC|Universal|W-SU|WET|Zulu)\n# ISO 8601 formats\nISO_DATE          # %Y-%m-%d## <regex> [0-9]{4}-(?:0[1-9]|1[0-2])-(?:0[1-9]|[12][0-9]|3[01])\nISO_TIME          # %H:%M:%S## <regex> (?:[01][0-9]|2[0-3]):[0-5][0-9]:(?:[0-5][0-9]|6[01])\nISO_DATETIME      # ISO_DATE followed by ISO_TIME, separated by space or \"T\".## <regex> [0-9]{4}-(?:0[1-9]|1[0-2])-(?:0[1-9]|[12][0-9]|3[01])(?:[ T](?:[01][0-9]|2[0-3]):[0-5][0-9]:(?:[0-5][0-9]|6[01]))?\n# Compound forms, separated by DTSEP\nYMD## <regex> [0-9]{1,4}[-/.:,\\ ]?(?:0[1-9]|1[0-2])[-/.:,\\ ]?(?:0[1-9]|[12][0-9]|3[01])\nYDM## <regex> [0-9]{1,4}[-/.:,\\ ]?(?:0[1-9]|[12][0-9]|3[01])[-/.:,\\ ]?(?:0[1-9]|1[0-2])\nMYD## <regex> (?:0[1-9]|1[0-2])[-/.:,\\ ]?[0-9]{1,4}[-/.:,\\ ]?(?:0[1-9]|[12][0-9]|3[01])\nMDY## <regex> (?:0[1-9]|1[0-2])[-/.:,\\ ]?(?:0[1-9]|[12][0-9]|3[01])[-/.:,\\ ]?[0-9]{1,4}\nDYM## <regex> (?:0[1-9]|[12][0-9]|3[01])[-/.:,\\ ]?[0-9]{1,4}[-/.:,\\ ]?(?:0[1-9]|1[0-2])\nDMY## <regex> (?:0[1-9]|[12][0-9]|3[01])[-/.:,\\ ]?(?:0[1-9]|1[0-2])[-/.:,\\ ]?[0-9]{1,4}\nHMS## <regex> (?:[01][0-9]|2[0-3])[-/.:,\\ ]?[0-5][0-9][-/.:,\\ ]?(?:[0-5][0-9]|6[01])\nHM## <regex> (?:[01][0-9]|2[0-3])[-/.:,\\ ]?[0-5][0-9]\nMS## <regex> [0-5][0-9][-/.:,\\ ]?(?:[0-5][0-9]|6[01])\n# We have some dates - both valid and invalid\nsome_dates <- c(\"2000-13-01\", \"2025-08-09\",\"2000-01-32\", \"2000-00-01\", \"2000-01-00\", \"2020-05-20\")\n\nstr_view(some_dates, ISO_DATE)## [2] │ <2025-08-09>\n## [6] │ <2020-05-20>\n# Similarly some time formats\nsome_times <- c(\"24:00:00\", \"23:60:59\", \"23:59:62\", \"23 59 59\", \"23:55:55\", \"00:00:00\")\nstr_view(some_times, ISO_TIME)## [5] │ <23:55:55>\n## [6] │ <00:00:00>"},{"path":"regex-in-human-readble-format-using-rebus.html","id":"roman-numerals","chapter":"30 Regex in human readble format using rebus","heading":"30.9 Roman numerals","text":"match Roman numerals constant ROMAN well function roman(lo, hi) rebus.Example-20:","code":"\n# Some Roman numerals, both valid and invalid\nsome_numbers <- c(\"MMMDCCCXLVIII\", \"MMMCMDCCCXLVIIV\", \"MCD\", \"XIL\", \"LIX\", \"XL\")\n# Find valid roam numerals\nstr_view(some_numbers, exactly(roman()))## [1] │ <MMMDCCCXLVIII>\n## [3] │ <MCD>\n## [5] │ <LIX>\n## [6] │ <XL>"},{"path":"factors.html","id":"factors","chapter":"31 Factors","heading":"31 Factors","text":"often requirement, need character variable representing categorical data take values fixed known set finite values (categories). Additionally sometimes categories need sorted specific order may alphabetical. Categorical data like plays important role data analytics.deal categorical data, special class factor R (Readers may remember learnt data type, though short , section 1.5.1). data analytical tasks often need create use factors, let us discuss bit detail chapter.","code":""},{"path":"factors.html","id":"factors-in-base-r","chapter":"31 Factors","heading":"31.1 Factors in base R","text":"Factors R objects built atomic data-type integer. two primary functions create (coerce) factors character vectors.factoras.factor(), factor provides us full customisation basic function create factor objects base R. Let us discuss .","code":""},{"path":"factors.html","id":"creating-factors-from-scratch","chapter":"31 Factors","heading":"31.1.1 Creating factors from scratch","text":"Now, let us create factor character vector values shirt_sizes.Let us say, 10 shirts sizes randomly.output get NAs silently place values available allowed values (read levels).Now check type class.can see factor class actually built upon integer class underneath labels (taking values levels default) shown output.can, however, modify labels without modifying levels providing values (vector) labels argument. Let us now create another factor labels different levels.Function summary gives us count level given factor.","code":"\nshirt_sizes <- c(\"S\", \"M\", \"L\", \"XL\", \"XXL\")\n# 10 shirts with following sizes\n# Notice one shirt with size small case l and one XXXL\nshirts <- c(\"S\", \"s\", \"L\", \"XL\", \"XXXL\", \"S\", \"M\", \"M\", \"L\", \"L\")\n\n# Let us create a factor of shirt_sizes\nshirt_f <- factor(shirts, levels = shirt_sizes)\nshirt_f##  [1] S    <NA> L    XL   <NA> S    M    M    L    L   \n## Levels: S M L XL XXL\ntypeof(shirt_f)## [1] \"integer\"\nclass(shirt_f)## [1] \"factor\"\nunclass(shirt_f)##  [1]  1 NA  3  4 NA  1  2  2  3  3\n## attr(,\"levels\")\n## [1] \"S\"   \"M\"   \"L\"   \"XL\"  \"XXL\"\nset.seed(123)\n# 100 Shirts\nshirts2 <- sample(shirt_sizes, 100, replace = TRUE)\nshirts2_f <- factor(shirts2, levels = shirt_sizes, labels = c(\"Small\", \"Medium\", \"Large\", \"Extra Large\", \"Extra Extra large\"))\n# Let's view some shirts\nhead(shirts2_f)## [1] Large             Large             Medium            Medium           \n## [5] Large             Extra Extra large\n## Levels: Small Medium Large Extra Large Extra Extra large\n# Check its levels\nlevels(shirts2_f)## [1] \"Small\"             \"Medium\"            \"Large\"            \n## [4] \"Extra Large\"       \"Extra Extra large\"\nsummary(shirts2_f)##             Small            Medium             Large       Extra Large \n##                21                20                23                17 \n## Extra Extra large \n##                19"},{"path":"factors.html","id":"coercing-objects-to-factor","chapter":"31 Factors","heading":"31.1.2 Coercing objects to factor","text":"now, created factor set allowable finite values (read levels) displayed using meaningful labels. Function .factor() check whether given vector factor . hand, function .factor coerce existing character vector factor variable using distinct values available therein levels, sorted alphabetically.problem earlier factor created scratch, took levels provided vector sorted meaningfully , thus control.","code":"\nis.factor(iris$Species)## [1] TRUE\nshirts2_coerced <- as.factor(shirts2)\nhead(shirts2_coerced)## [1] L   L   M   M   L   XXL\n## Levels: L M S XL XXL\nlevels(shirts2_coerced)## [1] \"L\"   \"M\"   \"S\"   \"XL\"  \"XXL\"\n# Check summary too\nsummary(shirts2_coerced)##   L   M   S  XL XXL \n##  23  20  21  17  19"},{"path":"factors.html","id":"ordfact","chapter":"31 Factors","heading":"31.1.3 Order in factors","text":"sort factors meaningful way, can actually create ordered factor. Ordered factor can created either byusing ordered = TRUE argument function factor; bycoercing given factor ordered factor using function ordered().Clearly, latter method order given factor per levels present factor.Ordering factor R another benefit, can actually perform calculations ordered factor.\nSuppose want find many shirts sizes “Large” greater.can check given factor ordered factor using function .ordered(). E.g.","code":"\nshirts2_ordered <- factor(\n  shirts2,\n  levels = shirt_sizes,\n  labels = c(\"Small\", \"Medium\", \"Large\", \n             \"Extra Large\", \"Extra Extra large\"),\n  ordered = TRUE\n)\n# How many shirts are there of sizes L or greater?\nsum(shirts2_ordered >= \"Large\")## [1] 59\n# But unordered factor will result in error.\nsum(shirts2_f >= \"Large\")## [1] NA\nis.ordered(shirts2_ordered)## [1] TRUE\nis.ordered(shirts2_f)## [1] FALSE"},{"path":"factors.html","id":"functions-returning-factors-as-output","chapter":"31 Factors","heading":"31.1.4 Functions returning factors as output","text":"Readers may remember section 4.12.9 learnt function cut returns factor variable output. next sections learn functions useful working factor variables, either input output .","code":""},{"path":"factors.html","id":"factors-in-forcats","chapter":"31 Factors","heading":"31.2 Factors in forcats","text":"Package forcats part core tidyverse provides us robust useful ways create deal factor variables. forcats function fct creating factor variables. produce errors value available given levels, avoid bugs/typographical errors code. E.g.","code":"\nlibrary(forcats)\nmonths_31 <- c(\"Jan\", \"Mar\", \"May\", \"Jul\", \"Aug\", \"Oxt\", \"Dec\")\nfct(months_31, levels = month.abb)## Error in `fct()`:\n## ! All values of `x` must appear in `levels` or `na`\n## ℹ Missing level: \"Oxt\""},{"path":"factors.html","id":"inspecting-factors","chapter":"31 Factors","heading":"31.3 Inspecting Factors","text":"","code":""},{"path":"factors.html","id":"summarising-factors","chapter":"31 Factors","heading":"31.3.1 Summarising factors","text":"summary() method works well give counts level.Like count dplyr, fct_count() give us level wise counts /proportions. difference summary() however output type. Function fct_count() returns tibble instead. E.g.","code":"\nshirts2_fct <- fct(shirts2)\nsummary(shirts2_fct)##   L   M XXL  XL   S \n##  23  20  19  17  21\nfct_count(shirts2_fct)## # A tibble: 5 × 2\n##   f         n\n##   <fct> <int>\n## 1 L        23\n## 2 M        20\n## 3 XXL      19\n## 4 XL       17\n## 5 S        21\n# Sort in decreasing counts\nfct_count(shirts2_fct, sort = TRUE)## # A tibble: 5 × 2\n##   f         n\n##   <fct> <int>\n## 1 L        23\n## 2 S        21\n## 3 M        20\n## 4 XXL      19\n## 5 XL       17\n# Include proportions also\nfct_count(shirts2_fct, sort = TRUE, prop = TRUE)## # A tibble: 5 × 3\n##   f         n     p\n##   <fct> <int> <dbl>\n## 1 L        23  0.23\n## 2 S        21  0.21\n## 3 M        20  0.2 \n## 4 XXL      19  0.19\n## 5 XL       17  0.17"},{"path":"factors.html","id":"unique-levels-only","chapter":"31 Factors","heading":"31.3.2 Unique levels only","text":"Function fct_unique() package, returns factor unique values, removing duplicates. E.g.","code":"\nfct_unique(shirts2_fct)## [1] L   M   XXL XL  S  \n## Levels: L M XXL XL S"},{"path":"factors.html","id":"order-in-factors","chapter":"31 Factors","heading":"31.4 Order in Factors","text":"","code":""},{"path":"factors.html","id":"default-ordering-in-factors","chapter":"31 Factors","heading":"31.4.1 Default ordering in factors","text":"Orders created factor variables using fct sorted per levels given level argument. argument supplied sorted basis first appearance (alphabetically factor), observed output example.learn functions forcats use gss_cat data frame part forcats package GSS stands General Social Survey. actually consists many factor variables. use cases, also use economics_long data part tidyr package.","code":""},{"path":"factors.html","id":"reordering-factors","chapter":"31 Factors","heading":"31.4.2 Reordering factors","text":"analyse (mean) number hours spent per day TV watching across different religions gss_cat, can see -\nFigure 31.1: Factors default order\ncase (31.1), order relig factor meaningful sorted basis summarised values another numerical variable present data. Function fct_reorder helpful scenarios.-.f factor variable sorted..x numerical variable based .f sorted..fun optional function (default median) used multiple values .x level .f., example, can re-order levels using function. See figure 31.2 -\nFigure 31.2: Factors reordered levels\nfunction also useful sorting box-plots. example refer Figure 31.3.\nFigure 31.3: Unsorted boxes unordered levels\n\nFigure 31.4: Boxes sorted Median reordering factor levels\n","code":"\nlibrary(tidyverse)\ngss_cat |>\n  summarise(tv = mean(tvhours, na.rm = TRUE), .by = relig) |>\n  ggplot(aes(relig, tv)) +\n  geom_col() +\n  coord_flip()fct_reorder(\n  .f,\n  .x,\n  .fun = median,\n  ...,\n  .na_rm = NULL,\n  .default = Inf,\n  .desc = FALSE\n)\ngss_cat |>\n  summarise(tv = mean(tvhours, na.rm = TRUE), .by = relig) |>\n  ggplot(aes(fct_reorder(relig, tv), tv)) +\n  geom_col() +\n  coord_flip() +\n  labs(x = \"Religion\")\neconomics_long |> \n  ggplot(aes(x = value01, y = variable)) +\n  geom_boxplot() +\n  ggtitle(\"Unsorted Boxes\")\neconomics_long |> \n  mutate(variable = fct_reorder(variable, value01)) |> \n  ggplot(aes(x = value01, y = variable)) +\n  geom_boxplot() +\n  ggtitle(\"Boxes sorted on Median\")"},{"path":"factors.html","id":"reordering-factors-with-two-other-variables.","chapter":"31 Factors","heading":"31.4.3 Reordering factors with two other variables.","text":"Sometimes, factor variable needs sorted basis first (last) values two variables. fct_reorder2() useful. compared fct_reorder() takes extra argument .y syntax likeHere default function last2 simply means levels .f sorted basis last values .y plotted .x grouped line charts. E.g. See Figure 31.5.\nFigure 31.5: Factors reordered two criteria\n","code":"fct_reorder2(\n  .f,\n  .x,\n  .y,\n  .fun = last2,\n  ...,\n  .na_rm = NULL,\n  .default = -Inf,\n  .desc = TRUE\n)\nlibrary(patchwork)\nlibrary(conflicted)\nconflicts_prefer(dplyr::filter)\n\ndefault <- economics_long |> \n  filter(date < dmy(\"31122003\"), date >= dmy(\"01011995\")) |> \n  ggplot(aes(date, value01)) +\n  geom_line(aes(group = variable, color = variable)) +\n  ggtitle(\"Default legend\")\n\naligned <- economics_long |> \n  filter(date < dmy(\"31122003\"), date >= dmy(\"01011995\")) |> \n  mutate(variable = fct_reorder2(variable, date, value01)) |> \n  ggplot(aes(date, value01)) +\n  geom_line(aes(group = variable, color = variable)) +\n  ggtitle(\"Legend aligned with \\nlast values of each line\")\n\ndefault + aligned"},{"path":"factors.html","id":"changing-orders-of-few-factor-labels-only","chapter":"31 Factors","heading":"31.4.4 Changing orders of few factor labels only","text":"Sometimes, may factor whose levels already meaningfully sorted. E.g. income levels gss_cat. Check plot Figure 31.6.\nFigure 31.6: Default Income levels\nlevels income already sorted meaningful way. However, sometimes may want change order levels . E.g. applicable Figure 31.6 re-leveled end may meaningful. cases, may use fct_relevel. function takes factor variable thereafter may pass levels arguments want move end. rearranging bars (levels), plot look like Figure 31.7\nFigure 31.7: Modifying levels manually\n","code":"\ngss_cat |> \n  ggplot(aes(rincome)) +\n  geom_bar() +\n  coord_flip()\ngss_cat |> \n  ggplot(aes(fct_relevel(rincome, \"Not applicable\"))) +\n  geom_bar() +\n  coord_flip() +\n  labs(x= \"Income levels\")"},{"path":"factors.html","id":"infreq","chapter":"31 Factors","heading":"31.4.5 Ordering bar charts in order of frequency","text":"Function fct_infreq() helpful sorting factor decreasing order frequency thus, can used sort bar charts (Refer charts Figure 31.8).\nFigure 31.8: Default increasing order Frequency\n","code":"\ndefault <- mpg |> \n  ggplot(aes(trans)) +\n  geom_bar()\n\nincreasing <- mpg |> \n  ggplot(aes(fct_infreq(trans))) +\n  geom_bar()\ndefault + increasing"},{"path":"factors.html","id":"reversing-the-factor-levels","chapter":"31 Factors","heading":"31.4.6 Reversing the factor levels","text":"Using fct_rev() can reverse order levels factor. E.g. Figure 31.9.\nFigure 31.9: Reversing levels\n","code":"\nmpg |> \n  mutate(trans = fct_rev(fct_infreq(trans))) |> \n  ggplot(aes(trans)) +\n  geom_bar()"},{"path":"factors.html","id":"other-reordering","chapter":"31 Factors","heading":"31.4.7 Other reordering","text":"two functions can used reorder factor levels -fct_inorder(): order first appear.fct_inseq(): numeric value level.Readers may explore functions .","code":""},{"path":"factors.html","id":"more-on-ordering-factors","chapter":"31 Factors","heading":"31.4.8 More on ordering factors","text":"section 31.1.3 saw unordered factor can turned ordered factor. ordering can cause one side effect plotting ggplot2. Ordered factor use scale_color_viridis default whereas unordered factor doesn’t. See following example (Notice color scale Figure 31.10 Figure 31.11).\nFigure 31.10: Use color scale ordered unordered factors\n\nFigure 31.11: Use color scale ordered unordered factors\n","code":"\nmtcars %>% \n  mutate(cyl = ordered(factor(cyl))) %>% \n  ggplot(aes(wt, mpg)) +\n  geom_point(size = 5, aes(color = cyl)) +\n  ggtitle(\"Ordered Factor\")\nmtcars %>% \n  mutate(cyl = factor(cyl)) %>% \n  ggplot(aes(wt, mpg)) +\n  geom_point(size = 5, aes(color = cyl)) +\n  ggtitle(\"Unordered Factor\")"},{"path":"factors.html","id":"levels-in-factors","chapter":"31 Factors","heading":"31.5 Levels in Factors","text":"","code":""},{"path":"factors.html","id":"modifying-factor-levels-by-applying-a-function","chapter":"31 Factors","heading":"31.5.1 Modifying factor levels by applying a function","text":"Function fct_relabel forcats powerhouse applies function .fun level .f factor supplied . E.g. Change case variable name economics_long (Figure 31.12).\nFigure 31.12: Applying function labels\n","code":"\neconomics_long |> \n  mutate(variable = fct_relabel(variable, str_to_upper)) |> \n  ggplot(aes(x = value01, y = variable)) +\n  geom_boxplot() "},{"path":"factors.html","id":"modifying-factor-levels-manually","chapter":"31 Factors","heading":"31.5.2 Modifying factor levels manually","text":"Using function fct_recode() can change levels given factor manually. provide new levels manually sequence named character vectors name gives new level, value gives old level. Levels otherwise mentioned left . Levels can removed naming NULL. See ExampleTo collapse multiple levels (lumping) one can use cousin fct_collapse(). Example","code":"\nx <- factor(c(\"apple\", \"bear\", \"banana\", \"dear\"))\nfct_recode(x, fruit = \"apple\", fruit = \"banana\")## [1] fruit bear  fruit dear \n## Levels: fruit bear dear\nx <- factor(c(\"apple\", \"bear\", \"banana\", \"dear\"))\nfct_collapse(x, fruit = c(\"apple\", \"banana\"))## [1] fruit bear  fruit dear \n## Levels: fruit bear dear"},{"path":"factors.html","id":"lump-uncommon-factor-levels-into-other","chapter":"31 Factors","heading":"31.5.3 Lump uncommon factor levels into other","text":"Package forcats provides us family 4 functions useful lumping together levels meeting given criteria. arefct_lump_min(): lumps levels appear fewer min times.fct_lump_prop(): lumps levels appear fewer (equal ) prop * n times.fct_lump_n() lumps levels except n frequent (least frequent n < 0)fct_lump_lowfreq() lumps together least frequent levels, ensuring \"\" still smallest level.functions, apart taking factors f argument, also take one argument, fits case-n Positive n preserves common n values. Negative n preserves least common -n values.prop Positive prop lumps values appear least prop time. Negative prop lumps values appear -prop time.min Preserve levels appear least min number times.w optional numeric vector giving weights frequency value (level) f.other_level: Value level used \"\"(default) values. Always placed end levels.examples-\nFigure 31.13: Lumping Factors\nFigure 31.13 can see five religions plus \"\" category placed last. may also notice bars sorted yet.may also make use w argument, already factor counts levels another vector. See figure 31.14 -\nFigure 31.14: Lumping Factors making use w argument\nsort levels increasing order frequency can use fct_infreq() function learnt section 31.4.5. may also take w argument (optionally, course) factor levels already counted. , sort levels Figure 31.14, may make use function one step lumping. See figure 31.15.\nFigure 31.15: Sorting Lumping Factors\n","code":"\n# We can check the religion in `gss_cat` and followers count\ngss_cat %>% \n  count(relig)## # A tibble: 15 × 2\n##    relig                       n\n##    <fct>                   <int>\n##  1 No answer                  93\n##  2 Don't know                 15\n##  3 Inter-nondenominational   109\n##  4 Native american            23\n##  5 Christian                 689\n##  6 Orthodox-christian         95\n##  7 Moslem/islam              104\n##  8 Other eastern              32\n##  9 Hinduism                   71\n## 10 Buddhism                  147\n## 11 Other                     224\n## 12 None                     3523\n## 13 Jewish                    388\n## 14 Catholic                 5124\n## 15 Protestant              10846\n# Let's restrict these to 5 religions only\ngss_cat %>% \n  ggplot(aes(fct_lump_n(relig, n = 5))) +\n  geom_bar()\ngss_cat %>% \n  count(relig) %>% \n  # Making use of `w` argument\n  mutate(relig = fct_lump_n(relig, n = 5, w = n)) %>% \n  ggplot(aes(relig, n)) +\n  geom_bar(stat = \"identity\")\ngss_cat %>% \n  count(relig) %>% \n  # Sorting making use of `w` argument\n  mutate(relig = fct_infreq(relig, w = n),\n         relig = fct_lump_n(relig, n = 5, w = n)) %>% \n  ggplot(aes(relig, n)) +\n  geom_bar(stat = \"identity\")"},{"path":"text-analytics-in-r.html","id":"text-analytics-in-r","chapter":"32 Text Analytics in R","heading":"32 Text Analytics in R","text":"content development/finalisationText analytics crucial data analytics, text data becoming increasingly significant across various applications, including marketing analytics. Text often replacing forms unstructured data cost-effective --date. fully leverage potential text data, need understand process, clean, summarize, model . chapter, use R workhorse tools efficiently begin working text. gain skills wrangling visualizing text, able perform sentiment analysis next chapter. also discuss little running interpreting topic models, thereby highlighting indispensable role text analytics modern data analysis.Text data processing faces unique challenges due complexity variability human language. Unlike structured data, text unstructured highly diverse, encompassing different languages, dialects, slang, abbreviations. variability complicates standardization requires sophisticated preprocessing techniques clean prepare data. Additionally, context-dependent nature language makes accurate interpretation difficult, words phrases can varying meanings based usage.","code":""},{"path":"text-analytics-in-r.html","id":"text-pre-processing","chapter":"32 Text Analytics in R","heading":"Text pre-processing","text":"","code":""},{"path":"text-analytics-in-r.html","id":"tokenisation","chapter":"32 Text Analytics in R","heading":"32.1 Tokenisation","text":"","code":""},{"path":"text-analytics-in-r.html","id":"stop-words","chapter":"32 Text Analytics in R","heading":"32.2 Stop words","text":"","code":""},{"path":"text-analytics-in-r.html","id":"stemming","chapter":"32 Text Analytics in R","heading":"32.3 Stemming","text":"","code":""},{"path":"text-analytics-in-r.html","id":"text-cleaning-vy-removing-punctuation-and-other-unwanted-characterstext","chapter":"32 Text Analytics in R","heading":"32.4 Text Cleaning vy removing punctuation and other unwanted characters/text","text":"","code":""},{"path":"sentiment-analysis.html","id":"sentiment-analysis","chapter":"33 Sentiment Analysis","heading":"33 Sentiment Analysis","text":"content development","code":""},{"path":"visualising-text-analytics-through-wordcloud-etc..html","id":"visualising-text-analytics-through-wordcloud-etc.","chapter":"34 Visualising Text analytics through Wordcloud, etc.","heading":"34 Visualising Text analytics through Wordcloud, etc.","text":"content development following finalised.","code":""},{"path":"visualising-text-analytics-through-wordcloud-etc..html","id":"frequency-plots","chapter":"34 Visualising Text analytics through Wordcloud, etc.","heading":"34.1 Frequency plots","text":"","code":""},{"path":"visualising-text-analytics-through-wordcloud-etc..html","id":"wordclouds","chapter":"34 Visualising Text analytics through Wordcloud, etc.","heading":"34.2 Wordclouds","text":"","code":""},{"path":"visualising-text-analytics-through-wordcloud-etc..html","id":"step-1prepare-data-and-load-libraries","chapter":"34 Visualising Text analytics through Wordcloud, etc.","heading":"34.2.1 Step-1:Prepare data and load libraries","text":"example create word cloud Budget Speech made Finance Minister Budget speech36 2022-23. budget speech available file called budget.txt.Load LibrariesLoad data","code":"\nlibrary(tidyverse)\nlibrary(tidytext) #install.packages(\"tidytext\")\nlibrary(wordcloud) #install.packages(\"wordcloud\")\nlibrary(ggtext)\nlibrary(ggalt)\nlibrary(ggthemes)\nlibrary(ggpubr)\nlibrary(conflicted)\nconflicts_prefer(dplyr::filter)\ndat <- read.table('data/budget.txt', header = FALSE, fill = TRUE)"},{"path":"visualising-text-analytics-through-wordcloud-etc..html","id":"step-2-reshape-the-.txt-data-frame-into-one-column","chapter":"34 Visualising Text analytics through Wordcloud, etc.","heading":"34.2.2 Step-2: Reshape the .txt data frame into one column","text":"steps create one row per line. Let’s create tidy data frame data.","code":"\ntidy_dat <- dat %>% \n  pivot_longer(everything(), values_to = 'word', names_to = NULL)"},{"path":"visualising-text-analytics-through-wordcloud-etc..html","id":"step-3-tokenize-the-datawords","chapter":"34 Visualising Text analytics through Wordcloud, etc.","heading":"34.2.3 Step-3: Tokenize the data/words","text":"tokenize words use function unnest_tokens() tidytext library. step count word, using dplyr::count create column n word.","code":"\ntokens <- tidy_dat %>% \n  unnest_tokens(word, word) %>% \n  count(word, sort = TRUE) "},{"path":"visualising-text-analytics-through-wordcloud-etc..html","id":"step-4-clean-stop-words","chapter":"34 Visualising Text analytics through Wordcloud, etc.","heading":"34.2.4 Step-4: Clean stop words","text":"library tidytext default database can eliminate stop words data. Let’s load default stop words data.may remove stop words using dplyr::anti_join.may remove additional stop words specific data/input. idea stop words, may firt, skip step altogether proceed generate word cloud next step directly. first look, can identify remove additional stop words seen first round(s).","code":"\ndata(\"stop_words\")\ntokens_clean <- tokens %>%\n  anti_join(stop_words, by='word') %>% \n  # remove numbers\n  filter(!str_detect(word, \"^[0-9]\"))\nuni_sw <- data.frame(word = c(\"cent\", \"pm\", \"crore\", \n                              \"lakh\", \"set\",\n                              \"level\", \"sir\"))\n\ntokens_clean <- tokens_clean %>% \n  anti_join(uni_sw, by = \"word\")"},{"path":"visualising-text-analytics-through-wordcloud-etc..html","id":"step-5-plotgenerate-word-cloud","chapter":"34 Visualising Text analytics through Wordcloud, etc.","heading":"34.2.5 Step-5: Plot/generate word cloud","text":"Output/Word cloud following code can seen figure 34.1.\nFigure 34.1: Word Cloud FM’s Budget Speech 2022\n","code":"\npal <- RColorBrewer::brewer.pal(8,\"Dark2\")\n\n# plot the 40 most common words\ntokens_clean %>% \n  with(wordcloud(word, \n                 n, \n                 random.order = FALSE, \n                 max.words = 40, \n                 colors=pal,\n                 scale=c(2.5, .5)))"},{"path":"finding-string-similarity.html","id":"finding-string-similarity","chapter":"35 Finding string similarity","heading":"35 Finding string similarity","text":"Comparison two () numeric fields easy job sense can use multiple statistical methods available measure comparison . hand, comparing strings way, shape form trivial task. Despite complexity, comparing text strings common fundamental task many text-processing algorithms. Basic objective string similarity algorithms quantify similarity two text strings terms string metrics.fuzzy matching problems input two strings return score quantifying likelihood expressions entity. (Geeta Gita) get high score (Apple Microsoft). several decades, various algorithms fuzzy string matching emerged. varying strengths weaknesses. fall two broad categories: lexical matching phonetic matching.","code":""},{"path":"finding-string-similarity.html","id":"lexical-matching","chapter":"35 Finding string similarity","heading":"35.1 Lexical matching","text":"Lexical matching algorithms match two strings based model errors. Typically meant match strings differ due spelling typing errors. Consider Atharv ahtarv. lexical matching algorithm pick ht transposition th. transposition errors common. Given , rest two strings match exactly long enough, score match high.Normally, algorithms find lexical matching, can classified ‘edit distance based’ ‘token based’.","code":""},{"path":"finding-string-similarity.html","id":"levenshtein-algorithm","chapter":"35 Finding string similarity","heading":"35.1.1 Levenshtein algorithm","text":"named Vladimir Levenshtein, considered distance 1965. Levenshtein distance two words minimum number single-character edits (.e. insertions, deletions substitutions) required change one word . Levenshtein distance may also referred edit distance, although may also denote larger family distance metrics. closely related pairwise string alignments.two words helo hello, obvious missing character \"l\". Thus transform word helo hello need insert character. distance, case, 1 one edit needed.","code":""},{"path":"finding-string-similarity.html","id":"hamming-distance","chapter":"35 Finding string similarity","heading":"35.1.2 Hamming distance","text":"distance computed overlaying one string another finding places strings vary. Note, classical implementation meant handle strings length. implementations may bypass adding padding prefix suffix. Nevertheless, logic find total number places one string different .","code":""},{"path":"finding-string-similarity.html","id":"jaro-winkler","chapter":"35 Finding string similarity","heading":"35.1.3 Jaro-Winkler","text":"algorithms gives high scores two strings ,contain characters, within certain distance one another, andthe order matching characters .exact, distance finding similar character 1 less half length longest string. longest strings length 5, character start string 1 must found ((5/2)–1) ~ 2nd position string 2 considered valid match. , algorithm directional gives high score matching beginning strings.","code":""},{"path":"finding-string-similarity.html","id":"q-gram","chapter":"35 Finding string similarity","heading":"35.1.4 Q-Gram","text":"Q-Grams based difference occurences Q consecutive characters two strings. illustrate take case Q=3 (special case also called trigrams). atharv possible typo ahtarv trigrams beFor atharv {ath tha har arv}ahtarv {aht hta tar arv}can see total 7 unique trigrams formed 1 similar. Thus, 3-gram similarility 1/7=14%. can see algorithm effective transpositions.","code":""},{"path":"finding-string-similarity.html","id":"phonetic-matching","chapter":"35 Finding string similarity","heading":"35.2 Phonetic matching","text":"Phonetic matching algorithms match strings based similar sound. Consider Geeta Gita. sound similar enough one person might spell Geetha Geeta, another Gita. case, one necessarily misspelling . just sounds similar.","code":""},{"path":"finding-string-similarity.html","id":"soundex","chapter":"35 Finding string similarity","heading":"35.2.1 Soundex","text":"Created Robert Russel Margaret King Odell 1918, algorithm intended match names surnames based basic rules English pronunciation, hence, similar names get value.","code":""},{"path":"finding-string-similarity.html","id":"metaphone","chapter":"35 Finding string similarity","heading":"35.2.2 Metaphone","text":"Developed Lawrence Philips 1990, Metaphone also accurate compared Soundex method takes consideration groups letters. disadvantage shows apply reconcile strings English, based rules English pronunciation.","code":""},{"path":"finding-string-similarity.html","id":"double-metaphone","chapter":"35 Finding string similarity","heading":"35.2.3 Double Metaphone","text":"Following Metaphone, Philips also designed Double Metaphone. name suggests, returns two codes, chances match items, however, time, means higher probability error. According algorithm, three matching levels:primary key primary key = strongest match,secondary key primary key = normal match,secondary key secondary key = weakest match.","code":""},{"path":"finding-string-similarity.html","id":"metaphone-3","chapter":"35 Finding string similarity","heading":"35.2.4 Metaphone 3","text":"Philips refined double metaphone algorithm produce better results. algorithm (Metaphone 3) however, proprietary open-source.","code":""},{"path":"finding-string-similarity.html","id":"examples","chapter":"35 Finding string similarity","heading":"35.3 Examples","text":"R, can use stringdist package calculate many mentioned distances. function vectorised. synatx iswhere -b two strings/vectors similarity/distance measured.method used. Default \nosa Optimal String Alignment. methods -\nlv Levenstein distance,\ndl Damerau-Levenshtein\nhamming Hamming distance\nlcs Longest Common Substring\nqgram Q-Grams\ncosine cosine\njaccard Jaccard’s algorithm\njw Jaro-Winkler\nsoundex Soundex\nosa Optimal String Alignment. methods -lv Levenstein distance,dl Damerau-Levenshteinhamming Hamming distancelcs Longest Common Substringqgram Q-Gramscosine cosinejaccard Jaccard’s algorithmjw Jaro-Winklersoundex SoundexOther arguments needed basis algorithm chosen.calculate ‘metaphone’ index can use phonics package ‘Double Metaphone’ can use PGRdup package R.Example - Suppose set two names.Note distances/similarity indices cases sensitive, therefore use methods bit cleaning first. can convert cases strings lower-case eliminate () unwanted errors.Creating Metaphone Double MetaphoneNote calculate metaphone special characters even spaces.Double metaphone vectorised. use apply family functions .","code":"stringdist(\n  a,\n  b,\n  method = c(\"osa\", \"lv\", \"dl\", \"hamming\", \"lcs\", \"qgram\", \"cosine\", \"jaccard\", \"jw\",\n    \"soundex\"),\n  useBytes = FALSE,\n  weight = c(d = 1, i = 1, s = 1, t = 1),\n  q = 1,\n  p = 0,\n  bt = 0,\n  nthread = getOption(\"sd_num_thread\")\n)\nnameset1 <- c('Geeta', 'Susheel', 'Ram', 'Dr. Suchitra')\nnameset2 <- c('Gita', 'Sushil', 'Rama', 'Suchitra')\nlibrary(stringdist)\nsuppressPackageStartupMessages(library(dplyr))\n\ndata.frame(\n  nameset1 = tolower(nameset1),\n  nameset2 = tolower(nameset2)\n) %>% \n  mutate(lv_dist = stringdist(nameset1, nameset2, method = 'lv'),\n         jw_dist = stringdist(nameset1, nameset2, method = 'jw'),\n         qgram_3 = stringdist(nameset1, nameset2, method = 'qgram', q=3))##       nameset1 nameset2 lv_dist    jw_dist qgram_3\n## 1        geeta     gita       2 0.21666667       5\n## 2      susheel   sushil       2 0.15079365       5\n## 3          ram     rama       1 0.08333333       1\n## 4 dr. suchitra suchitra       4 0.25694444       4\nlibrary(phonics)\n\ndata.frame(\n  nameset1 = tolower(nameset1),\n  nameset2 = tolower(nameset2)\n) %>% \n  mutate(metaphone_1 = metaphone(nameset1),\n         metaphone_2 = metaphone(nameset2))##       nameset1 nameset2 metaphone_1 metaphone_2\n## 1        geeta     gita          JT          JT\n## 2      susheel   sushil         SXL         SXL\n## 3          ram     rama          RM          RM\n## 4 dr. suchitra suchitra        <NA>        SXTR\nsuppressPackageStartupMessages(library(PGRdup))\nlibrary(purrr)\n\ndata.frame(\n  nameset1 = tolower(nameset1),\n  nameset2 = tolower(nameset2)\n) %>% \n  mutate(DMP_1_1 = map_chr(nameset1, ~DoubleMetaphone(.x)[[1]]),\n         DMP_1_2 = map_chr(nameset1, ~DoubleMetaphone(.x)[[2]]),\n         DMP_2_1 = map_chr(nameset2, ~DoubleMetaphone(.x)[[1]]),\n         DMP_2_2 = map_chr(nameset2, ~DoubleMetaphone(.x)[[2]]))##       nameset1 nameset2 DMP_1_1 DMP_1_2 DMP_2_1 DMP_2_2\n## 1        geeta     gita      JT      KT      JT      KT\n## 2      susheel   sushil     SXL     SXL     SXL     SXL\n## 3          ram     rama      RM      RM      RM      RM\n## 4 dr. suchitra suchitra    TRSX    TRSK    SXTR    SKTR"},{"path":"part-viii-geo-computation-in-r.html","id":"part-viii-geo-computation-in-r","chapter":"Part-VIII: Geo computation in R","heading":"Part-VIII: Geo computation in R","text":"","code":""},{"path":"maps-in-r.html","id":"maps-in-r","chapter":"36 Maps in R","heading":"36 Maps in R","text":"content development","code":""},{"path":"geo-coding-in-r.html","id":"geo-coding-in-r","chapter":"37 Geo-Coding in R","heading":"37 Geo-Coding in R","text":"content development","code":""},{"path":"reverse-geo-coding.html","id":"reverse-geo-coding","chapter":"38 Reverse geo-coding","heading":"38 Reverse geo-coding","text":"","code":""},{"path":"reverse-geo-coding.html","id":"introduction","chapter":"38 Reverse geo-coding","heading":"38.1 Introduction","text":"Geo-coding process converting human-readable address geographic coordinates, latitude longitude. Geocoding process determining location address map. , plotting places geographical map layers require extraction latitudes longitudes master databases.Reverse geo-coding, hand, process converting geographic coordinates (latitude longitude) human-readable address, street address, city, state/province, country. words, ’s process determining location’s address based geographic coordinates.geo-coding reverse geo-coding important geospatial data analysis, allow us link spatial information non-spatial information, demographic data, land use data, forms spatial data. information can used gain insights patterns relationships data, make informed decisions based location information.Reverse geo-coding can used forensic data analysis help identify location transactions events. example, can used determine location ATM point sale device fraudulent transaction occurred. can also used analyze patterns activity particular geographic area, frequency timing certain types transactions. another example, may think application data, mobile hand held devices used capture data time place intended service delivery. Auditors may use geo-spatial data captured devices cross check/verify actual points service delivery.","code":""},{"path":"reverse-geo-coding.html","id":"widely-used-databases","chapter":"38 Reverse geo-coding","heading":"38.2 Widely used databases","text":"OpenStreetMap (OSM) one popular widely used master databases geo-coding reverse geo-coding. OSM free open-source map world, created maintained community volunteers. provides rich detailed set spatial data, including street names, addresses, points interest.popular master databases geo-coding reverse geo-coding include Google Maps. Google Maps provides rich set geo-coding reverse geo-coding APIs widely used web mobile applications. Google Maps provides comprehensive set spatial data, including street names, addresses, points interest.","code":""},{"path":"reverse-geo-coding.html","id":"example-in-r","chapter":"38 Reverse geo-coding","heading":"38.3 Example in R","text":"","code":""},{"path":"reverse-geo-coding.html","id":"prerequisites","chapter":"38 Reverse geo-coding","heading":"38.3.1 Prerequisites","text":"","code":"\nlibrary(tidygeocoder)"},{"path":"reverse-geo-coding.html","id":"function-to-be-used","chapter":"38 Reverse geo-coding","heading":"38.3.2 Function to be used","text":"use reverse_geo() function library. Syntax isHere -lat long , names suggest, latitude longitudes place, address extracted.method argument provides geo service used. Default osm. services available - arcgis, geocodio, google, etc.address provides column name used.full_results default FALSE. set TRUE returns available data geocoding service.arguments, please check help function.","code":"reverse_geo(\n  lat,\n  long,\n  method = \"osm\",\n  address = \"address\",\n  full_results = FALSE\n  ...\n)"},{"path":"reverse-geo-coding.html","id":"practical-data-set","chapter":"38 Reverse geo-coding","heading":"38.3.3 Practical data-set","text":"Remember one thing, function vectorised. use apply family purrr::map family functions get reverse geo-coding information.","code":"\n# Coordinates of tajmahal\nreverse_geo(lat = 27.1751, \n            long = 78.421, \n            full_results = TRUE, \n            method = 'osm')## # A tibble: 1 × 25\n##     lat  long address     place_id licence osm_type osm_id osm_lat osm_lon class\n##   <dbl> <dbl> <chr>          <int> <chr>   <chr>     <int> <chr>   <chr>   <chr>\n## 1  27.2  78.4 Sailai, Fi…   2.13e8 Data ©… way      4.56e8 27.175… 78.421… high…\n## # ℹ 15 more variables: type <chr>, place_rank <int>, importance <dbl>,\n## #   addresstype <chr>, name <chr>, suburb <chr>, town <chr>, county <chr>,\n## #   state_district <chr>, state <chr>, `ISO3166-2-lvl4` <chr>, postcode <chr>,\n## #   country <chr>, country_code <chr>, boundingbox <list>"},{"path":"reverse-geo-coding.html","id":"using-it-in-a-dataset","chapter":"38 Reverse geo-coding","heading":"38.3.4 Using it in a dataset","text":"Let’s build sample dataset say 4 values/places interest IndiaNow extracting information using purrr::map family","code":"\nlibrary(dplyr, warn.conflicts = FALSE)\nlibrary(purrr, warn.conflicts = FALSE)\n\nresults <- pmap_dfr(sample,\n     ~ reverse_geo(..1, # first column in sample\n                   ..2, # second column in sample\n                   full_results = TRUE)) %>% \n  select(lat, long, country, state, city, village, address, postcode)"},{"path":"reverse-geo-coding.html","id":"view-above-results","chapter":"38 Reverse geo-coding","heading":"38.3.5 View above results","text":"","code":""},{"path":"reverse-geo-coding.html","id":"alternate-syntax-for-those-who-do-not-want-to-use-purrrpmap_dfr","chapter":"38 Reverse geo-coding","heading":"38.3.6 Alternate syntax (for those who do not want to use purrr::pmap_dfr)","text":"","code":"sapply(seq(nrow(sample)),\n       function(x) reverse_geo(sample$lat[x], sample$long[x], full_results = TRUE)) %>% \n  map_dfr(rbind)"},{"path":"part-ix-identifying-anamolous-observations-for-audit.html","id":"part-ix-identifying-anamolous-observations-for-audit","chapter":"Part-IX: Identifying anamolous observations for audit","heading":"Part-IX: Identifying anamolous observations for audit","text":"","code":""},{"path":"benford-testsanalysis.html","id":"benford-testsanalysis","chapter":"39 Benford Tests/Analysis","heading":"39 Benford Tests/Analysis","text":"","code":""},{"path":"benford-testsanalysis.html","id":"introduction-and-historical-context","chapter":"39 Benford Tests/Analysis","heading":"39.1 Introduction and Historical context","text":"Benford’s Law stands analysis method visualizing evaluating numerical data, especially focused detecting fraud. ’s really handy catching potential trickery, especially spotting fraud. rule tells us often different digits (like 1, 2, 3, etc.) show first number lots real-world data. law describes frequency distribution first digit, left hand side, many real-life data sets counter-intuitively uniform, shown Figure 39.2. Significant differences anticipated occurrence rates signal data questionable might altered. instance, eligibility government assistance often hinges meeting specific criteria, like income certain level. result, data might manipulated meet criteria. kind manipulation precisely Benford’s Law can detect since fabricated numbers won’t align expected frequency pattern outlined law.Law named physicist Frank Benford, worked theory 1938 result paper titled Law Anomalous Numbers published.37. However, discovery associated five decades earlier astronomer Simon Newcomb observed initial pages log tables booklet worn later pages published two page article titled Note Frequency Use Different Digits Natural Numbers 1881.38More researchers continued work Benford’s law extensions. However, took several decades find truly practical application. last decade twentieth Century, Dr. Mark J. Nigrini, accounting Professor, used law fraud detection/anaytics came practical fraud application. reviewed multiple data sources like sales figures, insurance claim costs, expense reimbursement claims studies detecting overstatements understatement financial figures. research confirmed law’s proven usefulness fraud examiners auditors accounting engagements.theory - somebody tries falsify, say, tax return invariably invent data. trying , tendency people use many numbers starting digits mid range, 5,6,7 thus enough numbers starting 1.\nFigure 39.1: (L R) Frank Benford, Simon Newcomb, Mark Nigrini (Source: Wiki)\n","code":""},{"path":"benford-testsanalysis.html","id":"benfords-law-properties-and-extensions","chapter":"39 Benford Tests/Analysis","heading":"39.2 Benford’s Law, properties and extensions","text":"","code":""},{"path":"benford-testsanalysis.html","id":"law-of-first-digit","chapter":"39 Benford Tests/Analysis","heading":"39.2.1 Law of first digit","text":"considering likelihood digit first position (left), initial assumption might simple one nine scenario, following uniform distribution. However, notion challenged Canadian-American astronomer Simon Newcomb 1881, noticed unusual wear patterns logarithmic tables. casually flipping logarithmic tables booklet, discerned curious pattern— initial pages exhibited wear tear later counterparts.Subsequently, Frank Benford conducted comprehensive analysis 20 diverse datasets encompassing river sizes, chemical compound weights, population data, . findings revealed successive diminishment probability digit 1 9. essence, probability digit 1 occurring initial position highest, digit 9 lowest.Mathematically, Benford’s Law Law first digits states probability digit first place follow equation (39.1).\\(d_i\\) ranges \\(1\\) \\(9\\).probabilities plotted generate plot depicted Figure 39.2.\nFigure 39.2: Diminishing Probabilities First Digits - Benford Law\ntest proposed law, Benford analysed 20 different data-sets observed nearly follow distribution mentioned equation (39.1).Let us also try see whether law holds anlaysing six different datasets, included R’s package called benford.analysis. Though discuss package detail later section 39.5. six datasets mentioned Table 39.1.\nTable 39.1: Table 39.2: List six datasets testing Benford Analysis\nresults Benford’s law first digit six datasets calculated mentioned Table 39.3. can seen actual frequencies first digits, six datasets follow Benford’s Law. can even plot actual frequencies inspect results visually. Actual Frequencies six datasets plotted 39.3 may seen follow Benford’s Law largely.\nTable 39.3: Table 39.4: Results First order tests six datasets\n\nFigure 39.3: Distribution first digit frequencies six datasets\n","code":""},{"path":"benford-testsanalysis.html","id":"scale-invariance","chapter":"39 Benford Tests/Analysis","heading":"39.2.2 Scale Invariance","text":"Later 1961, Roger Pinkham showed law invariant scaling.39 Scale Invariance, actually showed law invariant measurements units. words, law still holds convert units one unit another. example, price amount figures measured either USD INR, length measured either KMs Miles, digit frequencies still follow Benford’s Law.Let us check one six datasets mentioned , namely census.2009. data-set contains figures population towns cities United States, July 2009. can see first digit frequencies follow Benford’s Law/Pinkham’s Corollary Figure 39.4. Left plot shows frequencies original data whereas right plot shows randonly scaled data.\nFigure 39.4: First Digit Analysis US Census 2009 data (Left) Scaled Data (Right)\nFigure 39.4 (Left) shows law holds data. Let us also test Pinkham’s corollary aforesaid data. let’s multiply figures population random positive number. figure 39.4 (Right) clear law still holds scaling.","code":""},{"path":"benford-testsanalysis.html","id":"first-two-digits","chapter":"39 Benford Tests/Analysis","heading":"39.2.3 First two digits","text":"Nigrini’s contributions gained widespread recognition among scholars practitioners, highlighting applicability Benford’s Law valuable forensic accounting auditing tool across various datasets, particularly financial domain. Theodore P. Hill further40 extended scope law, demonstrating validity beyond just first digit encompass digits well. Hill’s work expanded utility Benford’s Law, affirming effectiveness detecting irregularities patterns leading digits throughout numerical sequences.formula second significant digit can written equation (39.2).\\(k\\) represents first digit,\\(d_i\\) represents second digit.probabilities calculated, depicted Table 39.5. cell depicts probability occurrence two digit, left side, first digit rows second digit columns. may also verify , row totals thereby depicting probability occurrence first digit corresponds Benford’s Law First Digit. example, probability first two digits 10 highest 4.14%.\nTable 39.5: Table 39.6: First Second Digit distributions\nlaw second digit combined original Benford’s Law first digit thus, gives us Law first two digits. can verify example census.2009 data. resultant plot depicted figure 39.5 shows us law first two digits also holds.\nFigure 39.5: Law holds first two digits well\n","code":""},{"path":"benford-testsanalysis.html","id":"second-order-test","chapter":"39 Benford Tests/Analysis","heading":"39.2.4 Second order test","text":"Nigrini Miller, 2009,41 introduced another advanced test based Benford’s Law. test states :Let \\(x_1\\), …, \\(x_N\\) data set comprising \\(N\\) observations, let \\(y_1\\), …, \\(y_N\\) observations \\(x_i\\)’s ascending order. , many natural data sets, large \\(N\\), digits differences adjacent observations \\(y_{+1} – y_i\\) close Benford’s Law. Large deviations Benford’s Law indicate anomaly investigated., steps may listed asSort data smallest largestcalculate \\(N-1\\) differences \\(N\\) consecutive observationsApply Benford’s law calculated new data.Nigrini showed digits expected closely follow frequencies Benford law. Using four different datasets showed test can detect () anomalies occurring data, (ii) whether data rounded (iii) use fake data ‘statistically generated data’ place actual (transactional) data.","code":""},{"path":"benford-testsanalysis.html","id":"summation-test","chapter":"39 Benford Tests/Analysis","heading":"39.2.5 Summation Test","text":"summation test, another second order test, looks excessively large numbers dataset. identifies numbers large compared norm data. test also proposed Nigrini42 based fact sums numbers Benford distribution first-two digits (10, 11, 12, …99) . Therefore, 90 first-two digits groups sum proportions equal, .e. 1/90 0.011. spikes, indicate large single numbers set numbers.next section, see implement tests R.","code":""},{"path":"benford-testsanalysis.html","id":"limitations-of-benford-tests","chapter":"39 Benford Tests/Analysis","heading":"39.2.6 Limitations of Benford Tests","text":"Benford’s Law may hold following circumstances-data-set comprised assigned numbers. Like cheque numbers, invoices numbers, telephone numbers, pincodes, etc.Numbers may influenced viz. ATM withdrawals, etc.amounts either lower bound, upper bounds . E.g. passengers onboard airplane, hourly wage rate, etc.Count transactions less 500.carrying analyics let us also see evaluation metrics help us evaluate goodness fit data Benford’s law. Three statistics commonly used.","code":""},{"path":"benford-testsanalysis.html","id":"goodness-of-fit-metrics","chapter":"39 Benford Tests/Analysis","heading":"39.3 Goodness of fit metrics","text":"table 39.3 saw digit frequencies largely followed Benford’s Law six different datasets. However, evaluate close actual distribution theoretical distribution, need evaluate fit metrics. use three different metrics follows.","code":""},{"path":"benford-testsanalysis.html","id":"chis","chapter":"39 Benford Tests/Analysis","heading":"39.3.1 Chi-square statistic","text":"first test, use Chi Square Statistic. statistic used test statistical significance whole distribution observed frequency first digit first two digits expected frequency Benford’s Law (BL). Null hypothesis states digits follow Benford’s Law. Mathematical formula ,-\\(O_i\\) observed frequency -th digit.\\(E_i\\) expected frequency -th digit predicted Benford’s Law.calculated chi-square statistic compared critical value. critical value Chi-Square Test, comes chi-square distribution available easily Statistical textbook43. However, first digit test first two digits test, critical values reproduced Table 39.7.\nTable 39.7: Table 39.8: Critical values Chi-Square Test\ncheck goodness fit, compare calculated \\(χ^2\\) statistic critical values. observed value critical values may conclude initial hypothesis data follows BL, rejected. simply data conforms Benford law/Distribution.example, census.2009 data chi-square statistic calculates 17.524 less 2.5% critical value 17.535. Thus, can say 5% confidence census.2009 data follows BL (first digit law).","code":""},{"path":"benford-testsanalysis.html","id":"z-score","chapter":"39 Benford Tests/Analysis","heading":"39.3.2 Z-score","text":"Z-statistic checks whether individual distribution significantly differs Benford’s Law distribution. Mathematically, Z-Statistics considers absolute magnitude difference actual expected, size data expected proportion.-\\(p\\) observed frequency leading digits dataset.\\(p_0\\) expected frequency Benford’s Law.\\(n\\) number recordsIn equation (39.4), last term numerator \\(\\frac{1}{2N}\\) continuity correction term used smaller first term numerator. Mark Nigrini proposed values Z-statistic exceed critical value 1.96, null hypothesis \\(H_{0A}\\) rejected 5% significance level. Also note Null hypothesis , states digits follow Benford’s Law.significant levels 1% 10%, corresponding critical values 2.57 1.64 respectively.","code":""},{"path":"benford-testsanalysis.html","id":"mean-absolute-deviation","chapter":"39 Benford Tests/Analysis","heading":"39.3.3 Mean absolute deviation","text":"Another Statistic, Mean Absolute Deviation also sometimes referred M..D., measures absolute deviations observed frequencies theoritical ones. mathematical formula written equation (39.5).\\[\\begin{equation}\nMAD = \\frac{1}{9} \\sum_{=1}^{9} |O_i - E_i|\n\\tag{39.5}\n\\end{equation}\\]objective critical scores absolute deviations, critical values prescribed Mark J Nigrini given table 39.9 .Table 39.9:  Critical Scores MAD test","code":""},{"path":"benford-testsanalysis.html","id":"other-descriptive-statistics","chapter":"39 Benford Tests/Analysis","heading":"39.3.4 Other descriptive Statistics","text":"data follows Benford’s Law, numbers close shown table 39.10 following, suggested Mark Nigrini.Table 39.10:  Ideal Statistics data follows Benford’s Law","code":""},{"path":"benford-testsanalysis.html","id":"important","chapter":"39 Benford Tests/Analysis","heading":"39.4 Important","text":"Benford’s Law analysis serves powerful tool uncovering potential irregularities datasets, ’s crucial note deviations statistical phenomenon don’t always signify fraudulent activities. highlights notable discrepancies expected observed frequencies digits naturally occurring datasets, variations might stem various legitimate factors data entry errors, fluctuations processes, different sources data. Understanding Benford’s Law offers signal rather definitive confirmation fraud allows nuanced interpretation, encouraging investigation discern true nature behind deviations.Conversely, just dataset adheres Benford’s Law, doesn’t guarantee absence fraud. conformity statistical principle generally suggests consistency within data, sophisticated fraudsters might deliberately manipulate information mimic expected distributions, masking illicit activities. Therefore, adherence Benford’s Law might lessen suspicion, doesn’t serve absolute assurance fraudulent behavior.Benford’s Law acting warning signal indicates potential irregularities numbers. ’s vital dive deeper investigate figures seem odd. scrutiny helps differentiate minor data hiccup potentially significant issue. additional examination might mean cross-checking data, validating records, engaging connected information. thorough approach crucial unraveling story behind uncommon figures.","code":""},{"path":"benford-testsanalysis.html","id":"pracben","chapter":"39 Benford Tests/Analysis","heading":"39.5 Practical approach in R","text":"already stated use package benford.analysis carrying analytics Benford’s Law, R. Let us load .package provides tools make easier validate data using Benford’s Law. package developed Carlos Cinelli. package author states main purpose package identify suspicious data need verification, always kept mind analytics provide us red-flagged transactions validated .Apart useful functions package, also loads default datasets specially used Frank Benford proposing law. Let us load census 2009 data containing population towns cities United States, July 2009.Let us view top 6 rows data.fact, contains 19509 records.Problem Statement: Let us test Benford’s law 2009 population data. Let us see whether data conforms Benford’s law.main function benford() takes vector values tested input, creates output special class benford syntax iswhere-data numeric vector analysis performed.number..digits number digits analysis performed. Default value 2.syntax create census_first_digit object store various useful information Benford Analytics. may view summary -Let us also print object see stored therein.Results Chi-Square distribution test, MAD etc. printed apart top deviations. MAD value 0.003 shows close conformity Benford’s law. Chi Square statistic 17.524 slightly greater 5% critical value 15.507. second example see results print command benford object can customised, using arguments.Let us also visualise plots. use plot command generate plots.\nFigure 39.6: Benford Analysis Results Census 2009 Data\ncan see default five charts printed.Digits distributionSecond Order Test digit distributionSummation test - digit distributionChi-Square differencesSummation differencesSimilarly, second example see customise plot outputs.can see first digits census 2009 data, follows Benford’s Law closely.","code":"\nlibrary(benford.analysis)\ndata(\"census.2009\")\nhead(census.2009)##     state             town pop.2009\n## 1 Alabama   Abbeville city     2930\n## 2 Alabama  Adamsville city     4782\n## 3 Alabama     Addison town      709\n## 4 Alabama       Akron town      433\n## 5 Alabama   Alabaster city    29861\n## 6 Alabama Albertville city    20115benford(data, number.of.digits=2)\ncensus_first_digit <- benford(census.2009$pop.2009, number.of.digits = 1)\nsummary(census_first_digit)##                   Length Class      Mode     \n## info               4     -none-     list     \n## data               4     data.table list     \n## s.o.data           2     data.table list     \n## bfd               13     data.table list     \n## mantissa           2     data.table list     \n## MAD                1     -none-     numeric  \n## MAD.conformity     1     -none-     character\n## distortion.factor  1     -none-     numeric  \n## stats              2     -none-     list\nprint(census_first_digit)## \n## Benford object:\n##  \n## Data: census.2009$pop.2009 \n## Number of observations used = 19509 \n## Number of obs. for second order = 7950 \n## First digits analysed = 1\n## \n## Mantissa: \n## \n##    Statistic  Value\n##         Mean  0.503\n##          Var  0.084\n##  Ex.Kurtosis -1.207\n##     Skewness -0.013\n## \n## \n## The 5 largest deviations: \n## \n##   digits absolute.diff\n## 1      1        134.79\n## 2      2        104.64\n## 3      3         95.43\n## 4      6         63.94\n## 5      8         45.07\n## \n## Stats:\n## \n##  Pearson's Chi-squared test\n## \n## data:  census.2009$pop.2009\n## X-squared = 17.524, df = 8, p-value = 0.0251\n## \n## \n##  Mantissa Arc Test\n## \n## data:  census.2009$pop.2009\n## L2 = 4.198e-05, df = 2, p-value = 0.4409\n## \n## Mean Absolute Deviation (MAD): 0.003119261\n## MAD Conformity - Nigrini (2012): Close conformity\n## Distortion Factor: 0.7404623\n## \n## Remember: Real data will never conform perfectly to Benford's Law. You should not focus on p-values!\nplot(census_first_digit)"},{"path":"benford-testsanalysis.html","id":"other-useful-functions-in-package","chapter":"39 Benford Tests/Analysis","heading":"39.5.1 Other Useful functions in package","text":"may wondering whether depend upon print function every time get analytics insights object created. fact several functions package useful carrying risk analysis Benford’s Law.chisq: Gets Chi-squared test Benford object. Takes benford object input.duplicatesTable Shows duplicates data. Similarly, takes benford object input.extract.digits Extracts leading digits data. Takes data input. useful, carrying analysis manually.getBfd Gets statistics first Digits benford object. E.g.getSuspects Gets ‘suspicious’ observations according Benford’s Law. Takes data well benford object, inputs. Example second case study.MAD Gets MAD Benford object.suspectsTable Shows first digits ordered mains discrepancies Benford’s Law. Notice difference getSuspects","code":"\ngetBfd(census_first_digit)##    digits  data.dist data.second.order.dist benford.dist\n##     <int>      <num>                  <num>        <num>\n## 1:      1 0.29412066             0.55811321   0.30103000\n## 2:      2 0.18145471             0.15471698   0.17609126\n## 3:      3 0.12004716             0.08968553   0.12493874\n## 4:      4 0.09467425             0.05761006   0.09691001\n## 5:      5 0.07991184             0.04364780   0.07918125\n## 6:      6 0.07022400             0.03308176   0.06694679\n## 7:      7 0.05976729             0.02553459   0.05799195\n## 8:      8 0.05346250             0.01987421   0.05115252\n## 9:      9 0.04633759             0.01773585   0.04575749\n##    data.second.order.dist.freq data.dist.freq benford.dist.freq\n##                          <num>          <num>             <num>\n## 1:                        4437           5738         5872.7942\n## 2:                        1230           3540         3435.3644\n## 3:                         713           2342         2437.4298\n## 4:                         458           1847         1890.6174\n## 5:                         347           1559         1544.7469\n## 6:                         263           1370         1306.0649\n## 7:                         203           1166         1131.3649\n## 8:                         158           1043          997.9346\n## 9:                         141            904          892.6829\n##    benford.so.dist.freq data.summation abs.excess.summation difference\n##                   <num>          <num>                <num>      <num>\n## 1:            2393.1885       51237849             29880783 -134.79419\n## 2:            1399.9255       33272136             11915070  104.63563\n## 3:             993.2630       22810354              1453288  -95.42981\n## 4:             770.4346       15763499              5593567  -43.61744\n## 5:             629.4909       15799838              5557228   14.25307\n## 6:             532.2270       14527377              6829689   63.93508\n## 7:             461.0360       11371006              9986060   34.63511\n## 8:             406.6626       18814056              2543010   45.06544\n## 9:             363.7720        8617475             12739591   11.31712\n##    squared.diff absolute.diff\n##           <num>         <num>\n## 1:    3.0938378     134.79419\n## 2:    3.1870315     104.63563\n## 3:    3.7362508      95.42981\n## 4:    1.0062752      43.61744\n## 5:    0.1315102      14.25307\n## 6:    3.1297790      63.93508\n## 7:    1.0603039      34.63511\n## 8:    2.0350972      45.06544\n## 9:    0.1434744      11.31712"},{"path":"benford-testsanalysis.html","id":"example-2-corporate-payments-data","chapter":"39 Benford Tests/Analysis","heading":"39.5.2 Example-2: Corporate payments data","text":"Problem Statement-2: Let us analyse red-flags, dataset 2010’s payments data (189470 records) division West Coast utility company. data, corporate.payments also available package. time use first two digits analysis.Step-1: Load dataset view top rows. Let’s also see summary.can see 189470 records havingStep-2: Create benford objectStep-3: Let us first visually inspect results. time use another argument plot function benford.analysis library except. Actually can create seven different plots default creates five plots stated earlier. Thus, writing except = \"none\" can include seven plots want. Otherwise mention exclusions c(\"digits\", \"second order\", \"summation\", \"mantissa\", \"chi squared\", \"abs diff\", \"ex summation\"). one argument namely multiple TRUE default plots multiple charts window.let us build () Digit distribution (ii) Second order digit distribution plots.\nFigure 39.7: Benford Analysis results Corporate payments Data\ncan see largely data follows Benford’s Law except abnormal peak 50.Step-4: Let us now see inside object. Function print benford.analysis package another argument .many simply tells us print many absolute differences.can see digit 50 indeed largest abolute difference. One reasons availability invoices digit group may due tax capping reason, auditor may need investigate .Using suspectsTable() can also get similar information.Step-5: Let us also get Chi Square metricsGoing strictly numbers p-value, depend upon Benford Analytics, see Null hypothesis (Ref: section 39.3.1) rejected. words, chi-square statistic tells us data follow Benford Law.get Mean Absolute DeviationWhether value conforms values suggested Mark Nigrini, can doStep-6: Let us generate duplicate values avilable , data. sake brevity , print top-5 results.Examining output , can see 6022 invoices amount USD50 . Probably reason failing null hypothesis data.Step-7: can extract distribution data using getBFD function.Step-8: get suspected/high risk records, may make use getSuspects function. already stated requires benford object data inputs.Moreover, using slice_max function dplyr can also get n high-valued ‘suspects’.","code":"\ndata(\"corporate.payment\")\nhead(corporate.payment)##   VendorNum       Date  InvNum Amount\n## 1      2001 2010-01-02 0496J10  36.08\n## 2      2001 2010-01-02 1726J10  77.80\n## 3      2001 2010-01-02 2104J10  34.97\n## 4      2001 2010-01-02 2445J10  59.00\n## 5      2001 2010-01-02 3281J10  59.56\n## 6      2001 2010-01-02 3822J10  50.38\nsummary(corporate.payment)##   VendorNum              Date               InvNum              Amount        \n##  Length:189470      Min.   :2010-01-02   Length:189470      Min.   :  -71388  \n##  Class :character   1st Qu.:2010-02-28   Class :character   1st Qu.:      50  \n##  Mode  :character   Median :2010-06-04   Mode  :character   Median :     200  \n##                     Mean   :2010-06-16                      Mean   :    2588  \n##                     3rd Qu.:2010-09-30                      3rd Qu.:     835  \n##                     Max.   :2010-12-31                      Max.   :26763476+   Vendor Numbers\n+   Date of Transaction\n+   Invoice Number\n+   Amount of invoice/transaction\ncorp_bfd <- benford(corporate.payment$Amount, number.of.digits = 2)\nplot(\n  corp_bfd,\n  except = c(\n    \"summation\",\n    \"mantissa\",\n    \"chi squared\",\n    \"abs diff\",\n    \"ex summation\",\n    \"chisq diff\",\n    \"legend\"\n  ),\n  multiple = TRUE\n)\nprint(corp_bfd, how.many = 7)## \n## Benford object:\n##  \n## Data: corporate.payment$Amount \n## Number of observations used = 185083 \n## Number of obs. for second order = 65504 \n## First digits analysed = 2\n## \n## Mantissa: \n## \n##    Statistic  Value\n##         Mean  0.496\n##          Var  0.092\n##  Ex.Kurtosis -1.257\n##     Skewness -0.002\n## \n## \n## The 7 largest deviations: \n## \n##   digits absolute.diff\n## 1     50       5938.25\n## 2     11       3331.98\n## 3     10       2811.92\n## 4     14       1043.68\n## 5     98        889.95\n## 6     90        736.81\n## 7     92        709.01\n## \n## Stats:\n## \n##  Pearson's Chi-squared test\n## \n## data:  corporate.payment$Amount\n## X-squared = 32094, df = 89, p-value < 2.2e-16\n## \n## \n##  Mantissa Arc Test\n## \n## data:  corporate.payment$Amount\n## L2 = 0.0039958, df = 2, p-value < 2.2e-16\n## \n## Mean Absolute Deviation (MAD): 0.002336614\n## MAD Conformity - Nigrini (2012): Nonconformity\n## Distortion Factor: -1.065467\n## \n## Remember: Real data will never conform perfectly to Benford's Law. You should not focus on p-values!\nsuspectsTable(corp_bfd) |> \n  head(7)##    digits absolute.diff\n##     <int>         <num>\n## 1:     50     5938.2544\n## 2:     11     3331.9798\n## 3:     10     2811.9177\n## 4:     14     1043.6833\n## 5:     98      889.9470\n## 6:     90      736.8084\n## 7:     92      709.0129\nchisq(corp_bfd)## \n##  Pearson's Chi-squared test\n## \n## data:  corporate.payment$Amount\n## X-squared = 32094, df = 89, p-value < 2.2e-16\nMAD(corp_bfd)## [1] 0.002336614\ncorp_bfd$MAD.conformity## [1] \"Nonconformity\"\nduplicatesTable(corp_bfd) |> \n  head(5)##     number duplicates\n##      <num>      <int>\n## 1:   50.00       6022\n## 2: 1153.35       2264\n## 3: 1083.45       1185\n## 4:  150.00       1056\n## 5:  988.35       1018\ngetBfd(corp_bfd) |> \n  head(10)##     digits  data.dist data.second.order.dist benford.dist\n##      <int>      <num>                  <num>        <num>\n##  1:     10 0.05658542            0.374786273   0.04139269\n##  2:     11 0.05579119            0.015922692   0.03778856\n##  3:     12 0.03236926            0.014609795   0.03476211\n##  4:     13 0.03116440            0.013266365   0.03218468\n##  5:     14 0.02432422            0.011113825   0.02996322\n##  6:     15 0.03038637            0.011510747   0.02802872\n##  7:     16 0.02385416            0.010365779   0.02632894\n##  8:     17 0.02179563            0.009129213   0.02482358\n##  9:     18 0.02085011            0.009358207   0.02348110\n## 10:     19 0.02043408            0.008106375   0.02227639\n##     data.second.order.dist.freq data.dist.freq benford.dist.freq\n##                           <num>          <num>             <num>\n##  1:                       24550          10473          7661.082\n##  2:                        1043          10326          6994.020\n##  3:                         957           5991          6433.875\n##  4:                         869           5768          5956.838\n##  5:                         728           4502          5545.683\n##  6:                         754           5624          5187.640\n##  7:                         679           4415          4873.039\n##  8:                         598           4034          4594.423\n##  9:                         613           3859          4345.952\n## 10:                         531           3782          4122.982\n##     benford.so.dist.freq data.summation abs.excess.summation difference\n##                    <num>          <num>                <num>      <num>\n##  1:             2711.386       28701407             23224143  2811.9177\n##  2:             2475.302       22324748             16847484  3331.9798\n##  3:             2277.057       16258127             10780863  -442.8749\n##  4:             2108.225       15520165             10042901  -188.8378\n##  5:             1962.711       27393259             21915996 -1043.6833\n##  6:             1835.994       49191988             43714724   436.3597\n##  7:             1724.651       12523174              7045911  -458.0390\n##  8:             1626.044       11994778              6517515  -560.4233\n##  9:             1538.106        7545939              2068675  -486.9517\n## 10:             1459.193        6987397              1510133  -340.9820\n##     squared.diff absolute.diff\n##            <num>         <num>\n##  1:  1032.084049     2811.9177\n##  2:  1587.368773     3331.9798\n##  3:    30.485235      442.8749\n##  4:     5.986347      188.8378\n##  5:   196.418497     1043.6833\n##  6:    36.704517      436.3597\n##  7:    43.053153      458.0390\n##  8:    68.359901      560.4233\n##  9:    54.561565      486.9517\n## 10:    28.200147      340.9820\n# We are printing 10 records only\ngetSuspects(corp_bfd, corporate.payment) |> \n  head(10)##     VendorNum       Date      InvNum  Amount\n##        <char>     <Date>      <char>   <num>\n##  1:      2001 2010-01-02     3822J10   50.38\n##  2:      2001 2010-01-07    100107-2 1166.29\n##  3:      2001 2010-01-08 11210084007 1171.45\n##  4:      2001 2010-01-08     1585J10   50.42\n##  5:      2001 2010-01-08     4733J10  113.34\n##  6:      2001 2010-01-08     6263J10  117.22\n##  7:      2001 2010-01-08     6673J10   50.80\n##  8:      2001 2010-01-08     9181J10  114.78\n##  9:      2001 2010-01-09     1510J10   50.49\n## 10:      2001 2010-01-09     1532J10   50.45\ngetSuspects(corp_bfd, corporate.payment) |>\n  slice_max(order_by = Amount, n = 10, with_ties = FALSE)##     VendorNum       Date                InvNum    Amount\n##        <char>     <Date>                <char>     <num>\n##  1:      2817 2010-10-27                10-10A 1156428.2\n##  2:     17141 2010-04-05                040510 1135003.6\n##  3:      2817 2010-11-30            1033500002 1112304.3\n##  4:     16721 2010-09-16 SEE ATTACHED BALSHEET 1100000.0\n##  5:      6118 2010-12-17             103511001  509093.7\n##  6:      2817 2010-05-28                 40821  506971.5\n##  7:     17284 2010-03-24                032400  504580.6\n##  8:      6118 2010-08-26             102381001  504334.6\n##  9:     17284 2010-03-10                 31000  502132.2\n## 10:      2088 2010-03-24            1008300003  500000.0"},{"path":"benford-testsanalysis.html","id":"conclusion","chapter":"39 Benford Tests/Analysis","heading":"Conclusion","text":"Though statistics (goodness fit metrics), data conform BL, yet observed abnormally high records starting digits 50. reasons can investigated. charts also observed , otherwise data conform BL. also extracted suspected records investigation parameters/tests/verification. sum , can say , Benford Analysis can good starting point fraud/forensic analytics auditing. closing, let us also delve one example.","code":""},{"path":"benford-testsanalysis.html","id":"example-3-lakes-perimeter","chapter":"39 Benford Tests/Analysis","heading":"39.5.3 Example-3: Lakes Perimeter","text":"Let us apply lakes.perimeter44 data available package.Let us see plots, metrics top outliers\nFigure 39.8: Benford Analysis - lake Perimeter Data\n","code":"\n# load sample data\ndata(lakes.perimeter) \n# Number of rows\nnrow(lakes.perimeter)## [1] 248607\n# View top rows\nhead(lakes.perimeter)##   perimeter.km\n## 1          1.0\n## 2          1.0\n## 3          1.1\n## 4          1.1\n## 5          1.1\n## 6          1.1\n# Generate Benford Object\nlake_ben <- benford(lakes.perimeter$perimeter.km, number.of.digits = 2)\nplot(lake_ben)\n# Chisq test\nchisq(lake_ben)## \n##  Pearson's Chi-squared test\n## \n## data:  lakes.perimeter$perimeter.km\n## X-squared = 88111, df = 89, p-value < 2.2e-16\n# MAD\nMAD(lake_ben)## [1] 0.006012766\n# Whether it conforms?\nlake_ben$MAD.conformity## [1] \"Nonconformity\"\n# Get top-10 suspects\ngetSuspects(lake_ben, lakes.perimeter) |>\n  head(10)##     perimeter.km\n##            <num>\n##  1:          1.5\n##  2:          1.5\n##  3:          1.5\n##  4:          1.5\n##  5:          1.5\n##  6:          1.5\n##  7:          1.5\n##  8:          1.5\n##  9:          1.5\n## 10:          1.5\n# Get top-10 suspects on Squared Differences\ngetSuspects(lake_ben, lakes.perimeter, \n            by = \"squared.diff\") |>\n  head(10)##     perimeter.km\n##            <num>\n##  1:          3.6\n##  2:          3.6\n##  3:          3.6\n##  4:          3.6\n##  5:          3.6\n##  6:          3.6\n##  7:          3.6\n##  8:          3.6\n##  9:          3.6\n## 10:          3.6\n# Get top-10 suspects on Absolute Excess Summation\ngetSuspects(lake_ben, lakes.perimeter, \n            by = \"abs.excess.summation\") |>\n  head(10)##     perimeter.km\n##            <num>\n##  1:          1.0\n##  2:          1.0\n##  3:          1.3\n##  4:          1.3\n##  5:          1.3\n##  6:          1.3\n##  7:          1.3\n##  8:          1.3\n##  9:          1.3\n## 10:          1.3"},{"path":"benford-testsanalysis.html","id":"conclusion-1","chapter":"39 Benford Tests/Analysis","heading":"Conclusion","text":"observed data conform Benford’s law evident plot well MAD value. Chi-Squared Value 88111 also exceeds critical value significantly. Nigrini Miller gave plausible explanations Research paper45 non-conformity. One possible reasons, propose, perimeter correct measurement size lake.","code":""},{"path":"benford-testsanalysis.html","id":"conclusion-2","chapter":"39 Benford Tests/Analysis","heading":"39.6 Conclusion","text":"conclude chapter Benford Analytics, ’s clear statistical phenomenon holds remarkable potential across diverse fields. inherent simplicity Benford’s Law belies complexity applicability. ability unveil anomalies, authenticate data integrity, aid forensic investigations underscores significance modern data analysis. delve deeper intricacies practical applications, unravel tool scrutinizes numbers also illuminates new avenues precision, authenticity, trust data-driven world.Reading-ISACA JOURNAL ARCHIVES - Understanding Applying Benford’s Law - 1 May 2011ISACA JOURNAL ARCHIVES - Understanding Applying Benford’s Law - 1 May 2011Newcomb, Simon. “Note Frequency Use Different Digits Natural Numbers.” American Journal Mathematics, vol. 4, . 1, 1881, pp. 39–40. JSTOR, https://doi.org/10.2307/2369148. Accessed 15 Jun. 2022.Newcomb, Simon. “Note Frequency Use Different Digits Natural Numbers.” American Journal Mathematics, vol. 4, . 1, 1881, pp. 39–40. JSTOR, https://doi.org/10.2307/2369148. Accessed 15 Jun. 2022.Durtschi, Cindy & Hillison, William & Pacini, Carl. (2004). Effective Use Benford’s Law Assist Detecting Fraud Accounting Data. J. Forensic Account.Durtschi, Cindy & Hillison, William & Pacini, Carl. (2004). Effective Use Benford’s Law Assist Detecting Fraud Accounting Data. J. Forensic Account.","code":""},{"path":"anomaly-detection.html","id":"anomaly-detection","chapter":"40 Anomaly Detection","heading":"40 Anomaly Detection","text":"Anomalies play significant role field audit. auditors start examining data whether financial statements, transactions, relevant data, detection anomalies can provide valuable insights help identify potential issues irregularities. Anomalies often indicate presence fraudulent activities. Unusual unexpected patterns financial transactions account balances may suggest potential fraud misappropriation assets. Auditors actively search anomalies may indicate fraudulent behavior, fictitious transactions, unauthorized access, unusual changes financial data.Anomalies audit context serve indicators potential issues, including fraud, material misstatements, control weaknesses, compliance violations, process inefficiencies. Detecting investigating anomalies crucial auditors provide accurate reliable financial information, enhance internal controls, support informed decision-making stakeholders.","code":""},{"path":"anomaly-detection.html","id":"definition-and-types-of-anomalies","chapter":"40 Anomaly Detection","heading":"40.1 Definition and types of anomalies","text":"Anomalies patterns data points deviate significantly expected normal behavior within dataset. also known outliers, novelties, deviations can provide valuable insights unusual events behavior various domains.Types anomalies:Point Anomalies: Individual data points considered anomalous compared rest dataset. example, temperature sensor reading significantly different expected range.Point Anomalies: Individual data points considered anomalous compared rest dataset. example, temperature sensor reading significantly different expected range.Contextual Anomalies: Data points considered anomalous within specific context subset dataset. instance, sudden increase obsolete website traffic.Contextual Anomalies: Data points considered anomalous within specific context subset dataset. instance, sudden increase obsolete website traffic.Collective Anomalies: Groups data points exhibit anomalous behavior analyzed together might appear normal considered individually. example sudden drop sales multiple related products.Collective Anomalies: Groups data points exhibit anomalous behavior analyzed together might appear normal considered individually. example sudden drop sales multiple related products.Anomaly detection, R","code":""},{"path":"anomaly-detection.html","id":"anomaly-detection---by-inspection","chapter":"40 Anomaly Detection","heading":"40.2 Anomaly detection - by inspection","text":"Several times, anomalies outliers detectable observation. summary() function prints maximum, minimum, upper lower quartiles, mean median, can give sense far extreme point lies rest data. E.g. Let’s visualise mean annual temperatures degrees Fahrenheit New Haven, Connecticut, 1912 1971, boxplot (Refer Figure 40.1. data available base R, nhtemp.easiest way get sense unusual particular value using graphical summary like boxplot. R, created using boxplot function. boxplot function takes column values input argument, illustrated temperature data, produces box whiskers representation distribution values. extreme values represented distinct points, making easier spot. can also make use ggplot2. Examples base R ggplot2 shown Figure 40.1.\nFigure 40.1: Outliers Boxplot\n’s important note point anomaly necessarily always extreme. point anomaly can also arise unusual combination values across several attributes.collective anomaly, hand, collection similar data instances can considered anomalous together compared rest data. example, consecutive 5 day period high temperatures shown red points plot. daily temperatures unusual occur together, likely caused underlying weather event. Refer Figure 40.2.\nFigure 40.2: Collective Anomalies\n","code":"\nsummary(nhtemp)##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   47.90   50.58   51.20   51.16   51.90   54.60\nboxplot(nhtemp, main = \"Box plot of nhtemp data\")\nggplot(iris, aes(Species, Petal.Length, color = Species )) +\n  geom_boxplot(outlier.colour = \"red\", outlier.size = 2) +\n  theme_bw() +\n  labs(title = \"Petal Lengths in Iris Data\")"},{"path":"anomaly-detection.html","id":"grubbs-test","chapter":"40 Anomaly Detection","heading":"40.3 Grubb’s Test","text":"saw visual assessment outliers works well majority data points grouped together rest lie separate. statistics several tests detect anomalies. Grubb’s Test one . Grubb’s test assesses whether point lies farthest mean dataset outlier. point either largest smallest value data. test however based assumption data points normally distributed. proceeding test, must sure plausible explanation assumption. (can check normality data points plotting histogram. methods please refer chapter linear regression.)Let’s run test nhtemp data example, already seen 40.1 (Left). let’s check normality assumption.\nFigure 40.3: Histogram Grubb’s test\nfigure 40.3 can see assumption nearly satisfied. Let’s run test.p value 0.15 strong evidence reject null hypothesis extreme maximum value outlier.","code":"\nhist(nhtemp, breaks = 20)\nlibrary(outliers)\noutliers::grubbs.test(nhtemp)## \n##  Grubbs test for one outlier\n## \n## data:  nhtemp\n## G = 2.71806, U = 0.87266, p-value = 0.1539\n## alternative hypothesis: highest value 54.6 is an outlier"},{"path":"anomaly-detection.html","id":"seasonal-hybrid-esd-algorithm","chapter":"40 Anomaly Detection","heading":"40.4 Seasonal Hybrid ESD Algorithm","text":"seen test may appropriate anomaly detection (normality assumption well detection extreme values ), particularly detecting anomalies time series may seasonal variations. may install AnomalyDetection package development version github using devtools::install_github(\"twitter/AnomalyDetection\"). Example: JohnsonJohnson data quarterly sales data, can use following syntax.\nFigure 40.4: Seasonal Hybrid ESD Algorithm\nFigure 40.4, can see output $anoms containing anomalies denoted blue dots.","code":"\n#devtools::install_github(\"twitter/AnomalyDetection\")\nlibrary(AnomalyDetection)\nAnomalyDetectionVec(as.vector(JohnsonJohnson), \n                    period = 4, # Sesaonality\n                    plot = TRUE, # set FALSE when plot not needed\n                    direction = 'both' # set 'pos' for higher values\n                                       # or 'neg' for lower values\n                    )## $anoms\n##   index anoms\n## 1    74 12.06\n## 2    75 12.15\n## 3    77 14.04\n## 4    78 12.96\n## 5    79 14.85\n## 6    81 16.20\n## 7    82 14.67\n## 8    83 16.02\n## \n## $plot"},{"path":"anomaly-detection.html","id":"k-nearest-neighbour-distance-score","chapter":"40 Anomaly Detection","heading":"40.5 k-Nearest Neighbour Distance Score","text":"One greatest limitation two methods , applicable univariate data series, whereas, real world, data analysis rarely univariate. knn technique works multivariate data, visulaisation purposes, first visulaise results bivariate data .K-Nearest Neighbour KNN distance-based classifier, meaning implicitly assumes smaller distance two points, similar . bivariate data, can understand algorithm plotting data-points two dimensional scatter plot. Now distance two points usually calculated using Euclidean Distance Metric, ensure data normalised/scaled proceeding distance calculation.Problem Statement-1: Let us try identify outliers virginica Species’ flower measurements. let’s prepare data. make use scale function available base R, normalise data. Remember scale returns matrix, may need convert data.frame using ggplot2.Let us visualise data (first two dimensions ). However, data scaled already, can try visualise dimensions using boxplot. See figures 40.5. points/ouliers numbered (basis row numbers) interpretaion purposes.\nFigure 40.5: Sepal Length Vs. Widths Virginica\nNow, let’s proceed identify outliers R. make use get.knn function FNN package. However, k parameter required beforehand.knn object created two sub-objects (matrices columns equal chosen k), one nearest neighbors’ indices another distances . Let’s view top 6 rows.Using rowMeans can calculate mean distance data point. bigger score , chances record outlier relatively higher. Let’s also store mean distance variable/column say score main dataset, visualise results setting point-size mean distance (actually square root). Figure 40.6, may notice points lying far away bigger becuase chances outliers high.\nFigure 40.6: k-Nearest Neighbour Distance\nNow, can run algorithm find outlier basis variables dataset.","code":"\n# Sample data\ndf <- iris[101:150, 1:4]\n#  Scale the data\ndf <- scale(df)\ndf %>% \n  as.data.frame() %>% \n  mutate(row = row_number()) %>% \n  ggplot(aes(Sepal.Length, Sepal.Width)) +\n  geom_point(color = \"seagreen\") +\n  ggrepel::geom_text_repel(aes(label = row), arrow = grid::arrow()) +\n  theme_bw()\n\n# Helper Function\nfind_outlier <- function(x) {\n  return(x < quantile(x, .25) - 1.5*IQR(x) | x > quantile(x, .75) + 1.5*IQR(x))\n}\n\ndf %>% \n  as.data.frame() %>% \n  mutate(row = row_number()) %>% \n  pivot_longer(-row, names_to = \"Dimension\", values_to = \"Values\") %>% \n  group_by(Dimension) %>% \n  mutate(outlier = ifelse(find_outlier(Values), row, NA)) %>% \n  ggplot(aes(Dimension, Values)) +\n  geom_boxplot(outlier.colour = \"red\") +\n  geom_text(aes(label=outlier), na.rm=TRUE, hjust=-.5) +\n  theme_bw()\n# Load the library\nlibrary(FNN)\n# get kNN object, using k = 5\nviginica_knn <- FNN::get.knn(df[, 1:2], 5)\nhead(viginica_knn$nn.index)##      [,1] [,2] [,3] [,4] [,5]\n## [1,]   37   16   49   11   45\n## [2,]    2   15   22   35   14\n## [3,]   30   40   42    8   13\n## [4,]   34   27   29   33   28\n## [5,]    5   17   46   38   41\n## [6,]   36    8   30   23   31\nhead(viginica_knn$nn.dist)##           [,1]      [,2]      [,3]      [,4]      [,5]\n## [1,] 0.3100808 0.3476803 0.3476803 0.4416741 0.6290499\n## [2,] 0.0000000 0.3100808 0.4416741 0.5645648 0.6397904\n## [3,] 0.1572625 0.4416741 0.4416741 0.4416741 0.4717874\n## [4,] 0.3100808 0.3476803 0.3476803 0.3476803 0.4416741\n## [5,] 0.0000000 0.0000000 0.3145250 0.3476803 0.4416741\n## [6,] 0.1572625 0.5645648 0.6290499 0.6397904 0.6953605\nscore <- rowMeans(viginica_knn$nn.dist)\ndf %>% \n  as.data.frame() %>% \n  mutate(row = row_number(),\n         score = score) %>% \n  ggplot(aes(Sepal.Length, Sepal.Width)) +\n  geom_point(aes(size = score),color = \"seagreen\") +\n  ggrepel::geom_text_repel(aes(label = row), arrow = grid::arrow()) +\n  theme_bw() +\n  labs(title = \"KNN method of outlier\")\nvirginica_knn <- FNN::get.knn(df, 5)\nscore <- rowMeans(virginica_knn$nn.dist)\n# Which point is farthest-Outlier\nwhich.max(score)## [1] 32"},{"path":"anomaly-detection.html","id":"local-outlier-factor","chapter":"40 Anomaly Detection","heading":"40.6 Local Outlier Factor","text":"kNN uses distances k neighbors, algorithm uses density data point vis--vis density nearest neighbors. kNN distance seems good detecting points really far neighbors, sometimes called global anomalies, sometimes fail capture points might considered anomalous like local anomalies. Local Anomalies may lie near cluster still won’t like neighbors. understand better, see plot Figure 40.7 consisting dummy data.\nFigure 40.7: Local Outlier factor\nred points may global outliers, far immediate neigbors, yet blue points may local anomalies like immediate neighbors may local anomalies.stated, LOF segregates data points based ratio density point densities neighbors. score > 1 thus indicates data point may anomaly. Let’s see problem statement.\nFigure 40.8: LOF\nClearly, Figure 40.8 (left), plotted data 2 dimensions despite attempted find LOF basis four dimensions. can see presence local anomalies, within clustered data points. E.g. points nos. 31, 23, etc. earlier given weight instead point nos.15, 10, etc. now given weight. However, want visaulise first two principal components, can (Refer Figure 40.8 (Right)). can also verify .Histograms knn LOF scores can also drawn, Figure 40.9.\nFigure 40.9: Histogram - KNN(Left) LOF (Right)\n","code":"\nlibrary(dbscan)\nlof_score <- lof(df, 5)\ndf %>% \n  as.data.frame() %>% \n  mutate(row = row_number(),\n         score = lof_score) %>% \n  ggplot(aes(Sepal.Length, Sepal.Width)) +\n  geom_point(aes(size = score),color = \"seagreen\") +\n  ggrepel::geom_text_repel(aes(label = row), arrow = grid::arrow()) +\n  scale_size_continuous(guide = FALSE) +  \n  theme_bw() +\n  labs(title = \"LOF method of outlier\")\n\ndf_prcomp <- prcomp(df, scale. = TRUE)\n\ndf %>% \n  as.data.frame() %>% \n  mutate(row = row_number(),\n         score = lof_score,\n         PC1 = df_prcomp$x[,1],\n         PC2 = df_prcomp$x[,2]) %>% \n  ggplot(aes(PC1, PC2)) +\n  geom_point(aes(size = score),color = \"seagreen\") +\n  ggrepel::geom_text_repel(aes(label = row), arrow = grid::arrow()) +\n  scale_size_continuous(guide = FALSE) +  \n  theme_bw() +\n  labs(title = \"LOF method of outlier\\nVisualised on principal components\")\n# Biggest Anomaly - kNN\nwhich.max(score)## [1] 32\n# Biggest Anomaly - LOF\nwhich.max(lof_score)## [1] 7\nhist(score, breaks = 40, \n     main = \"Histogram of KNN scores in IRIS-Virginica data\")\nhist(lof_score, breaks = 40, \n     main = \"Histogram of LOF scores in IRIS-Virginica data\")"},{"path":"anomaly-detection.html","id":"isolation-treesforest","chapter":"40 Anomaly Detection","heading":"40.7 Isolation Trees/Forest","text":"Isolation Forest unsupervised tree based model, actually works path length instead distance density measures. Tree based models use decision tree(s) determine value/class observation given value. words, determines try identify path root node leaf node based value observation question. Forest (Random Forest) actually collection smaller trees thus using ensemble learning methods make decision, instead single complex tree.Let us work problem statement . Firstly, build single decision tree. use development version package isofor can downloaded using remotes::install_github(\"Zelazny7/isofor\"). build forest/tree use function iForest. argument nt determine number trees built ensemble. Another important argument phi determines number samples draw without replacement construct tree. let’s use nt = 1 phi = 100.Score interpretations: closer score 1, likely point anomaly. However, scores 0.5, probably just normal points within trend.Let’s just visualise scores Principal Component plot .\nFigure 40.10: Isolation Tree Method\nLet’s also try forest (ensemble) approach. Refer Figure 40.11, can see scores modified using ensemble methods. However, gradually increase number trees, scores stabilise.\nFigure 40.11: 100 Isolation Trees (Left) Comparison 1 vs. 100 trees (Right)\nContour plots: can also visualise results scores anomaly detection, using contour plots. See plot Figure 40.12.\nFigure 40.12: Contour Plot\nIncluding categorical variables One benefit using forest tree method anomaly detection, can include categorical values also. condition factor type.Problem Statement-2: Let’s now try find anomalies full iris data. can check column types proceeding.condition met. can proceed directly build decision tree/Forest. sake simplicity, let’s build simple tree (one). Refer Figure 40.13.\nFigure 40.13: Including categorical variable\n","code":"\n# remotes::install_github(\"Zelazny7/isofor\")\nlibrary(isofor)\n# Generate a single tree\n# Specify number of samples explicitly\nviginica_1 <- iForest(df, nt = 1, phi = 100)\n\n# Generate isolation score\niso_score_1 <- predict(viginica_1, df)\n\n# View fisrt 10 scores\niso_score_1[1:10]##  [1] 0.3372822 0.3437950 0.3694749 0.4425250 0.3694749 0.4517359 0.8198250\n##  [8] 0.6085593 0.4989121 0.6085593\ndf %>% \n  as.data.frame() %>% \n  mutate(row = row_number(),\n         score = iso_score_1,\n         PC1 = df_prcomp$x[,1],\n         PC2 = df_prcomp$x[,2]) %>% \n  ggplot(aes(PC1, PC2)) +\n  geom_point(aes(size = score),color = \"seagreen\") +\n  ggrepel::geom_text_repel(aes(label = row), arrow = grid::arrow()) +\n  scale_size_continuous(guide = FALSE) +  \n  theme_bw() +\n  labs(title = \"Isolation Tree based outlier\\nVisualised on principal components\")\n\ndf %>% \n  as.data.frame() %>% \n  mutate(row = row_number(),\n         score = iso_score_1,\n         PC1 = df_prcomp$x[,1],\n         PC2 = df_prcomp$x[,2],\n         is_outlier = factor(ifelse(ntile(iso_score_1, 10) >= 10, \"O\", \"N\"))) %>% \n  ggplot(aes(PC1, PC2)) +\n  geom_point(aes(size = score, color = is_outlier)) +\n  ggrepel::geom_text_repel(aes(label = row), arrow = grid::arrow()) +\n  scale_color_manual(values = c(O = \"red\", N = \"black\"), guide = \"none\") +\n  scale_size_continuous(guide = FALSE) +  \n  theme_bw() +\n  labs(title = \"Isolation Tree based outlier\\nTop-10%\")\niso_100 <- iForest(df, nt = 100, phi = 100)\n\n# Generate scores\n\niso_score_100 <- predict(iso_100, df)\n\n# View Results\ndf %>% \n  as.data.frame() %>% \n  mutate(row = row_number(),\n         score = iso_score_100,\n         PC1 = df_prcomp$x[,1],\n         PC2 = df_prcomp$x[,2],\n         is_outlier = factor(ifelse(ntile(iso_score_100, 10) >= 10, \"O\", \"N\"))) %>% \n  ggplot(aes(PC1, PC2)) +\n  geom_point(aes(size = score, color = is_outlier)) +\n  ggrepel::geom_text_repel(aes(label = row), arrow = grid::arrow()) +\n  scale_color_manual(values = c(O = \"red\", N = \"black\"), guide = \"none\") +\n  scale_size_continuous(guide = FALSE) +  \n  theme_bw() +\n  labs(title = \"Isolation Tree based outlier\\nNumber of Trees = 100\")\n\nplot(iso_score_1, iso_score_100, xlim = c(0, 1), ylim = c(0, 1), \n     main = \"Comparision of Tree Vs. Forest Method\")\nabline(a = 0, b = 1)\n# Create PRCOMP data\ndf_grid <- data.frame(\n  PC1 = df_prcomp$x[,1],\n  PC2 = df_prcomp$x[,2]\n)\n\n# Create Sequences\npc1_seq <- seq(min(df_prcomp$x[,1]), max(df_prcomp$x[,1]), length.out = 25)\npc2_seq <- seq(min(df_prcomp$x[,2]), max(df_prcomp$x[,2]), length.out = 25)\n# Create Grid\nmy_grid <- expand.grid(PC1 = pc1_seq, PC2 = pc2_seq)\n\n# Create model for Pr comp\niso_model <- iForest(df_grid, nt = 100, phi = 100)\n# append scores\nmy_grid$scores <- predict(iso_model, my_grid)\n\n# Draw Plot\nlibrary(lattice)\ncontourplot(scores ~ PC1 + PC2, data = my_grid, region = TRUE)\nsapply(iris, class)## Sepal.Length  Sepal.Width Petal.Length  Petal.Width      Species \n##    \"numeric\"    \"numeric\"    \"numeric\"    \"numeric\"     \"factor\"\n# New Iforest Model\niso_model_new <- iForest(iris, nt = 1, phi = 100)\nnew_scores <- predict(iso_model_new, iris)\nhead(new_scores)## [1] 0.3687105 0.3687105 0.3687105 0.3687105 0.3687105 0.6607827\n# Full PRCOMP for Visual\niris_pr <- prcomp(iris[, 1:4])\n\ndata.frame(\n  PC1 = iris_pr$x[,1],\n  PC2 = iris_pr$x[,2]\n) %>% \n  mutate(score = new_scores,\n         Species = iris$Species) %>% \n  ggplot(aes(PC1, PC2, color = Species)) +\n  geom_point(aes(size = score)) +\n  guides(size = FALSE) +\n  theme_bw() +\n  labs(title = \"Decision Tree\\nVisualised on Principal Components\")"},{"path":"anomaly-detection.html","id":"including-categorical-variables-in-lof","chapter":"40 Anomaly Detection","heading":"40.8 Including categorical variables in LOF","text":"can also include categorical variable Local outlier factor using gower distance calculation method. Gower method lets us calculate distance categorical observations, importantly categories ordered. calculate can use function daisy library cluster R. Let’s see LOF score calculation example. plot generated can seen Figure 40.14.\nFigure 40.14: Including categorical variable\n","code":"\nlibrary(cluster)\niris_dist <- daisy(iris, metric = \"gower\")\niris_lof <- lof(iris_dist, minPts = 5)\n\ndata.frame(\n  PC1 = iris_pr$x[,1],\n  PC2 = iris_pr$x[,2]\n) %>% \n  mutate(score = iris_lof,\n         Species = iris$Species) %>% \n  ggplot(aes(PC1, PC2, color = Species)) +\n  geom_point(aes(size = score)) +\n  guides(size = FALSE) +\n  theme_bw() +\n  labs(title = \"LOF Scores\\nVisualised on Principal Components\")"},{"path":"anomaly-detection.html","id":"time-series-anomalies","chapter":"40 Anomaly Detection","heading":"40.9 Time Series Anomalies","text":"time series data, anomaly/outlier can termed data point following common collective trend seasonal cyclic pattern entire data significantly distinct rest data.calculate/detect anomalies, R, make use package timetk. package works tibble instead time series data, may prep data/time series accordingly.Problem Statement Let’s find anomalies, Sunspots data available base R46.\nFigure 40.15: Time Series Anomalies\ncan anomalies highlighted red, Figure 40.15 anomalies filtered code . may also implement Seasonal Hybrid ESD algorithm already discussed .","code":"\nstart_date <- as.Date(\"1749-01-01\")\nend_date <- as.Date(\"2013-09-01\")\ndate_sequence <- seq(start_date, end_date, by = \"months\")\n\nsunspot_df <- sunspot.month %>%\n  as.data.frame() %>%\n  set_names(\"value\") %>%\n  mutate(date = date_sequence)\n\nlibrary(timetk)\nsunspot_df %>% \n  plot_anomaly_diagnostics(date, value,\n                           .interactive = FALSE) \n\nsunspot_df %>% \n  tk_anomaly_diagnostics(date, value) %>% \n  filter(anomaly == 'Yes')## # A tibble: 37 × 11\n##    date       observed season trend remainder seasadj remainder_l1 remainder_l2\n##    <date>        <dbl>  <dbl> <dbl>     <dbl>   <dbl>        <dbl>        <dbl>\n##  1 1749-11-01     159. -1.07   76.3      83.4    160.        -64.8         66.7\n##  2 1769-09-01     149. -0.524  78.8      70.5    149.        -64.8         66.7\n##  3 1769-10-01     158.  1.17   79.3      77.7    157.        -64.8         66.7\n##  4 1769-11-01     148. -1.07   79.8      69.4    149.        -64.8         66.7\n##  5 1771-05-01     153.  0.963  77.5      74.3    152.        -64.8         66.7\n##  6 1777-11-01     146  -1.07   75.7      71.4    147.        -64.8         66.7\n##  7 1777-12-01     157. -0.495  77.8      80.0    158.        -64.8         66.7\n##  8 1778-01-01     177. -2.18   79.9      99.6    179.        -64.8         66.7\n##  9 1778-05-01     239.  0.963  87.3     151.     238.        -64.8         66.7\n## 10 1778-06-01     172.  0.368  89.2      82.1    171.        -64.8         66.7\n## # ℹ 27 more rows\n## # ℹ 3 more variables: anomaly <chr>, recomposed_l1 <dbl>, recomposed_l2 <dbl>"},{"path":"finding-anamolous-outliers.html","id":"finding-anamolous-outliers","chapter":"41 Finding Anamolous Outliers","heading":"41 Finding Anamolous Outliers","text":"content development","code":""},{"path":"dup.html","id":"dup","chapter":"42 Duplicate Detection","heading":"42 Duplicate Detection","text":"content development","code":""},{"path":"detecting-gaps-in-sequences.html","id":"detecting-gaps-in-sequences","chapter":"43 Detecting gaps in sequences","heading":"43 Detecting gaps in sequences","text":"content development finalisation\nAudit analytics often requires us check gaps sequences numbers. Gaps Sequentially numbered objects purchase orders, invoice numbers, cheque numbers, etc accounted . Thus, auditors may require exercise audit check part audit analytics.","code":""},{"path":"detecting-gaps-in-sequences.html","id":"when-sequence-numbers-are-available-as-numeric-column","chapter":"43 Detecting gaps in sequences","heading":"43.1 When sequence numbers are available as numeric column","text":"starting ending numbers sequence. Let us allocate two variables.means 3200 terms (say cheques) issued series. suppose, cheque numbers issued stored column say cheque_no given data frame, total count say 3177. simulateTo find gaps may simply use function setdiff two.","code":"\nstart_num <- 500301 # say\nend_num <- 503500 # say\nset.seed(123)\ncheque_no <- sample(start_num:end_num, 3177, replace = FALSE)\nsetdiff(start_num:end_num, cheque_no)##  [1] 500398 500445 500457 500716 500747 500862 501017 501018 501109 501333\n## [11] 501459 501609 501823 501908 502160 502191 502609 502937 502974 503002\n## [21] 503284 503385 503422"},{"path":"detecting-gaps-in-sequences.html","id":"when-sequence-numbers-are-available-as-character-column","chapter":"43 Detecting gaps in sequences","heading":"43.2 When sequence numbers are available as character() column","text":"may easily replicate procedure gap detection, even sequence column character type. E.g. cheque numbers prefix say, ‘’, cheque numbers may look like-case, may first substitute prefix nothing proceed .","code":"##  [1] \"A502763\" \"A502811\" \"A502527\" \"A500826\" \"A500495\" \"A503286\" \"A502142\"\n##  [8] \"A501442\" \"A501553\" \"A501568\"\nmodified_cheques <- sub(\"A\", \"\", cheque_no) |> as.integer()\nmissing_cheques <- setdiff(start_num:end_num, modified_cheques)\nmissing_cheques##  [1] 500398 500445 500457 500716 500747 500862 501017 501018 501109 501333\n## [11] 501459 501609 501823 501908 502160 502191 502609 502937 502974 503002\n## [21] 503284 503385 503422"},{"path":"data-envelopment-analysis.html","id":"data-envelopment-analysis","chapter":"44 Data Envelopment Analysis","heading":"44 Data Envelopment Analysis","text":"Data Envelopment Analysis (DEA) method measuring efficiency set decision-making units (DMUs) developed late 1970s Abraham Charnes, William Cooper, Edwardo Rhodes. originally developed evaluate performance U.S. banks, since applied wide range industries sectors.provides way measure relative efficiency different DMUs, even produce different combinations outputs use different combinations inputs. can useful identifying best practices areas improvement, making decisions DMUs invest prioritize.","code":""},{"path":"data-envelopment-analysis.html","id":"how-it-works","chapter":"44 Data Envelopment Analysis","heading":"44.1 How it works?","text":"high level, DEA works comparing input-output combinations different decision-making units (DMUs) determine DMUs operating efficiently . Specifically, DEA tries identify “efficient frontier” DMUs using inputs produce optimum amount outputs possible., DEA uses mathematical model takes account inputs outputs DMU, well weights constraints may apply. model calculates “score” DMU based well performs relative DMUs group. DMUs score higher considered efficient score lower.","code":""},{"path":"data-envelopment-analysis.html","id":"use-cases","chapter":"44 Data Envelopment Analysis","heading":"44.2 Use-cases","text":"DEA wide range applications across different industries sectors. examples:Firm Performance Evaluation: DEA can used evaluate performance firms given industry, comparing input-output combinations identify firms operating efficiently others. can help investors policymakers make decisions firms invest support.Firm Performance Evaluation: DEA can used evaluate performance firms given industry, comparing input-output combinations identify firms operating efficiently others. can help investors policymakers make decisions firms invest support.Education Quality Assessment: DEA can used assess quality schools, comparing input-output combinations identify schools producing better outcomes (e.g., higher test scores) fewer resources. can help policymakers allocate education funding effectively.Education Quality Assessment: DEA can used assess quality schools, comparing input-output combinations identify schools producing better outcomes (e.g., higher test scores) fewer resources. can help policymakers allocate education funding effectively.Healthcare Efficiency Analysis: DEA can used analyze efficiency hospitals healthcare providers, comparing input-output combinations identify providers delivering better outcomes (e.g., lower mortality rates) lower costs. can help healthcare systems optimize resource allocation improve patient outcomes.Healthcare Efficiency Analysis: DEA can used analyze efficiency hospitals healthcare providers, comparing input-output combinations identify providers delivering better outcomes (e.g., lower mortality rates) lower costs. can help healthcare systems optimize resource allocation improve patient outcomes.Regional Development Analysis: DEA can used compare efficiency different regions countries, comparing input-output combinations identify regions/countries making best use resources. can help policymakers identify areas improvement develop targeted interventions promote economic growth.Regional Development Analysis: DEA can used compare efficiency different regions countries, comparing input-output combinations identify regions/countries making best use resources. can help policymakers identify areas improvement develop targeted interventions promote economic growth.","code":""},{"path":"data-envelopment-analysis.html","id":"possible-uses-in-audit","chapter":"44 Data Envelopment Analysis","heading":"44.3 Possible uses in Audit","text":"Data Envelopment Analysis (DEA) can used audit evaluate efficiency organization process. Auditors can use DEA identify areas inefficiency within organization comparing inputs outputs different departments processes. example, auditor use DEA compare efficiency different production lines within manufacturing plant efficiency different branches within bank.using DEA, auditors can identify areas organization using resources effectively recommend changes improve efficiency. can help organizations reduce costs, improve productivity, increase profits. DEA can also used evaluate efficiency different audit methods procedures. example, auditor use DEA compare efficiency different sampling methods determine method effective detecting errors fraud. Undoubtedly, DEA powerful tool can help auditors identify areas inefficiency recommend changes improve organizational performance.","code":""},{"path":"data-envelopment-analysis.html","id":"a-practical-example-in-r","chapter":"44 Data Envelopment Analysis","heading":"44.4 A practical example in R","text":"now see complete practical example DEA R.","code":""},{"path":"data-envelopment-analysis.html","id":"pre-requisites-1","chapter":"44 Data Envelopment Analysis","heading":"44.4.1 Pre-requisites","text":"use library deaR measuring DEA efficiency.","code":"\nlibrary(deaR)"},{"path":"data-envelopment-analysis.html","id":"sample-data-set","chapter":"44 Data Envelopment Analysis","heading":"44.4.2 Sample data-set","text":"Let us assume, data/KPIs six stores located across India.sample, two inputs, x1 x2 two outputs, y1 y2. -x1 x2 represents machine hours employee + management hours per week;y1 y2 represents quantity products B produced per week, locations given.","code":""},{"path":"data-envelopment-analysis.html","id":"performing-dea-in-r","chapter":"44 Data Envelopment Analysis","heading":"44.4.3 Performing DEA in R","text":"Now objective first find effective DMUs, .e. DMUs using inputs effectively. also assume constant returns scale .e. output doubled inputs doubled.DEA deaR two step process.Step-1: make data form required package; using make_deadata() function. inputs, outputs dmus require column references data provided.Step-2: Performing actual analysis, using model_basic takes previously created data (step-1) two essential arguments,\norientation: takes string value; \"io\" (input oriented), \"oo\" (output oriented), \"dir\" (directional)\nrts: takes string; specifying type returns scale, equal \"crs\" (constant), \"vrs\" (variable), \"nirs\" (non-increasing), \"ndrs\" (non-decreasing) \"grs\" (generalized)\narguments may passed per need. See ?model_basic help.\norientation: takes string value; \"io\" (input oriented), \"oo\" (output oriented), \"dir\" (directional)rts: takes string; specifying type returns scale, equal \"crs\" (constant), \"vrs\" (variable), \"nirs\" (non-increasing), \"ndrs\" (non-decreasing) \"grs\" (generalized)arguments may passed per need. See ?model_basic help.stored DEA model model variable just created. Now can use following functions get efficiencies, targets lambdas respectively.","code":"\nstore_dea <- make_deadata(store, inputs = 2:3, outputs = 4:5, dmus = 1)\nmodel <- model_basic(store_dea, orientation = 'io', rts = 'crs')\nefficiencies(model)##     Delhi   Gurgaon Bengalore   Chennai    Mumbai    Nagpur \n## 0.8254374 1.0000000 1.0000000 1.0000000 1.0000000 0.9685549\ntargets(model)## $target_input\n##                 x1       x2\n## Delhi     41.74913 31.36662\n## Gurgaon   60.00000 45.00000\n## Bengalore 43.00000 33.00000\n## Chennai   53.00000 43.00000\n## Mumbai    43.00000 38.00000\n## Nagpur    42.61641 33.38805\n## \n## $target_output\n##            y1  y2\n## Delhi     169 119\n## Gurgaon   243 167\n## Bengalore 173 158\n## Chennai   216 138\n## Mumbai    155 161\n## Nagpur    169 157\nlambdas(model)##           Delhi Gurgaon Bengalore Chennai  Mumbai Nagpur\n## Delhi         0 0.64348   0.07303       0 0.00000      0\n## Gurgaon       0 1.00000   0.00000       0 0.00000      0\n## Bengalore     0 0.00000   1.00000       0 0.00000      0\n## Chennai       0 0.00000   0.00000       1 0.00000      0\n## Mumbai        0 0.00000   0.00000       0 1.00000      0\n## Nagpur        0 0.00000   0.85459       0 0.13649      0"},{"path":"data-envelopment-analysis.html","id":"interpreting-above-results","chapter":"44 Data Envelopment Analysis","heading":"44.4.4 Interpreting above results","text":"evident named function efficiencies returned efficiencies DMUs. can see DMUs except Delhi Nagpur working efficiently.targets give us optimum values inputs /outputs given values. can see reduce x1 x2 given values Delhi Nagpur.lambdas give lambdas DEA solution., plot() function output following plots easy inerpret.\nFigure 44.1: Data Envelopment Analysis Plots\nexample simple enough. However, can easily perform DEA number inputs outputs, using R.","code":"\nplot(model)## Press [enter] to continue## Press [enter] to continue"},{"path":"COLORR.html","id":"COLORR","chapter":"A Colors in R","heading":"A Colors in R","text":"visualisation without colours? Colours crucial conveying meaning effectively, colours specific psychological associations. instance, red can represent emotions love anger, also often used indicate declines losses data visualizations. context data visualization, color sets mood emphasizes message display. creates particular atmosphere can turn simple visualization compelling data narrative.tools like ggplot2 excellent crafting impactful data visualizations, default color schemes may sometimes require customization. Carefully selected colors can make easier quicker users interpret data grasp intended message.strategic use colors can highlight critical data points, differentiate categories, depict gradients way text alone achieve. Effective color choices make complex data accessible engaging, fostering better understanding retention information. See following example (Figure .1), wherein mean highway mileage class cars mpg data-set shown. Color used distinguish classes mileage greater average mileage.\nFigure .1: Default colors GGPLOT2\nmay notice default color choice R’s GGPLOT2 effective picked negative shades classes good mileage.addition, crucial consider color-blind friendly palettes ensure visualizations accessible users. Approximately 8% men 0.5% women globally experience form color blindness. using color schemes distinguishable color vision deficiencies, provided tools like ColorBrewer, designers can create inclusive visualizations. broadens audience also demonstrates commitment accessibility inclusivity. Thoughtful color selection, including use color-blind friendly palettes, can significantly enhance effectiveness reach data visualizations.Readers wish learn usage appropriate colors data visualizations may refer blog post. let us learn tweak colors data visualizations make convey message clear effective.","code":"\nlibrary(tidyverse, warn.conflicts = FALSE, quietly = TRUE)\nmpg %>% \n  summarise(hwy = mean(hwy), .by = class) %>% \n  mutate(class = fct_reorder(class, hwy),\n         mean = mean(hwy),\n         performance = ifelse(hwy >= mean, \"Above\", \"Below\")) %>% \n  ggplot(aes(y = class, x = hwy, fill = performance)) +\n  geom_col() +\n  geom_vline(aes(xintercept = mean), linetype = \"dashed\", linewidth = 1) +\n  theme_bw() +\n  ggtitle(\"Class wise Mean Highway mileage of cars\")"},{"path":"COLORR.html","id":"choosing-colors-by-own-choice","chapter":"A Colors in R","heading":"A.1 Choosing colors by own choice","text":"need different colors making visualizations R. base R plots, colors usually specified using “col” whereas ggplot2 manipulated using ‘fill’ ‘color’ aesthetics. 600+ built colors R. can look using colors() function.can see colors() function produces color names, can use color names directly plotting.alternatively, can use HEX color code values colors.following plot, readers may see random 100 colors printed colors() R.","code":"\n# See names of random 20 colors\nset.seed(123)\nsample(colors(), 20)##  [1] \"lightgoldenrodyellow\" \"mediumorchid1\"        \"gray26\"              \n##  [4] \"palevioletred2\"       \"gray42\"               \"deeppink2\"           \n##  [7] \"grey38\"               \"gray76\"               \"gray91\"              \n## [10] \"azure1\"               \"indianred2\"           \"slategray3\"          \n## [13] \"slategray4\"           \"darkorange1\"          \"grey87\"              \n## [16] \"grey94\"               \"blue\"                 \"paleturquoise\"       \n## [19] \"lightsalmon2\"         \"gray58\"\n# In base R\nbarplot(c(5,4,3,2), col = c(\"lightgoldenrodyellow\", \"mediumorchid1\", \"indianred2\", \"paleturquoise\"))\nbarplot(c(2, 3, 4, 5), col = c(\"#fedcba\", \"#abcdef\", \"#123456\", \"#654321\"))"},{"path":"COLORR.html","id":"colors-by-hex-values","chapter":"A Colors in R","heading":"A.2 Colors by hex values","text":"Colors R can also directly selected entering hex value color preceded # hash character. hex values generally contain six hexadecimal digits, -first two represent hex value rednext two represent greenand last two represent bluethus forming rgb/RGB value color. Readers may already know sixteen hexadecimal digits, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, , b, c, d, e f. two hexadecimal digits actually represent \\(16{\\times}16 = 256\\) values base-10. Thus,#000000 represents perfect black (color )#ff0000 represents red color#00ff00 represents green color#0000ff represents blue color#ffffff represents white color.See following example.","code":"\nbarplot(2:6, col = c(\"#000000\", \"#ff0000\", \"#ffff00\", \"#00ffff\", \"#ff00ff\"), main = \"Some primary color Mixing\")"},{"path":"COLORR.html","id":"color-palettes-in-base-r","chapter":"A Colors in R","heading":"A.3 Color palettes in Base R","text":"color palette combination colors used UI designers designing interface. Base R certain palettes may used visualizations.See colors action following plots","code":"\nheat.colors(5)## [1] \"#FF0000\" \"#FF5500\" \"#FFAA00\" \"#FFFF00\" \"#FFFF80\"\nrainbow(7)## [1] \"#FF0000\" \"#FFDB00\" \"#49FF00\" \"#00FF92\" \"#0092FF\" \"#4900FF\" \"#FF00DB\"\nterrain.colors(6)## [1] \"#00A600\" \"#63C600\" \"#E6E600\" \"#EAB64E\" \"#EEB99F\" \"#F2F2F2\"\ntopo.colors(5)## [1] \"#4C00FF\" \"#004CFF\" \"#00E5FF\" \"#00FF4D\" \"#FFFF00\"\ncm.colors(4)## [1] \"#80FFFF\" \"#BFFFFF\" \"#FFBFFF\" \"#FF80FF\"\nhcl.colors(5)## [1] \"#4B0055\" \"#00588B\" \"#009B95\" \"#53CC67\" \"#FDE333\"\npar(mfrow = c(2, 3))\nbarplot(1:5, col = heat.colors(5), main = \"Heat Colors\")\nbarplot(1:7, col = rainbow(7), main = \"Rainbow colors\")\nbarplot(1:7, col = terrain.colors(7), main = \"Terrain Colors\")\nbarplot(1:5, col = topo.colors(5), main = \"Topgraphical Colors\")\nbarplot(1:6, col = cm.colors(6), main = \"CM colors\")\nbarplot(1:7, col = hcl.colors(7), main = \"HCL Colors\")"},{"path":"COLORR.html","id":"color-brewer","chapter":"A Colors in R","heading":"A.4 Color Brewer","text":"color palettes created specifically purpose let us choose best colors story telling. Cynthia Brewer American cartographer, worked visibility color theory cartography developed colorblind-friendly sets colors, known Brewer palettes. RColorBrewer package R presents us palettes use data visualizations.Let us load package display available palettes .actually divided three categories, namely-Sequential: use preferably continuous scales;Qualitative: used non-ordered categorical things – factor, like country continent;Diverging: use continuous scales showing two diverging trends like positives negatives, etc.display single palette, can use display.brewer.pal(n, name) n name arguments requiring number colors palette name respectively.See following examples showing us pick color palette usingscale_fill_brewer scale_color_brewer ggplot2brewer.pal base R plots.","code":"\n#install.packages(\"RColorBrewer\")\nlibrary(RColorBrewer)\ndisplay.brewer.all()\n# View a single RColorBrewer palette by specifying its name\ndisplay.brewer.pal(n = 7, name = 'BrBG')\nlibrary(RColorBrewer)\nggplot(mpg, aes(year, fill = class)) +\n  geom_bar() +\n  scale_fill_brewer(palette = \"Dark2\")\n# Bar-plot using RColorBrewer\nbarplot(c(2,4, 5, 7, 3), col = brewer.pal(n = 5, name = \"Accent\"))"},{"path":"COLORR.html","id":"package-colorspace","chapter":"A Colors in R","heading":"A.5 Package colorspace","text":"colorspace package provides broad toolbox selecting individual colors color palettes, manipulating colors, employing various kinds visualizations.colorspace package provides three types palettes based HCL model:Qualitative: Designed coding categorical information, .e., particular ordering categories available every color receive perceptual weight. Function: qualitative_hcl().Sequential: Designed coding ordered/numeric information, .e., colors go high low (vice versa). Function: sequential_hcl().Diverging: Designed coding ordered/numeric information around central neutral value, .e., colors diverge neutral two extremes. Function: diverging_hcl().quick overview can gained easily hcl_palettes() function:provide access HCL color palettes within ggplot2 graphics suitable discrete /continuous ggplot2 color scales provided. scales named via scheme scale_<aesthetic>_<datatype>_<colorscale>(), <aesthetic> name aesthetic (fill, color, colour), <datatype> type variable plotted (discrete continuous) <colorscale> sets type color scale used (qualitative, sequential, diverging, divergingx).illustrate diverging color palette, can plot figure .1, ","code":"\nlibrary(\"colorspace\")\nhcl_palettes(plot = TRUE)\nmpg %>%\n  summarise(hwy = mean(hwy), .by = class) %>%\n  mutate(class = fct_reorder(class, hwy)) %>%\n  ggplot(aes(y = class, x = hwy, fill = hwy)) +\n  geom_col() +\n  scale_fill_continuous_diverging(palette = \"Red-Green\", mid = mean(mpg$hwy)) +\n  geom_vline(aes(xintercept = mean(mpg$hwy)), linetype = \"dashed\", linewidth = 1) +\n  theme_bw() +\n  ggtitle(\"Class wise Mean Highway mileage of cars\")"},{"path":"various-datasets-in-base-r-used-in-this-book.html","id":"various-datasets-in-base-r-used-in-this-book","chapter":"B Various Datasets, in base R, used in this book","heading":"B Various Datasets, in base R, used in this book","text":"Nile:\nMeasurements annual flow river Nile Aswan (formerly\n‘Assuan’), 1871-1970, 10^8 m^3, “apparent changepoint near\n1898” (Cobb(1978), Table 1, p.249).Nile:\nMeasurements annual flow river Nile Aswan (formerly\n‘Assuan’), 1871-1970, 10^8 m^3, “apparent changepoint near\n1898” (Cobb(1978), Table 1, p.249).lynx:\nAnnual numbers lynx trappings 1821-1934 Canada. Taken \nBrockwell & Davis (1991), appears series considered \nCampbell & Walker (1977).lynx:\nAnnual numbers lynx trappings 1821-1934 Canada. Taken \nBrockwell & Davis (1991), appears series considered \nCampbell & Walker (1977).mtcars:\ndata extracted 1974 Motor Trend US magazine, \ncomprises fuel consumption 10 aspects automobile design \nperformance 32 automobiles (1973-74 models).mtcars:\ndata extracted 1974 Motor Trend US magazine, \ncomprises fuel consumption 10 aspects automobile design \nperformance 32 automobiles (1973-74 models).sunspot.month:\nMonthly numbers sunspots, World Data Center, aka SIDC.\nversion data occasionally updated \nnew counts become available.sunspot.month:\nMonthly numbers sunspots, World Data Center, aka SIDC.\nversion data occasionally updated \nnew counts become available.nhtemp:\nmean annual temperature degrees Fahrenheit New Haven,\nConnecticut, 1912 1971.nhtemp:\nmean annual temperature degrees Fahrenheit New Haven,\nConnecticut, 1912 1971.airquality:\nDaily air quality measurements New York, May September 1973.airquality:\nDaily air quality measurements New York, May September 1973.JohnsonJohnson:\nQuarterly earnings (dollars) per Johnson & Johnson share 1960-80.JohnsonJohnson:\nQuarterly earnings (dollars) per Johnson & Johnson share 1960-80.iris:\nfamous (Fisher’s Anderson’s) iris data set gives \nmeasurements centimeters variables sepal length width \npetal length width, respectively, 50 flowers 3iris:\nfamous (Fisher’s Anderson’s) iris data set gives \nmeasurements centimeters variables sepal length width \npetal length width, respectively, 50 flowers 3attitude:\nsurvey clerical employees large financial\norganization, data aggregated questionnaires \napproximately 35 employees 30 (randomly selected)\ndepartments. numbers give percent proportion favourable\nresponses seven questions department.attitude:\nsurvey clerical employees large financial\norganization, data aggregated questionnaires \napproximately 35 employees 30 (randomly selected)\ndepartments. numbers give percent proportion favourable\nresponses seven questions department.longley:\nmacroeconomic data set provides well-known example \nhighly collinear regression.longley:\nmacroeconomic data set provides well-known example \nhighly collinear regression.USArrests:\ndata set contains statistics, arrests per 100,000 residents \nassault, murder, rape 50 US states 1973. Also\ngiven percent population living urban areas.USArrests:\ndata set contains statistics, arrests per 100,000 residents \nassault, murder, rape 50 US states 1973. Also\ngiven percent population living urban areas.state.x77:\nData sets related 50 states United States America.state.x77:\nData sets related 50 states United States America.","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
