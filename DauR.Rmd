--- 
title: "Data Analytics in Audit, using R"
author: "Anil Goyal"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib, packages.bib]
# url: your book url like https://bookdown.org/yihui/bookdown
# cover-image: path to the social sharing image like images/cover.jpg
description: |
  This book is intended for auditors performing exploratory data analytics using R programming language, mainly baseR and Tidyverse.
biblio-style: apalike
csl: chicago-fullnote-bibliography.csl
---


# Preface {-}

>"R is the Swiss Army knife of data science. It's a versatile and powerful tool that can handle nearly any data-related task, from data cleaning and preparation to statistical modeling and machine learning. And because it's open-source, there's a vast ecosystem of packages and tools available to extend its capabilities even further." - Norman Matloff, professor of computer science at the University of California, Davis.


```{r include=FALSE, message=FALSE}
library(knitr)
```


Welcome to my book on R language, designed specifically for auditors who are interested in learning data analytics using open source resources. In today's digital age, data is abundant and ubiquitous, and its importance cannot be overstated. As a result, auditors must be equipped with the necessary skills to leverage this data and draw insights from it. This book is a result of my love for R and my passion for data analytics. In this book, I have tried to present R programming concepts and case studies that are useful in audit analysis as well as forensic audit and fraud investigation.

This book is written based on my notes on R while I was learning the language myself. I have included sufficient figures and examples to make the concepts easy to understand for readers with little or zero knowledge of programming.  Most of the figures have been created by me except a few fore which due credit has been given.

R is an open-source programming language that has been gaining popularity in recent years due to its versatility and flexibility in data analysis. My journey with R began during Covid-19 lockdown, and I was introduced to R for data analysis by one of my colleagues. Since then, R has become an essential tool in my toolbox for data analysis.

I always admire hadley Wickham's contributions to the development of R programming concepts, that has made data analysis more accessible and efficient. The tidyverse, a collection of R packages developed by Wickham, has transformed the way data analysts and data scientists work with data. The tidyverse promotes a consistent and coherent way of working with data, making it easier to write code that is easier to read, understand, and maintain. Wickham's contributions to R have become the foundation of many data analysis tools in other programming languages, including Python and Julia. I hope that this book will inspire readers to explore the vast potential of R and the contributions made by Wickham in data analytics.

One of the advantages of using free/open source tools like R is that they can be easily customized and extended to suit the specific needs of the user. Additionally, free/open source tools are often updated more frequently than licensed tools, ensuring that users have access to the latest features and bug fixes. Using licensed data analytics tools like Caseware IDEA, Tableau, and others can be expensive, and their licensing fees can be a significant burden on smaller organizations or individuals. By using open source tools like R, users can significantly reduce their costs while still having access to powerful data analytics capabilities.

Another strength of R is its extensive library of packages, which includes many tools for statistical analysis and data visualization. In this book, I have tried to make the concepts of R programming and data analytics as accessible as possible for auditors who may have little or zero knowledge of programming. The first part of the book covers R programming concepts which are absolutely necessary to work in R.  Second part onwards covers data wrangling/transformation techniques as well as case studies in forensic audit and fraud investigation using R.

I hope that this book will be a useful resource for auditors who want to learn data analytics using open source resources like R. I invite readers to share their suggestions and comments on the book to help me improve it further.

Happy reading and happy learning!

## Acknowledgments {-}

A lot of people helped me when I was writing the book.

## Overview {-}
R programming language is the extended version of the S programming language. John Chambers, the creator of the S programming language in 1976 at Bell laboratories. In 1988, the official version of the S language came into existence with the name S-PLUS. The R language is almost the unchanged version of S-PLUS. 

In 1991, R was created by **Ross Ihaka** \index{Ihaka, Ross} and **Robert Gentleman** \index{Gentleman, Robert} in the Department of Statistics at the University of Auckland. Ross’s and Robert’s experience developing R is documented in a 1996 paper in the Journal of Computational and Graphical Statistics [@10.2307/1390807]. In 1997 the R Core Group was formed, containing some people associated with S and S-PLUS. Currently, the core group controls the source code for R and is solely able to check in changes to the main R source tree. Finally, in 2000 R version 1.0.0 was released to the public. \index{History of R}

```{r history, echo=FALSE, fig.cap="A Brief History of R", fig.show='hold', fig.align='center', out.width="90%"}
include_graphics("images/history.png")
```

## Advantages of R {-}
R programming language is an open-source programming language for statistical computation. It supports n number of statistical analysis techniques, machine learning models, and graphical visualization for data analysis. It serves the purpose of the free software environment for statistical computation and graphics. R is easy to understand and implement. The packages are available to create an effective R program, data models, and graphical charts. For research and analytics purposes,  it is a popular language among statisticians and data scientists.

```{r whyr, echo=FALSE, fig.cap="Why R", fig.show='hold', fig.align='center', out.width="90%"}
include_graphics("images/whyR.png")
```


# About author {-}
Anil Goyal is a data analytics enthusiast who has been working in the Indian Audit and Accounts Department since 1998. Anil has a passion for learning and applying programming languages/other tools such as R and Tableau to solve data-related challenges.

Anil is a self-taught expert in R, and has been involved in a variety of data analytics projects and audits.

Anil holds a post-graduate degree in Mathematics from the University of Rajasthan, Jaipur, which he received in 1998. This book is his first book. He continues to expand his skill set and knowledge in this field.  Anil also loves to solve problems raised by various users on StackOverflow.com mainly related to R language.

When not working with data, Anil enjoys pursuing his personal interests, which include photographying birds, nature, and watching movies, etc..


<!--chapter:end:index.Rmd-->

\mainmatter

# Gearing up {-}
```{r echo=FALSE, message=FALSE}
library(knitr)
```

## Download and installation
The R programming language for your local computer can be downloaded from web portal of **The Comprehensive R Archive Network**,\index{CRAN} in short mostly referred to as **CRAN**,\index{CRAN} which is a network of ftp and web servers around the world that store identical, up-to-date, versions of code and documentation for R.  The portal address is [https://cran.r-project.org/](https://cran.r-project.org/) -

```{r cranportal, echo=FALSE, fig.cap="CRAN Portal", fig.show='hold', fig.align='center', out.width="60%"}
include_graphics("images/CRAN.png")
```

Download the specific (as per the operating system) file from the port and install it following the instructions.  The R programming interface looks like-

```{r workspace, echo=FALSE, fig.cap="R Workspace", fig.show='hold', fig.align='center', out.width="70%"}
include_graphics("images/workspace.png")
```

## Writing your first code
Writing code in R is pretty easy.  Just type the command in front of `>`, as shown in figure \@ref(fig:workspace) prompt and press `Enter(Return)` key.  R will display the results in next line.

```{r first, echo=FALSE, fig.cap="Left - Writing first Code in R; Right - Indenting code not necessary but recommended", out.width="49%", out.height= "49%", fig.show='hold', fig.align='center'}
knitr::include_graphics(c("images/first_code.png", "images/indent.png"))
```

## Remember

1. R is case sensitive.  This will have to be remebered while writing/storing/calling functions or other objects.  So `Anil`, `ANIL`, `anil` all are different objects in R.
2. White spaces between different pieces of codes don't matter.  See figure-\@ref(fig:first) above.  Both `3+4` and `3 + 4` will evaluate same.  However, for better readability it is always better to use spaces.
3. Parenthesis `()` are generally used to change the natural order of precedence.  Moreover, these are also used in passing arguments to functions, which will be discussed in detail in chapter-\@ref(func) and onwards.
4. Multi-line code(s) aren't required to be indented in R. In R, indents have no meaning.  However, following best practices to write a code that is understandable by readers, proper indentation is suggested. See figure-\@ref(fig:first) (right) above.
5. If an incomplete code is written in the first line of the code (useful when a single line is not sufficient to write complete code), R will automatically prompt as displaying `+` at the beginning of line, instead of a `>`.  See figure-\@ref(fig:first) (right) above.
6. Indices in R always start from 1 (and not from 0).  This has been discussed in detail in chapter-\@ref(subset).
7. Code that start with hash symbol `#` do not execute. Even in a line if `#` appears in between the line, the code from that place does not get executed.  See the following example.  Comments may be used in codes for either of the purposes -
    + Code Readability
    + Explanation of code
    + Inclusion of metadata, other references, etc.
    + Prevent execution of certain line of code
    

```{r}
# 1 + 3 (this won't be executed)
1 + 3 # +5
```

> Tip: to clear the workspace, just click `ctrl` + `l`.

Normally R code files have an extension `.R` but other R files may have other extensions, such as project files `.Rproj`, markdown files `.Rmd`, and many more.

All of the programming/code writing may be done in R.  But you may have noticed that code once executed cannot be edited.  The code has to written again (Tip: To get previous executed command just use scroll up key on keyboard). Thus, in order to use many other smart features, we will write our code as R scripts i.e. in `.R` files using most popular IDE for R which is `R Studio`. 

> Using R studio IDE is so much popular that many persons using R, do not distinguish between R and its IDE.  Even Stack Overflow which is a popular forum to seek online help explicitly asks users not to tag 'R studio' in general R code problems^[[https://stackoverflow.com/tags/rstudio/info](https://stackoverflow.com/tags/rstudio/info)].

## R studio IDE 
RStudio\index{R studio} is free and open source IDE (Integrated Development Environment) for R, which is available for Windows, Mac OS and LINUX. It can be downloaded from its portal [https://www.rstudio.com](https://www.rstudio.com/products/rstudio/download/). For our most of the data analytics needs, we require Rstudio desktop version, which is available for free to download and installation.

It includes a console, syntax-highlighting editor that supports direct code execution, and a variety of robust tools for plotting, viewing history, debugging and managing your work-space. After downloading and installing it the local machine, a work-space/UI similar to that shown in following figure, is opened.

```{r rstud, echo=FALSE, fig.cap="R Studio interface", fig.show='hold', fig.align='center', out.width="90%"}
include_graphics("images/rstudio.png")
```

There are four panels

- Top-left: 
    + **scripts and files:** The script files which we will be working on, will be opened and displayed here.  To open a new script, you just need to click the new script button ![](images/new_script.png) which is just below the _file menu._; or using keyboard shortcut `ctrl + Shift + n`
- Bottom-left:
    + **R console:** is where the R commands can be written and see the output.  Even the commands run on script will show the output in this panel.
    + **Terminal:** Her we can access our system shell.
- Top-right: 
    + **Environment:**  To see the objects saved in current environment.  This panel is also used to import data in current environment.
    + **history** To view the history of commands run, in the current session
    + **Connections:** Used to connect/import with external database/data
- Bottom-right: 
    + **Files** having tree of folders, to see the file structure of current working directory
    + **Plots** graph window, if the output of r command is a plot/graph, it will be generated here
    + **packages**, to download and load the external packages using mouse click
    + **Help**, window to get help on desired functions.  Even the help sought through r command will be displayed in this window.
    + **viewer:** can be used to view local web content.

## Packages and libraries
As already stated, one of the strength of R is that numerous user-written packages (or _libraries_)\index{packages, external} \index{library} are available on __Comprehensive R Archive Network__ i.e. [CRAN](https://cran.r-project.org/)\index{CRAN}. Package installation is perhaps easiest of the jobs in R.

The command \index{install.packages() function} is fairly simple -
```
install.packages("library_name")
```
which downloads the given package name (to be given in quotes and is case-sensitive), compiles it and then load it into the specified/default directory.  This will however, not load into the memory.  The library once downloaded need not be downloaded every time but need to be loaded every time using the command- \index{library()}
```
library(library_name)
```
Quotes here are optional but package name is still case sensitive.  So to install and load `tidyverse` we need to run first command once (which will download the package into your local computer) but second command (to load it in the current R session) at every new session.
```
install.packages('tidyverse')
library(tidyverse)
```

### Double Colon operator `::`
In R, we can use double colon operator \index{double colon operator, ::} i.e. `::` to access functions that are defined as part of the internal functions that a package uses. These may be used in at least two cases-

1. To call a function say `filter` from package `dplyr` we may use `dplyr::filter()` without actually loading it.  
2. In cases of conflicts (e.g. when two or more packages have same function names) if we want to use the function specifically from a package.  E.g. While loading `dplyr` the function `filter` masks the function with same available in `stats` package (a part of base R).  So, if the requirement is to use function masked `filter` from `stats` we can use `stats::filter()`.


## Getting Help within R {#help}
Once R is installed, there is a comprehensive built-in help system. We can use any of the following commands-
```
help.start()   # general help
help(foo)      # help about function `foo`
?foo           # same as above
apropos("foo") # show all functions containing word `foo`
example(foo)   # show an example of function `foo`
```
Alternatively, features under the Help menu or help pane, can also be used.


## tidyverse
The [tidyverse](https://www.tidyverse.org/) is a _package of packages_ that work in harmony because they share common data representations and 'API' design. This package is designed to make all these easy to install and load multiple 'tidyverse' packages in a single step.

Though `tidyverse`  is a collection 20+ packages (in fact 80+ packages will be installed including depended packages) which are all installed by `install.packages("tidyverse")` command, yet `library(tidyverse)` load [nine](https://www.tidyverse.org/packages/) of them.  Others (like `readxl`) will have to loaded explicitly.

1. [**ggplot2**](https://ggplot2.tidyverse.org/) is a system for declaratively creating graphics, based on [*The Grammar of Graphics*](https://link.springer.com/book/10.1007/0-387-28695-0). 
2. [**dplyr**](https://dplyr.tidyverse.org/) provides a grammar of data manipulation, providing a consistent set of verbs that solve the most common data manipulation challenges. 
3. [**tidyr**](https://tidyr.tidyverse.org/) provides a set of functions useful for data transformation. 
4. [**readr**](https://readr.tidyverse.org/) is used to read and write rectangular/tabular data formats. 
5. [**purrr**](https://purrr.tidyverse.org/) is functional programming (FP) toolkit for working with functions and vectors. 
6. [**tibble**](https://tibble.tidyverse.org/) provides functionalities related to displaying data frames. 
7. [**stringr**](https://stringr.tidyverse.org/) provides set of functions designed to work with strings.  It is built on top of another package [stringi](https://cran.r-project.org/package=stringi). 
8. [**forcats**](https://forcats.tidyverse.org/) provides a suite of useful tools that solve common problems with factors. 
9. [**lubridate**](https://lubridate.tidyverse.org/) makes it easier to do the things R does with date-times.

With latest version of Tidyverse, while loading it [**lubridate**](https://lubridate.tidyverse.org/) also loads with default.

```{r tidyverse, echo=FALSE, fig.cap="tidyverse", fig.show='hold', fig.align='center', out.width="70%"}
knitr::include_graphics('images/tidyverse.png')
```

There are several other `tidyverse` packages which we will be working with-

- `hms`
- `readxl`


<!--chapter:end:00-Zero.Rmd-->

# Part-I: Basic R Programming Concepts {.unnumbered}

# R Programming Language

## Use R as a calculator {#calculator}
To start learning R, just start entering equations directly at the command prompt `>` and press enter. So, `3+4` will give you result `7`. Common mathematical operators are listed in table \@ref(tab:table3).\index{common mathematical operators}

Table: (#tab:table3) Common Mathematical Operators in R

```{r table3, echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}
tabl1 <- "
| Operator/ function      | Meaning             | Example                    |
|:-------------:|:-----------------------------|:--------------------------------------------|
| `+` | Addition | `4 + 5` is `9` |
| `-` | Substraction | `4 - 5` is `-1` |
| `*` | Multiplication | `4 * 5` is `20` |
| `/` | Division | `4/5` is `0.8` |
| `^` | Exponent | `2^4` is `16` |
| `%%` | Modulus (Remainder from division) | `15 %% 12` is `3` |
| `%/%` | Integer Division | `15 %/% 12` is `1` |
"
cat(tabl1) 
```

Strings or Characters have to be enclosed in single `'` or double`"` quotes (more on strings in section \@ref(string)). So a few examples of calculations that can be performed in R could be-

```{r}
4 + 3 ^ 2
8 * (9 + 4)
```

> Note that R follows common mathematical order of precedence while evalauting expressions. That may be changed using simple parenthesis i.e. `()`. Also note that other brackets/braces i.e. curly braces `{}` and `[]` have been assigned different meaning, so to change nested order of operations only `()` may be used.

## Basic Concepts

### Object Assignment
R is an object-oriented language.[@R-base] This means that *objects* \index{objects in R}are created and stored in R environment so that they can be used later.

So what is an object? An object can be something as simple as a number (value) that can be assigned to a variable. Think of it like this; Suppose we have greet each user by his/her name prefixing *hello* to his/her name. Now user's name may be saved in our work environment for later use. Thus, once the user name is saved in a variable then can be retrieved later on, by calling the variable name instead of asking the user name again and again. An object can be also be a data-set or complex model output or some function. Thus, an object created in R can hold multiple values.

The other important thing about objects is that objects are created in R, using the assignment operator\index{assignment operator} `<-`. Use of equals sign `=` to set something as an object is not recommended thought it will work properly in some cases. For now we will stick with the assignment operator, and interpret it as the left side is the object name that is storing the object information specified on the right side. *If `->` right hand side assignment is used, needless to say things mentioned above will interchange.*\index{right assignment operator}

```{r}
# user name
user_name <- 'Anil Goyal'

# when the above variable is called
user_name
```

> **Case sensitive nature:**\index{case sensitive nature} Names of variables even all objects in R are case sensitive, and thus `user`, `USER` and `useR`; all are different variables.

## Atomic data types in R
\index{atomic data types}We have seen that objects in R can be created to store some values/data. Even these objects can contain other objects as well. So a question arises, what is the most atomic/basic data type in R. By atomic we mean that the object cannot be split any further. Thus, the atomic objects created in R can be thought of variables holding one single value. E.g. user's name, user's age, etc. Now atomic objects created in R can be of six types-

-   logical (or Boolean i.e. TRUE FALSE etc.)
-   integer (having non-decimal numeric values like 0, 1, etc.)
-   double ( or floating decimal type i.e. having numeric values in decimal i.e. 1.0 or 5.25, etc.)
-   character (or string data type having some alphanumeric value)
-   complex (numbers having both real and imaginary parts e.g. 1+1i)
-   raw (not discussed here)

```{r datatypes, fig.cap="Data types in R", fig.show="hold", fig.align="center", echo=FALSE, out.height="60%"}
knitr::include_graphics("images/datatypes.png")
```

Let us discuss all of these.

>Note: We will use a pre-built function `typeof()` to check the type of given value/variable. However, functions as such will be discussed later-on.

### Logical
In R logical \index{logical data type}values are stored as either `TRUE` or `FALSE` (all in caps)

```{r}
TRUE
typeof(TRUE)

my_val <- TRUE
typeof(my_val)
```

**`NA`**: There is one special type of logical value i.e. `NA`\index{missing values in R} (short for *Not Available*)\index{NA in R}. This is used for missing data. 

>Remember missing data is not an empty string. The difference between the two is explained in section \@ref(string).

### Integer
Numeric \index{Numeric data types}values can either be integer\index{integer data type} (i.e. without a floating point decimal) or with a floating decimal value (called `double` in r)\index{double data type in R}. Now integers in R are differentiated by a suffix `L`\index{L suffix}. E.g.

```{r}
my_val1 <- 2L
typeof(my_val1)
typeof(2)
```

### Double

Numeric values with decimals are stored in objects of type `double`. It should be kept in mind that if storing an integer value directly to a variable, suffix `L` must be used otherwise the object will be stored as `double` type as shown in above example.

In double type, exponential formats or hexadecimal formats to store these numerals may also be used.

```{r}
my_val2 <- 2.5
my_val3 <- 1.23e4
my_val4 <- 0xcafe # hexadecimal format (prefixed by 0x)

typeof(my_val2)
typeof(my_val3)
typeof(my_val4)
```

> Note: Suffix `L` may also be used with numerals in hexadecimal (e.g. `0xcafeL`) or exponential formats (e.g. `1.23e4L`), which will coerce these numerals in `integer` format.

```{r}
typeof(0xcafeL)
```

Thus, both `integer` and `double` data types may be understood in R as having sub-types of `numeric` data. There are three other types of special numerals (specifically doubles) `Inf`\index{Inf data type}, `-Inf` and `NaN`\index{NaN data type}. The first two are infinity (positive and negative) and the last one denotes an indefinite number (`NaN` short for *Not a Number*).

```{r}
1/0
-45/0
0/0
```

### Character {#string}

Strings \index{characters in R}are\index{strings} stored in R as a character type. Strings should either be surrounded by single quotes `''` or double quotes `""`\index{quotes}[^02-basics-1].

[^02-basics-1]: Single and double quotes can be used interchangeably and won't have any difference in the objects created using any of these. Only thing that should be kept in mind is that the quote used to start the string must be used to close the string, otherwise an error may be thrown. However, this may be used to store objects with either type of string in the data itself.

```{r}
my_val5 <- 'Anil Goyal'
my_val6 <- "Anil Goyal"
my_val7 <- "" # empty string
my_missing_val <- NA # missing value

typeof(my_val5)
typeof(my_val6)
typeof(my_val7)
typeof(my_missing_val)
```

> [[Notes:\\\\](Notes:){.uri}](%5BNotes:\%5D(Notes:)%7B.uri%7D){.uri} 1. Though `NA` is basically of type logical yet it will be used to store missing values in any other data type also as shown in subsequent chapter(s). 2. Special characters are escaped with `\`; Type `?Quotes` in console and check documentation for full details. 3. A simple use of `\` escape character may be to use `"` or `'` within these quotes. Check Example-3 below.

Example-1: Usage of double and single quote interchangeably.

```{r}
my_val8 <- "R's book"
my_val8
```

Example-2: Usage of escape character.

```{r}
cat("This is first line.\nThis is new line")
```

Example-3: Usage of escape character to store single/double quotes as string themselves.

```{r}
cat("\' is single quote and \" is double quote")
```

**Note:** If absence of indices has been noticed in above code output, learn more about `cat` function [here](#cat).

### NULL

`NULL` (note: all caps) is a specific data type used to create an empty vector\index{NULL in R}\index{empty vector in R}. Even this `NULL` can be used as a vector in itself.

```{r}
typeof(NULL)
vec <- 1:5
vec
vec <- NULL
vec
```

### Complex

Complex numbers \index{complex numbers - data type}are made up of real and imaginary parts. As these will not be used in the data analysis tasks, it is not discussed in detail here.

```{r}
my_complex_no <- 1+1i
typeof(my_complex_no)
```

## Data structures/Object Types in R

Objects\index{data structures in R} in R can be either homogeneous or heterogeneous.

```{r datastr, fig.cap="Objects/Data structures in R, can either be homogeneous (left) or heterogeneous (right)", echo=FALSE, out.width="49%", out.height="49%", fig.show='hold', fig.align='center'}
knitr::include_graphics(c("images/homgeneous.jpg", "images/hetero.jpg"))
```

### Homogeneous objects {.unnumbered}

### Vectors {#vectors}

What is a vector? A vector is simply a collection of values/data of same type.\index{vectors in R}

```{r vecs, fig.cap="Vectors are homegeneous data structures in R", fig.show="hold", fig.align="center", echo=FALSE, out.width="99%"}
knitr::include_graphics("images/vec_buck.jpg")
```

#### Simple vectors (Unnamed vectors)

Though, `Vector` is the most atomic data type used in R, yet it can hold multiple values (of same type) simultaneously. In fact vector is a collection of multiple values of same type. So why vector is atomic when it can hold multiple values? You may have noticed a `[1]` printed at the start of line of output whenever a variable was called/printed. This `[1]` actually is the index of that element. Thus, in R instead of having *scalar(s)* as most atomic type, we have *vector(s)* containing only one element. Whenever a vector is called all the values stored in it are displayed with its index at the start of each new line only.

Even processing of multiple values simultaneously, stored in a vector, to produce a desired output, is one of the most powerful strengths of R. The three variables shown in the figure below, all are vectors.

```{r exvecs, fig.cap="Examples of Vectors", fig.show="hold", fig.align="center", echo=FALSE, out.width="99%"}
knitr::include_graphics("images/example_vecs.png")
```

How to create a vector? Vectors in R are created using either -

-   `c()` function\index{c() function} which is shortest and most commonly used function in r. The elements are concatenated (and hence the shortcut `c` for this function) using a comma `,` ; *OR*
-   `vector()`\index{vector() function} produces vector of given `length` and `mode`.

```{r}
my_vector <- c(1, 2, 3)
my_vector

my_vector2 <- vector(mode = 'integer', length = 15)
my_vector2
```

Function `c()` can also be used to **join two or more vectors**\index{vector concatenation}.

```{r}
vec1 <- c(1, 2)
vec2 <- c(11, 12)
vec3 <- c(vec1, vec2)
vec3
```

```{r vecconcat, fig.cap="Vector Concatenation", fig.show="hold", fig.align="center", echo=FALSE, out.height="20%"}
knitr::include_graphics("images/vector_concatenation.png")
```

#### Useful Functions to create new vectors {.unnumbered}

There are some more useful functions to create new vectors in R, which we should discuss here as we will be using these vectors in subsequent chapters.

#### Generate integer sequences with Colon Operator `:` {.unnumbered}

This function generates a sequence from the number preceding `:` \index{colon operator}\index{: operator}to next specified number, in arithmetical difference of `1` or `-1` as the case may be. Notice that output vector type is of `integer`.

```{r}
1:25
25:30
10:1
typeof(2:250)
```

> Note: One of the common mistakes with colon operator is assuming its **operator precedence**. In R, colon operator has calculation precedence over any mathematical operator. Think of outputs you may get with these-

```         
n <- 5
1:n+1
1:n*2
```

#### Generate specific sequences with function `seq` {.unnumbered}

This function\index{seq() function} generates a sequence from a given number to another number, similar to `:`, but it gives us more control over the output desired. We can provide the difference specifically (`double` type also) in the `by` argument. Otherwise if `length.out` argument is provided it calculates the difference automatically.

```{r}
seq(1, 5, by = 0.3)
seq(1, 2, length.out = 11)
```

#### Repeat a pattern/vector with function `rep` {.unnumbered}

As the name suggests `rep`\index{rep() function} is short for *repeat* and thus it repeat a given element, a given number of times.

```{r}
rep('repeat this', 5)
# We can even repeat already created vectors
vec <- c(1, 10)
rep(vec, 5)
rep(vec, each = 5) # notice the difference in results
```

#### Generate english alphabet with `LETTERS` / `letters` {.unnumbered}

These are two inbuilt vectors in R having all 26 alphabets in upper and lower cases respectively.\index{LETTERS}\index{letters}

```{r}
LETTERS
letters
```

#### Generate gregorian calendar month names with `month.name` / `month.abb` {.unnumbered}

```{r}
month.name
month.abb
```

#### Named Vectors

Vectors in R, can be named also, i.e. where each of the element has a name.\index{named vectors} E.g.

```{r}
ages <- c(A = 10, B = 20, C = 15)
ages
```

```{r namedvec, fig.cap="Vector elements can have names", fig.show="hold", fig.align="center", echo=FALSE, out.width="60%"}
knitr::include_graphics("images/named_vector.png")
```

**Note** here that while assigning names to each element, the names are not enclosed in quotes similar to variable assignment. Also notice that this time R has not printed the numeric indices/index of first element (on each new line). There are other ways to assign names to an existing vector. We can use `names()` function\index{names() function}, which displays the names of all elements in that vector ( *and this time in quotes as these are displayed in a vector*).

```{r}
names(ages)
```

Using this function we can assign names to existing vector. See

```{r}
vec1
names(vec1) <- c('first_element', 'second_element')
vec1
```

Names may also be assigned using `setNames()`\index{setNames() function} while creating the vector simultaneously.

```{r}
new_vec <- setNames(1:26, LETTERS)
new_vec
```

Function `unname()`\index{unname() function} may be used to remove all names. Even all the names can be removed by assigning `NULL` to `names` of that vector. Also remember that `unname` does not modify vector in place. To have this change we will have to assigned unnamed vector to that vector again. Check this,

```{r}
unname(new_vec)
new_vec
new_vec <- unname(new_vec)
new_vec
```

#### Type coercion {.unnumbered}

There are occasions\index{type coercion} when different classes of R objects get mixed together. Sometimes this happens by accident but it can also happen on purpose. Let us deal with each of these.

But prior to this let us learn how to check the type of a vector. Of course we can check the type of any vector using function `typeof()` but what if we want to check whether any vector is of a specific type. So there are `is.*()`\index{is.*() functions} functions to check this, and all these functions return either `TRUE` or `FALSE`.

-   `is.logical()`\index{is.logical() function}
-   `is.integer()`\index{is.integer() function}
-   `is.double()`\index{is.double() function}
-   `is.character()`\index{is.character() function}
-   `is.complex()`\index{is.complex() function}

```{r}
is.integer(1:10)
is.logical(LETTERS)
```

#### Implicit Coercion {.unnumbered}
As already stated\index{implicit coercion}, vector is the most atomic data object in R. Even all the elements of a vector (having multiple elements) are vectors in themselves. We have also discussed that vectors are homogeneous in types. So what happens when we try to mix elements of different types in a vector.

In fact when we try to mix elements of different types in a vector, the resultant vector is coerced to the type which is most feasible. Since a numeral say `56` can easily be converted into a complex number (`56+0i`) or character (`"56"`), but alphabet say `A`, cannot be converted into a numeral, the atomic data types normally follow the order of precedence, tabulated in table \@ref(tab:rank).

| Rank |   Type    |
|:----:|:---------:|
|  1   | Character |
|  2   |  Complex  |
|  3   |  Double   |
|  4   |  Integer  |
|  5   |  Logical  |

: (#tab:rank) Order of Precedence for Atomic Data Types

For e.g. in the following diagram, notice all individual elements in first vector. Out of the types of all elements therein, character type is having highest rank and thus resultant vector will be silently coerced to a character vector. Similarly, second and third vectors are coerced to `double` (second element) and `integer` (first element) respectively.

```{r impcoer, fig.cap="Implicit Coercion of Vectors", fig.show="hold", fig.align="center", echo=FALSE, out.width="99%"}
knitr::include_graphics("images/implicit_coercion.png")
```

It is also important to note here that this implicit coercion is without any warning and is silently performed. This implicit coercion is also carried out when two (or more) vectors having different data types are concatenated together.

Example- `vec` is an existing vector of type `integer`. When we try to add an extra element say of `character` type, `vec` type is coerced to `character`.

```{r}
vec <- 1:5
typeof(vec)
vec <- append(vec, 'ABCD')
typeof(vec)
```

R also implicitly coerces vectors to appropriate type when we try to perform calculations on vectors of other types. Example

```{r}
(TRUE == FALSE) + 1
typeof(TRUE + 1:100)
typeof(FALSE + 56)
```

#### Explicit Coercion {.unnumbered}
We can explicitly coerce\index{explicit coercion} by using an `as.*()` function, like `as.logical()`, `as.integer()`, `as.double()`, or `as.character()`. **Failed coercion of strings generates a warning and a missing value:**

```{r}
as.double(c(TRUE, FALSE))
as.integer(c(1, 'one', 1L))
```

#### Coercion precedence
Sometimes, inside R both coercion happen at same time. So which one to precede other? Actually, implicit coercion will precede explicit coercion always. Consider this example. However, without seeing the result try to guess the output.

```{r}
as.logical(c('TRUE', 1))
```

Explanation: the vector `c('TRUE', 1)` coerces to `c('TRUE', '1')` due to implicit coercion first and thereafter explicit coercion forces second element `as.logical('1')` to `NA`. Though `as.logical(1)` would have resulted into `TRUE` but `as.logical("1")` would result into `NA`.

#### Checking dimensions {.unnumbered}
Now a vector can have `n` number of vectors (recall that each element is a vector in itself) and at times we may need to check how many elements a given vector contains. Using function `length()`, we can check the number of elements.

```{r}
length(1:100)
length(LETTERS)
length('LENGTH') # If you thought its output should have been 6, check again.
```

### Matrix (Matrices)
Matrix (or plural matrices) is a two dimensional arrangement (similar to a matrix in linear algebra and hence its name) of elements of again same type as in vectors. E.g.

$$\begin{array}{ccc}
x_{11} & x_{12} & x_{13}\\
x_{21} & x_{22} & x_{23}
\end{array}$$

Thus, matrices are vectors with an attribute named *dimension*.

> The dimension attribute is itself an integer vector of length 2 (number of rows, number of columns).

#### Create a new matrix {.unnumbered}
A new matrix can be created using function `matrix()` where a vector is given which is to be converted into a matrix and either number of rows `nrow` or number of columns `ncol` may be given.

```{r}
matrix(1:12, nrow = 3)
matrix(1:12, ncol=3)
```

Another useful argument is `byrow` \index{byrow argument in matrix function}which by default is `FALSE`. So if it is explicitly changed, we get

```{r}
matrix(1:12, ncol=3, byrow = TRUE)
```

```{r byrow, fig.cap="Arrangement of Matrix, if byrow argument is used", fig.show="hold", fig.align="center", echo=FALSE, out.width="60%"}
knitr::include_graphics("images/byrow.png")
```

Matrix can be of any type. But rules of explicit and implicit coercion (as explained in vectors) also apply here.

```{r}
matrix(LETTERS, nrow = 2)
matrix(c(LETTERS, 1:4), nrow=5)
```

#### Names in matrices {.unnumbered}
Similar to vectors, rows or columns or both in matrices may have names\index{named matrix}. Check `?matrix()` for complete documentation.

#### Dimension {.unnumbered}
To check dimension of a matrix\index{dimensions of matrix} we can use `dim()`\index{dim function} (short for dimension) (similar to `length` in case of vectors) which will return a vector with two numbers (rows first, followed by columns).

```{r}
my_mat <- matrix(c(LETTERS, 1:4), nrow=5)
dim(my_mat)
```

This gives us another method to create matrix from a vector. See

```{r}
my_mat2 <- 1:10
dim(my_mat2) <- c(2,5)
my_mat2
```

#### Have a check on replication {.unnumbered}
What happens when product of given dimensions is less than or greater than given vector to be converted. It replicates but it is advised to check these properly as resultant vector may not be as desired. Check these cases, and notice when R gives result silently and when with a warning.

```{r}
matrix(1:10, nrow=5, ncol=5)
matrix(1:1000, nrow=2, ncol=3)
```

#### Combining matrices {.unnumbered}
Using `cbind()` or `rbind()` we can combine two matrices column-wise or row-wise respectively.

```{r bind, fig.cap="Binding of Two or more matrices together", fig.show="hold", fig.align="center", echo=FALSE, out.height="60%"}
knitr::include_graphics("images/cbind_vs_rbind.png")
```

See these two examples.

```{r}
mat1 <- matrix(1:4, nrow = 2)
mat2 <- matrix(5:8, nrow = 2)
cbind(mat1, mat2)
```

Example-2

```{r}
rbind(mat1, mat2)
```

### Arrays
Till now we have seen that elements in one dimension are represented as vectors and in two dimension as matrices. So a question arises here, how many dimensions we can have. Actually we can have n number of dimensions in r, in object type `array`, but they'll become increasingly difficult to comprehend and are not thus discussed here. Check these however for your understanding,

```{r}
array(1:24, dim = c(3,2,4)) # a three dimensional array
```

Try creating 4 or 5 dimensional arrays in your console and see the results.

Further properties of vectors, matrices will be discussed in next chapter on sub-setting and indexing where we will learn how to retrieve specific elements of vector/matrices/etc. But till now we have created objects which have elements of same type. What if we want to have different types of elements/data retaining their types, together in a single variable? Answer is in next section, where we will discuss hetergeneous objects.

### Heterogeneous objects {.unnumbered}

### Lists
So lists are used when we want to combine elements of different types together. Function used to create a list is `list()`. Check this

```{r}
list(1, 2, 3, 'My string', TRUE)
```

Pictorially this list can be depicted as

```{r exlist, fig.cap="A list in R is a heterogeneous object", fig.show="hold", fig.align="center", echo=FALSE, out.width="99%"}
knitr::include_graphics("images/list_ex.png")
```

Interestingly list can contain vectors, matrices, arrays as individual elements. See

```{r}
list(1:3, LETTERS, TRUE, my_mat2)
```

```{r exlist2, fig.cap="A list in R, can contain vector, matrices, array or even lists", fig.show="hold", fig.align="center", echo=FALSE, out.width="99%"}
knitr::include_graphics("images/list_ex2.png")
```

Similar to vectors these elements can be named also.

```{r}
list(first_item = 1:5, second_item = my_mat2)
```

OR

```{r}
my_list <- list(first=c(A=1, B=2, C=3),second=my_mat2)
my_list
```

```{r namedlist, fig.cap="Similar to vector elements, the elements in list can be named also", fig.show="hold", fig.align="center", echo=FALSE, out.width="99%"}
knitr::include_graphics("images/named_list.png")
```

OR 

More interestingly, lists can even contain another lists.

```{r}
my_list2 <- list(my_list, new_item = LETTERS)
my_list2
```

Number of items at first level can be checked using `length` as in vectors. Checking number of items in second level onward will be covered in subsequent chapter(s).

```{r}
length(my_list)
length(my_list2) # If you thought its output should have been 3, think again.
```

### Data Frame

Data frames are used to store tabular data (or rectangular) in R. They are an important type of object in R.

```{r dframe, fig.cap="An example data frame", fig.show="hold", fig.align="center", echo=FALSE, out.width="99%"}
knitr::include_graphics("images/dataframe.png")
```

Data frames are represented as a special type of list where every element of the list has to have the same length. Each element of the list can be thought of as a column and the length of each element of the list is the number of rows.

```{r listvsdf, fig.cap="A data frame in R, is just a special kind of list", fig.show="hold", fig.align="center", echo=FALSE, out.width="99%"}
knitr::include_graphics("images/list_vs_df.png")
```

Unlike matrices, data frames can store different classes of objects in each column. (Remember that matrices must have every element be the same class).

To create a data frame from scratch we will use function `data.frame()`. See

```{r}
my_df <- data.frame(emp_name = c('Thomas', 'Andrew', 'Jonathan', 'Bob', 'Charles'),
                    department = c('HR', 'Accounts', 'Accounts', 'Execution', 'Tech'),
                    age = c(40, 43, 39, 42, 25),
                    salary = c(20000, 22000, 21000, 25000, NA),
                    whether_permanent = c(TRUE, TRUE, FALSE, NA, NA))
my_df
```

**Note** that R, on its own, has allocated row names that are numbers to each of the row on its own.

Of course at most of the times we will have data frames ready for us to analyse and thus we will learn to import/read external data in r, in subsequent chapters. To check dimensions of a data frame use `dim` as in matrix.

```{r}
dim(my_df)
```

Thus, the object types in R, can be depicted as in adjoining figure.

```{r impdstr, fig.cap="Most important Data structures, in R", fig.show="hold", fig.align="center", echo=FALSE, out.width="99%"}
knitr::include_graphics("images/Objects.png")
```

## Other Data types
Of course, there are other data types in R of which three are particularly useful `factor`, `date` and `date-time`. These types are actually built over the base atomic types, `integer`, `double` and `double` respectively and that's why these are being discussed separately. These types are built as `S3 objects` in R, and users may also define their own data types in `object oriented programming`. **OOP** being concept of core programming concepts and therefore are out of the scope here.

However, to understand the S3 objects better, we have to understand that atomic objects (for the sake of simplicity consider only vectors) can have attributes. 

**Example** One of the attributes that each vector has is `names`, which for unnamed vector is empty (NULL). Attributes of any object can be viewed/called from function `attributes()`.

```{r}
# Let us create a vector
vec <- 1:26
# Convert this to a named vector using function setNames()
# This function takes first argument as vector
# Second argument should be a character vector of equal length.
vec <- setNames(vec, LETTERS)
# let's check what are the attributes of `vec`
attributes(vec)
```
Using `attr()` we may assign any new attribute to any R object/variable.

```{r}
# Let's also assign a new attribute say `x` having value "New Attribute" to `vec`
attr(vec, "x") <- "New Attribute"
# Now let's check its attributes again
attributes(vec)
```
We can see, in above example, how a new attribute has been added to a vector.  It should have been clear by now that apart from `names`, other `attributes` may also be assigned to a vector.

### Factors

A factor is a vector that can contain only predefined values. It is used to store categorical data. Factors are built on top of an integer vector with two attributes: a *class*, 'factor', which makes it behave differently from regular integer vectors, and *levels*, which defines the set of allowed values. To create factors we will use function `factor`.

```{r}
fac <- factor(c('a', 'b', 'c', 'a'))
fac
typeof(fac) # notice its output
attributes(fac)
```

So if `typeof` of a factor is returning integer, how will we check its type? We may use `class` or `is.factor` in this case.

```{r}
class(fac)
is.factor(fac)
```

Now a factor can be ordered also. We may use its argument `ordered = TRUE` along with another argument `levels`.

```{r}
my_degrees <- c("PG", "PG", "Doctorate", "UG", "PG")
my_factor <- factor(my_degrees, levels = c('UG', 'PG', 'Doctorate'), ordered = TRUE)
my_factor # notice output here
is.ordered(my_factor)
```

Another argument `labels` can also be used to display the labels, which may be different from levels.

```{r}
my_factor <- factor(my_degrees, levels = c('UG', 'PG', 'Doctorate'), 
                    labels = c("Under-Graduate", "Post Graduate", "Ph.D"),
                    ordered = TRUE)
my_factor # notice output here
is.factor(c(my_factor, "UG"))
```

Attribute `levels` can be used as a function to retrieve/modify these.

```{r}
levels(my_factor)
levels(my_factor) <- c("Grad", "Masters", "Doctorate")
my_factor
```

Remember that while factors look like (and often behave like) character vectors, they are built on top of integers. Try to think of output of this `is.factor(c(my_factor, "UG"))` before running it in your console.

### Date

Date vectors are built on top of double vectors. They have class "Date" and no other attributes. A common way to create `date` vectors in R, is converting a character string to date using `as.Date()` (see case carefully),

```{r}
my_date <- as.Date("1970-01-31")
my_date
attributes(my_date)
```

Do check other arguments of as.Date by running `?as.Date()` in your console. To check whether a given variable is of type Date in r, there is no function like `is.Date` in base r, so we may use `inherits()` in this case.

```{r}
inherits(my_date, 'Date')
```

### Date-time (`POSIXct`)
Times are represented by the `POSIXct` or the `POSIXlt` class.

-   POSIXct is just a very large integer under the hood. It use a useful class when you want to store times in something like a data frame.
-   POSIXlt is a list underneath and it stores a bunch of other useful information like the day of the week, day of the year, month, day of the month.

```{r}
my_time <- Sys.time()
my_time
class(my_time)
my_time2 <- as.POSIXlt(my_time)
class(my_time2)
names(unclass(my_time2))
```

### Duration (`difftime`)

Duration, which represent the amount of time between pairs of dates or date-times, are stored in `difftimes`. `Difftimes` are built on top of doubles, and have a units attribute that determines how the integer should be interpreted.

```{r}
two_days <- as.difftime(2, units = 'days')
two_days
```

These over the top, data types will be discussed in more detail in subsequent chapters.

<!--chapter:end:02-basics.Rmd-->

# Subsetting R objects or accesing specific elements {#subset}

There are multiple methods for sub-setting R objects (vectors, matrices, data frames, lists, etc.) and each have its own uses and benefits.  We will discuss each one of them.  Three operators `[`, `[[` & `$` will be used.

## Subsetting vectors
Let us first start sub-setting vectors, which is as we have learned, atomic object in R.  To subset the vectors we will use `[`.
For this we will use following `x` vector, which has 6 elements (names) each starting with alphabets A to F.
```{r}
x <- c('Andrew', 'Bob', 'Chris', 'Danny', 'Edmund', 'Freddie')
```

![](images/subset_vec.png)

### Subsetting through a vector of positive integers
Sub-setting through positive integers will give us elements at those given position (indices). See this
```{r}
# fourth element
x[4]
# third to fifth element
x[3:5]
# first and fifth element
x[c(1,3)]
```
_Note: Check what happens when the integer vector has repeated integers._

### Subsetting through a vector of negative integers 
Sub-setting through negative integers will give us all elements __except__ those at given indices.  See
```{r}
# all elements except that at fourth
x[-4]
# all elements except third to fifth
x[-(3:5)]
# all elements except first and fifth
x[-c(1,5)]
```

_Note:  Try mixing sub-setting with a vector having both positive and negative integers in your console and check what happens._

### Subsetting through a logical vector
We can also subset a given vector through another vector having `logical` values i.e. `TRUE` and `FALSE`.  As you can understand output/result will have elements at places having `TRUE` only.
```{r}
# First, third and fifth element only
x[c(TRUE, FALSE, TRUE, TRUE, FALSE, FALSE)]
```
__Recycling__ is an important concept while sub-setting though a logical vector.  It recycles the given logical vector up to the length of vector to be subset. Thus, `x[TRUE]` will give us original `x` only.
```{r}
x[TRUE]
x[c(TRUE, FALSE)] # will give elements at odd indices
```
_Note: Try to subset a vector through a logical vector having missing values i.e. `NA` along with `TRUE` and/or `FALSE` in your console and check what happens._

__Sub-setting through logical vector is most important and used sub-setting method as we will see it subsequent chapter/sections when we will filter a vector on the basis of some conditions.__

### Subsetting through a character vector
This method is used when the given vector is named.  We can pass desired names inside `[]` to get/filter those desired elements.  See this example.
```{r}
# let us create a named vector `y`
y <- setNames(1:6, LETTERS[1:6])
# display `y`
y
# subset elements named `A` and `C`
y[c('A', 'C')]
```
_Note that we have used quotes in above method of sub-setting._ We can use this method when we have names saved in another variable.  See this
```{r}
var <- c('A', 'C', 'E')
# subset those elements from `y` which are named as per `var`
y[var] # notice that since `var` is a variable, we have not used quotes.
```
_Note: Similar to positive integer indexing we will get repeated values if character vector has repeated names._
```{r}
y[c("A", "A", "C", "A")]
```

__Other two methods of indexing will not be used frequently but are important to know for debugging the code as sometimes your subset vector may be `NULL` or `zero`__

### Subsetting through nothing
Indexing through nothing i.e. simply with `[]` will give us original vector.
```{r}
x[]
```

### Subsetting through Zero
Sub-setting through `NULL` or `0` will give us a zero length vector.
```{r}
x[NULL]
y[0]
is.null(x[NULL])
```
It is important and interesting to note here that subsetting through NULL is not NULL and is a zero length vector instead.

## Subsetting Matrices and arrays

We can subset higher dimensional structures (Matrix - 2 dimensional and arrays - dimension greater than 2) using (i) multiple vectors, (ii) single vector and (iii) matrix.

Let us first create a 5x5 matrix say `mat` with elements named $A_{mn}$ where `m` will denote row number and `n` will denote column number.
```{r echo=FALSE}
mat <- outer(1:5, 1:5, FUN = function(x, y) paste0('A', x, y))
mat
```

### Indexing through Multiple vectors
This is extension of all sub-setting methods explained for a vector.  In objects with higher dimensionality we will have to provide one vector for each dimension.  Blank values, as you may understood (ref - sub-setting through nothing explained above) will do nothing and return that dimension complete.
```{r}
# first and second row with third and fifth column
mat[1:2, c(3,5)]
# third to fifth column, all rows
mat[,3:5]
# all columns except third
mat[, -3]
# Odd rows, all columns
mat[c(TRUE, FALSE),]
```

The idea can be extended to a named matrix also.
```{r}
# First create a named matrix
rownames(mat) <- paste0("Row", 1:5)
colnames(mat) <- paste0("Col", 1:5)
mat
# filter desired rows/columns
mat[c("Row1"), c("Col2", "Col3")]
```
In the above example you must have noticed that indexing objects with higher dimensionality may return the objects with lower dimensionality.  E.g. sub-setting a matrix may return a vector.  __We can control the dimensionality reduction through the argument `drop=FALSE` which is by default TRUE and may thus introduce bugs in the code.__
```{r}
mat[c("Row1"), c("Col2", "Col3"), drop=FALSE]
#check this
dim(mat[c("Row1"), c("Col2", "Col3"), drop=FALSE])
#versus this
dim(mat[c("Row1"), c("Col2", "Col3")])
```

### Subsetting through one vector
By now it should be clear that objects with higher dimensionality like matrices, array are actually vectors at the core of r, displayed and acting like objects having more than one dimension.  So sub-setting with single vector on these objects coerce the behavior of these objects as vectors only and give output exactly as shown in previous section.
```{r}
mat[c(1, 10, 15, 25)]
# OR
mat[c(TRUE, FALSE)]
```


### Subsetting through a matrix
We can also subset objects with higher dimensionality with integer matrix (having number of columns equal to dimensions).  In other words, to subset a matrix (2D) with the help of other matrix we will need a 2 column matrix where first column will indicate row number and second column will indicate column number.  See
```{r}
selection_matrix <- matrix(c(1,1, # Element at Row 1 Col 1
                             2,2, # Element at Row 2 Col 2
                             3,3), # Element at Row 3 Col 3
                           ncol = 2, 
                           byrow = TRUE)
mat[selection_matrix]
```


## Subsetting lists
List sub-setting can be done using either `[]`, `[[]]` or `$`.  To understand the difference between these, let us consider these one by one.  As done earlier let us consider a list of 4 elements - one vector, one matrix, one list and one data frame.  For now let us consider that list is unnamed.
```{r}
my_list <- list(
  11:20,                                                       # first element
  outer(1:4, 1:4, FUN = function(x, y) paste0('B', x, y)),     # second element
  list(LETTERS[1:8], TRUE),                                    # third element  
  data.frame(col1 = letters[1:4], col2 = 5:8)                  # fourth element
)
# display the list
my_list
```


### Subsetting lists with `[]`
Sub-setting lists with `[]` will always result a list containing desired element(s).
```{r}
my_list[2]
class(my_list[1])
```
We can apply other ideas of vector sub-setting as explained earlier with this list sub-setting.  The output will also be list containing one or more items.

### Subsetting lists with `[[]]`
Sub-setting list with `[[]]` will return that specific item (as per index given) but the output will be of type of that specific item.
```{r}
my_list[[2]]
class(my_list[[4]])
```
_Notice the difference in outputs created with `my_list[2]` and `my_list[[2]]` in above 2 code blocks._

_Needless to say, one cannot index/subset lists using multiple indices._  Check `my_list[[1:2]]` in your console as the results may not be as what you think.

### Chaining or multiple subsetting
We can further subset/index a vector/variable in R using __chaining__ i.e. by combining one or methods as we have discussed here.
```{r}
# third element of second element
my_list[[2]][3] # recall that by default matrix is by column
# or
my_list[[2]][1:3,2:4]
```


### Subsetting with `$`
`$` is a shorthand operator: `x$y` is roughly equivalent to `x[["y"]]`. To check this let us assign our list some names.

```{r}
names(my_list) <- c("first", "second", "third", "fourth")
# Now see
my_list$first
my_list$fourth$col2
```
_Notice that rules for dimensionality reduction also applies with `$`_.

Another difference between `[[` sub-setting versus `$` sub-setting is partial matching (_left to right only_), which is possible with `$` only and not with `[[`.  See
```{r}
my_list$fir
my_list[['fir']]
```

## Data frames
As already explained data frames are basically lists with each element having equal length, rules for sub-setting lists all apply with data frames.  One addition is that data frames can also be subset using rules for matrix sub-setting.

```{r}
mtcars # it is a default data frame in r
# list type sub-setting
mtcars[[2]]  # second column 
# matrix type
mtcars[1:4, 2:3] # first four rows with second & third columns
```

**Remember** 

1. If sub-setting data frames with single vector, data frame behave like lists, by default.  If we have to get output as data.frame we will have use `drop= FALSE` argument.
2. If however, sub-setting data frame through two vectors, these behave like matrices.

Examples
```{r}
#Default
mtcars[1:5, 2]
# using drop argument
mtcars[1:5, 2, drop = FALSE]
```

## Subsetting and assignment
All the sub-setting that we have seen can be used for assignment as well.
```{r}
my_list$first <- mtcars[1:4, 2:3]
my_list
```


<!--chapter:end:03-Subsets.Rmd-->

# Functions and operations in R {#func}

What is a function? Mathematically, a function $f$ is a relationship which map an input $x$ to an specific output, which is denoted as $f(x)$.  There are only two conditions i.e. every input should have an output, and same input if passed into same function multiple times, it should produce same output each time.  So if $x=y$ we should have $f(x)=f(y)$.  

```{r echo=FALSE, fig.show='hold', fig.align='center', fig.cap="Author's illustration of a function", out.width="99%"}
knitr::include_graphics("images/function.png")
```

For example `squaring` if considered on numbers is a function.  We denote this as $f(x)=x^2$.  Or, `square-root` on positive numbers is also a function.

Now there may be more than one input, let us assume three inputs `x`, `y` and `z` and our function's job is to add three times `x`, two times `z` and one time `y` together.  We will write this function as $f(x,y,z) = 3x+y+2z$.  Each programming language has some pre-defined functions.  Here inputs are usually termed as `arguments`.  Normally values to `arguments` should be passed by users, but many times there's a default value for these arguments. So if the value of that argument is not supplier by the user/coder explicitly, that function uses that default value silently and produces a result.  

R's engine then calculates the output as per definition of that function and gives us the output.  If that output is assigned to some variable R does not displays/prints anything but if function is performed only the output is displayed usually, with the exception that many times function is carried out silently and nothing is returned.

In this chapter we will learn about some of the pre-defined functions which shall be used in our data analysis operations.  We can also define our own custom functions which we will learn in chapter \@ref(cust).

As an example, `sum()` is a predefined function available in R, which produces sum of one or more vectors passed in the function as arguments.
```{r}
sum(1:10, 15:45)
```

To check the arguments available for any pre-defined function, we can use another function `args()` which take a _function name_ as an argument and returns all the available arguments to that function.  
```{r}
args(sum)
```

Here we see that there is an argument (which is anmed argument0 `na.rm` having a default value `FALSE`.  Actually, this argument silently takes default value and produces results.  But if `TRUE` is required as a value to this argument that need to be explicitly mentioned.
```{r}
sum(1:10, NA)
sum(1:10, NA, na.rm = TRUE)
```


To get the definition of any existing function, we may just type its name without parenthesis on console, and the definition will be returned as an output.

```{r}
sum
```

To get further help about any existing function, refer section \@ref(help).

## Custom Functions {#cust}

One of R’s greatest strengths is the user’s ability to add functions\index{custom function}. In fact, many of the functions in R are functions of existing functions. The structure of a function looks like this:

```
myfunctionname <- function(arg1, arg2, ... ){
  statements
  return(object)
}
```
**Note:** Objects in the function are local to the function. The object returned can be any data type, from scalar to list. 

Let’s take a look at an example.  We will create a function which will take 3 numbers, will give an output by adding thrice of first, second and twice of third.
```{r}
my_fun1 <- function(first,second,third){
  first*3+second+third*2
}
# let's check whether it is working as desired
my_fun1(3,1,10)
```
- If the arguments provided are not named, it will take all arguments in the order these are defined.
- However, we can provide named arguments in any order.  See this
```{r}
my_fun1(second=3, first=1, third=10)
```
- Partial matching of names are also allowed.  Example
```{r}
my_fun1(sec=3,fir=1,thi=10)
```
- We can also provide default values to any argument.  These default values are however, overridden when specific values are given.  See this example.
```{r}
# let's create a new function which adds twice the second argument to first argument, which in turn by default is 10
my_fun2 <- function(first=10, second){
  first+second*2
}
my_fun2(second = 10)
my_fun2(1, 10)
```
- There may be functions which do not require any argument.  See this example
```{r}
my_fun3 <- function(){
  print('Hi')
}
my_fun3()
```

### Special argument ellipsis `...` {-}

While searching for help of a function in r, you may have came across something like this `sum(..., na.rm = FALSE)`.  The three dots `...`\index{... ellipsis} here are referred to as ellipsis\index{ellipsis}.  Basically it means that the function is designed to take any number of named or unnamed arguments.

Thus it means we can provide any number of arguments in place of `...`.  Now the point to be noted here is that values to all agruments occurring after `...` must only be named.  See this example-
```{r}
sum(1:100, NA, TRUE)
sum(1:100, NA, na.rm = TRUE)
```
Now we can even use these three dots in our own custom functions.  Just unpack these before writing the actual statement for that function.  See this simple example-
```{r}
my_ellipsis_func <- function(...){
  l <- list(...) # unpack ellipsis
  length(l) # return length of l
}
my_ellipsis_func(1:10, 11:20, 'a string') # we are passing three arguments
```

### Environment issues {-}

- Any of the argument values are not saved/updated in global environment\index{global environment}.  See this example
```{r}
x <- 10
my_fun4 <- function(x){
  x*2
}
my_fun4(2)
x
```
- Even if we create another variable inside the function, that variable is not available outside that function's environment.
```{r}
y <- 5
my_fun5 <- function(){
  y <- 1
  return(y)
}
my_fun5()
y
```
- If however, we want to create a variable (or update existing variable) inside the function intentionally, we may use `forced assignment` denoted as `<<-`.  See this example
```{r}
y <- 5
my_fun5 <- function(){
  y <<- 1
  return(y)
}
my_fun5()
y
```
- As already stated, we can create object of any type using a custom function.
```{r}
my_list_fun <- function(x){
  list(sum=sum(x),
       mean = mean(x),
       sd = sd(x))
}
my_list_fun(1:10)
```

## Existing and useful functions in base R
R has a lot of inbuilt/existing functions that are useful and therefore it is good to know about them.  Let us discuss a few of these existing functions which are useful for data analytics and other allied jobs.

Firstly, let's learn logical operators that will be useful to check various conditions.  For those who doesn't know what operators are, they may simply think of operators being special kind of functions having exactly two arguments.

### Conditions and logical operators/operands
Table: (\#tab:table2) Conditions and logical operators/operands

```{r table2, echo=FALSE, message=FALSE, warnings=FALSE, results='asis', fig.env="table", fig.cap="A dummy"}
tabl <- "
| Operator/ function      | Meaning             | Example                    |
|:-------------:|:-----------------------------|:--------------------------------------------|
|   `==`        | Is RHS equal to LHS?             | `5 == 2` will return FALSE |
|               |                                  | `'Anil' == 'anil'` is FALSE |
| `!=` | Is RHS not equal to LHS? | `'ABCD' != 'abcd'` is TRUE |
|   `>=`        | Is LHS greater than or equal to RHS?      | `5 >= 2` will return TRUE  |
| `<=`          | Is LHS less than or equal to RHS?      | `15 <= 2` will return FALSE|
| `>`  | Is LHS strictly greater than RHS? | `2 > 2` will return FALSE |
| `<`  | Is LHS strictly less than RHS? | `12 < 12` will return FALSE |
| `is.na()` | Whether the argument passed is NA | `is.na(NA)` is TRUE |
| `is.null()` | Whether the argument passed is null | `is.null(NA)` is FALSE |
| `|` | Logical OR | `TRUE | FALSE ` will return `TRUE` |
| `&` | Logical AND | `TRUE & FALSE ` will return `FALSE` |
| `!` | Logical NOT | `!TRUE` will return `FALSE` |
| `||` | Element wise Logical OR | Examines only the first element of the operands resulting into a single length logical vector |
| `&&` | Element wise Logical AND | Examines only the first element of the operands resulting into a single length logical vector |
| `%in%` | LHS __IN__ RHS |Checks whether LHS elements are present in RHS vector |

"
cat(tabl) 
```

### Vectorisation of operations and functions {-}

All the above mentioned operators are __vectorised__.  Except `||` and `&&` will return vector of same length as we are comparing.  Check
```{r}
LETTERS[1:4] == letters[1:4]
10:1 >= 1:10
# TRUE will act as 1 and FALSE as 0
x <- c(TRUE, FALSE, FALSE, TRUE)
y <- c(1, 0, 1, 10)
x == y
# Examples of element wise operations
x & y
x | y

# character strings may be checked for alphabetic order
'ABCD' >= 'AACD'
```

### Recycling {-}
__Recycling__ rules apply when two vectors are not of equal length.  See these examples.
```{r}
# Notice that results are displayed silently
LETTERS[1:4] == 'A'
#Notice that results are displayed with a warning
LETTERS[1:5] == LETTERS[1:3]
```

The __operator `%in%`__ behaves slightly different from above.  Each searches each element of LHS in RHS and gives result in a logical vector equal to length of LHS vector.  See these examples carefully.
```{r}
'A' %in% LETTERS
LETTERS %in% LETTERS[1:4]
```

### Handling Missing values `NA` in these operations {-}
While checking for any condition to be `TRUE` or `FALSE` missing values `NA` and/or `NaN` should be handled carefully or a bug may be introduced.  See these examples-
```{r}
FALSE != NA
TRUE != NA
```
Thus, if any of the condition is evaluated on a vector, we can have `NA` in our output along with `TRUE` and `FALSE`.  See this example
```{r}
x <- c(1, 5, 15, NA, 2, 3)
x <= 5
```
These missing values however behaves slightly different with logical operators `&` `|`.  See these examples.
```{r}
TRUE | NA
FALSE & NA
```

### Use of above logical operators for subsetting {-}
Since the logical operations on vectors gives a `logical` vector as output, these can be used for sub-setting as well.  See these examples.
```{r}
my_ages <- c(40, 45, 31, 51, 25, 27, 59, 45)
# filter ages greater than or equal to 30
my_ages[my_ages >= 30]

my_names <- c("Andrew", "Bob", "Carl", "Daven", "Earl")
# filter names which start with alphabet either A, B or C
my_names[my_names <= "D"]
```

### Conditions with `ifelse` {-}
Syntax `ifelse(test, yes, no)` will be used to return value (of same shape as `test`) which is filled with elements selected from either `yes` or `no` depending on whether the elements of `test` are `TRUE` or `FALSE`.  See this example 
```{r}
x <- c(1:5, NA, 16:20)
ifelse(x>5, 'Greater than 5', 'Upto 5')
```

### Functions `all()` and `any()` {-}
These are shortcut functions to tell us whether `all` or `any` of the elements of given object are `TRUE`.  See This example
```{r}
x <- 11:20
all(x > 5)
any(x > 20)
```

All of the above mentioned operators (along with those listed in section \@ref(calculator)) are __vectorised__. Check these examples.
```{r}
x <- 1:5
y <- 6:10

x + y
x - y
x * y
x / y
x ^ y
# Caution: here RHS is not a vector
y %% 3
y %/% 3
```
__Recycling__ also applies on mathematical operators.  See these examples and notice when R gives results silently and when with a warning.
```{r}
10:15 + 4
100:110 - 50
# when length of one vector is multiple of length of smaller vector
x <- c(5, 2, 7, 9)
y <- c(7, 8)
x + y
# when length of one vector is not multiple of length of smaller vector
x + c(1, 2, 3)
```

All the above-mentioned operators/functions may also be used on matrices, arrays of larger dimension, since we have already seen that matrices/arrays are actually vectors at the core.
```{r}
mat1 <- matrix()
```

## Common arithmetical Functions
Table: (\#tab:table4) Common Arithmetical Functions
```{r table4, echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}
tabl2 <- "
| Function      | Meaning             | Input                    | Output   |
|:-------------:|:----------------------|:-----------------------|:-------------------------------|
| `sum()` | Adds all elements  | One or more Vector, matrix, array | Vector having 1 element only |
| `prod()` | Returns product of all elements | One or more Vector, matrix, array | Vector having 1 element only |
| `mean()` | Returns the arithmetic mean | One Vector, matrix, array | Vector having 1 element only |
| `max()` | Returns maximum value | One or more Vector, matrix, array | Vector having 1 element only |
| `min()` | Returns minimum value | One or more Vector, matrix, array | Vector having 1 element only |
| `ceiling()` | Returns integer(s) not less than given values | One Vector, matrix, array | Vector, matrix, array having same `dim` |
| `floor()` | Returns largest integers not greater than given values | One Vector, matrix, array | Vector, matrix, array having same `dim` |
| `trunc()` | returns integers formed by truncating the values towards 0 | One Vector, matrix, array | Vector, matrix, array having same `dim` |
| `round(x, digits = 0)` | Rounds the given value(s) to number of decimal places provided | One Vector, matrix, array | Vector, matrix, array having same `dim` |
| `signif(x, digits = 6)` | Round to `significant` digits | One Vector, matrix, array | Vector, matrix, array having same `dim` |
| `factorial()` | Returns factorial | One Vector, matrix, array of `integer` type | Vector having 1 element |
| `sqrt()` | Returns square root | One Vector, matrix, array | Vector, matrix, array having same `dim` |
| `log10()` or `log2()` | Logrithm with base 10 or 2 respectively | One Vector, matrix, array | Vector, matrix, array having same `dim` |
| `exp(x)` | returns exponential | One Vector, matrix, array | Vector, matrix, array having same `dim` |
"
cat(tabl2) 
```

See these examples.
```{r}
sum(1:100, 1:10)

Mat1 <- matrix(1:10, nrow = 2)
Mat2 <- matrix(1:4, nrow = 2)

prod(Mat1, Mat2)
sqrt(Mat2)
log10(Mat1)
factorial(10:1)
```

### Missing values {-}
If the vector on which we are calculating `sum` etc., has missing values, we will have to use argument `na.rm = TRUE` in these functions (Check documentation of these functions individually once).  See these examples -
```{r}
x <- c(1:50, NA)
sum(x)
sum(x, na.rm = TRUE)
mean(x, na.rm = TRUE)
```

## Some Statistical functions
Table: (\#tab:table5) Some commonly used Statistical Functions
```{r table5, echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}
tabl3 <- "
| Function      | Meaning             | Input                    | Output   |
|:-------------:|:----------------------|:-----------------------|:-------------------------------|
| `sd()` | Returns standard deviation | One Vector, matrix, array | Vector having 1 element only |
| `var()` | Returns variance | One or more Vector, matrix, array | Vector having 1 element only |
| `median()` | Returns median value | One Vector, matrix, array | Vector having 1 element only |
| `range()` | Returns range | One Vector, matrix, array | Vector having 2 elements |
| `IQR()` | Computes interquartile range of the x values | One Vector, matrix, array | Vector having 1 element only |
| `quantile()` | Computes percentile of given values for the given probabilities in `probs` argument | One Vector, matrix, array | Named Vector having 5 elements by default, OR equal to the length of `probs` vector given |

"
cat(tabl3) 
```
Examples-
```{r}
median(1:100)
range(1:100, 45, 789)
quantile(1:100)
quantile(0:100, probs = 1:10 / 10)
```

## Functions related to sampling and probability distributions {#prob}

#### Set the random seed with `set.seed()` {-}
It is a way to specify the random seed which is an integer vector, containing the random number generator (RNG) state for random number generation in R.  It does not given any output but makes your code reproducible for further use.  

#### Generate random numbers with `rnorm()` / `runif()` / `rpois()` etc. {-}
Used to generate random numbers from normal, uniform and poisson distributions respectively.  Of course there are numerous other functions not only to calculate random numbers but to calculate probability, density of these and other probability distributions (such as binomial, t), but those are beyond the scope of this book.  E.g.
```{r}
rnorm(n=10) #default mean is 0 and SD is 1
rnorm(n=10) # notice these will produce different results each time.
# If however seed is fixed as above, these will be reproducible.
set.seed(123)
runif(10) # default min and max are 0 and 1 respectively
set.seed(123)
runif(10)
```

#### Random Sample with `sample()` {-}
Used to take a sample of the specified `size` from the elements of x using either with or without replacement. E.g.
```{r}
set.seed(123)
sample(LETTERS, 5, replace = FALSE)
set.seed(111)
sample(LETTERS, 15, replace = TRUE)
```
If the sampling is proportionate to given `probabilities` the same can be provided in `prob` argument.
```{r}
set.seed(12)
sample(LETTERS, 5, replace = FALSE, prob = 1:26)
```

## Other Mathematical functions

#### Progressive calculations with `cumsum()` /`cumprod()` {-}
Used to calculate running total or product.  Output vector length will be equal to that of input vector.
```{r}
cumsum(1:10)
cumprod(-5:5)
```
Other similar functions like `cummax()` (cumulative maximum) and `cummin()` may also be useful.
```{r}
set.seed(1)
x <- sample(1:100, 10)
cummin(x)
cummax(x)
```

#### Progressive difference `diff()` {-}
Used to calculate running difference (difference between two consecutive elements) in the given numeric vector. Output will be shorter by one element. E.g.
```{r}
set.seed(123)
x <- rnorm(10)
x
diff(x)
length(diff(x))
```

## String Manipulation functions

#### Concatenate strings with `paste()` and `paste0()` {-}
R's inbuilt function `paste()` concatenates each element of one or more vectors given as argument.  Argument `sep` is used to provide separator is any, which by default is a space i.e. `" "`.  On the other `sep` argument is not available in `paste0` which thus concatenates elements without any separator.
```{r}
paste(LETTERS, letters)
paste0(letters, '_', 1:26) # check replication here
```
_Note:_ that both `paste` and `paste0` returns vector with length equal to length of larger vector.  __Thus if the requirement is to concatenate each of the element in the given vector(s), use another argument `collapse`.  See this example.__
```{r}
paste0(letters, 1:26, collapse = '+')
```

#### Functions `startsWith()` / `endsWith()` {-}
To check whether the given string vector say `x` start or end with string (entries of) `prefix` or `suffix` we can use `startsWith(x, prefix)` or `endsWith(x, suffix)` respectively.  E.g.
```{r}
x <- c('apples', 'oranges', 'apples and oranges', 'oranges and apples', 'apricots')
startsWith(x, 'apples')
startsWith(x, 'ap')
endsWith(x, 'oranges')

```
Note that both these functions return logical vectors having same length as `x`.

#### Check number of characters in string vector using `nchar()` {-}
To count the number of characters in each of the element in string vector, say `x`, we can use `nchar(x)` which will return a vector of integer types. E.g.
```{r}
nchar(x)
y <- c('', ' ', '   ', NA)
nchar(y)
```

#### Change case using `toupper()` / `tolower()` {-}
Changes the case of given vector to all UPPER or lower case respectively.
Example-
```{r}
x <- c('Andrew', 'Bob')
tolower(x)
toupper(x)
```


#### Extract a portion of string using `substr()` {-}
To extract the characters from a given vector say `x` from a given `start` position to `stop` position (both being integers) we will use `substr(x, start, stop)`.  E.g.
```{r}
substr(x, 2, 8)
```

#### Split a character vector using `strsplit()` {-}
To split the elements of a character vector `x` into sub-strings according to the matches to sub-string `split` within them.  E.g.
```{r}
strsplit(x, split = ' ')
```
**Notice that output will be of `list` type.**

#### Replace portions of string vectors `sub()` / `gsub()` {-}

These two functions are used to perform replacement of the first and all matches respectively.  E.g.
```{r}
#Replace only first match
sub(pattern = 'B', replacement = '12', x, ignore.case = TRUE)
# Replace all matches
gsub(pattern = 'B', replacement = '12', x, ignore.case = TRUE)
```

#### Match patterns using `grep()` / `grepl()` / `regexpr()` / `gregexpr()` {-}
These functions are used to match string passed as argument `pattern` under a string vector.  These four however, differ in output/results.  E.g.
```{r}
grep(pattern = 'an', x) # will give indices.  
#                         Output will be integer vector and length may be shorter than that of `x`
grepl(pattern = 'an', x) # will give a logical vector of same length as `x`
regexpr(pattern = 'an', x) # output will have multiple attributes
```
Note that `regexpr()` outputs the character position of first instance of pattern match within the elements of given vector.
`gregexpr()` is same as `regexpr()` but finds all instances of pattern. Output will be in `list` format. E.g.
```{r}
gregexpr(pattern = 'an', x)
```

## Other functions

#### Transpose a matrix using `t()` {-}
Used to return transpose of given matrix. E.g.
```{r}
mat <- outer(1:5, 1:5, FUN = \(x, y) paste0('A', x, y))
mat
t(mat)
```

#### Generate a frequency table using `table()` {-}
Returns a frequency/contingency table of the counts at each combination of factor levels.  E.g.
```{r}
set.seed(123)
x <- sample(LETTERS[1:5], 100, replace = TRUE)
table(x)
```
If more than one argument is passed-
```{r}
set.seed(1234)
df <- data.frame(State_code = x,
                 Code2 = sample(LETTERS[11:15], 100, replace = TRUE))
my_table <- table(df$State_code, df$Code2)
my_table
```

#### Generate proportion of frequencies using `prop.table()` {-}
This function takes a table object as input and calculate the proportion of frequencies.
```{r}
prop.table(my_table)
```


#### Column-wise or Row-wise sums using `colSums()` / `rowSums()` {-}
Used to sum rows/columns in a matrix/data.frame.  E.g.
```{r}
# Row sums
rowSums(my_table)
# Col sums
colSums(my_table)
```
Note Similar to `colSums()`/ `rowSums()` we also have `colMeans()` and `rowMeans()`.
```{r}
rowMeans(my_table)
```

#### Extract unique values using `unique()` {-}
Used to extract only unique values/elements from the given vector. E.g.
```{r}
unique(x) # note the output
```

#### Check if two vectors are identical using `identical()` {-}
Used to check whether two given vectors/objects are identical.
```{r}
identical(unique(x), LETTERS)
```

#### Retreive duplicate items in a vector using `duplicated()` {-}
Used to check which elements have already appeared in the vector and are thus duplicate.
```{r}
set.seed(123)
x <- sample(LETTERS[1:5], 8, replace = TRUE)
x
duplicated(x)
```

#### Generate sequences using other objects with `seq_len()` / `seq_along()` {-}
Used to generate sequence of given integer length starting with 1, or with length equal to given vector, respectively.  E.g.
```{r}
seq_len(5)
x <- c('Andrew', 'Bob')
seq_along(x)
```

#### Divide a vector into categories (factor) using `cut()` {-}
The function divides the range of `x` into intervals and codes the values in `x` according to which interval they fall. The leftmost interval corresponds to level one, the next leftmost to level two and so on.  The output vector will be of type `factor.`  

Example-1:
```{r}
x <- c(1,2,3,4,5,2,3,4,5,6,7)
cut(x, 3)
```
Example-2:
```{r}
cut(x, 3, dig.lab = 1, ordered_result = TRUE)
```
**Note:** that the output `factor` above is ordered.

#### Scale the columns of a matrix using `scale()` {-}
Used to scale the columns of a numeric matrix.
```{r}
x <- matrix(1:10, ncol = 2)
x
scale(x)
```
**Note:** The output will always be of a matrix type with two more attributes.  See this example
```{r}
scale(1:5)
```

#### Output the results using `cat()` {- #cat}
Outputs the objects, concatenating the representations. `cat` performs much less conversion than `print`.
```{r}
cat('ABCD')
```
**Note:** that indices are now _not printed._  `cat` may print objects also.  Example-2:
```{r}
cat(month.name)
```

`cat` is useful to print _special characters._  Example-3:
```{r}
cat('Budget Allocation is \u20b91.5 crore')
```

#### Sort a vector using `sort()` {-}
Used to **sort** the given vector.  Example-1:
```{r}
vec <- c(5, 8, 4, 1, 6)
sort(vec)
```
Argumemt `decreasing = TRUE` is used to sort the vector in descending order instead of default ascending order.
Example-2:
```{r}
sort(vec, decreasing =  TRUE)
```


#### Arrange the elements of a vector using `order()` {-}
In contrast to `sort()` explained above, `order()` returns the indices of given vector in ascending order.  Example
```{r}
order(vec)
```
Thus, `sort(vec)` will essentially perform the same operations as `vec[order(vec)]`.  We may check-
```{r}
identical(vec[order(vec)], sort(vec))
```

#### Check structure using `str()` {-}
The short `str` is not to be confused with strings as it instead is short for `structure`.  Thus, `str` returns structure of given object.  Example
```{r}
str(vec)
```
Extremely useful when we need to inspect data frames.
```{r}
str(iris)
```

#### Generate a summary using `summary()` {-}
In addition to `str` explained above, `summary()` is also useful is getting result summaries of given objects.  Example-1: When given object is vector
```{r}
summary(vec)
```
We observe that when numeric vector is passed, it produces quantile summary.  Example-2: When input object is data frame.
```{r}
summary(iris)
```


## Pipes
```{r echo=FALSE, message=FALSE, warning=FALSE}
library(magrittr)
```

Now here I would like to introduce you with the concept of pipes\index{pipes} in R. There are two types of pipes used-

- `|>` is native pipe of R.  It was introduced in R version 4.1
- `%>%` pipe introduced in `magrittr` package[@R-magrittr], now part of `tidyverse` which we will use extensively in our data analysis tasks.

```{r pipe2, echo=FALSE, fig.align='center', fig.show='hold', fig.cap="Magrittr, the R package, is named after the surrealist painter Rene' Magritte. His painting was self captioned, 'This is not a pipe.'", out.width="99%"}
knitr::include_graphics('images/pipemag.png')
```


Actually `%>%` is predecessor to native R's pipe `|>`.  The pipes are powerful tools for clearly expressing a sequence of operations that transform an object, without the need of actually creating that object in each step.  Let us understand this concept with the following example.  Suppose, we have to three functions say `FIRST` , `SECOND` and `THIRD` to an object `OBJ` in sequence.  So the order of operations would either be like-
```
THIRD(SECOND(FIRST(OBJ)))
```
or with creating intermediate objects, when instead we actually do not need those intermediate objects. 
```
OBJ1 <- FIRST(OBJ)
OBJ2 <- SECOND(OBJ1)
OBJ3 <- THIRD(OBJ2)
```
Here actually we do not require `OBJ1` and `OBJ2`.  So in these cases we either have to compromise with the readability of code i.e. inside out or have to create unwanted objects.  Pipes actually mitigate both these issues simultaneously.  With pipes we can write above operations as either of these -
```
OBJ1 |> FIRST() |> SECOND() |> THIRD()
OBJ1 %>% FIRST() %>% SECOND() %>% THIRD()
```
A diagrammatic representation is given in figure \@ref(fig:pipe).

```{r pipe, echo=FALSE, fig.align='center', fig.show='hold', fig.cap="A diagrammtic illustation of Pipe concept in base R and tidyverse", out.width="99%"}
knitr::include_graphics("images/pipe.png")
```

Now two questions may arise here-

1. What if there are multiple arguments to be passed in any of the operations?
2. Is there any difference between the two pipes?  If yes, which is better OR what are the pros and cons of each?

To answer these questions, we will discuss both pipes separately.

```{r pipe3, echo=FALSE, fig.align='center', fig.show='hold', fig.cap="Using pipes in R", out.width="99%"}
knitr::include_graphics('images/pipes in R.png')
```


### Magrittr/Dplyr pipe `%>%`
Pipes usually pass result of previous operation silently into first argument of next/right expression.  So `data %>% filter(col == 'A')` means `filter(data, col=='A')`.  But there may be cases when result of previous (LHS) expression is required to be passed on second or other argument in RHS expression.  A simple example may be of function `lm`, where `data` argument is second argument. In such cases we can make use special placeholder `.` as result of LHS specifically.  In other words aforesaid filter example can be written with placeholder as `data %>% filter(. , col == 'A')`.  Now using this placeholder we can use result of LHS wherever we want.  See this example
```{r}
iris %>% lm(Sepal.Length ~ Sepal.Width, data = .)
```
Thus `x %>% f(y)` is equivalent to `f(x, y)` but `x %>% f(y, .)` is equivalent to `f(y, x)`.

### Base R pipe `|>` (Version 4.2.0 +)
R version 4.2.0 introduced concept of placeholder `_` similar to dplyr/magrittr, but with a few differences.

- The argument where `_` is to be used, must be named.  So `f(y, z = x)` can be written as `x |> f(y, z= _)`.

```{r}
iris |> lm(Sepal.Length ~ Sepal.Width, data = _) |> summary()
```
The requirement of named argument is not there in dplyr pipe.  So essentially, `iris %>% lm(Sepal.Length ~ Sepal.Width, .)` will also work. But in base R `iris |> lm(Sepal.Length ~ Sepal.Width, _)` would not work and throw an error.  Thus, in cases where the argument of placeholder is not named, we have to use anonymous function. See these-

```
# placeholder without named argument
iris |> lm(Sepal.Length ~ Sepal.Width, _)
```
```{r}
# Correct way to use unnamed argument
iris |> {\(.x) lm(Sepal.Length ~ Sepal.Width, .x)}() |> summary()
```
Type ```` ?`|>` ```` in console and see help page for more details.


<!--chapter:end:04-functions.Rmd-->

# Control statements
In the previous chapter we learnt about many of the useful pre-built functions in R.  In this chapter we will learn how to create customized functions suited to our needs.

**_Though these are core concepts of a programming language, yet a reading to this chapter is advised for better understanding and better application while using r for data analytics._**

## Control flow/Loops

### `if else` {-}
The basic form(s) of `if else` \index{if else statement}statement in R, are-
```
if (test) do_this_if_true
if (test) do_this_if_true else else_do_this
```
So, if `test` is true, `do_this_if_true` will be performed and optionally if `test` is not true `else_do_this` will be performed.  See this example-
```{r}
x <- 50
if(x < 10){
  'Smaller than 10'
} else {
  '10 or more'
}
```
**Note** that the `if`/`if else` are evaluated for a single `TRUE` or `FALSE` i.e. this control flow is **not vectorised** as we have in the case of `ifelse()` function which was vectorised.

### `for` loop {-}
The `for` loops \index{for loops}in r are used to iterate over _given_ items.  So, the basic structure of these loops are -

```
for(item in vector) perform_some_action

# OR

for(item in vector) {
  perform_some_action
}
```

Thus, for each item in `vector`, `perform_some_action` is called once; updating the value of item each time.  This can be understood by the following simple example-
```{r}
for(i in 1:3){
  print(i)
}
```
Conventionally `i` has been used in above example to iterate over given vector `1:3`, however any other symbol may also be used.
```{r}
for(item in 1:3){
  print(item)
}
```
If we use the name of any existing variable as `item` to iterate over the given object, `for loop` assigns the `item` to the current environment, overwriting any existing variable with the same name.  See this example -
```{r}
x <- 500
for(x in 1:3){
  # do nothing
}
x
```

```{r echo=FALSE, fig.cap="A Diagrammatic representation of For Loop", fig.align='center', fig.show='hold', out.width="99%"}
knitr::include_graphics("images/for_loop.png")
```

The idea can also used to iterate over any object any *number of times* as we want.  See these two examples.

Example-1
```{r}
my_names <- c('Andrew', 'Bob', 'Charles', 'Dylan', 'Edward')
# If we want first 4 elements
for(i in 1:4){
  print(my_names[i])
}
```
Example-2
```{r}
# if we want all elements
for(i in seq_along(my_names)){
  print(my_names[i])
}
```

There are 2 ways to terminate any `for loop` early-

- `next` which exits the current iteration only\index{next}
- `break` which breaks the entire loop.\index{break}

See these examples.

Example-1
```{r}
for(i in 1:5){
  if (i == 4){
    next
  }
  print(i)
}
```
Example-2
```{r}
for(i in 1:5){
  if (i == 4){
    break
  }
  print(i)
}
```

### `while` loop {-}
We have seen that `for` loop is used to iterate over a set of `known values` or at least known number of times.  If however, we want to perform some iterative action unknown number of times, we may use `while` loop\index{while loop} which iterates till a given `condition` is `TRUE`.  Another option is to have `repeat` loop\index{repeat loop} which can be used to iterate any number of times till it encounters a `break`.

The basic syntax of `while` loop is-
```
while (condition) action
```
See these examples-

Example-1
```{r}
i <- 1
while(i <=4){
  print(LETTERS[i])
  i <- i+1
}
```
We may check the value of `i` here after executing the loop
```{r}
i
```
Example-2: 
```{r}
i <- 4
while(i >=0){
  print(LETTERS[i])
  i <- i-1
}
```
**Note:** We have to make sure that the statements inside the brackets modify the `while condition` so that sooner or later the given condition is no longer `TRUE` otherwise the loop will never end and will go on forever.

```{r echo=FALSE, fig.cap="Author's illustration of While Loop", fig.align='center', fig.show='hold', out.width="99%"}
knitr::include_graphics("images/while_loop.png")
```

> **_Looping in R can be inefficient and time consuming when you’re processing the rows or columns of large data-sets. Even one of greatest feature in R is its parallel processing of vector objects.  Thus, whenever possible, it’s better to use R’s built-in numerical and character functions in conjunction with the `apply` family of functions.(We will discuss these in detail in the chapter related to functional programming)_**



<!--chapter:end:05-conditionals.Rmd-->

# Functional Programming

```{r echo=FALSE}
library(knitr)
```

## What is functional programming?

Conceptually functional programming philosophy is based on lambda calculus. Lambda calculus is a framework developed by Alonzo Church^[Alan Turing, who created Turing machine which in turn laid the foundation of imperative programming style, was a student of __Alonzo Church__.] to study computations with functions.

Functional programming is a programming paradigm in which we try to bind everything in pure mathematical functions style. It is a declarative type of programming style. Its main focus is on __what to solve__ in contrast to an imperative style where the main focus is __how to solve__.  For a more elaborated definition readers may see this wikipedia [link](https://en.wikipedia.org/wiki/Functional_programming).  Simply putting functional programming is like doing something repeatedly but in declarative style.  Here, functions are the primary method with which we carry out tasks. All actions are just implementations of functions we are using.

Functional programming use high order functions.  A high order function is actually a function that accepts a function as an argument, or returns a function; in short, function that operates upon a function.  We have already seen one such example may be without noticing it, `args()` function take a function as an argument and in turn return its arguments.

Let us learn a bit more here.

### Usage of functional programming in R 

Strictly speaking R is not a functional programming language.  But we have already seen that one of the greatest strengths of R is parallel operations on vectors. In fact we need functional programming where concurrency or parallelism is required.  Till now we have seen that most of the functions work on all atomic objects (vectors, matrices, arrays, etc.), but what about working of these functions on recursive objects i.e. lists?  Check this example (in your console)-
```
list1 <- list(50000, 5000, 56)
sum(list1)
```
> Of course, we can solve the above problem by using for loops.  See
```{r}
list1 <- list(50000, 5000, 56)
# for loop strategy
x <- c()
for(i in seq_along(list1)){
  x[i] <- list1[[i]]
}
sum(x)
```
Consider another list, where we want to calculate mean of each element of that list.
```{r}
list2 <- list(
  1:10,
  11:20,
  21:30
)
```

Of course, we may use a for loop again, but in R these operations can be done easily with *apply* group of functions, which are one of the most famous and most used features in R.  

## `apply` family of functions

First of these functions is `apply()` which works on matrices/data frames.

### Function `apply()`
The basic syntax of `apply` is
```
apply(m, MARGIN, FUN, f_args)
```
where 

- `m` is the matrix
- `MARGIN` is the dimension.  If we want to _apply_ function to each row then use `1` or else if it is to be column-wise use `2`
- `FUN` is the desired function which we want to apply
- `f_args` are the optional set of arguments, if needed to be supplied to `fun`.

An illustrative construction of `apply` function can be seen in \@ref(fig:applyimage).

```{r applyimage, out.width="100%", echo=FALSE, fig.align='center', fig.show='hold', fig.cap="Illustration of function apply"}
knitr::include_graphics('images/apply.png')
```

Check this example
```{r}
(mat <- matrix(1:10, nrow = 5))
apply(mat, 1, mean)
```
**Note:** `rowMeans(mat)` in above example would have given similar results, but for sake of simplicity we have provided a simplest example.

**Further note that we may also write our own customised function in the argument.**  See this another example, where we will take sum of squares of each row.  We may define our own custom function for the purpose and then `apply` it.
```{r}
my_fun <- function(x){
  sum(x^2)
}
apply(mat, 1, my_fun)
```
The need to writing a custom function before hand may be eliminated if the function so defined is not be used further.  We may write anonymous function directly in the `apply` syntax - 

```{r}
apply(mat, 1, FUN = function(x) sum(x^2))
```

::: {.rmdnote}
**In R version 4.1 and onwards R has devised shorthand style of defining inline custom functions, where we can write backslash i.e. `\` instead of writing `function`.**  We could have written above expression as-
```{r collapse=TRUE}
apply(mat, 1, FUN = \(x) sum(x^2))
```
:::

#### `apply()` need not necessarily output vectors only {-}
If `FUN` applied on rows/columns of matrix outputs vector of length more than 1, the output will be in matrix format.  But the thing to note here is that matrix will be displayed columnwise always irrespective of fact whether `MARGIN` is `1` or `2`.  As an easy example we could have shown this using function like `sqrt`, but `apply(matrix, MARGIN, sqrt)` will work like `sqrt(matrix)` only.  So let's take a different example.  Suppose we want to calculate `column-wise` cumulative sum in a given matrix.
```{r}
apply(mat, 2, cumsum)
```
The output here is eaxctly what was desired.  But what if, our requirement was to take `row-wise` cumulative sum?
```{r}
apply(mat, 1, cumsum)
```
It may now be noticed that the output is actually _transpose_ of what we were expecting.  Actually the output of each iteration of `apply` function is displayed in one column always. Let us check our understanding with one more example taking function which may give output that is not dependent on input vector length.  

```{r}
set.seed(1)
apply(mat, 1, sample, 4, TRUE)
```

Thus we may conclude that- 

- If `FUN` outputs a scalar, the output of `apply` will be a vector of __length__ equal to 
   - number of `rows` in input matrix given that `MARGIN` selected in 1,
   - number of `columns` in input matrix given that `MARGIN` selected in 2.
- if `FUN` outputs a vector(of length >1) then output of `apply` will be a matrix having __number of columns__ equal to -
   - number of `rows` in input matrix given that `MARGIN` selected in 1,
   - number of `columns` in input matrix given that `MARGIN` selected in 2.
   
These have been tabulated in table \@ref(tab:apply).

Table: (\#tab:apply) Relation between input and output data structure in `apply`

| Input matrix $m*n$      | `MARGIN = 1`  | `MARGIN = 2`  |
|-------------------------|:-------------:|:-------------:|
| FUN gives scalar        | Vector size $m$ | Vector size $n$ |
| FUN gives vector size $p$| Matrix $p*m$ | Matrix $p*n$ |


We may thus have to be careful while getting the output from `apply` function as it may lead to introduction of bug in our code.

#### `apply()` on data frames {-}
Now we know that data frames despite being special type of lists also behave like matrices, we may use `apply` on data frames too.  See this example.
```{r}
(my_df <- as.data.frame(mat))
apply(my_df, 2, sum)
```


### Function `lapply()` 

Another cousin of apply is `lapply` which can thought of `apply` to `l`ists.  So as the name suggests it is applied on lists instead of matrices. Now since `data frame` is also a list `lapply` can be applied on these.  The basic syntax of `lapply()` is -
```
lapply(l, FUN, f_args)
```
where 

- `l` is the list
- `FUN` is the desired function which we want to apply
- `f_args` are the optional set of arguments, if needed to be supplied to `fun`.

It may be noted that `MRAGIN` argument is not available here.  See these examples.
```{r}
lapply(my_df, sum)
```
Now you may have noticed two things here -

1. The output is of `list` type.
2. Unlike `apply` as `MARGIN` is not passed/available here, it applies `FUN` to every element of list.  When we consider any `data.frame` as a list its each column is a separate element of that list.  So `FUN` cannot be applied to `rows` in a `data.frame`.

Thus `lapply()` -

- loops over a list, iterating over each element in that list
- then _applies_ the function `FUN` to each element
- and then returns a list.

Example-2: Let's try to find type of each column in a given data frame.
```{r}
lapply(iris, typeof)
```

Similar to `apply` we can define `FUN` inline here (anonymously) also.  Example-3:
```{r}
lapply(my_df, \(a) a^2) 
```

Example-4:
```{r}
set.seed(1)
lapply(1:4, runif, min=0, max=10)
```

**Note** that even if `lapply` is applied over a vector, it returns a list only.

### Function `sapply()`
There is not much of the difference between `lapply()` and `sapply()`, as `sapply` is actually `s`implified l`apply`.  It simplifies the argument as much as possible.

Example:
```{r}
sapply(my_df, sum)
```

## Other loop functions 

### Function `replicate()`

Function `replicate()`\index(replicate) is used for repeated evaluation of an expression.  Syntax is
```
replicate(n, expr, simplify = "array")
```
where -

 - `n` is integer denoting the number of replications
 - `expr` is the expression to evaluate repeatedly
 - `simplify` takes either 'character' or 'logical' to value to indicate whether the results should be simplified.
 
Example:
 
```{r}
set.seed(123)
# Default value of simplify will simplify the results as much possible
replicate(5, runif(3))
# Notice the difference with simplify=FALSE
replicate(3, runif(5), simplify = FALSE)
```

### Function `split()` 

The `split()`\index(split() function) function takes object (vector or other) and splits it into groups determined by a given factor.  The basic syntax is-
```
split(x, f, drop=FALSE, ...)
```
where 

- `x` is input object - `vector` or `list` or `data.frame`
- `f` is a factor or a list of factors.  If a factor is not provided, it will be coerced to factor.
- `drop` argument indicates whether empty factors should be dropped.

Example: (To divide the given list by alternate elements)- 
```{r}
split(LETTERS, rep(1:2, 13))
```
Example-2: Find out sum of every odd and even number from 1:100-
```{r}
split(1:100, (1:100) %% 2) |> lapply(sum)
```

Example-3: Find out mean of `mpg` column splitting the `mtcars` data by `cyl`
```{r}
split(mtcars$mpg, mtcars$cyl) |> sapply(mean)
```

### `tapply()` 

The `tapply()` function\index(tapply() function) can be thought of combination of `split` and `sapply` for vectors, exactly as used in above example.  It actually applies the function over subsets of a given vector.  The basic syntax is-
```
tapply(X, INDEX, FUN = NULL, ..., default = NA, simplify = TRUE)
```
Where -

- `X` is a vector
- `INDEX` is factor or list of factors
- `FUN` is function to be applied
- `...` are other arguments, if any, of `FUN` to be passed
- `simplify` if TRUE simplifies the result.

See this example
```{r}
tapply(mtcars$mpg, mtcars$cyl, mean)
```

Needless to say if `simplify` is `FALSE` the results will not be simplified. See this example-
```{r}
# month-wise mean of temperatures from `airquality` data
tapply(airquality$Temp, airquality$Month, mean, simplify = FALSE)
```

### `by()` function

This\index(by() function) works something like `tapply` but with the difference that input object here is `data.frame`.  See this example
```{r}
# Split the data by `cyl` column and subset first six rows only
by(mtcars, mtcars$cyl, head)
```

### Specifying the output type with `vapply()`
Function `vapply()`\index(vapply() function) works exactly like `sapply()` described above, with only difference that type of return value (output) has to be specifically provided through `FUN.VALUE` argument.  Its syntax is -
```
vapply(X, FUN, FUN.VALUE, ..., USE.NAMES = TRUE)
```
 - In the argument `FUN.VALUE` we have to provide the format type of output.  See this example.
```{r}
vapply(mtcars, max, FUN.VALUE = double(1))
```
Through `FUN.VALUE = double(1)` we have specifically provided that our output should be of `double` type with length `1`.  So in case we have to find out `range` of each column-
```{r}
vapply(mtcars, range, FUN.VALUE = double(2))
```
If we will try this function on a dataset having mixed type columns like `iris` dataset, `vapply` will throw an error.
```
vapply(iris, range, FUN.VALUE = double())
```
## Functional Programming in `purrr` {#purrr}
Package `purrr`^[[https://purrr.tidyverse.org/](https://purrr.tidyverse.org/)], which is part of core `tidyverse`, enhances R’s functional programming (FP) toolkit by providing a complete and consistent set of tools for working with functions and vectors. 

```{r}
library(purrr)
```

### Iterate over single list/vector with `map_*()` family of functions
This package has many families of functions; and most primary family is `map` family of functions.  `map_*()` works nearly similar to `vapply` where we can control the type of output.  The syntax style of each of these functions is nearly same, where these accept one object (list or vector) as `.x` argument, one function (or alternatively a formula) as `.f` argument; and outputs an object of specified type.

Example
```{r}
map(mtcars, .f = sum)
```
Note that output type is list.  If the output can be simplified to an atomic vector we can use either of these functions depending upon the output type of that vector.

- `map_lgl` for `logical` format
- `map_int` for `integer` format
- `map_dbl` for `double` format
- `map_chr` for `character` format

`map` always return a list.
See these further examples.

Example-1:
```{r}
map_dbl(mtcars, max)
```

### Iterate over two or more lists/vectors using `map2_*()`/ `pmap_*()` family

So far we have seen that `map_*()` family of functions are used to iterate over elements of a list.  Even if extra lists/vectors are provided as extra arguments, these are used as it is, in each iteration, as can be seen in first illustration in figure^[Source: [Advanced R](https://adv-r.hadley.nz/functionals.html) by Hadley Wickham] \@ref(fig:map2).

```{r map2, fig.cap="Working of map vs map2 family of functions \\hspace{\\textwidth} Source Advanced R by Hadley Wickham", echo=FALSE, fig.show='hold', fig.align='center', out.width="49%"}
knitr::include_graphics(c("images/map1.png", "images/map2.png" ))
```
In order to iterate over two vectors/lists, we will however, need `map2_*()` family of functions (Refer second illustration in figure \@ref(fig:map2)).

See the following example
```{r}
x <- list(1, 2, 3)
y <- list(11, 12, 13)
map2(x, y, `*`)
```
Similarly, to iterate over multiple lists we will use `pmap_*()`, with the only difference being that here all the vectors/list should be collectively passed on to `pmap` in a list.  This can be better understood with the illustration used by Hadley Wickham in his [book](https://adv-r.hadley.nz/functionals.html).  For reference see figure \@ref(fig:pmap).

```{r pmap, fig.cap="Working of pmap family of functions \\hspace{\\textwidth} Source Advanced R by Hadley Wickham", echo=FALSE, fig.show='hold', fig.align='center', out.width="49%"}
knitr::include_graphics(c("images/pmap.png", "images/pmap-arg.png"))
```

<!--chapter:end:06-functionalprog.Rmd-->

# Visualisations in Base R
It is a common practice to visualize data as soon as we start analysing it.  While small data can be easily visualised in data format, the most common being opening it in MS Excel format, it becomes a hurdle to look at it when the data is bigger.  Undoubtedly, visualization is an essential tool for data analysis because it enables us to see patterns and relationships that may not be apparent from a simple numerical summary. For example, a histogram can show the distribution of a variable, which can help identify outliers or skewness that may be hidden in the summary statistics. Similarly, a scatter plot can show the relationship between two variables, which can help identify correlation or causation that may not be apparent from summary statistics.

Visualization also plays a crucial role in communicating data analysis results to others. A well-designed graph or chart can convey information more effectively than a table of numbers or a lengthy report. It can also help identify errors or anomalies in the data and facilitate better decision-making.

In R, the base graphics system provides a powerful set of functions for creating various types of graphs and plots. In this chapter, we will cover the most commonly used functions for creating visualizations in base R, including `plot()`, `boxplot()`, `barplot()`, `hist()`, `pie()`, `dotchart()` and kernel density plots. 

> Note:  In next chapter we will cover visulaisation through `ggplot2` library which is very extensive, and is developed on theortical framework known as 'grammar of graphics'.  This grammar, created by Leland Wilkinson, has been implemented in a variety of data visualisation software like Tableau^[[https://statmodeling.stat.columbia.edu/2021/05/14/tableau-and-the-grammar-of-graphics/](https://statmodeling.stat.columbia.edu/2021/05/14/tableau-and-the-grammar-of-graphics/)], Plotly, etc.

## Using `plot()`
The `plot()` function in R is a very versatile function that can create a wide variety of visualizations. Its primary purpose is to create scatterplots, but it can also create line plots, bar plots, box plots, and many other types of plots. The "plot" function can be customized in many ways, allowing users to create high-quality visualizations that are tailored to their specific needs.

The basic syntax of the "plot" function is as follows:
```
plot(x, y, ...)
```
The `x` and `y` arguments specify the variables to be plotted on the x-axis and y-axis, respectively. The `...` argument can include a wide range of optional arguments that customize the plot. Some of the most commonly used optional arguments include:

- `xlab`: Specifies the label for the x-axis.
- `ylab`: Specifies the label for the y-axis.
- `main`: Specifies the main title for the plot.
- `xlim`: Specifies the limits for the x-axis.
- `ylim`: Specifies the limits for the y-axis.
- `type`: Specifies the type of plot to be created (e.g., `"p"` for points, `"l"` for lines,`"b"` for both points and lines, etc.).
- `col`: Specifies the color of the plot elements (e.g., points, lines, etc.).
- `pch`: Specifies the symbol used for the points in the plot.
- `cex`: Specifies the size of the plot elements (e.g., points, lines, etc.).

> Note: Here `x` and `y` refer to vectors instead of column names, as we will see in next chapter on ggplot2 where we will use column names, instead of individual vectors.  You'll understand the difference in the following example.

Example of scatterplot: (This will create a scatterplot with "wt" on the x-axis and "mpg" on the y-axis, with axis labels and a main title.)
```{r p1, fig.align='center', out.height="55%", fig.show='hold'}
plot(mtcars$wt, 
     mtcars$mpg, 
     xlab = "Car Weight (1000 lbs)", 
     ylab = "Miles per Gallon", 
     main = "Scatterplot of MPG vs Car Weight")
```

Example-2: Lineplot (This will create a line plot with "pressure"^[The "pressure" dataset contains measurements of vapor pressure of mercury as a function of temperature, measured at 10 different times. The line plot will show the trend of vapor pressure over time.] on the y-axis and "time" on the x-axis, with axis labels and a main title.)
```{r p2, fig.align='center', out.height="55%", fig.show='hold'}
plot(pressure, 
     type = "l", 
     xlab = "Time", 
     ylab = "Pressure", 
     main = "Line Plot of Pressure vs Time")

```
Note in above example that, the "type" argument is set to "l" to specify that a line plot should be created. 

Example-3: Bar-Plot (bar plot of the number of cars by "cyl" i.e. number of cylinders)
```{r p3, fig.align='center', out.height="55%", fig.show='hold'}
barplot(table(mtcars$cyl), 
        xlab = "Number of Cylinders", 
        ylab = "Frequency", 
        main = "Barplot of Number of Cars by Cylinders")

```
Note: In above example we have used `table()` function to get the frequency table of `mtcars$cyl`.

Example-4: Boxplot ()
```{r p4, fig.align='center', out.height="55%", fig.show='hold'}
# Create a boxplot of "Sepal.Length" by "Species" in the iris dataset
plot(x = iris$Species, 
     y = iris$Sepal.Length, 
     main = "Boxplot of Sepal Length by Species",
     xlab = "Species", 
     ylab = "Sepal Length", 
     col = "darkgray")

```

Example-5: density plot
```{r p5, fig.align='center', out.height="55%", fig.show='hold'}
# Create a density plot of "Sepal.Length" in the iris dataset
plot(density(iris$Sepal.Length), main = "Density Plot of Sepal Length in the iris dataset",
     xlab = "Sepal Length", ylab = "Density", col = "blue")

```
Note that we have used `density()` function within `plot()` to create a density plot.

Let us learn about other plotting functions in base R.

## Bar plots using `barplot()`
As the name suggests `barplot()` function is used to create bar charts in R. The basic syntax is `barplot(height, ...)` where `height` should be a vector providing heights of each bar.  Other arguments in ellipsis `...` can be checked by using `?barplot()`.

Here are a few examples of how to use the function:

Example: Basic Bar Chart
```{r p6, fig.align='center', out.height="55%", fig.show='hold'}
# Create a basic bar chart of the "mpg" dataset

barplot(mtcars$mpg, names.arg = rownames(mtcars), main = "Miles Per gallon - mtcars", xlab = "Car Model", ylab = "MPG")

```

Example- Bar chart with summarised data
```{r p7, fig.align='center', out.height="55%", fig.show='hold'}
# Create a summarised bar chart of the "PlantGrowth" dataset
data(PlantGrowth)
barplot(height = t(tapply(PlantGrowth$weight, 
                          list(PlantGrowth$group), 
                          mean)), 
        main = "Mean Weight of Plants by Group", 
        ylab = "Group", 
        xlab = "Weight", 
        col = c("red", "green", "blue"), 
        beside = TRUE, 
        horiz = TRUE)
```

Example- Grouped Bar Chart
```{r p8, fig.align='center', out.height="55%", fig.show='hold'}
# Create a grouped bar chart of the "ChickWeight" dataset
data(ChickWeight)
barplot(height = t(tapply(ChickWeight$weight, 
                          list(ChickWeight$Diet, ChickWeight$Time), 
                          mean)), 
        main = "Mean Weight of Chicks by Diet and Time", 
        xlab = "Diet and Time", ylab = "Weight", 
        col = c("red", "green", "blue", "yellow"), 
        beside = TRUE,
        legend.text = c("Diet 1", "Diet 2", "Diet 3", "Diet 4"))
```


Example: Stacked Bar Chart
```{r p9, fig.align='center', out.height="55%", fig.show='hold'}
# Create a stacked bar chart of the "VADeaths" dataset
data(VADeaths)
barplot(as.matrix(VADeaths), 
        main = "Death Rates by Age Group and Gender", 
        xlab = "Age Group", 
        ylab = "Death Rate", 
        col = c("red", "green", "blue", "yellow", "purple"), 
        legend.text = c("Females", "Males"), 
        beside = FALSE)

```

In fact, we can also create barplots using formula directly.  See this example
```{r p10, fig.align='center', out.height="55%", fig.show='hold'}
barplot(GNP ~ Year, data = longley)
```

## Histograms using `hist()`
The `hist()` function in R is used to create a histogram, which is a graphical representation of the distribution of a numeric variable. The basic syntax for `hist()` is as follows:

```
hist(x, breaks = "Sturges", freq = TRUE, main = NULL,
     xlab = NULL, ylab = "Frequency", ...)
```
where -

- `x`: The data to be plotted, which should be a numeric vector or a matrix.
- `breaks`: The number of bins to use in the histogram. By default, R uses the Sturges formula to determine the number of bins, but you can also specify a different number of bins or a vector of breakpoints.
- `freq`: A logical value indicating whether to plot the frequency or density of the data. If `TRUE`, the y-axis represents the number of observations in each bin. If `FALSE`, the y-axis represents the density of the data.
- `main`: A character string specifying the title of the histogram.
- `xlab`: A character string specifying the label of the x-axis.
- `ylab`: A character string specifying the label of the y-axis.
- `...`: Additional arguments passed to the `plot()` function, such as `col`, `border`, and `xlim`.

Example -
```{r p11, fig.align='center', out.height="55%", fig.show='hold'}
hist(iris$Sepal.Length, breaks = 10, col = "blue",
     xlab = "Sepal Length", ylab = "Frequency",
     main = "Histogram of Sepal Length in Iris Dataset")
```

Another example using two layers
```{r p12, fig.align='center', out.height="55%", fig.show='hold'}
hist(mtcars$mpg,
     freq=FALSE,
     breaks=12,
     col="red",
     xlab="Miles Per Gallon",
     main="Histogram, with density curve")
# add density curve
lines(density(mtcars$mpg), col="blue", lwd=2)
```

In above example we have used `lines()` function to add another layer to existing plot.  

## Boxplot(s) and variants using `boxplot()`

A Box-plot is also called box and whiskers plot and is used to show distribution of a numerical variable using graphical summaries.  Deciphering box plot can be understood using the following illustration.
```{r boxplots, echo=FALSE, fig.cap="Understanding boxplots", fig.align='center', fig.show='hold', out.height="45%"}
knitr::include_graphics('images/boxplots.png')
```

Using `boxplot()` function is simple, as the syntax is `boxplot(x, ...)` where `x` is a numeric vector or a list of numeric vectors to be plotted, and `...` represents any additional arguments that can be used to customize the appearance of the box plot.

Example-
```{r p13, fig.align='center', out.height="55%", fig.show='hold'}
boxplot(mtcars$mpg)
```

However, the boxplots are particularly useful when drawn in parallel over several categories.  To draw these, we can directly use formula as we did in example above.  E.g. the following code will create box-plots of for each type of spray, from InsectSprays^[a base R data-set having the counts of insects in agricultural experimental units treated with different insecticides.] data.
```{r p14, fig.align='center', out.height="55%", fig.show='hold'}
boxplot(count ~ spray, 
        data = InsectSprays, 
        col = "lightgray")
```

## Saving and exporting charts
See last section of next chapter.

## Use of `par()`
One of the useful functions while using base R's graphics is `par()` which is used to set or query graphical **par**ameters.  See the following example, where one of its argument `mfrow = c(nr, nc)` has been used inside this function to draw subsequent plots in an `nr`-by-`nc` array on the device.  
```{r}
par(mfrow = c(2,2))
purrr::walk(1:4, 
     ~plot(anscombe[[paste0('x', .x)]], 
           anscombe[[paste0('y', .x)]], 
           xlab = paste0('x', .x),
           ylab = paste0('y', .x),
           main = paste("Anscombe's Quartet Chart No.", .x))
     )
```


<!--chapter:end:07-plot.Rmd-->

# Visualisation with `ggplot2`

## Core concepts of **grammar of graphics**
__ggplot2__^[[https://ggplot2.tidyverse.org/](https://ggplot2.tidyverse.org/)] [@R-ggplot2] is the package developed by Hadley Wickham, which is based on concepts laid (2005) down by Leland Wilkinson in his _The Grammar of Graphics_.^[[https://link.springer.com/book/10.1007/0-387-28695-0](https://link.springer.com/book/10.1007/0-387-28695-0)]  Basically, a grammar of graphics is a framework which follows a layered approach to describe and construct visualizations or graphics in a structured manner. Even the letters `gg` in ggplot2 stand for `g`rammar of `g`raphics.

Hadley Wilkinson, in his paper titled __A Layered Grammar of Graphics__^[[http://vita.had.co.nz/papers/layered-grammar.pdf](http://vita.had.co.nz/papers/layered-grammar.pdf)](2010) [@layered-grammar] proposed his idea of layered grammar of graphics in detail and simultaneously put forward his idea of _ggplot2_ as an open source implementation framework for building graphics.  Readers/Users are advised to check the paper as it describes the concept of grammar of graphics in detail.  By the end of the decade the package progressed^[Version 3.3.0 was released in March 2020] to one of the most used and popular packages in R.

The relationship between the components explained in both the grammars can be illustrated with the following figure^[Source: Hadley Wickham's paper on _the layered grammar of graphics_].  The components on the left have been put forward by Wilkinson whereas those on right were proposed by Wickham.  It may be seen that `TRANS` has no relation in `ggplot2` as its role is played by in-built features of R.

```{r echo=FALSE, fig.cap="Layers in Grammar of Graphics mapped in GGPLOT2", fig.align='center', fig.show='hold', out.height="75%"}
knitr::include_graphics("images/layers_gg.png")
```

Thus, to build a graphic having one or more dimensions, from a given data, we use _seven_ major components -

1. **Data:** Unarguably, a graphic/visualisation should start with a data.  It is also the first argument in most important function in the package i.e. `ggplot(data =)`.
2. **Aesthetics:** or `aes()` in short, provide a mapping of various data dimensions to axes so as to provide positions to various data points in the output plot/graphic. 
3. **Geometries:** or `geoms` for short, are used to provide the _geometries_ so that data points may take a concrete shape on the visualisation.  For e.g. the data points should be depicted as bars or scatter points or else are decided by the provided `geoms.`
4. **Statistics:** or `stat` for short, provides the statistics to show in the visualisation like measures of central tendency, etc.
5. **Scale:** This component is used to decide whether any dimension needs some scaling like logrithmic transformation, etc.
6. **Coordinate System:**  Though most of the time _cartesian coordinate system_ is used, yet there are times when _polar coordinate system_ (e.g. pie chart) or _spherical coordinate system_ (e.g. geographical maps) are used.
7. **Facets:** Used when based on certain dimension, the plot is divided into further sub-plots.

## Prerequisites:

```{r}
library(ggplot2)
```

## GGPLOT2 in action

> Out of the afore-mentioned components, three are to be explicitly provided and thus can be understood as mandatoty components.  These three componenets are `data`, `aesthetics` and `geometries`.  Whilst these three compoenents are mandatorily provided, it is not that others are not mandatory.  basically other componenets have their defaults (e.g. default coordinate system is cartesian coordinate system).  Let us dive into these three essential components and build a plot using these.

### Building a basic plot
We will use `mtcars` datasets, a default dataset to learn the concepts.

See what happens when `data` is provided to `ggplot` function-
```{r fig_blank, fig.height=2, fig.align='center'}
ggplot(data=mtcars)
```

We can see that a blank chart/plot space has been created as our data `mtcars` has now mapped with ggplot2. Now let us provide aesthetic mappings to this using function `aes()`

```{r fig_gg_2, fig.height=2, fig.align='center'}
ggplot(data = mtcars, mapping = aes(x=wt, y=mpg))
```
You may now notice, apart from creating a blank space for plot, the two dimensions provided, i.e. `wt` and `mpg` have been _mapped_ with `x` and `y` axes respectively.  Since no geometry has been provided, the plot area is still blank.  Now we will provide geometry to our dimension say _point_.  To do this we will use another layer of function `geom_*` (`geom_point()` in this case specifically).  

```{r fig_3, fig.height=3, fig.align='center'}
ggplot(mtcars, aes(wt, mpg)) +
  geom_point()
```
Notice that another layer has been added to function `ggplot()` using a `+` sign here.  

We could have used another geometry say boxplot here.
```{r fig_4, fig.height=3, fig.align='center'}
ggplot(mtcars, aes(y = wt)) +
  geom_boxplot()
```

That's basic architecture of this package. Now lets discuss more on `aesthetics` and `geometries` before moving on to another compoenents.

### More on Aesthetics
Now what if color is provided inside `geom_*` function. 
```{r fig_5, fig.height=3, fig.align='center'}
ggplot(mtcars, aes(wt, mpg)) +
  geom_point(color='red')
```
As the argument `color='red'` was mentioned inside the `geom_point()` function, it turned every point to red.  But if we have to pass a vector/column based on which the points should be colored, it should be wrapped within aesthetics function `aes()` -
```{r fig_6, fig.height=3, fig.align='center'}
ggplot(mtcars, aes(wt, mpg)) +
  geom_point(aes(color=cyl))
```
Since the `cyl` column was a numeric column, ggplot2 thought it to be a `continuos` column and thus produced a color scale instead of a legend.  We however, know that this is a categorical column here, and thus if we want to produce a color legend we will have to convert it to a factor first.  See now the changes-
```{r fig_7, fig.height=3, fig.align='center'}
ggplot(mtcars, aes(wt, mpg)) +
  geom_point(aes(color=as.factor(cyl)))
```
One more thing - `aes()` function wrapped in `geom_point()` function could have been wrapped in `ggplot()` also.  So basically the following code will also produce exactly the same chart-

```
ggplot(mtcars, aes(wt, mpg, color = as.factor(cyl))) +
  geom_point()
```
Two questions arise here - 

1. Is there any difference between the two?

**Ans:** Yes, basically aesthetics if provided under the `geoms`, will override those aesthetics which are already provided under `ggplot` function.  See the result of following command in your console-
```{r fig_8, fig.height=3, fig.align='center'}
ggplot(mtcars, aes(wt, mpg, color = as.factor(cyl))) +
  geom_point(color='red')
```

2. What if `color='red'` (or blue) is passed inside `aes()`?

**Ans:** In this case ggplot will try to map it some aesthetics called 'blue'.  Let's see

```{r fig_9, fig.height=3, fig.align='center'}
ggplot(mtcars, aes(wt, mpg)) +
  geom_point(aes(color='blue'))
```
Interesting!  GGPLOT2 has not only mapped a dummy variable called `'blue'` with color of points, but also created a legend.  More interestingly the color is not what we wanted.

Different types of aesthetic attributes work better with different types of variables. For example, `color` and `shape` work well with discreet variables, while `size` or `alpha` (transparency) works well for continuous variables. In your console run the following command and check results

```
ggplot(mtcars, aes(wt, mpg, shape=as.factor(cyl))) +
  geom_point()
# OR
ggplot(mtcars, aes(wt, mpg, size=cyl)) +
  geom_point()
# OR
ggplot(mtcars, aes(wt, mpg, alpha=cyl)) +
  geom_point()
```
Multiple aesthetics can be mapped simultaneously, as per requirement.  See this example-
```{r fig_10, fig.height=3, fig.align='center'}
ggplot(mtcars, aes(wt, mpg, shape=as.factor(cyl), color=as.factor(gear), alpha=wt)) +
  geom_point()
```

Some commonly used aesthetics are -

- `shape` = Display a point with `geom_point()` as a dot, star, triangle, or square
- `fill` = The interior color (e.g. of a bar or boxplot)
- `color` = The exterior line of a `bar`, `boxplot`, etc., or the point color if using `geom_point()`
- `size` = Size (e.g. line thickness, point size)
- `alpha` = Transparency (`1 = opaque`, `0 = invisible`)
- `binwidth` = Width of histogram bins
- `width` = Width of “bar plot” columns
- `linetype` = Line type (e.g. solid, dashed, dotted)

**A few shapes available in `shape` aesthetics**

```{r echo=FALSE, fig.show='hide', message=FALSE, warning=FALSE}
library(tidyverse)
df <- data.frame(
           x = c(0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,
                 0.5,0.5,0.5,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,
                 0.25,0.25,0.25,0.25),
           y = c(1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,
                 11L,12L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L),
           z = c(0L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,
                 11L,12L,13L,14L,15L,16L,17L,18L,19L,20L,21L,22L,
                 23L)
)

df %>% 
  ggplot(aes(x=y, y=x, label=z))+
  geom_point(aes(shape=z), size=7)+
  scale_shape_identity() +
  labs(x="", y="") +
  geom_text(nudge_y = 0.08) +
  ylim(c(0.2,0.6)) +
  theme_void()

ggsave('shapes.png', width = 12, height = 2)
```


```{r shapes, echo=FALSE, fig.cap="Some Shapes available in GGplot",fig.show='hold', out.width="99%", fig.align='center'}
knitr::include_graphics('shapes.png')
```


### More on Geoms {#geoms}
In previous section we have seen that as soon as we passed a `geom_*` function/layer to `data` & `aesthetics` layers, the chart/graph was constructed.  Actually, `geom_point()` function, in the background added three more layers i.e. `stat`, `geom` and `position`.  Why?  The answer is simple, `geom_*` are generally shortcuts, which add these three layers.  So in our example, `ggplot(mtcars, aes(wt, mpg)) + geom_point()` is actually equivalent to -
```
ggplot(mpg, aes(displ, hwy)) +
  layer(
  mapping = NULL, 
  data = NULL,
  geom = "point", 
  stat = "identity",
  position = "identity"
)
```
A complete list of `geoms` available in ggplot2 is given in Annex-.  Some common geoms are listed below:

- Histograms - `geom_histogram()`
- Bar charts - `geom_bar()` or `geom_col()`
- Box plots - `geom_boxplot()`
- Points (e.g. scatter plots) - `geom_point()`
- Line graphs - `geom_line()` or `geom_path()`
- Trend lines - `geom_smooth()`

**Note that in ggplot2 `color` aesthetic represent border color of geometry and `fill` aesthetic represent color used to be fill that geometry.**

#### List of geoms available in `ggplot2`
Table: List of GEOMS available in `ggplot2` package
```{r geoms, echo=FALSE}
geoms <- help.search("geom_", package = 'ggplot2')
geoms <- unique(geoms$matches[, 1:2])
row.names(geoms) <- NULL
knitr::kable(geoms)
```

### Faceting
The amount of data also makes a difference: if there is a lot of data it can be hard to distinguish different groups. An alternative solution is to use faceting, as described next.  Facets, or “small-multiples”, are used to split one plot into a multi-panel figure, with one panel (“facet”) per group of data. The same type of plot is created multiple times, each one using a sub-group of the same dataset.

In `ggplot2` faceting can be acheived using either of the functions -

- `facet_grid()` creates a grid of plots, with each plot showing a subset of the data.  We may also specify the number of columns to use in the grid using the `ncol` argument.
- `facet_wrap()` creates a grid of plots with different variables on each axis. We may also specify the scales to use for each axis using the `scales` argument. 

Let us understand this, with these examples. 

Example-1
```{r}
ggplot(mpg, aes(x = displ, y = hwy)) + 
  geom_point() + 
  facet_wrap(~ class, ncol = 2)
```

Example-2
```{r}
ggplot(mpg, aes(x = displ, y = hwy)) + 
  geom_point() + 
  facet_grid(year ~ class)
```

Notice that `facet_grid()` arranges the plots in a grid with different variables on each axis. We specify the variables to use for faceting using the `~` operator. For example, `facet_grid(variable1 ~ variable2)` will create a grid of plots with `variable1` on the y-axis and `variable2` on the x-axis. This is useful when we want to compare the relationship between two variables across different levels of a third variable.

On the other hand, `facet_wrap()` creates a grid of plots, each showing a subset of your data based on a single variable. We specify the variable to use for faceting using the same `~` operator here too. For example, `facet_wrap(~ variable)` will create a grid of plots, each showing a different level of the variable. This is useful when you have a single categorical variable that you want to use for faceting.

### Labels
Labeling is an essential aspect of data visualization because it provides context and information about the data being presented. Labels can include titles, axis labels, legends, and annotations that describe the data and provide important information that helps the viewer understand what they are looking at.  Proper labeling can help to make the data more understandable, clear, and accessible, which enhances its overall value and impact.

#### Labeling Data points
To label data points in ggplot2, we can use the `geom_text()` function. This function adds text to the plot at the specified x and y coordinates.  Moreover, we can customize the appearance of the labels by adding additional arguments to `geom_text()` -

- `size` to set font size
- `color` to color the fonts
- `hjust` or `vjust` to adjust the labels vertically or horizontally, respectively.

Example-
```{r}
ggplot(mtcars, aes(x = hp, y = mpg)) +
  geom_point() +
  geom_text(aes(label = rownames(mtcars)),
            size = 3,
            color = "red",
            vjust = -1)

```

**TIP: Use package `ggrepel` to show all labels, when needed, without overlapping and in a better way.** E.g.
```{r}
library(ggrepel)
ggplot(mtcars, aes(x = hp, y = mpg)) +
  geom_point() +
  geom_text_repel(aes(label = rownames(mtcars)),
            size = 3,
            color = "red",
            vjust = -1)
```

#### Labeling Charts
There are several ways to add labels to ggplot2 charts, but we will focus on using the `labs()` function, which allows us to add `titles`, `subtitles`, `axis labels`, and other annotations like `caption`, etc. to the plot.  Example -
```{r}
ggplot(mtcars, aes(x = hp, y = mpg)) +
  geom_point() +
  labs(title = "Scatter plot of mpg vs. hp",
       subtitle = "Data from mtcars dataset",
       x = "Horsepower",
       y = "Miles per gallon",
       caption = "Source: R datasets")
```

We can also customize the appearance of the labels by using the `theme()` function, which allows us to modify the font size, font family, and other visual properties of the labels. Example-
```{r}
ggplot(mtcars, aes(x = hp, y = mpg)) +
  geom_point() +
  labs(title = "Scatter plot of mpg vs. hp",
       x = "Horsepower",
       y = "Miles per gallon") +
  theme(plot.title = element_text(size = 20, color = 'seagreen'),
        axis.title = element_text(size = 16, color = "blue"))

```

### Modifying scales
Several times the requirement is to modify x or/and y axis minimum and/or maximum values i.e. axis limits; or otherwise the axis itself is to be transformed.  For these requirements, we have `sacle_*_*()` group of functions in `ggplot2`.

For example we have these two functions for continuos axis/variables.
```
scale_x_continuous(name, breaks, labels, limits, trans)
scale_y_continuous(name, breaks, labels, limits, trans)
```
In arguments to above functions, we can see that axis title (name), axis breaks, axis labels, axis limits, and transformations can be dealt with.

Example-
```{r scale1, warning=FALSE, message=FALSE, fig.align='center', fig.show='hold', out.width="49%", fig.cap="Modifying Scales in GGplot2"}
# Basic Scatter Plot
ggplot(cars, aes(x = speed, y = dist)) + 
  geom_point()
# Modifying scales both axis title and axis limits
ggplot(cars, aes(x = speed, y = dist)) + 
  geom_point() + 
  scale_x_continuous(name="Speed of cars", limits=c(0, 30)) +
  scale_y_continuous(name="Stopping distance", limits=c(0, 150))
```

As for transformation we can use `trans` argument

```{r scale2, warning=FALSE, message=FALSE, fig.align='center', fig.show='hold', out.width="49%", fig.cap="Transforming Axes in GGplot2"}
ggplot(cars, aes(x = speed, y = dist)) + 
  geom_point()+
  scale_x_continuous(trans='log10') +
  scale_y_continuous(trans='log10')

state.x77 %>% 
  as.data.frame() %>% 
  ggplot(aes(Area, Illiteracy/100)) +
  geom_point() +
  scale_x_continuous(name = "Area in Square Miles", labels = scales::comma) +
  scale_y_continuous(name = "Illiteracy as % of Population", labels = scales::percent)
```


### Themes
We can customize the appearance of plots, such as the axis labels, titles, background colors, and font sizes by applying themes to the plot.  In the above, notice that we have used `theme()` function to modify font etc. of labels in the plot.  Now see the following example-
```{r theme1, warning=FALSE, message=FALSE, fig.align='center', fig.show='hold', out.width="49%", fig.cap="Modifying Themes in GGplot2 theme_bw (left) and theme_void (right)"}
ggplot(mtcars, aes(x = wt, y = mpg)) +
  geom_point() +
  labs(title = "MPG vs. Weight",
       x = "Weight (1000 lbs)",
       y = "Miles per Gallon") +
  theme_bw()

ggplot(mtcars, aes(x = wt, y = mpg)) +
  geom_point() +
  labs(title = "MPG vs. Weight",
       x = "Weight (1000 lbs)",
       y = "Miles per Gallon") +
  theme_void()
```

In this example, we've used the `theme_bw()` and `theme_void()` functions to apply a black-and-white theme to the plot.  Some theme functions that can be used to modify plot themes are -

- `theme_bw()`: A black and white theme that is useful when you want a simple, clean plot.
- `theme_classic()`: A classic theme that adds gray borders and gridlines to the plot.
- `theme_void()`: A theme with a transparent background and no gridlines or borders.
- `theme_minimal()`: A minimalistic theme that removes the gridlines and reduces the size of the axis labels.
- `theme(axis.title = element_text(size = 16), plot.title = element_text(size = 20))`: This code sets the font size of the axis and plot titles to 16 and 20, respectively.
- `theme(panel.background = element_rect(fill = "gray90"))`: This code sets the background color of the plot to a light gray color.

We can also customize specific elements of the plot using `element_*()` functions. For example:
```{r theme2, warning=FALSE, message=FALSE, fig.align='center', fig.show='hold', out.width="49%", fig.cap="Customising Themes in GGplot2"}
ggplot(mtcars, aes(x = wt, y = mpg)) +
  geom_point() +
  labs(title = "MPG vs. Weight",
       x = "Weight (1000 lbs)",
       y = "Miles per Gallon") +
  theme_bw() +
  theme(plot.title = element_text(color = "blue", size = 20, face = "bold"))

```

We can combine multiple customization options together to create a customized theme that fits our specific needs. The possibilities for customization are endless, so feel free to experiment and create your own unique theme!

### Saving/exporting plots
Of course, after creating charts/plots we would like to save them for further usage in our reports/documents, etc.  Though there may be many options to save a plot to disk, we will be focussing on three different methods.

#### Saving through Rstudio menu {-}
To save a graph using the RStudio menus, go to the Plots tab and choose Export.

```{r export1, echo=FALSE, fig.align='center', fig.show='hold', fig.cap="Exporting Charts", out.width="95%"}
knitr::include_graphics('images/export.png')
```


Three options are available here.

- Save as Image
- Save as PDF
- Copy to clipboard.

#### Saving through code {-}
We may also save our plots using function `ggsave()` here.  Its syntax is simple
```
ggsave(
  filename,
  plot = last_plot(),
  device = NULL,
  path = NULL,
  scale = 1,
  width = NA,
  height = NA,
  units = c("in", "cm", "mm", "px"),
  dpi = 300,
  limitsize = TRUE,
  bg = NULL,
  ...
)
```
All arguments are simple to understand.  Thus for example if we need to save the following violin plot, 

```{r export2, echo=FALSE, fig.align='center', fig.show='hold', fig.cap="Violin Plot", out.width="95%"}
# Create sample data
set.seed(123)
df <- data.frame(group = rep(c("A", "B"), each = 100), 
                 value = c(rnorm(100, 0, 1), rnorm(100, 1, 1)))

# Create base plot with violin plots and data labels
base_plot <- ggplot(df, aes(x = group, y = value, fill = group)) +
  geom_violin(trim = FALSE, alpha = 0.7) +
  geom_boxplot(width = 0.1, alpha = 0.8) +
  ggrepel::geom_text_repel(
    aes(label = round(value, 2)),
    size = 3,
    box.padding = 0.5,
    nudge_y = 0.2
  ) +
  labs(x = "Group", y = "Value") +
  scale_fill_manual(values = c("#CC6666", "#9999CC"))
base_plot
```

we can use this code

```
ggsave('violin.png', base_plot, height = 10, width = 8)
```
#### Graphics Devices (Base R Plots) {-}
If we create plots outside of ggplot (with `plot()`, `hist()`, `boxplot()`, etc.), we cannot use `ggsave()` to save our plots since it only supports plots made with ggplot.

Base R provides a way to save these plots with its graphic device functions.  There are three steps involved in this process-

- Specify the file extension and properties (size, resolution, etc.) along with units
- create the plot, in base R or/and ggplot2
- Signal that the plot is finished and save it by running `dev.off()`.  Thus, using this way we can insert as many charts in a single pdf without turning off the device till our pdf is ready.

Example-
```{r}
# Creates a png file
png(
  filename = "scatter.png",
  width = 5,
  height = 3,
  units = "in",
  res = 300
)
# Prints a ggplot2 in it
ggplot(mtcars, aes(x = wt, y = mpg)) +
  geom_point() +
  geom_abline(intercept = 5,
              slope = 3,
              color = "seagreen")
# Device is off
dev.off()

# Creates a new PDF file
pdf(file = "two_page.pdf",
    width = 6,
    height = 4)
#first plot
plot(mtcars$wt, mtcars$mpg)
abline(a = 5, b = 3, col = "red")
# Second Plot
ggplot(mtcars, aes(x = wt, y = mpg)) +
  geom_point() +
  geom_abline(intercept = 5,
              slope = 3,
              color = "seagreen")
# Device Off
dev.off()
```

### GGPLOT2 Tips and other extensions/supporting libraries

- [Extensions](https://exts.ggplot2.tidyverse.org/gallery/)
- [R graph Gallery](https://r-graph-gallery.com/ggplot2-package.html)

### More to read

- Book [@ggplot22016]
- [ggplot2 cheatsheet](https://posit.co/wp-content/uploads/2022/10/data-visualization-1.pdf) 
- [Another Cheatsheet](http://r-statistics.co/ggplot2-cheatsheet.html)




<!--chapter:end:08-ggplot2.Rmd-->

# Part-II: Dealing with tabular data {-}

# Getting data in and out of R {#read}

Till now, we have created our own datasets or we have used sample datasets available in R.  In practical usage, there will hardly be any case when we get the data imported in R.  So we have to import and load our data sets in R, for our analytics tasks.  Even after completing the analytics, the need for summarised data, other reports, charts, etc. need to be exported. This chapter is intended to do all these tasks.  

First section deals about functions related to reading external data i.e. importing objects into R.  This is followed by another section dealing with writing data to external files i.e. exporting data out of R. 

## Importing external data in R - Base R methods

Base R has many functions which can fulfill nearly any of our jobs to import external data into R.  However, there are packages which are customised to do certain tasks in easier way and thus, we will also learn two of the packages from `tidyverse` also.

There are some important functions in base R, which are used frequently to import external data into R environment.  Let us discuss these one by one.

### Reading tables through `read.table()` and/or `read.csv()` {-}

Basically these two functions are most commonly used functions in R, to get tabular data out of flat files.  The two functions namely `read.table()` and `read.csv()` are used respectively to read tabular data out of flat files having simple text format (.txt) and comma separated values (.csv) formats respectively.  There are three more cousins to these functions.

- `read.csv2()` to read csv files where `;` is used as delimiter and `,` is used for decimals instead.
- `read.delim()` to read delimited files where `tab character` has been used as delimiter and `.` as decimal.
- `read.delim2()`, similarly to read delimited files where `tab character` has been used as delimiter and `,` as decimal.

Most important arguments to these functions are -

- `file` the name of the file along with complete path as string^[If backward slash `\` is used in file paths, these must be `escaped` as R recognises `\` as escape character itself.  So, `"my/location/here/file.txt"` and `"my\\file\\here\\file.txt"` are the correct way of giving file name].
- `header` a logical value indicating if first line of the file has to be read as header or not.
- `sep` a string indicating a separator value to separate columns.
- `colClasses` a character vector indicating type of the columns if these are to be read explicitly in these types/formats only.
- `skip` an integer, indicating how many rows (from beginning) are to be skipped.

Readers may check results of `?read.table()` to get a complete list of arguments of these functions.

Example:  Let us try to download data related to _World Happiness Report 201=21_ which is available on data.world portal.

```
wh2021 <- read.csv("https://query.data.world/s/qbsbmxlfj54sl4mq3y6uxsr3pkhhmo")
# Check dimensions
dim(wh2021)
# Check column names
colnames(wh2021)
```
We can see that dataset named `wh2021` having `20` columns and `149` rows is now available in our environment.

> __Tip: Use `"clipboard"` in file argument for pasting copied data into R. E.g. Copy a few rows and cells from excel spreadsheet and run this command `read.delim("clipboard", header = FALSE)`.__

### Read data into a vector through `scan()` or `readline()` {-}

As afore-mentioned functions `read.table()` _et al_ are used in reading data into tables, we may also require reading data into a vector or simple list.  The two functions `scan()` and `readline()` are used for these purposes.

The basic syntax of `scan()` is -
```
scan(file = "",
     what = double(),
     nmax = -1,
     ...
)
```

Where -

- `file` is name of the file, or link
- `what` is format/data type to be read.  Default type is double
- `nmax` is mux number of data values to be read, default is `-1` which means all.

There are many other usefule arguments, for which please check results of `?scan` in your console.

Example -
```
scan("http://mattmahoney.net/iq/digits.txt", nmax = 10)
```

This function is sometimes useful to read data from keyboard into a vector.  Just use a blank string `""` in file name.  See this example

![](images/scan.png)

Function `readline()` on the other hand does similar job, but with a prompt.  See this example

![](images/readline.png) 

### Reading text files through `readLines()` {-}

Function `readLines()` is used to read text lines from a file (or connection).  To see this in action, prepare a text file (say `"txt.txt"`) and try reading it using `readLines("txt.txt")`.


## Exporting data out of R - Base R methods

Since the nature of most of the data anaytic jobs carried out in R, will be followed after reading the external data which will be followed by wrangling, transformation, modelling, etc., all in R.  Exporting files will not be used as much as reading external data.  Still, there will be times, when wrangled data tables need to be exported out of R.  For each of the different use cases, the following functions will almost complete our export requirements.

Let's learn these.

### Writing tabular data through `write.table()` and/or `write.csv()` {-}

Exporting data frames, whether after cleaning, wrangling, or transformation, etc., can be exported using these functions.  Latter will be used to write data frames in csv formats specifically.  The syntax is -

```
write.table(x, file = "", sep = " ", ...)
write.csv(x, file = "", ...)
write.csv2(x, file = "", ...)
```
where -

- `x` is the data frame object to be exported
- `file` is used to give file name (along with path)
- `sep` is separator
- `...` - there are many more arguments which are used to cutomised export needs.  See `?write.table()` for full details.

E.g. - The following command will export `iris` data frame as `iris.csv` file in the current working directory.
```{r eval=FALSE}
write.csv(iris, 'iris.csv')
```

### Writing character data line by line to a file through `writeLines()` {-}

Similar to `readLines`, function `writeLines()` will write the text data into a file with given file name.  Type the following code in your console and check that a new file with name `my_new_file.txt` has been created with the given contents in your current working directory.

```{r eval=FALSE}
writeLines("Andrew 25
           Bob 45
           Charles 56", "my_new_file.txt")
```


### Using `dput()` to get a code representation of R object {-}

This function will output the code representation of the given R object.  This function is particularly useful, when you are searching for help online and you need to give some sample data to reproduce the problem.  E.g. on [_Stack Overflow_](https://stackoverflow.com/) when asking for a solution to a specific problem, [reproducible data](https://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example) needs to be furnished.  Please also refer to section \@ref(reprex) for more.

Example-1:
```{r}
my_data <- data.frame(Name = c("Andrew", "Bob", "Charles"),
                      Age = c(25, 45, 56))
dput(my_data)
```
And while reproducing someone else's dput-
```{r}
now_mydata <- structure(list(Name = c("Andrew", "Bob", "Charles"), Age = c(25, 
45, 56)), class = "data.frame", row.names = c(NA, -3L))

now_mydata
```

## Using external packages for reading/writing data

### Package `readr`

The `readr` package [@R-readr] is part of core `tidyverse` and is loaded directly when we load it through `library(tidyverse)`.  It provides a range of analogous functions for each of the reading functions in base R.

| Base R     | readr        | Uses                                    |
| :---------:| :----------: | :-------------------------------------- |
| read.table | read\_table  | Reading table                           |
| read.csv   | read\_csv    | Reading CSV file with comma as sep      |
| read.csv2  | read\_csv2   | Reading CSV file with semi-colon as sep |
| read.delim | read\_delim  | Reading files with any delimiter        |
| read.fwf   | read\_fwf    | Reading fixed width files               |
| read.tsv   | read\_tsv    | Reading tab delimited file              |
| \--        | write\_delim | Writing files with any delimiter        |
| write.csv  | write\_csv   | Writing files with comma delimiter      |
| write.csv2 | write\_csv2  | Writing files with semi-colon delimiter |
| \--        | write\_tsv   | writing a tab delimited file            |

So a question may be asked here that what's the difference between these two sets of functions. Firstly, `readr` alternatives are much faster than their base R counterparts.  Secondly, it provides an informative problem report when parsing leads to unexpected results.  

For these, check results of these examples-
```{r echo=FALSE, message=FALSE}
library(tidyverse)
```

```{r}
read_csv(readr_example("chickens.csv"))
```

Note that the column types, while parsing the data frame, have now been printed.  These column types have been guessed by `readr` actually.  If the column types are not what were actually required to be parsed, then argument `col_types` may be used.  We can also use `spec()` function to retrieve the data types guessed by `readr` later-on, so these can be modified and used again in `col_types` argument.  See this example

```{r}
write.csv(iris, "iris.csv") #write a dummy data
spec(read_csv("iris.csv"))
read_csv("iris.csv",
         col_select = 2:6,
         col_types = cols(
           Sepal.Length = col_double(),
           Sepal.Width = col_double(),
           Petal.Length = col_double(),
           Petal.Width = col_double(),
           Species = col_factor(levels = c('setosa', 'versicolor', 'virginica'))
         )
) %>% head(2)
```

### Package `readxl`

This package is also part of `tidyverse` but this one has to be loaded specifically using `library(readxl)`.  As the name suggests, it has functions which are useful to read/write data to/from excel files.  Excel files (having extension .xls or .xlsx) are slightly different in a way that these may contain several _sheets_ of data at once.  The function `read_excel()` has been designed for reading sheets from excel files.  The syntax is
```
read_excel(path, sheet = NULL, range = NULL)
```

### Package `reprex` {#reprex}

This is again part of tidyverse, but has to be loaded specifically by calling `library(reprex)`. The name `reprex` is actually short for reproducible example.  This is useful particularly when we are stuck in some problem and seek for online help on some forum such as [Stack Overflow](https://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example), [R Studio Community](https://community.rstudio.com), etc.  

As an example do this in your console 

```
library(reprex)

x <- dput(head(iris))
x
```
Thereafter run `reprex()`, a small window in the `Viewer` tab will be opened like this.  Moreover, the code has been copied on the clipboard.

![](images/reprex.png)

For more reading please refer [this page.](https://www.tidyverse.org/help/)^[https://www.tidyverse.org/help/]

<!--chapter:end:09-reading.Rmd-->

# Data Transformation in `dplyr`

## Prerequisites
Obviously `dplyr` [@R-dplyr] will be needed.  This package also comes with matrittr pipe i.e. `%>%` and therefore in dplyr syntax we will be using these pipes.  `library(magrittr)` is not needed.
```{r}
library(dplyr)
library(knitr)
```


The package `dplyr` (\@ref(fig:dplyrr)) calls its functions as 'verbs' because these are actually doing some action.  So `dplyr verbs` can be divided in three classifications depending upon where they operate -

- 'Row' verbs that operate on Rows
- 'Column' verbs 
- 'group' verbs that operate on table split into different groups.

```{r dplyrr, echo=FALSE, fig.cap="Package Dplyr", fig.show='hold', out.width="99%", fig.align='center'}
knitr::include_graphics("images/dplyr.png")
```

Let's learn each of these -

## Column verbs

### `select()`
In real world data sets we will often come across with data frames having numerous columns.  However for many of the data analysis tasks, most of these columns are not needed.  As already stated `select` (figure \@ref(fig:selectr)) operates on columns.  Like `SELECT` in `SQL`, it just _select_ the column(s) from the given data frame.  The basic syntax is - `select(data_frame, column_names, ...)`.  So with pipes the same syntax goes like this
```
data_frame %>% 
  select(column_name)
```
```{r selectr, echo=FALSE, fig.cap="Illustration of dplyr::select()", fig.show='hold', out.width="99%", fig.align='center'}
knitr::include_graphics("images/select_dplyr.png")
```

For example, let's try our hands on `mtcars` dataset.  
Example-1
```{r}
mtcars %>% 
  select(mpg)
```

**Note** that output is still a data frame unlike the `mtcars[['mpg']]` which returns a vector.  We can subset multiple columns here.  Example-2
```{r}
mtcars %>% 
  select(mpg, qsec) %>% 
  head()
```

We can also provide column numbers instead of names. Example-3
```{r}
mtcars %>% 
  select(4, 6) %>% 
  tail()
```

We can also use `select` to reorder the columns in output, by using a `dplyr` _helping verb_ `everything()` which is basically _everything else._  See this example-
```{r}
mtcars %>% 
  select(qsec, mpg, everything()) %>% 
  names()
```

We may also use mix and match of _column names_ and _column numbers_.  See
```{r}
mtcars %>% 
  select(5, 7, mpg, everything()) %>% 
  names()
```

Operator `:` can also be used with column names.  Ex-
```{r}
mtcars %>% 
  select(mpg:drat) %>% 
  head(n=4)
```


**Other helping verbs**
There are other helping verbs, apart from `everything()` that can be used within `select()` just to eliminate need to type the column names and select columns based on some conditions.  These verbs are self explanatory-

- `starts_with('ABC')` will select all columns the names of which __starts with__ string `ABC`
- `ends_with('ABC')` will select all columns the names of which __ends with__ string `ABC`
- `contains('ABC')` will select all columns the names of which __contains__ string `ABC`
- `num_range('A', 1:3)` will select all columns named `A1`, `A2` and `A3`

Some Examples-
```{r}
starwars %>% 
  select(ends_with('color'))
```
Example-2
```{r}
starwars %>% 
  select(contains('or'))
```


### `mutate()`
This perhaps is one of the most important functions in `dplyr` kitty.  It enables us to create new column(s) that are functions of one or more existing columns. Refer figure \@ref(fig:mutater)

```{r mutater, echo=FALSE, fig.cap="Illustration of dplyr::mutate()", fig.show='hold', out.width="99%", fig.align='center'}
knitr::include_graphics("images/mutate_dplyr.png")
```

More than one column can be added simultaneously.  Newly created column may also be used for creation of another new column.  See example.
```{r}
starwars %>% 
  select(name:mass) %>% 
  mutate(name_upper = toupper(name),
         BMI = mass/(height/100)^2)
```

By default the new column will be added to the last of data frame.  As shown in above example, more operations can be combined in one using `%>%`.  There is a cousin `transmute()` of _mutate_ which drops all the old columns and keeps only newly created columns.  Example
```{r}
starwars %>% 
  transmute(name_upper = toupper(name))
```

**Other useful dplyr functions**  Another good use of `mutate` is to generate summarised result and display it corresponding to each row in data.  For example if the requirement is to calculate proportion of say `wt` column in `mtcars` data.
```{r}
mtcars %>% 
  head() %>% 
  select(wt) %>% 
  mutate(total_wt = sum(wt),
         wt_proportion = wt*100/total_wt) 
```

1. `n()` is used to count number of rows
2. `n_distinct()` is used to count number of distinct values for the given variable

```{r}
mtcars %>% 
  select(1:5) %>% 
  mutate(total_cars = n()) %>% 
  head()
  
```

### `rename()`
It is used to _rename_ the column names.  Refer figure \@ref(fig:renamer) for illustration.

```{r renamer, echo=FALSE, fig.cap="Illustration of dplyr::rename()", fig.show='hold', out.width="99%", fig.align='center'}
knitr::include_graphics("images/rename_dplyr.png")
```

See this example
```{r}
mtcars %>% 
  rename(miles_per_gallon = mpg) %>% 
  head(3)
```

**Note** that `select` can also rename the columns but will drop all unselected columns.  Check this
```{r}
mtcars %>% 
  select(miles_per_gallon = mpg) %>% 
  head(3)
```

### `relocate()`
It _relocates_ column or block of columns simultaneosly either before the column mentioned in argument `.before` or after mentioned in `.after`.  See the example-
```{r}
starwars %>% 
  relocate(ends_with('color'), .after = name) %>% 
  head(5)
```


## Row verbs
### `filter`
This verb/function is used to subset the data, or in other words filter rows of data frame based on certain condition. Refer figure \@ref(fig:filterr) for illustration.

```{r filterr, echo=FALSE, fig.cap="Illustration of dplyr::filter()", fig.show='hold', out.width="99%", fig.align='center'}
knitr::include_graphics("images/filter_dplyr.png")
```

See this example-
```{r}
starwars %>% 
  filter(eye_color %in% c('red', 'yellow'))
```

Multiple conditions can be passed simultaneously. Example
```{r}
starwars %>% 
  filter(skin_color == 'white',
         height >= 150)
```
**Note** that these conditions act simultaneously as in operator `AND` is used.  So if `OR` is to be used, use `|` explicitly
```{r}
starwars %>% 
  filter(skin_color == 'white' | height >= 150) %>% 
  nrow()
```

### `slice()` / `slice_*()` {#slice_func}
`slice()` and its cousins also filters rows but based on rows placement.  So, `data_fr %>% slice(1:5)` will filter out first five rows of the `data_fr`.  See example
```{r}
starwars %>% 
  slice(4:10) # filter 4 to 10th row
```
Other `slice()` cousins -

- `slice_head(5)` will slice out first 5 rows
- `slice_tail(10)` will slice out last 10 rows
- `slice_min()` or `slice_max()` will slice rows with highest or lowest values of given variable. The full syntax is `slice_max(.data, order_by, ..., n, prop, with_ties = TRUE)` or equivalent
- `slice_sample()` will randomly select the rows.  Its syntax is `slice_sample(.data, ..., n, prop, weight_by = NULL, replace = FALSE)`

Example-1:
```{r}
starwars %>% 
  slice_min(height, n=3)
```
Example-2:
```{r}
set.seed(2022)
starwars %>% 
  slice_sample(prop = 0.1) #sample 10% rows
```

### `arrange()`
This verb also act upon rows and it actually _rearranges_ them on the basis of some condition.  Refer figure \@ref(fig:arranger) for illustration.

```{r arranger, echo=FALSE, fig.cap="Illustration of dplyr::arrange()", fig.show='hold', out.width="99%", fig.align='center'}
knitr::include_graphics("images/arrange_dplyr.png")
```

Example-
```{r}
starwars %>% 
  arrange(height) %>% 
  slice(1:5)
```
## Group verbs

### `group_by()`
A data analyst will be hard to find who is not using `group_by`.  It basically groups the rows on the basis of values of a given variable or block of variables.  The returned result is still a data frame (and one too) but now the rows are grouped.  Refer figure \@ref(fig:groupby) for illustration. So any of the above functions we learnt above will give a different result after group by.

```{r groupby, echo=FALSE, fig.cap="Illustration of Grouped Operations in dplyr", fig.show='hold', out.width="99%", fig.align='center'}
knitr::include_graphics("images/groupby_dplyr.png")
```

Note the output of this simple example
```{r}
starwars %>% 
  group_by(sex)
```
**Note** that output now has 5 groups, though nothing different is seen in the displayed data.

This operation/verb is thus more useful if used in combination with other verbs.

Example-1:  How many total characters are with same skin_color?
```{r}
starwars %>% 
  select(name, skin_color) %>% 
  group_by(skin_color) %>% 
  mutate(total_with_s_c = n())
```

Example- 2: Sample 2 rows of each `cyl` size from `mtcars`?
```{r}
set.seed(123)
mtcars %>% 
  group_by(cyl) %>% 
  slice_sample(n=2)
```
Also note that `grouped` varaible(s) will always be available in the output.
```{r}
mtcars %>% 
  group_by(cyl) %>% 
  select(drat) %>% # despite not selecting cyl
  head() # it is available in output
```


### `summarise()`
This verb creates a summary row for each group if grouped data frame is in input, otherwise one single for complete operation.  

Example-1:
```{r}
mtcars %>% 
  summarise(total_wt = sum(wt))
```
Example-2:
```{r}
mtcars %>% 
  group_by(cyl) %>% 
  summarise(total_wt = sum(wt))
```

## Other Useful functions in `dplyr`

### `if_else()` {-}
This function operates nearly as similar to base R's `ifelse()` with two exceptions-

- There is an extra argument to provide values when missing values are encountered.  (See example-1)
- `NA` will have to be provided specifically. (See Example-2)

See these examples.  Example-1:
```{r}
x <- c(-2:2, NA)
if_else(x>=0, "positive", "negative", "missing")
```
Example-2:
```{r}
x <- c(-2:2, NA)
if_else(x>=0, "positive", "negative", NA_character_)
```
Due to the additional restrictions, this function is sometimes faster than its base R alternative and may also be useful in prevention of bugs in code as the output will be known beforehand.

### `case_when()` {-}
Though both `ifelse` and `if_else` variants provide for nesting multiple conditions, yet `case_when` provides a simpler alternative in these conditions as here multiple conditions can be provided simultaneously.  Syntax follows this style-
```
case_when(
  condition1 ~ value_if_true,
  condition2 ~ value_if_true,
  ...,
  TRUE ~ value_if_all_above_are_false
)
```
See this example.
```{r}
set.seed(123)
income <- runif(7, 1, 9)*100000
income

# tax brackets say 0% upto 2 lakh, then 10% upto 5 Lakh
# then 20% upto 7.5 lakh otherwise 30%
tax_slab <- case_when(
  income <= 200000 ~ 0,
  income <= 500000 ~ 10,
  income <= 750000 ~ 20,
  TRUE ~ 30
)

# check tax_slab
data.frame(
  income=income,
  tax_slab = tax_slab
)
```

## Window functions/operations
We learnt that by using `group_by` function we can create windows in data and we can make our calculations in each separate window specifically.

Dplyr provides us with some useful window functions which will operate on these windows.

1. `row_number()` can be used to generate row number
2. `dense_rank` / `min_rank` / `percent_rank()` / `ntile()` / `cume_dist()` are other windowed functions in dplyr.  Check `?dplyr::ranking` for complete reference.
3. `lead()` and `lag()` will give leading/lagging value in that window.
 

These functions can be very helpful while analysing time series data. 

Example-1:
```{r}
# example data
df <- data.frame(
  val = c(10, 2, 3, 2, NA)
)

df %>% 
  mutate(
    row = row_number(),
    min_rank = min_rank(val),
    dense_rank = dense_rank(val),
    perc_rank = percent_rank(val),
    cume_dist = cume_dist(val)
  )
```
Example-2:
```{r}
Orange %>% 
  group_by(Tree) %>% 
  mutate(prev_circ = lag(circumference))
```
# Combining Tables/tabular data

```{r fig.cap="Most of times, joining two or more tables will be required to perform analytics", fig.align="center", echo=FALSE, out.width="75%", out.height="75%"}

knitr::include_graphics("images/conf.jpg")

```

In real world scenarios, there may hardly be a case when we have to analyse one single table.  There may be cases when we have to either join tables split into multiple smaller tables (e.g. we can have smaller tables split States-wise), or the tables may be divided into various smaller master and transaction tables (relational databases).  

We may thus divide the data tables joining requirements into three broad categories-

- Simple joins or concatenation
- Relational Joins
- Filtering Joins

Let us discuss each of these with examples.

## Simple joins/concatenation
Many times tables split into smaller tables have to be joined back before proceeding further for data analytics.  We may have to join two or more tables either columnwise (e.g. some of the features for all rows have been split into a separate table) or row wise (e.g. all the fields/columns are split into smaller tables like a separate table for each State).  Diagramatically these joins may be depicted as shown in figure \@ref(fig:joins).

```{r joins, echo=FALSE, fig.cap="Illustration of Simple joins/concatenation", fig.show='hold', out.width="99%", fig.align='center'}
knitr::include_graphics("images/concat_joins.png")
```

### Column binding
As we have already seen that data frames act like matrices in many ways except to that fact that these support heterogeneous data unlike matrices.  We have also discussed the ways two matrices can be joined.  Base R has two dedicated functions i.e. `cbind()` and `rbind()` for these operations.

In _tidyverse_ (dplyr specifically) we have two similar functions `bind_cols()` and `bind_rows` respectively which provide us better functionality for these use cases.  The syntax for finction `bind_cols()` used for concatenating two or more tables _column wise_ is -
```
bind_cols(
  ...,
  .name_repair = c("unique", "universal", "check_unique")
)
```
Where -

- `...` represent data frames to be combined
- `.name_repair` argument chooses method to rename duplicate column names, if any.

Example-1:
```{r}
df1 <- iris[1:3, c(1,2,5)]
df2 <- iris[1:3, 3:5]

bind_cols(df1, df2, .name_repair = 'universal')
```

> Note:  The data frames to be merged should be row-consistent for column binding.  Try this `bind_cols(iris[1:3, 1:2], iris[1:4, 3:4])` and see the results.

### Row binding
The syntax used for appending rows of one or more tables together in one table is -

```
bind_rows(..., .id = NULL)
```
where -

- `...` represent data frames to be combined
- `.id` argument creates a new column of identifiers.

To understand it better, let us see this example
```{r}
setosa <- iris[1:3, 1:4]
versicolor <- iris[51:53, 1:4]
virginica <- iris[101:103, 1:4]

bind_rows(setosa, versicolor, virginica, .id = 'groups')
```

> Note: In the above example if the requirement is to store data tables names into the new _identifier_ column just list convert the databases into a list and `.id` will take element_names as the values in identifiers.  Try this `bind_rows(list(setosa=setosa, versicolor=versicolor, virginica=virginica), .id = 'Species')` 

## Relational joins

Relational Joins are usually needed to join multiple tables based on _primary key_ and _secondary keys_.  The joins may either be _one-to-one_ key join or _one-to-many_ key joins.  Broadly these can either be _inner joins_ or _outer joins_. Diagrammatically these may be represented as shown in figure \@ref(fig:joinm).

```{r joinm, echo=FALSE, fig.cap="Illustration of Mutating Joins in dplyr", fig.show='hold', out.width="99%", fig.align='center'}
knitr::include_graphics("images/mutating_joins.png")
```

The syntax of all these joins is nearly same-
```
*_join(x, y, by = NULL, copy = FALSE, suffix = c('.x', '.y'), ... , keep = FALSE)
```
where -

- `x` and `y` are data frames to be joined
- `by` is a character vector of column names to be joined by
- `suffix` argument provides suffixes to be added to column names, if any of those are duplicate
- `keep` argument decides whether the join keys from both x and y be preserved in the output?

Let's discuss each of these joins individually.

### Inner Joins

Inner Joins keeps only those rows where matching keys are present in both the data frames.

Example-1:
```{r}
band_members
band_instruments

inner_join(band_members, band_instruments)
```

### Left Joins

Left Joins on the other hand preserves all rows of data frame passed as `x` i.e. first argument irrespective of the fact that matching key record is available in second data table or not.

Example-1:
```{r}
left_join(band_members, band_instruments)
```

### Right Joins

Right join, is similar to left join and preserves all rows of data frame passed as `y` i.e. second argument irrespective of the fact that matching key record is available in first data table or not.

Example-1:
```{r}
right_join(band_members, band_instruments)
```

### Full Joins

Full join returns all the rows of both the data tables despite non-availability of matching key in either of the tables.

Example-1:
```{r}
full_join(band_members, band_instruments)
```

You must have noticed that each of the examples shown above has thrown a warning that join has been performed on variable `name`.  We may override this warning by specifically providing the joining _key_ column name(s) in `by` argument i.e. `by = "name"`.

There may be cases when the joining _key_ column(s) in the two data frames are of different names.  These cases can also be handled by using `by` argument.  

Example-
```{r}
band_instruments2
left_join(band_members, band_instruments2, by = c('name' = 'artist'))
```

> Note: Each of the `*_join()` can be joined on multiple keys/columns (i.e. more than one) using `by` argument as explained above.

### Many-to-many Joins {-}
Example of four of the above-mentioned joins are many to many joins and thus need to be used carefully.  See the following example
```{r}
df1 <- data.frame(
  x = c(1, 1, NA),
  y = c(11, 12, 21)
)
df1
df2 <- data.frame(
  x = c(1, 1, NA, NA),
  z = c(101, 102, 201, 202)
)
df2
df1 %>% left_join(df2, by = 'x')
```
In fact, we can use argument `relationship` explicitly to silence this warning and avoid errors.  This argument can take one of these values

- `NULL` (default)
- `"one-to-one"`
- `"one-to-many"`
- `"many-to-one"`
- `"many-to-many"`

Example above re-coded
```{r}
df1 %>% left_join(df2, by = 'x', relationship = "many-to-many")
```



## Filtering Joins
The other joins available in `dplyr` are basically filtering joins.  

### Semi Joins

First of these is `semi_join` which essentially filters those rows from a data frame, which are based on another data frame.  See this example-
```{r}
df1 <- data.frame(
  x = c(1, 2, NA),
  y = c(11, 21, 100)
)
df1
df2 <- data.frame(
  x = c(1, 3, 4, NA),
  z = c(101, 301, 401, 501)
)
df2
df1 %>% semi_join(df2, by = 'x')
```

### Anti Joins
The `anti_join` is basically opposite to that of `semi_join`.  It keeps only those records from left data-frame which are not available in right data-frame.  Example
```{r}
df1 %>% anti_join(df2, by = 'x')
```


## Relational, but non-equi joins
Till now, we have learnt joins that use syntax of dplyr version before 1.1.0.  However, in dplyr newest version, 1.1.0, released in January 2023, a new helper function `join_by()` has been introduced.  It actually, constructs a specification that describes how to join two tables using a small domain specific language. The result can be supplied as the by argument to any of the join functions that we have learnt above.  For equal joins, we have learnt above, there are two slight but easier to implement changes if we use `join_by()` -

- we may pass variable names in by argument instead of using characters as described above.
- instead of using `=` we have to use `==` which is actually the equality operator as opposed to assignment operator we were using earlier. 

So our syntax for `left_join` example is
```{r}
band_instruments2
left_join(band_members, band_instruments2, by = join_by(name == artist))
```
Simple, isn't it.  But now using the potential of this new helper function, we can on the other hand join two (or more data frames) using inequality conditions.  

### Inequality Joins
Inequality joins match on an inequality, such as `>`, `>=`, `<`, or `<=`, and are common in time series analysis and genomics. To construct an inequality join using `join_by()`, we have to supply two column names separated by one of the above mentioned inequalities.  See the following example for better understanding of inequality joins.

Example- Suppose we have two tables, one containing product-wise `sales` and another containing `rates` as and when these are revised.  So, in order to have total sales amount, we can use inequality join.

```{r}
# sales
sales <- data.frame(
  product.id = c("A", "A", "A", "A", "A", "B", "B"),
  sales.date = as.Date(c("01-01-2022",
                 "02-02-2022","05-11-2022","05-04-2022",
                 "05-10-2022","01-02-2022","01-01-2023"), format = "%d-%m-%Y"),
  quantity = c(5L, 10L, 15L, 5L, 15L, 1L, 2L)
)
# Print it
sales
# rates
rates <- data.frame(
  product.id = c("A", "A", "A", "B", "B"),
  revision.date = as.Date(c("01-02-2022",
                    "03-04-2022","05-10-2022","01-01-2022",
                    "05-10-2022"), format = "%d-%m-%Y"),
  rate = c(50L, 60L, 70L, 500L, 1500L)
)
#print it
rates

# total sale(1st attempt)
sales %>% 
  left_join(rates,
            by = join_by(product.id,
                         sales.date >= revision.date))
```

We got the results, but the results/rates are obtained fixed on all earlier dates.  To solve this issue, we have rolling joins.  See next section.

### Rolling Joins
Rolling joins are a variant of inequality joins that limit the results returned from an inequality join condition. They are useful for "rolling" the closest match forward/backwards when there isn't an exact match. To construct a rolling join, we have to wrap inequality condition with `closest()`.


### Overlap Joins
Overlap joins are a special case of inequality joins involving one or two columns from the left-hand table overlapping a range defined by two columns from the right-hand table. There are three helpers that `join_by()` recognizes to assist with constructing overlap joins, all of which can be constructed from simpler inequalities.

- `between(x, y_lower, y_upper, ..., bounds = "[]")` which is just short for `x >= y_lower, x <= y_upper`

- `within(x_lower, x_upper, y_lower, y_upper)` which is just short for `x_lower >= y_lower, x_upper <= y_upper`

- `overlaps(x_lower, x_upper, y_lower, y_upper, ..., bounds = "[]")` which is short for `x_lower <= y_upper, x_upper >= y_lower`

Above example solved by wrapping date condition in `closest()`
```{r}
sales %>% 
  left_join(rates,
            by = join_by(product.id,
                         closest(sales.date >= revision.date)))
```

We may see that we do not have rates for product `A` for sales made on `01-01-2022`.  

Similarly, we can make use of other helper functions as per our need.  

Example-2: Suppose we have `orders` data where we have customer-wise orders, corresponsing order dates and ship dates.  Let us suppose we want to know about customers who placed another order, without waiting for shipping of any of their previous/earlier order.

```{r}
orders <- structure(list(customer_id = c("A", "A", "A", "B", "B", "C", 
"C"), order_id = 1:7, order_date = c("2019-01-01", "2019-01-05", 
"2019-01-16", "2019-01-05", "2019-01-06", "2019-01-07", "2019-01-09"
), ship_date = c("2019-01-30", "2019-02-10", "2019-01-18", "2019-01-08", 
"2019-01-08", "2019-01-08", "2019-01-10")), row.names = c(NA, 
-7L), class = "data.frame")

# print 
orders

# customers who placed the orders without
# waiting to ship their earlier order
orders %>% 
  inner_join(orders,
             by = join_by(customer_id,
                          overlaps(order_date, ship_date, order_date, ship_date),
                          order_id < order_id)) 
```


### Cross Joins
This join matches everything with everything and therefore can be thought of as a cross product of two data frames. We can use `cross_join` for this special join.  If we have `m` and `n` rows in two data frames, then as a result of cross join we will have `m*n` rows.

Example-
```{r}
df <- data.frame(products = c('A', 'B', 'C'))

df %>% 
  cross_join(df)
```


<!--chapter:end:10-dplyr.Rmd-->

# Data Wrangling in `tidyr`

In this chapter we will learn about reshaping data to the format most suitable for our data analysis work.  To reshape the data, we will use `tidyr` package [@R-tidyr] which is part of core `tidyverse` and can be loaded either by calling `library(tidyr)` or `library(tidyverse)`. 

## Prerequisites

```{r message=FALSE}
library(tidyverse)
```

## Concepts of tidy data

[Hadley Wickham](https://hadley.nz/), the chief scientist behind development of RStudio, tidyverse, and much more, introduced the concept of tidy data in a paper^[[https://www.jstatsoft.org/article/view/v059i10](https://www.jstatsoft.org/article/view/v059i10)] published in the Journal of Statistical Software [@JSSv059i10].  Tidy data is a framework to structure data sets so they can be easily analyzed and visualized. It can be thought of as a goal one should aim for when cleaning data. Once we understand what tidy data is, that knowledge will make our data analysis, visualization, and collection much easier. [@JSSv021i12] 

A tidy data-set has the following properties:

1. Each variable forms a column.
2. Each observation forms a row.
3. Each type of observational unit forms a table.

Diagrammatically^[Image taken from Hadley Wickham's book [R for data science](https://r4ds.had.co.nz/)] this can be represented as in figure \@ref(fig:tidy).

```{r tidy, echo=FALSE, fig.cap="Diagrammatic representation of tidy data", fig.show='hold', fig.align='center', out.width="99%"}
knitr::include_graphics("images/tidy-1.png")
```

Once a dataset is tidy, it can be used as input into a variety of other functions that may transform, model, or visualize the data.

Consider these five examples^[all taken from package `tidyr`].  All these examples^[These data-tables display the number of TB cases documented by the World Health Organization in Afghanistan, Brazil, and China between 1999 and 2000. The data contains values associated with four variables (country, year, cases, and population), but each table organizes the values in a different layout.] represent same data but shown in different formats-

```{r}
#Example-1
tidyr::table1
# Example-2
tidyr::table2
# Example-3
tidyr::table3
# Example-4 (Same data in 2 data tables now)
tidyr::table4a

tidyr::table4b
# Example-5
tidyr::table5
```
Let us discuss these example one by one -

- `table1` fulfills all three rules stated above and is thus, in tidy format.  Notice that every observation has its own row, and each variable is stored in a separate column.
- `table2` stores one observation in two columns (separately for cases and population) and is thus not tidy.
- `table3` stores two variables in one column (cases and population together) and is thus not tidy.
- `table4a` and `table4b` clearly stores one observation in two different tables and is thus not tidy.  We may further notice that both these tables use values as column headers, which also violate rule no.3 stated above.
- `table5` again stores one variable i.e. year in two separate columns, and thus does not follow rule no.2 stated above.

## Reshaping data

In real world problems we will mostly come across data-sets that are not in tidy formats and for data analysis, visualisation we will need `tidying` the datasets.  To do this we will first need to understand what actually is a value, a variable/field and an observation.  As a second step of _tidying_ we will require to reshape the data by either -

- re-organising the observation originally spread into multiple rows (e.g. `table2`), in one row; OR
- re-organising the variable spread into multiple columns (e.g. `table3`, etc.), in one single column.

To perform this _tidying_ exercise, we will need two most important functions from `tidyr` i.e. `pivot_longer` and `pivot_wider`.  So let us understand the functioning of these.

### LONGER format through function `pivot_longer()` 

Often we will come across data sets that will have values (instead of actual variable name) as column headers.  Let us take the example of `table4a` or `table4b` shown above.  Both these tables have values of variable `year` as column names.  So we need to re-structure these tables into a longer format where these values form part of columns instead of column names.  We will use `pivot_longer()` function for this.  Th basic syntax is-
```
pivot_longer(
  data,
  cols,
  names_to = "name",
  values_to = "value",
  ...
)
```
_Note that there many more useful arguments to this function, but first let us consider on these only._

- `cols` indicate names of columns (as a character vector) to be converted into longer format
- `names_to = "name"` argument will actually convert values used as column headers back to a column with  given "name"
- `values_to = "value"` argument will convert values of all those columns back into one column with given name "value" (e.g. population in table4b)

Basic functionality can be understood using the following example-
```{r}
iris_summary <- iris %>% 
  group_by(Species) %>% 
  summarise(mean = mean(Sepal.Width),
            st_dev = sd(Sepal.Width))
iris_summary
```

Using `pivot_wider` we can convert column headers into values.  Check the diagram in figure \@ref(fig:plong).

```{r plong, echo=FALSE, fig.cap="Diagrammatic representation of pivot\\_longer", fig.show='hold', fig.align='center', out.width="99%"}
knitr::include_graphics("images/pivot_longer.png")
```

Now we are ready to convert `table4a` into a tidier format.

#### Case-I when values are in column headers {-}

```{r}
pivot_longer(
  table4a, 
  cols = c('1999', '2000'), 
  names_to = 'year', 
  values_to = 'cases')
```
With _pipes_  and a little tweaking, the above syntax could have been written as-
```{r, eval=FALSE}
tidyr::table4a %>%                         # first argument passed through pipe 
  pivot_longer(cols = -country,     # all columns except country
               names_to = "year",
               values_to = "cases")
```

#### Case-II when both variables and variable names are combined together as column names {-}

We have seen a simple case to tidy the table when the values (e.g. years) were depicted as column names instead of variables i.e. actual data.  There may be cases when column names are a combination of both.  

Example - Say we have a table `table6` as
```{r, echo=FALSE}
(table6 <- left_join(table4a %>% set_names(c('country', 'cases_1999', 'cases_2000')),
          table4b %>% set_names(c('country', 'pop_1999', 'pop_2000')),
          by="country"))
```

We may use `names_sep` argument in this case, which will separate the combined variables from the column names -
```{r}
table6 %>% 
  pivot_longer(cols = !country,
               names_sep = "_",
               names_to = c("count_type", "year"),
               values_to = "count")
```

> Note that we have two column names in argument `names_to`.

Though the above table is still not in tidy format, yet the example was taken to show the functioning of other arguments of the `pivot_longer`.  We provided two static column names to the related argument and the variables were created after splitting the headers with sep `_`.  We actually require one dynamic value to be retained as column name (`cases` and `pop` here) but need to convert `year` to variables.

To do so we will use special value `".value"` in the related argument.  See

```{r}
table6 %>% 
  pivot_longer(cols = !country,
               names_sep = "_",
               names_to = c(".value", "year"),
               values_to = "count")
```
> Note that by using `.value` the argument `values_to` becomes meaning less.

### WIDER format through `pivot_wider()` 

As the name suggests, `pivot_wider()` does exactly opposite to what a `pivot_longer` does. Additionally, this function is used to create summary reports, as pivot functionality in MS Excel, through `values_fn` argument.  Diagrammatically this can be represented as in figure \@ref(fig:pwide). The basic syntax (with commonly used arguments) is-

```
pivot_wider(
  data,
  id_cols = NULL,
  names_from = name,
  values_from = value,
  values_fill = NULL,
  values_fn = NULL,
  ...
)
```
where-

- `id_cols` is a vector of columns that uniquely identifies each observation
- `names_from` is a vector of columns to get the name of the output column 
- `values_from` similarly provides columns to get the cell values from
- `values_fill` provides what each value should be filled in with when missing
- `values_fn` is a named list - to apply different aggregations to different `values_from` columns

> Also note that there are many other arguments for this function, which may be used to deal with complicated tables. 

```{r pwide, echo=FALSE, fig.cap="Diagrammatic representation of pivot\\_wider", fig.show='hold', fig.align='center', out.width="99%"}
knitr::include_graphics("images/pivot_wider.png")

```

Example-1:

```{r}
tidyr::table2 %>% 
  pivot_wider(names_from = "type",
              values_from = "count")
```

Example-2:
```{r}
tidyr::table2 %>% 
  pivot_wider(names_from = year,
              values_from = count)
```
Example-3: Summarisation
```{r}
tidyr::table2 %>% 
  pivot_wider(id_cols = country,
              names_from = type,
              values_from = count,
              values_fn = mean)
```

Example-4: Summarisation with different id_cols
```{r}
tidyr::table2 %>% 
  pivot_wider(id_cols = year,
              names_from = type,
              values_from = count,
              values_fn = sum)
```

Example-5: Use of multiple columns in `names_from` argument
```{r}
tidyr::table2 %>% 
  pivot_wider(names_from = c(year, type),
               values_from = count)
```

What if order is reversed in `names_from` arg.  See Example-6:
```{r}
tidyr::table2 %>% 
  pivot_wider(names_from = c(type, year),
               values_from = count)
```
Example-7: Multiple columns in `values_from`
```{r}
tidyr::table1 %>% 
  pivot_wider(names_from = year,
              values_from = c(cases, population))
```

Example-8: Use of names_vary argument to control the order of output columns
```{r}
tidyr::table1 %>% 
  pivot_wider(names_from = year,
              values_from = c(cases, population),
              names_vary = "slowest")
```

For more details please refer to [package vignette](https://tidyr.tidyverse.org/articles/pivot.html) or [Chapter-12 of R for Data Science book](https://r4ds.had.co.nz/tidy-data.html).

### Separate Column(s) into multiple columns/ Join columns into one column

#### Separate a character column into multiple with `separate()` {-}
As the name suggests, `separate()` function is used to separate a given character column into multiple columns either using a regular expression or a vector of character positions.
The syntax is -
```
separate(
  data,
  col,
  into,
  sep = "[^[:alnum:]]+",
  remove = TRUE,
  convert = FALSE,
  extra = "warn",
  fill = "warn",
  ...
)
```
Explanation of purpose of different arguments in above syntax -

1. `data` is as usual name of the data frame
2. `col` is the name of the column which is required to be separated.
3. `into` should be a character vector, which usually should be equal length of maximum number of new columns which will be created out of such separation. (Refer examples nos. 1)
4. `sep` provides a separator value. (Refer Example -3 below).
5. `remove` if `FALSE`, the original column is not removed from the output. (Refer Example -3 below).
6. `convert` if `TRUE`, the component columns are converted to double/integer/logical/NA, if possible. This is useful if the component columns are integer, numeric or logical. (Refer Example-1 below).
7. `extra` argument is used to control when number of desired component columns are less than the maximum possible count. (Refer Example-4 below).
8. `fill` argument is on the other hand, useful when the number of components are different for each row. (Refer Example-2 below)

Example-1:
```{r}
tidyr::table3 %>% 
  separate(rate, into = c("cases", "population"),
           convert = TRUE) # optional - will convert the values
```

Example-2:
```{r}
data.frame(
  x = c("a", "a+b", "c+d+e")
) %>% 
  separate(x,
           into=c('X1', 'X2', 'X3'),
           fill = "left")
```

Example-3:
```{r}
data.frame(
  x = c("A$B", "C+D", "E-F")
) %>% 
  separate(x, 
           sep = "\\-|\\$",
           into = c('X1', 'X2'),
           remove = FALSE)
```
Example-4:
```{r}
data.frame(
  x = c("a", "a+b", "c+d+e")
) %>% 
  separate(x,
           into=c('X1', 'X2'),
           extra = "merge")
```
#### Unite multiple character columns into one using `unite()` {-}
It complements `separate` by uniting the columns into one.  Its syntax is
```
unite(data, 
      col, 
      ..., 
      sep = "_", 
      remove = TRUE, 
      na.rm = FALSE)
```
Explanation of arguments in the above syntax-

1. `data` is as usual name of the data frame.
2. `col` should be the name of new column to be formed (should be a string),
3. `...` the names of columns to be united should be provided
4. `sep` is separator to be used for uniting
5. `remove` if `FALSE`, will not remove original component columns
6. `na.rm` if `TRUE`, the missing values will be removed beforehand.

Example-1a:
```{r}
tidyr::table5 %>% 
  unite("Year",
        c("century", "year"), 
        sep = "",
        remove = FALSE)
```
Example-1b:
We may complete the tidying process in the next step
```{r}
tidyr::table5 %>% 
  unite("Year",
        c("century", "year"), 
        sep = "") %>% 
  separate(rate, 
           into = c("cases", "population"),
           convert = TRUE)
```
Example-

### Separate row(s) into multiple rows

#### Split data into multiple rows with `separate_rows()` {-}
This function is used to separate delimited values placed in one single cell/column into multiple rows (as against in rows using `separate`).
See Example-
```{r}
tidyr::table3 %>% 
  separate_rows(
    rate,
    sep = "/",
    convert = TRUE
  )
```
To create `type` we have to however, add an extra step to the above example-
```{r}
tidyr::table3 %>% 
  separate_rows(
    rate,
    sep = "/",
    convert = TRUE
  ) %>% 
  group_by(country, year) %>% 
  mutate(type = c("cases", "pop"))
```

### Expand table to handle missing rows/values

Sometimes, when we deal with missing data, we require to handle implicit missing values as well.  Either we have to turn these values into explicit missing values or we have to fill appropriate values.  In such cases, two functions namely `complete` and `fill` both from same package are extremely useful.  Let's learn these as well.

#### Turn implicit missing values to explicit using `complete()` {-}
As th name suggests, this function is used to turn implicit missing values (invisible rows) to explicit missing values (visible rows with `NA`).  

As an example, let's suppose fuel prices are revised randomly.  Say, after revision on `1 January 2020` prices revise on `20 January 2020`.  So we will have only 2 rows in data for say `Janaury 2020`.  Thus, there are `r 31-2` implicit missing values in the data.

The syntax is

```
complete(data, 
         ..., 
         fill = list(), 
         explicit = TRUE)
```
Where -

- `data` is as usual argument to provide data frame
- `...` are meant to provide column names to be completed
- `fill` provides a list to supply a single value which can be provided instead of `NA` for missing combinations.

In the example mentioned above we can proceed as 
```{r}
# First create a sample data
df <- data.frame(
  date = c(as.Date("2020-01-01"), as.Date("2020-01-20")),
  price = c(75.12, 78.32)
)
df 

# Use tidyr::complete to see explicit missing values
df %>% 
  complete(date = seq.Date(as.Date("2020-01-01"), as.Date("2020-01-31"), by = "day"))
```
Though the above example created dates as per the criteria given, `complete` function can find all unique combinations in the set of columns provided and return complete set of observations. See this example
```{r}
#Let's create a sample data
set.seed(123)
df2 <- data.frame(
  year = c(2020, 2020, 2020, 2021, 2021),
  qtr = c(1,3,4,2,3),
  sales = runif(5, 100, 200)
)
df2
# use complete to find all combination
df2 %>% 
  complete(year, qtr, # cols provided
           fill = list(sales = 0))

```

**Note:** If `fill` argument would not have been used, the value of sales in missing columns would have been `NA` instead of provided value.

#### Fill missing values based on criteria `fill()` {-}
This function helps in filling the missing values using previous or next entry.  Think of a paper sheet where many cells in a table have been filled as `"-do-"`. Syntax is -
```{r eval=FALSE}
fill(data, # used to provide data
     ..., # provide columns to be filled
     .direction = c("down", "up", "downup", "updown"))
```
The arguments are pretty simple.  Most of the time `"down"` method is used. As an example we could see the example of fuel prices mentioned above.
```{r}
fill_df <- df %>% 
  complete(date = seq.Date(as.Date("2020-01-01"), as.Date("2020-01-31"), by = "day")) %>% 
  fill(price, .direction = "down")

head(fill_df)
tail(fill_df)
```


<!--chapter:end:11-tidyr.Rmd-->

# Part-III: Statistics with R {-}
```{r echo=FALSE, message=FALSE}
library(tidyverse)
```


# Descriptive statistics
Exploratory Data Analysis or often abbreviated as EDA, is mostly the first and foremost step before carrying out any data analytics task, is used to analyze and investigate data sets and summarize their main characteristics, often employing data visualization methods.  EDA is primarily used to see what data can reveal beyond the formal modeling or hypothesis testing task and provides a provides a better understanding of data set variables and the relationships between them. It can also help determine if the statistical techniques you are considering for data analysis are appropriate. Originally developed by American mathematician John Tukey in the 1960s, EDA techniques continue to be a widely used method in the data discovery process today.

## Using base R
Base R provides us with two functions used ato ascertain structure and summary statistics of a data frame.  First is `str` short for __structure__ (and not to be confused with **str**ing) which as its full name suggests gives us structure of the data.  Its usage is simple

```{r}
str(iris)
```

As can be seen it gives us number of variables (columns) as well as observations (rows) available in the given data.  It thereafter presents us names of all the columns/variables in the data along with their types.  That's not all.  It also prints few first values in all of the columns.  For `factor` columns it also gives us available levels in those factor variables.

Another function from base R is `summary` which can be used to generate some summary statistics from the given data frame.  Let's see what we can get from this function.

```{r}
summary(iris)
```

We can see that it nicely gives us five-point summary for all `numeric` variables and count of all values present in `factor` variables. Apart from the five point summary i.e. (1) minimum, (2) 1st quartile, (3) Median, (4) third quartile and (5) maximum; we also get mean (arithmetic) of all numeric variables.

Before moving forward, we can discuss again `table()` function here which is used to genrate counts of factor/character variable(s) in base R.

```{r}
with(iris, table(Species))
```
## Dplyr functions

For calculating other statistics we can use `dplyr::summarise` in combination with `across`.  For Example to calculate `mean`, `sd`, `variance` for all numeric variables of say `iris` data, we can do-

```{r}
library(dplyr)
iris %>%
  summarise(across(where(is.numeric),
                   .fns = list(
                     Mean = ~ mean(.),
                     SD = ~ sd(.),
                     Var = ~ var(.)
                   )))
```

Before trying to understand the output let's learn to use `dplyr::across`.  Actually `across` is used inside dplyr verbs mostly with `mutate` or `summarise` through which we can mutate/summarise multiple variables (columns) simultaneously.  So, at least two arguments are needed; first variable names which can be provided through a type checking variable, str detecting function, etc.; and second argument either a function name or a list of functions together.  So in above example we have summarised all numeric columns (see first argument is a function `is.numeric` which only operates on column names) and second argument is a list of three functions in lambda style notation.  In our example we are having 4 numeric columns and three aggregating functions, so 12 columns we are getting in output.  

We can further reshape/transform the data using `tidyr::pivot_longer`.  See

```{r}
library(tidyr)
iris %>%
  summarise(across(where(is.numeric),
                   .fns = list(
                     Mean = ~ mean(.),
                     SD = ~ sd(.),
                     Var = ~ var(.)
                   ))) %>%
  pivot_longer(everything(),
               names_sep = "_",
               names_to = c(".value", "Function"))
```

```{r}
iris %>%
  summarise(across(where(is.numeric),
                   .fns = list(
                     Mean = ~ mean(.),
                     SD = ~ sd(.),
                     Var = ~ var(.)
                   ))) %>%
  pivot_longer(everything(),
               names_sep = "_",
               names_to = c("Variable", ".value"))
```

Let us also discuss one more data summary statistics function of `dplyr` that is `glimpse`.  It is basically a pipe friendly version of `str()`.  See

```{r}
iris %>% 
  glimpse()
```
To calculate counts of factor variable (as generated by `table` in base R), we can use `dplyr::count` a pipe friendly function.

```{r}
iris %>% 
  count(Species)
```
We can generate counts of multiple combinations of variables
```{r}
ggplot2::diamonds %>% 
  count(cut, color, name = "count")
```


## Using another package `psych`
There are indeed some beautiful packages in R, which creates beautiful EDA summaries for us without much ado.  Package `psych` is one of these.

```{r}
library(psych)
describe(USArrests)
```

Note that output is in `data.frame` format ready to use.  Another function in `psych` is `describeBy` which creates grouped summaries.
```{r}
describeBy(ggplot2::diamonds, group = "cut")
```

There is one more function `describeData` is this package which also results in first as well as last four (default) values.

```{r}
describeData(ggplot2::diamonds)
```

## Using `skimr`

Package `skimr` generates beautiful data EDA summary reports which can be customised as per one's taste.  Full descriptions of this package may be seen [here](https://cran.r-project.org/web/packages/skimr/vignettes/skimr.html).  For basic purposes we can use function `skim` from this package to get data EDA summary reports.

```
library(skimr)
skim(iris)
```
```{r skimrr, fig.show='hold', out.width='50%', echo=FALSE}
knitr::include_graphics('images/skmi1.png')
```

## Viewing relationships between different variables

We can use package `PerformanceAnalytics` to generate and view relationships between different variables in the data.  For this purpose function `PerformanceAnalytics::chart.Correlation()` may be used as shown below.

```{r warning=FALSE, fig.show='hold', fig.align='center', fig.cap="Viewing relationships with PerformanceAnalytics"}
suppressMessages(library(PerformanceAnalytics))

USArrests %>% 
  select(where(is.numeric)) %>% 
  PerformanceAnalytics::chart.Correlation()
```
As can be seen that it generates visualization of a Correlation Matrix of the numeric variables in the given data.  


There is one more package `GGally` which also creates beautiful charts for viewing relationships.  There are two functions in this package which are particularly useful.

1. The `ggpairs()` function of the GGally package allows to build a great scatterplot matrix.  Scatterplots of each pair of numeric variable are drawn on the left part of the figure. Pearson correlation is displayed on the right. Variable distribution is available on the diagonal.

2. The `ggcorr()` function allows to visualize the correlation of each pair of variable as a square. Note that the method argument allows to pick the correlation type you desire.

See the following example-
```{r two-corr, fig.show='hold', out.width="50%", warning=FALSE, fig.cap="Scatterplot Matrix (Left) and Correlation plot (Right) produced in GGally", fig.align='center'}
suppressMessages(library(GGally))
USArrests %>% 
  select_if(is.numeric) %>% 
  ggcorr(label = TRUE)

USArrests %>% 
  select_if(is.numeric) %>%
  ggpairs()

```


# Probability in R
We will keep short here.  Instead of learning all the concepts of probability, we will see how to calculate probability, densities, quantiles for nearly any type of distribution.  R's powerhorse has four types of functions for each of the distributions associated called `pqdr` functions.  Actually all these are prefixes.  Consider a probability function $P(X=x) = p$ for a variable $x$ and $p$ be the associated probability.  

| Distribution                   | P |  Q | D | R  |
| ------------------------------ | --------- | ------ | ----- | -----|
| Beta                           | pbeta     | qbeta | dbeta | rbeta |
| Binomial                       | pbinom    | qbinom | dbinom | rbinom |
| Cauchy                         | pcauchy   | qcauchy | dcauchy | rcauchy |
| Chi-Square                     | pchisq    | qchisq | dchisq | rchisq |
| Exponential                    | pexp      | qexp | dexp | rexp |
| F                              | pf        | qf | df | rf |
| Gamma                          | pgamma    | qgamma | dgamma | rgamma |
| Geometric                      | pgeom     | qgeom | dgeom | rgeom |
| Hypergeometric                 | phyper    | qhyper | dhyper | rhyper |
| Logistic                       | plogis    | qlogis | dlogis | rlogis |
| Log Normal                     | plnorm    | qlnorm | dlnorm | rlnorm |
| Negative Binomial              | pnbinom   | qnbinom | dnbinom | rnbinom |
| Normal                         | pnorm     | qnorm | dnorm | rnorm |
| Poisson                        | ppois     | qpois | dpois | rpois |
| Student t                      | pt        | qt | dt | rt |
| Studentized Range              | ptukey    | qtukey | dtukey | rtukey |
| Uniform                        | punif     | qunif | dunif | runif |
| Weibull                        | pweibull  | qweibull | dweibull | rweibull |
| Wilcoxon Rank Sum Statistic    | pwilcox   | qwilcox | dwilcox | rwilcox |
| Wilcoxon Signed Rank Statistic | psignrank | qsignrank | dsignrank | rsignrank |

All these functions are vectorised. Let us explore these one by one.


## `p*()` set of functions
These set of functions give the cumulative **p**robability distribution of that probability function.  

Example-1.  What is the probability of a number being less than or equal to `25` in `Normal` distribution with `mean = 50` and `sd = 10`.

```{r}
pnorm(25, mean = 50, sd = 10)
```

On the contrary, the probability of a number being greater than or equal to 25 in the above distribution is-
```{r}
# Either deduct probability from 1 
1 - pnorm(25, mean = 50, sd = 10)
# Or provide FALSE to lower.tail argument
pnorm(25, mean = 50, sd = 10, lower.tail = FALSE)
```

Example-2: What is the probability of one or more heads out of two tosses of a fair coin (binomial distribution with `p = 0.5`).

```{r}
pbinom(1, size = 2, p = 0.5)
```


## `q*()` set of functions
These set of functions, give **q**uantile which is the inverse of cumulative probability function. So if $f$ is cdf (cumulative distribution function) of a given probability distribution then $F$ the quantile is inverse of `f` i.e. $F = f^{-1}$.  These are related by

\begin{equation} 
p = f(x)
(\#eq:s1)
\end{equation} 

\begin{equation} 
x = F(x) = f^{-1}(x)
(\#eq:s2)
\end{equation} 

Example- In the above same normal distribution (`mean = 50` and `sd = 10`) What is number below which 90% of population will be distributed.

```{r}
qnorm(0.9, mean = 50, sd = 10)
```

Similar to `cdf` here we may use `lower.tail` argument to find the number above which a population percent is distributed.
```{r}
qnorm(0.9, mean = 50, sd = 10, lower.tail = FALSE)
```

## `d*()` set of functions
We saw that `p` group denotes `cdf`, `q` group denotes `inverse cdf`, but `d` group actually denotes probability **d**ensity function of a given distribution.  Simply stating, this returns the height of probability distribution function for a given x value.

So what is expected probability of drawing exactly 2 heads out of two tosses of a single fair coin (i.e. from a binomial distribution with probability `p = 0.5`).

```{r}
dbinom(2, 2, prob = 0.5)
```

## `r*()` set of functions
These set of functions are used to generate **r**andom numbers from a Statistical distribution.  So to generate `10` random numbers from Normal distribution with `mean = 50` and `sd = 10`, we can use `rnorm`.

```{r}
rnorm(10, mean = 50, sd = 10)
```

We can actually check this using histogram.
```{r fig.align='center', fig.show='hold', fig.cap="Histogram of Random numbers generated out of Normal distribution", out.width="75%"}
set.seed(1234)
hist(rnorm(10000, 50, 10), breaks = 50)
```



<!--chapter:end:12-1stats.Rmd-->

# Linear Regression

Regression models are a class of statistical models that let us explore the relationship between a response variable and one or more explanatory variables. If such a relationship exists and is detected, we can make predictions about the value of the response variable given some explanatory variables. As an example, if a relationship between number of employees in an office and its monthly expenses on salary is established, we can predict the monthly expenses incurred by that office on salary if we know the number of employees.  That lets us do thought of experiments like asking how much the a new company had to pay say 10 employees on salary expenses monthly.  

Explanatory variables are sometimes also referred to as *regressors* or *independent* or *predictor* variables whereas response variable is also called as *dependent* or *outcome* variable.  When the response variable is numeric and the relationship is linear, the regression is called as linear regression. Of course, there are certain other assumptions, which we will discuss later on in the chapter. 

```{r helpfun, echo=FALSE, warning=FALSE, message=FALSE}
library(tidyverse)

my_chart <- function(data, x_var, y_var){
  # Model
  lmod <- eval(substitute(lm(y_var ~ x_var , data = data)))
  # Visualise
  data %>%
    mutate(pred = predict(lmod),
           res = residuals(lmod)) %>%
    ggplot(aes({{x_var}}, {{y_var}})) +
    geom_smooth(method = 'lm', color = 'lightgrey', se = FALSE, formula = 'y ~ x') +
    geom_segment(aes(xend = {{x_var}},  yend = pred), alpha = 0.2, linewidth = 0.9, color = 'seagreen') +
    geom_point(aes(color = abs(res), size = abs(res))) +
    scale_color_continuous(low = 'yellow', high = 'darkred') +
    guides(color = FALSE, size = FALSE) +
    geom_point(aes(y = pred), shape = 1) +
    theme(panel.grid.major = element_blank(),
          panel.grid.minor = element_blank(),
          panel.background = element_blank(),
          axis.line = element_line(colour = "black")
          )

}
```

## Basic concepts
Before we start creating regression models, it's always a good practice to visualize the data. It will help us ascertaining the nature of relationship between the predictors and response variable, if any. To visualize the relationship between two numeric variables, we can use a scatter plot. See the two examples in figure \@ref(fig:concept). In the first example (plot) we can see a near perfect linear relationship between GNP and Population of the countries, whereas a moderate but negative relationship between two variables is seen in second example. 

```{r concept, echo=FALSE, fig.align='center', fig.cap="Linear Regression - Intuition", out.height="45%", out.width="47%", fig.show='hold'}

longley |> 
  ggplot(aes(Population, GNP)) +
  geom_point(color = 'blue') +
  geom_smooth(method = 'lm', se = FALSE, color = 'red', formula = 'y ~ x') +
  labs(title = 'Ex-1: Gross Nation Product Vs.\nPopulation in longley dataset') +
  theme_bw()

mtcars |>
  ggplot(aes(disp, mpg)) +
  geom_point(color = 'blue') +
  geom_smooth(method = 'lm', se = FALSE, color = 'red', formula = 'y ~ x') +
  labs(title = 'Ex-2: Mileage in miles per gallon\nvs. Displacement') +
  theme_bw()


```

Linear regression is a perfect model to predict outcome or response variable when it has linear relationship with explanatory variables.  In that case our predicted values will lie on the assumed line (in linear relationship) ideally.  Maths behind estimating or predicting outcomes is thus, finding the following algebraic equation \@ref(eq:lr1).

\begin{equation} 
y = mx + c
(\#eq:lr1)
\end{equation} 

Where - 

- `m` is the slope of the line (in linear relationship)
- `c` is the intercept of Y-axis. (value of $y$ when $x$ is `0` )

Interpreting this equation in real world is like estimating the coefficients i.e. both $m$ and $c$ in above equation. The goal of our exercise will thus be to estimate the best values of $m$ and $c$.  These two parameters, $m$ and $c$ are sometimes also referred to as $\beta_1$ and $\beta_0$ respectively.  Those familiar with mathematics behind fitting the equation of a line (in two dimensional space), may know that we require only two variables (data points i.e. $x$ and $y$ value pairs) values to find these parameters.  So it means, that if we have a fair amount of data points available (which will in rarest of circumstances be collinear i.e. lying on one same line), we can actually get many such regression lines.  Our goal is thus, to find best of these lines.  But, how?

To answer this question, let us also understand that the values of $x$ and $y$ pairs in actual practice, don't lie on the regression line because these points will rarely be collinear (See plots in Figure \@ref(fig:concept) and note that none of the data point actually lie on the line).  So for each data point of response variable, there is an actual value and one fitted value (the one falling on the regression line).  The difference between these values is called *error term* or *residual*. Technically these are not errors but random noise that the model is not able to explain. Mathematically, if $\hat{y}$ is fitted value, $y$ is actual value, the difference also called error (of prediction) term say $\epsilon$ can be denoted as -

\begin{equation} 
\epsilon = y - \hat{y}
(\#eq:lr2)
\end{equation} 

OR, we can say that

\begin{equation} 
y = \beta_0 + \beta_1x + \epsilon
(\#eq:lr3)
\end{equation} 

Now, one method to find best fit regression line is to minimize the error terms. Theoretically, it means to capture pattern/relationship between data points as much as possible so that what's left behind is true random noise. One way could be to minimise the mean of these error terms.  But these error terms can be both positive or negative.  See figure \@ref(fig:errors). So to ensure that these are not cancelled out while taking mean, we can minimise either the mean of their absolute values or their squares. Most commonly accepted method is to take mean of squares and minimise it.  One of the benefit of adopting it over another, is that while squaring errors or residuals, large residuals get higher weight than lower residuals.  That's why this linear regression technique is also sometimes referred to as **Ordinary Least Squares** or **OLS** regression.  

```{r errors, echo=FALSE, warning=FALSE, fig.align='center', fig.cap="Residuals - Intuition and Concept", out.height="35%"}
my_chart(LifeCycleSavings, sr, pop75)
```

## Simple Linear Regression in R
Don't worry, in R we do not have to do this minimisation job ourselves.  In fact, base R has a fantastic function `lm` which can fit a best regression line, for a given set of variables, for us.  The syntax is simple.

```
lm(formula, data, ...)
```

where -

- `formula` an object of class "formula" (or one that can be coerced to that class): a symbolic description of the model to be fitted.  For our example above, we can write `y ~ x`
- `data` an optional data frame.

It actually returns a special object, which can be printed directly like other R objects.  However, it is best printed with the `summary` function.  Firstly we will see an example of simple linear regression which is a linear regression with only one independent variable.  

Example-1:  **Problem Statement:** `iris` which is a famous (Fisher's or Anderson's) data set, loaded by default in R, gives the measurements in centimeters of the variables sepal length (`Sepal.Length`) and width (`Sepal.Width`) and petal length (`Petal.Length`) and width (`Petal.Width`), respectively, for 50 flowers from each of 3 species (`Species`) of iris. The species are Iris `setosa`, `versicolor`, and `virginica`.  Let us try to establish a relationship between `Sepal.Length` and `Sepal.Length` variables of `setosa` Species i.e. iris` dataset, (first 50 records only).

As already stated above, it is always a good practice to visualize the data, if possible.  So let's make a scatterplot, as seen in Figure \@ref(fig:ex1plot).

```{r ex1plot, fig.align='center', fig.cap="Relationship between sepal widths and lengths in setosa species", out.height="30%"}

iris %>% 
  head(50) %>% 
  ggplot(aes(Sepal.Length, Sepal.Width)) +
  geom_point() +
  geom_smooth(method = 'lm', se=FALSE, formula = "y~x") +
  theme_bw()

```

The relationship seem fairly linear (Figure \@ref(fig:ex1plot)), so let's build the model.

```{r ex1}
lin_reg1 <- lm(formula = Sepal.Width ~ Sepal.Length, data = iris[1:50,])

# Let us print the object directly
lin_reg1

# Try printing it with summary()
summary(lin_reg1)
```

Observing the outputs above, we can notice that simply printing the object returns coefficients whereas printing with `summary` gives us a lot of other information.  But how to interpret this information?  Before proceeding to interpret the output, let us understand a few more concepts which are essential here.  These concepts are basically some assumptions, which we have made while finding the best fit line or in other words estimating the parameters statistically.

## Assumptions of Linear Regression
Linear regression makes several assumptions about the data, such as :

- *Linearity of the data*. The relationship between the predictor ($x$) and the outcome ($y$) is assumed to be linear.  Obviously, the relationship should be linear.  If we would try to fit non-linear relationship through linear regression our results wouldn't be correct.  Also when we use multiple predictors, as we will see shortly, we make another assumption that the model is additive in nature besides being linear.  Refer figure \@ref(fig:linearity).  It is clear that if we try to establish a linear relationship (red line) when it is actually cubic (green dashed line) our model will give erroneous results.

```{r linearity, echo=FALSE, fig.align='center', fig.cap="Is the relationship linear?", out.width="50%"}
set.seed(1234)
x <- 11:110
y <- x^3 + rnorm(100, 0, 80000)
data.frame(x = x, y = y) %>% 
  ggplot(aes(x, y)) +
  geom_point() +
  geom_smooth(method = "lm", color = "red", formula = 'y ~ x', linetype = "solid", se = FALSE) +
  geom_smooth(method = "lm", color = "green", formula = 'y ~ I(x^3)', se = FALSE, linetype = 'dashed') +
  theme_bw()
```

- *Normality of residuals*. The residuals are assumed to be normally distributed.  Actually, this assumption is followed by the assumption that our dependent variable is normally distributed and not concentrated anywhere.  Thus, if dependent variable is normally distributed, and if we have been able to capture the relationship available, then what has been left must be true noise and it should be normally distributed with a mean of $0$.  

- *Homogeneity of residuals variance*. The residuals are assumed to have a constant variance, statistically known as homoscedasticity.  It shows that residuals that are left out of regression model are true noise and not related to fitted values, which in that case would have meant that the model was insufficient to capture the actual relationship.  Heteroscedasticity (the violation of homoskedasticity) is present when the size of the error term differs across values of an independent variable.  This can be best understood by plots in Figures \@ref(fig:homod) where residuals in left plot indicate equal variance and thus homoskedasticity whereas in the right plot heteroskedasticity is indicated clearly.

```{r homod, echo=FALSE, fig.align='center', fig.cap="Homoskedasticity (left) Vs. Heteroskedasticity (right)\nSample data created by author for demonstration only", out.width="47%", fig.show='hold'}
library(tidyverse)

set.seed(1234)
data.frame(x = 1:100, y = rnorm(100, 5, 10)+(1:100)) %>% 
  ggplot(aes(x, y)) +
  geom_point()+
  geom_smooth(method = 'lm', se = FALSE, formula = "y ~ x") +
  labs(title = "Homoskedasticity of Residuals") +
  theme_bw()

data.frame(x = 1:100, y = rnorm(100, 0.5, 0.07)*(1:100)) %>% 
  ggplot(aes(x, y)) +
  geom_point()+
  geom_smooth(method = 'lm', se = FALSE, formula = "y ~ x") +
  labs(title = "Heteroskedasticity of Residuals") +
  theme_bw()


```

- *Independence* of residuals error terms.  This assumption is also followed by original assumption that our dependent variable is independent in itself and any $y$ value is not dependent upon another set of $y$ values.  In our example, we can understand it like that Sepal width of one sample is not affecting width of another.

## Interpreting the output of `lm`
Let us discuss each of the component or section of `lm` output, hereinafter referred to as model.

### `call`
The `call` section shows us the formula that we have used to fit the regression model.  So `Sepal.Width` is our dependent or response variable, `Sepal.Length` is predictor or independent variable.  These variables refer to our dataset which is, first 50 rows of `iris` or `iris[1:50,]`.

### `Residuals`
This depicts the quantile or five point summary of error terms or the residuals, which also as discussed, are the difference between the actual values and the predicted values. We can generate these same values by taking the actual values of $y$ variable and subtracting these from its predicted values of the model.

```{r resid}
summary(iris$Sepal.Width[1:50] - lin_reg1$fitted.values)
```

Ideally, the median of error values/residuals should be centered around `0` thus telling us that these are somewhat symmetrical and our model is predicting fairly at both positive/higher and negative/lower side.  Any skewness will thus show that errors are not normally distributed and our model may be biased towards that side.

In our example, we can observe a slight left-skewed distribution of error terms which indicates that our model is not doing that well for higher sepal lengths as it is doing for lower ones.  This can also be seen in Figure \@ref(fig:ex1hist) i.e. histogram of residuals.
```{r ex1hist, echo=FALSE, fig.align='center', fig.cap="Histogram of Resuduals"}
broom::augment(lin_reg1) %>% 
  ggplot(aes(.resid)) +
  geom_histogram(binwidth = 0.03) +
  geom_density(linewidth = 1, lty = 2) +
  theme_bw() +
  labs(x = "Residuals", y = NULL) + theme(panel.grid.major = element_line(linetype = "blank"),
    panel.grid.minor = element_line(linetype = "blank"))
```

### `Coefficients`
Remember that these were our goals of the exercise.  Here coefficients will be written as coefficient for that predictor and an intercept term, i.e. our $\beta_1$ and $\beta_0$ respectively.  We can easily interpret that positive coefficients means positive relation and negative coefficient means value of outcome variable will decrease as corresponding independent variable increase.  So, in our example, we can deduce that -

- for `0` sepal length, the sepal width will be `-0.5694` (Though mathematically only as physically having `0` and negative width and lengths are not possible).
- for every unit i.e. `1` i.e. unit increase in sepal length, width increases by `0.7985`

Thus our regression line equation is -

\begin{equation} 
Sepal.Width = -0.5694 + 0.7985 * Sepal.Length
(\#eq:lr4)
\end{equation} 


**Now one thing to note here that, since we are adopting OLS approach to find out the (estimated) equation of best line, the coefficients we have arrived at, are only the estimated values of mean of these coefficients.  Actually, we started (behind the scenes) with a null hypothesis that there is no linear relationship or, in other words, that the coefficients are zero.  Alternate hypothesis, in this case, as you may have guessed by now, was that these coefficients are not zero.  The coefficients may follow a statistical/ probabilistic distribution and thus, we may infer only its estimated (mean) value.**

**We can see the confidence intervals of each of the coefficient using function `confint`.  By default, the 95% confidence intervals may be generated.  See the following.**

```{r}
confint(lin_reg1)
```

Now, these estimated distribution must also have some standard error, a probability statistic and a p-value.  

- The *standard error* of the coefficient is an estimate of the standard deviation of the coefficient. It tells us how much uncertainty there is with our coefficient. We can build confidence intervals of coefficients using this statistic, as shown above. 
- The t-statistic is simply the estimated coefficient divided by the standard error. By now you may have understood that we are applying student's t-distribution while estimating the parameters.
- Finally `p value` i.e. **Pr(>|t|)** gives us the probability value and tells us how significant is our coefficient value.

### `Signif. codes`
These are nothing but code legends which are simply telling us how significant out p-value may be for each case.  Notice three asterisks in from of coefficient estimate of `Sepal.Length` which indicate that coefficient is extremely significant and we can reject null hypothesis that $\beta_1$ is $0$.

These codes give us a quick way to visually see which coefficients are significant to the model. 

### `Residual standard error`
The residual standard error is a measure, and one of the metrics, telling us how well the model fits the data.  This is actually the standard deviation of all error terms with the difference that instead of taking `n` terms we are taking `degrees of freedom`.

\begin{equation} 
RSE = \sqrt{\frac{1}{(n-2)} \sum_{i=1}^n (y_i - \hat{y_i})^2}
(\#eq:lr5)
\end{equation} 

Obviously $df$ is $n-2$, as there is one regressor and one intercept.  We can verify equation \@ref(eq:lr5) by calculation.

```{r rse}
sqrt(sum((iris[1:50, "Sepal.Width"] - lin_reg1$fitted.values)^2)/48)
```

### `R-Squared` both Multiple and Adjusted
*Multiple R-Squared* is also called the coefficient of determination.  Often this is the most cited measurement of how well the model is fitting to the data. It tells us what percentage of the variation within our dependent variable that the independent variable is explaining through the model. By looking at output we can say that about 55% of variation is explained through the model. We will discuss it in detail, in the next section.

*Adjusted R squared* on the other hand, shows us what percentage of the variation within our dependent variable that all predictors are explaining.  Thus, it is helpful when there are multiple regressors i.e. in Multiple Linear Regression. The difference between these two metrics might be subtle where we adjust for the variance attributed by adding multiple variables.  

### `F-Statistic` and `p-value`
So why a p-value again?  Is there a hypothesis again?  Yes, When running a regression model, a hypothesis test is being run on the global model, that there is no relationship between the dependent variable and the independent variable(s).  The alternative hypothesis is that there is a relationship. In other words, alternate hypothesis means at least one coefficient of regression is non-zero.  This hypothesis is tested on F-statistic and hence the two values.  `p-value` in our example is very small which lead us to reject the null hypothesis and conclude that there is strong evidence that a relationship does exist between `Sepal.Length` and `Sepal.Width`.

The reason for this test is based on the fact that if we run multiple hypothesis tests on our coefficients, it is likely that a variable is included which isn’t actually significant. 

## Model Evaluation Metrics
To evaluate the model's performance and accuracy, evaluation metrics are needed.  There are several types of metrics which are used to evaluate the performance of model we have built.  We will discuss a few of them here -

### MAE - Mean Absolute Error
As the name suggests it is mean of absolute values of errors or residuals.  The formula thus, can be written as equation \@ref(eq:lr6).

\begin{equation} 
{MAE} = \frac{1}{N}\sum_{i = 1}^{N}{\lvert}{y_i - \hat{y_i}}{\rvert}
(\#eq:lr6)
\end{equation} 

Clearly, it is average value of residuals and a larger value denotes lesser accurate model.  In isolation, the MAE is not very useful, however, to compare performance of several models while fitting a best regression model, obviously we can use this metric to choose a better model.

Moreover, once we extract `$residuals` out of the model, calculating the metric is easy.  In our example-
```{r ex1mae}
# Mean Absolute Error
lin_reg1$residuals |> abs() |> mean()
```

### MSE - Mean Square Error and RMSE - Root Mean Square Error
Again as the name suggests, the mean of square of all residuals is mean square error or MSE.  The formula may be written as in equation \@ref(eq:lr7).

\begin{equation} 
{MSE} = \frac{1}{N}\sum_{i = 1}^{N}({y_i - \hat{y_i}})^2
(\#eq:lr7)
\end{equation} 

MSE penalises the higher residuals by squaring them.  It may be thus thought as weighted average where more and more weight is allocated as the residual value rises.  Similar, to MAE, we can use this metric to choose a better model out of the several validating models. 

Interestingly, by definition it is also cost function in regression, as while finding parameters, we are actually minimising MSE only.  Similar to MAE, calculating this require no special skills.
```{r}
# Mean Square Error
lin_reg1$residuals^2 |> mean()
```

RMSE or root mean square error is square root of MSE. 

\begin{equation} 
{RMSE} = \sqrt{\frac{1}{N}\sum_{i = 1}^{N}({y_i - \hat{y_i}})^2}
(\#eq:lr8)
\end{equation} 

In our Example-

```{r}
# Root Mean Square Error
lin_reg1$residuals^2 |> mean() |> sqrt()
```

### MAPE - Mean Absolute Percentage Error
This metric instead of taking residual value in isolation, takes residual value as percentage of actual values.  The formula is thus,

\begin{equation} 
{MAPE} = \frac{1}{N}\sum_{i = 1}^{N}{\lvert}\frac{({y_i - \hat{y_i}})}{y_i}\cdot{100}\%{\rvert}
(\#eq:lr9)
\end{equation} 

Clearly, MAPE is independent of the scale of the variables since its error estimates are in terms of percentage.  In our example, we can calculate MAPE-
```{r}
# Mean Absolute Percentage Error
{lin_reg1$residuals/iris$Sepal.Width[1:50]} %>% 
  {abs(.)*100} %>% 
  mean(.) %>% 
  sprintf("MAPE is %1.2f%%", .)
```

### R - Squared and adjusted R-squared
As already discussed, it is coefficient of determination or goodness of fit of regression.  The  can be written as equation \@ref(eq:lr10).

\begin{equation} 
R^2 = 1 - \frac{\sum_{i=1}^{N}(y_i - \hat{y_i})^2}{\sum_{i=1}^{N}(y_i - \bar{y_i})^2}
(\#eq:lr10)
\end{equation} 

The numerator in the fraction above, is also called as $SSE$ or Sum of Squares of Errors; and denominator is also called as $TSS$ or Total Sum of Squares.  Actually the ratio (or fraction in above formula) i.e. $\frac{SSE}{TSS}$ denotes ratio of variance in errors to the variance (about mean) in the actual values.  Thus $R^2$ actually denotes how much variance is explained by the model; and clearly as the variance errors or $SSE$ minimises and approaches $0$, $R^2$ increases and approaches $1$ i.e. a perfect model.

We can easily verify the formula from the results obtained.
```{r}
## Sum of Squares of Errors
y_bar <- mean(iris[1:50,"Sepal.Width"])
(SSE <- sum(lin_reg1$residuals^2))
(TSS <- sum((iris[1:50,"Sepal.Width"] - y_bar)^2))
(r_sq <- 1 - SSE/TSS)
```

Now, we can think of this $R^2$ in one more way, as it is simply the square of the correlation between the actual and predicted values.  We can verify once again

```{r}
(cor(iris[1:50, 'Sepal.Width'], lin_reg1$fitted.values))^2
```

Usually, when we keep on adding independent variables to our regression model $R^2$ increases and it can easily incorporate over-fitting in itself. For this reason, sometimes adjusted r squared is used, which penalizes R squared for the number of predictors used in the model.

\begin{equation} 
{Adjusted}\;{ R^2} = 1 - \frac{(1-R^2)(N - 1)}{(N - p - 1)}
(\#eq:lr11)
\end{equation} 

where $p$ is the number of predictors included in model.  The `summary` function returns both these metrics. We can verify the calculation-
```{r}
1 - ((1-r_sq)*(50-1)/(50-1-1))
```


## Plotting the results and their interpretion
The output of `lm` can be plotted with `plot` command to see six diagnostics plots, one by one, which can be chosen using `which` argument.  These six plots are -

- Residuals Vs. Fitted Values
- Normal Q-Q
- Scale-Location
- Cook's distance
- Residuals vs. leverage
- Cook's distance vs. leverage

Let us see these, for the example above.

```{r plots1, fig.align='center', fig.cap="First two diagnostic plots", out.height="40%"}
par(mfrow = c(1, 2), oma = c(0, 0, 2, 0))
plot(lin_reg1, which = 1:2, sub.caption = "")
```

1. **Residuals Vs. Fitted Values**: This plot is used to determine if the residuals exhibit non-linear patterns. If the red line across the center of the plot is roughly horizontal then we can assume that the residuals follow a linear pattern.  In our example we can see that the red line deviates from a perfect horizontal line but not severely. We would likely declare that the residuals follow a roughly linear pattern and that a linear regression model is appropriate for this dataset. This plot is useful to check first assumption of linear regression i.e. linearity of the data.
2. **Normal Q-Q**: This plot is used to determine if the residuals of the regression model are normally distributed, which was our another assumption. If the points in this plot fall roughly along a straight diagonal line, then we can assume the residuals are normally distributed.

Moreover, notice that the extreme outlier values impacting our modelling will be labeled.  We can see that values from rows, 23, 33 and 42 are labeled.  

```{r plots2, fig.align='center', fig.cap="Next two diagnostic plots", out.height="40%"}
par(mfrow = c(1, 2), oma = c(0, 0, 2, 0))
plot(lin_reg1, which = 3:4, sub.caption = "")
```

3. **Scale-Location**: This plot is used to check the assumption of equal variance, i.e. “homoskedasticity” among the residuals in our regression model. If the red line is roughly horizontal across the plot, then the assumption of equal variance is likely met.
4. **Cook's Distance**: An influential value is a value, which inclusion or exclusion can alter the results of the regression analysis. Such a value is associated with a large residual. Not all outliers (or extreme data points) are influential in linear regression analysis. A metric called Cook’s distance, is used to determine the influence of a value. This metric defines influence as a combination of leverage and residual size.

```{r plots3, fig.align='center', fig.cap="Last two diagnostic plots", out.height="40%"}
par(mfrow = c(1, 2), oma = c(0, 0, 2, 0))
plot(lin_reg1, which = 5:6, sub.caption = "")
```

5. **Residuals vs. leverage**: This plot is used to identify influential observations. If any points in this plot fall outside of Cook’s distance (the dashed lines) then it is an influential observation.  Actually, a data point has high leverage, if it has extreme predictor x values.
6. **Sixth plot i.e. Cooks distance vs. leverage** is also used to identify extreme values that may be impacting our model.

Though the base R's command `plot` can generate all the plots, we may make use of library `performance` to generate all the relevant diagnostic plots beautifully and with small interpretaion.  See

```{r perf, fig.align='center', fig.cap="Output from package-performance", fig.height=8.5}
library(performance)
check_model(lin_reg1)
```

The warning is obvious, we cannot have `multi-collinearity` problem as there is only one regressor.  Actually multi-collinearity is about another assumption, we would have made in case there were more than one independent variables.  This assumption would have been that the independent variables are not mutually collinear.  We will see detailed explanation in case of multiple linear regression in the subsequent section.

## Using `lm` for predictions
The output of `lm` is actually a list which contains much more information than we saw above.  See which info is contained here -

```{r objs}
names(lin_reg1) |>
  as.data.frame() |>
  setNames('Objects')
```

We may extract any of the as per requirement. E.g.

```{r}
lin_reg1$coefficients
```

There is a package `broom` which uses `tidy` fundamentals to returns all the useful information by a single function `augment`.

```{r aug}
library(broom)
augment(lin_reg1)
```

Let's also visualise the predicted values vis-a-vis actual values/residuals in Figure \@ref(fig:predvsact).

```{r predvsact, echo=FALSE, fig.align='center', fig.show='hold', fig.cap="Predicted Vs. Actual Values (Left) and Residuals (Right)", out.width="48%"}
newdata <- iris[1:50, ]
lin_reg1 <- lm(Sepal.Width ~ Sepal.Length, data =newdata)
newdata$predictions <- predict(lin_reg1)
ggplot(newdata, aes(predictions, y = Sepal.Width)) +
  geom_point() +
  geom_abline(color = "blue") +
  labs(x = "Predicted Values",
       y = "Actual Values",
       title = "Predicted Values Vs. Actual Values") +
  theme_bw()

newdata$residuals <- lin_reg1$residuals
ggplot(newdata, aes(predictions, y = residuals)) +
  geom_pointrange(aes(ymin = 0, ymax = residuals), alpha = 0.5) +
  geom_hline(yintercept = 0, linetype = 3, color = "blue") +
  labs(x = "Predicted Values",
       y = "Residuals",
       title = "Predicted Values Vs. Residuals") +
  theme_bw()

  
```


So if we have predict output from a new data, just ensure that data is in exactly same format as of regressor and we can use `predict` from base R directly.  See this example.

```{r preds}
new_vals <- rnorm(10, 5, 1) |> 
  as.data.frame() |>
  setNames('Sepal.Length')
predict(lin_reg1, new_vals)
```

## Multiple Linear Regression

As the name suggests, multiple linear regression is the model where multiple independent variables may have linear relationship with dependent variable.  In this case, the regression equation will be -

\begin{equation} 
y = \beta_0 + \beta_1x_1 + \beta_2x_2 + \beta_3x_3 + ... + \epsilon
(\#eq:lr12)
\end{equation} 

where $x_1$, $x_2$, $x_3$ ..., $x_n$ will be $n$ independent variables and $y$ will be dependent variable as usual. 

It may be clear that this equation represents an equation of a plane if there are two regressors.  If there are $n$ regressors, the equation will represent equation of a hyperplane of $n-1$ dimensions. However, visualizing the variables and relation between them can be a bit tricky if there are multiple variables.  We may decide about the type of the visualization will suit the requirement in that case. 

Now, as already stated, there is an additional assumption, **that all the independent variables are mutually independent too i.e. do not have multi-collinearity between them.**  Let's build an example model again.  We have to just add the predictors (independent variables) using the `+` operator in the formula `call`.  

**Problem Statement:** Example-2. Let's try to establish the relationship between cars' mileage (`mpg` variable in `mtcars` dataset) with engine displacement `disp`, horse power `hp` and weight `wt`.  First of all let's visualise the individual relationships between the variables through three plots as in Figure \@ref(fig:ex2vis).

```{r ex2vis, echo=FALSE, warning=FALSE, message=FALSE, fig.align='center', fig.cap="Relationship between regressors and outcome variables", out.width="31%", fig.show='hold'}

mtcars %>% 
  ggplot(aes(disp, mpg)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE , formula = "y ~ x") +
  theme_bw()

mtcars %>% 
  ggplot(aes(hp, mpg)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE , formula = "y ~ x") +
  theme_bw()

mtcars %>% 
  ggplot(aes(wt, mpg)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE , formula = "y ~ x") +
  theme_bw()


```

Let's also visualise the correlation between all these variables. See Figure \@ref(fig:ex2vis2).

```{r ex2vis2, echo=FALSE, warning=FALSE, message=FALSE, fig.align='center', fig.cap="Correlation between the variables in Example-2"}
library(GGally)
my_fn <- function(data, mapping, method="p", use="pairwise", ...){
  
  # grab data
  x <- eval_data_col(data, mapping$x)
  y <- eval_data_col(data, mapping$y)
  
  # calculate correlation
  corr <- cor(x, y, method=method, use=use)
  
  # calculate colour based on correlation value
  # Here I have set a correlation of minus one to blue, 
  # zero to white, and one to red 
  # Change this to suit: possibly extend to add as an argument of `my_fn`
  colFn <- colorRampPalette(c("blue", "white", "red"), interpolate ='spline')
  fill <- colFn(100)[findInterval(corr, seq(-1, 1, length=100))]
  
  ggally_cor(data = data, mapping = mapping, ...) + 
    theme_void() +
    theme(panel.background = element_rect(fill=fill))
}


ggpairs(
  mtcars[, c("mpg", "disp", "hp", "wt")],
  upper = list(continuous = my_fn),
  lower = list(continuous = 'smooth', wrap=c(colour="blue")))

```

Now let's build the model.

```{r mult}
data <- mtcars[, c("mpg", "disp", "hp", "wt")]
lin_reg2 <- lm(mpg ~ ., 
               data = data)
summary(lin_reg2)
```

In formula call, please note that I have used `.` instead of naming all the variables.  In fact `.` is a shorthand style of mentioning that all the variables except that mentioned as y variable will be our independent variables/predictors.  In results, there is one coefficient for slope for each predictors and one intercept term in the output.  The output equation can be written as equation \@ref(eq:lr13).

\begin{equation} 
{mpg} = -0.000937\cdot{disp} - 0.031157\cdot{hp} - 3.800891\cdot{wt} + 37.105505
(\#eq:lr13)
\end{equation} 

**Interpretation:** Notice that all slopes are in negative meaning that mileage drops by increase in each of weight, displacement and horsepower.  Interpreting the equation would be on similar lines. We can now say that *keeping other variables constant*, the effect of change (increase) of 1 unit in `disp` will result in decrease of milaege by 0.000937 miles per gallon. Keeping other variables constant is important here.  Of course, by above equation we may deduce that if other factors change, the effect on response variable would be different.

**Results:** Analysing results, we may notice that our model is explains 83% of variance in data.  Of course, adjusted r-squared is lower which means that adding extra variables may have increased the r-squared. Global p-value is highly significant which means that at least of the coefficients is non-zero.  Of course, the least significant coefficient is that of `disp` which we can remove and re-run the model to check the parameters again.

## Including categorical or factor variables in `lm`
By now we  have seen that linear regression is useful for predicting numerical output and through the examples we have seen that our regressors were numerical too.  But what if there's an input variable which is categorical or nominal?

In such case, we will have to ensure that categorical variable is of type `factor` before proceeding to build a model. 

**Problem Statement:** Example-3.  Let's try to predict ${Sepal.Width}$ from ${Species}$ in the iris dataset. This time we will take complete dataset. Since, we know that `Species` is already of factor type we need not convert it into one.  Before moving on let's visualize the relation between the two variables using ggplot2.  Box-plots are best suited here.

```{r fact1, echo=FALSE, fig.align='center', fig.cap="Species Vs. Sepal Width", warning=FALSE, fig.show='hold', out.height="30%"}
ggplot(iris, aes(Species, Sepal.Width)) +
  geom_boxplot() +
  stat_summary(fun = mean, shape = 15) +
  labs(x = "Species", 
       y = "Sepal.Width") +
  theme_bw()
```

Now let's build the model.

```{r}
lm_fact <- lm(Sepal.Width ~ Species, data = iris)
summary(lm_fact)
```

In results we now got one intercept and two slopes for single regressor `Species`.  So what happened now?  

**Interpretation:** Actually, factor data type requirement for categorical regressor was due to the fact that this factor variable is encoded as dummy variable for each of the category available in it. Dummy variable is numeric and we can now run linear regression as earlier. The first category available in it will be baseline level.  Since there were three levels included in `Species` it has been encoded into two dummy variables.  To see what happened behind the scenes, we may use `contrasts` function.

```{r}
contrasts(iris$Species)
```

`model.matrix`

It is clear that when `versicolor` is `1` the other variable is `0` and vice versa.  Obviously when both are `0` it means that `Species` is `setosa` and that's why no separate slope for that is present in output.  Now we can write our regression line equation as \@ref(eq:lr14)

\begin{equation} 
{Sepal.Width} = 3.428 + (-0.658)\cdot{Speciesversicolor} + (-0.454)\cdot{Speciesvirginica}
(\#eq:lr14)
\end{equation} 

Interpreting above equation is now easy.  For each `versicolor` the `Sepal.Width` may be `3.428 - 0.658` or `r 3.428 - 0.658`. Obviously when two dummy variables are zero, the Sepal.Width would be equal to intercept; and thus, we can conclude that intercept is nothing but prediction for base-line level.  In fact we can subtract a `1` to obtain these interpreted results.

```{r}
lm_fact <- lm(Sepal.Width ~ Species -1, data = iris)
summary(lm_fact)
```

**Notice that other two coefficients have now been adjusted automatically.**

**Results:** Observing the figure \@ref(fig:fact1), we may notice that the coefficients are nothing but mean Sepal Widths for each Species, which is obvious and logical too.  Those, who are interested in seeing equation (without intercept) can also make one, as in equation \@ref(eq:lr15).

\begin{equation} 
{Sepal.Width} = 3.428\cdot{Speciessetosa} + (2.77)\cdot{Speciesversicolor} + (2.974)\cdot{Speciesvirginica}
(\#eq:lr15)
\end{equation} 

Since, these coefficients are not slopes in true sense, we will refer these as intercepts, in next examples/sections.

### Parallel slopes regression
To understand how categorical response variable acts, when there are other numerical variables in regression model, let us build a model step by step.   

**Problem statement:** Example-4. We are taking `mpg` dataset included by default with `ggplot2` package.  This dataset shows *Fuel economy data from 1999 to 2008 for 38 popular models of cars*.  Let us predict highway mileage `hwy` from engine displacement `displ` and year of the model `year`.  Let us visualize the variables.  From Figure \@ref(fig:ex4vis) it is clear that highway mileage is linearly associated with displacement.

```{r ex4vis, fig.align='center', fig.cap="Highway Mileage Vs. Displacement over cars manufactured in 1998 Vs. 2008", fig.show='hold', out.height="35%"}
ggplot(mpg, aes(hwy, displ)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, formula = "y ~ x") +
  facet_wrap(.~ year) +
  theme_bw()
```


**Step-1:**  Let us first include a single numerical variable `displ` to predict highway mileage `hwy`; and examine the coefficients.
```{r}
par_slop1 <- lm(hwy ~ displ, data = mpg)
coef(par_slop1)
```

We got one intercept and one slope coefficient.  We can even see related visualisation in figure \@ref(fig:parrslop) (left).

**Step-2:** Now let us try to predict mileage on the basis of `year` of manufacture only.  So as already stated, we have to convert it into factor.  We can do that directly in the formula.  *Also note that we are substracting $1$ from the response variables, which actually replaces intercept with the baseline level explicitly.*  Now see the coefficients-

```{r}
library(ggplot2)
par_slop2 <- lm(hwy ~ factor(year) - 1, data = mpg)
coef(par_slop2)
```

Notice that we got intercept for each of the category available in factor variable.  By seeing plot in Figure \@ref(fig:parrslop)-(Right) that these intercepts are nothing but means for each category.

```{r parrslop, warning=FALSE, echo=FALSE, fig.align='center', fig.cap="Mileage vs. Displacement (Left) and Year (right)", out.width="47%", fig.show='hold'}

ggplot(mpg, aes(displ, hwy)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, formula = 'y ~ x') +
  labs(x = "Engine Displacement in Litres", 
       y = "Highway Mileage in Miles per Gallon") +
  theme_bw()


ggplot(mpg, aes(factor(year), hwy)) +
  geom_boxplot() +
  stat_summary(fun = mean, shape = 15) +
  labs(x = "Year of Manufacture", 
       y = "Highway Mileage in Miles per Gallon") +
  theme_bw()


```

**Step-3:**  Now we will build the complete model by including both variables together; and examine the coefficients.

```{r}
par_slop <- lm(hwy ~ displ + factor(year) - 1, data = mpg)
coef(par_slop)
```

We can see one slope coefficient for numerical variable and two different intercepts for each of the Years.  Try visualising these.  In fact there are two different parallel lines (same slope). Refer figure \@ref(fig:parrslop2).

```{r parrslop2, echo=FALSE, fig.align='center', fig.cap="Parallel Slopes", out.width="95%", out.height="35%", fig.show='hold'}
library(moderndive)
ggplot(mpg, aes(displ, hwy, color = factor(year))) +
  geom_point() +
  geom_parallel_slopes(se = FALSE) +
  labs(x = "Engine Displacement in Litres", 
       y = "Highway Mileage in Miles per Gallon",
       color = "Year Manufactured") +
  theme(legend.position = "bottom") +
  theme_bw()
```

This is also evident, if we write out the equation \@ref(eq:lr16).

\begin{equation} 
{hwy} = -3.610986\cdot{displ} + 35.275706\cdot{year1999} + 36.677842\cdot{year2008}
(\#eq:lr16)
\end{equation} 

Either one of the dummy variables will be 0 and another 1, so that variable with `1` will act as intercept term, but the slope term will remain same. In other words, whatever be the year, the mileage will vary with displacement at the same rate.  This is in actual circumstances, rare.  Rate of change of response variable will change as per factor variables (regressor) change their values.  So how to incorporate these changes in our model?  The answer is `interaction` which has been discussed in next section.

### Extending multiple linear regression by including `interactions`

The parallel slopes model, we saw in previous section enforced a common slope for each category. That's not always the best option. 

**Problem Statement:**  In same example-4 (earlier section) we can introduce `interaction` between two predictors using special operator OR shorthand notation `:` in formula call.  See the following example-

```{r interact}
new_model <- lm(hwy ~ displ + factor(year) + displ:factor(year) -1 , data = mpg)
coef(new_model)
```

Now notice an extra coefficient, though small which is change in slope when moving from baseline category to category of year- 2008.  This, in fact represents that the model has a different slope for each category; refer Figure \@ref(fig:parrslop3).

```{r parrslop3, echo=FALSE, fig.align='center', fig.cap="Changing Slopes with interaction", out.width="95%", fig.show='hold', out.height="35%"}
library(moderndive)
ggplot(mpg, aes(displ, hwy, color = factor(year))) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, formula = "y~x") +
  labs(x = "Engine Displacement in Litres", 
       y = "Highway Mileage in Miles per Gallon",
       color = "Year Manufactured") +
  theme(legend.position = "bottom") +
  theme_bw()
```

Interpreting these models are now, not that difficult as it seem earlier. Let's generate summary first.

```{r}
summary(new_model)
```

We may write our equation now (equation \@ref(eq:lr17).

\begin{equation} 
{hwy} = -3.7684\cdot{displ} + 35.7922\cdot{year1999} + 36.1367\cdot{year2008} + 0.3052\cdot{displ}\cdot{year2008}
(\#eq:lr17)
\end{equation} 

Clearly, when year is 2008, the slope for `displ` changes by `0.3052`.

We can add as many interactions as we would like to, using the shorthand `:`; howwever, when there are many interactions we may make use of another shorthand operator `*`.  So `x*z` would mean `x + z + x:z` and `x*z*w` would mean `x + z + w + x:z + x:w + z:w`.  This obviously wouldn't make any difference but would save us a lot of typing.

## Multi-collinearity and Variance Inflation Factor (VIF)
Multi-collinearity indicates a strong linear relationship among the predictor variables. This can create challenges in the regression analysis because it becomes difficult to determine the individual effects of each independent variable on the dependent variable accurately.  Multi-collinearity can lead to unstable and unreliable coefficient estimates, making it harder to interpret the results and draw meaningful conclusions from the model. It is essential to detect and address multi-collinearity to ensure the validity and robustness of regression models.

But why it poses a problem in regression analysis.  Actually, multi-collinearity means that one independent variable can be predicted from another and it in turn means that independent variables are no longer independent.  

Multi-collinearity can be detected using many different methods.  One of the method can be to use correlation plots, which is explained in next section.  Another method is to use Variance Inflation Factor or VIF.  VIF determines the strength of the correlation between the independent variables. It is predicted by taking a variable and regressing it against every other variable. In other words, VIF score of an independent variable represents how well the variable is explained by other independent variables.  

\begin{equation} 
{VIF} = \frac{1}{1-R^2}
(\#eq:lr18)
\end{equation} 

So the closer $R^2$ value to $1$ the higher the ${VIF}$.  

- $VIF$ starts at $1$ and has no upper limit
- ${VIF} = 1$ means there is no correlation between the independent variable and the other variables
- ${VIF}$ exceeding $5$ indicates high multi-collinearity between that independent variable and others.

Again in R, we do not have to manually calculate ${VIF}$ for each variable.  Using `vif()` function, of library `car` we can calculate this.  Let's use it on another model built on `mtcars` data where `mpg` variable is predicted using all other variables.  For all other variables, instead of using names, we will use another shorthand operator `.`.

```{r}
mod <- lm(mpg ~ . , data = mtcars)
library(car)
car::vif(mod)
```

We may notice high collinearity among some of these predictors as also indicated in above output.

## One Complete Example
**Problem Statement:** Example-5. The data pertains to intercountry Life-Cycle Savings 1960-1970.  Under the life-cycle savings hypothesis as developed by *Franco Modigliani*, the savings ratio (aggregate personal saving divided by disposable income) `sr` is explained by per-capita disposable income `dpi`, the percentage rate of change in per-capita disposable income `ddpi`, and two demographic variables: the percentage of population less than 15 years old `pop15` and the percentage of the population over 75 years old `pop75`. The data are averaged over the decade 1960–1970 to remove the business cycle or other short-term fluctuations.

Let's try linear regression on `LifeCycleSavings` data.

```{r ex2}
# Visualise first 6 rows
head(LifeCycleSavings)

# Build a model
ex1 <- lm(sr ~ . , data = LifeCycleSavings)
```

In `call` formula above, notice the shorthand `.` operator which here means all variable other than y variable are treated as input variables.  See its output-

```{r summ1}
summary(ex1)
```

Multiple R squared is about 34% which means nearly 34% variability is explained by linear model.  Let us see some diagnostics plots (figure \@ref(fig:perf2))

```{r perf2, fig.align='center', fig.cap="Diagnostic Plots", fig.height=8.5}
performance::check_model(ex1)
```

We may notice some multi-collinearity, between two population variables.  See figures and \@ref(fig:perf2) and \@ref(fig:multi2).

```{r multi2, fig.align='center', fig.cap="Are x and y correlated?", out.height="30%"}
LifeCycleSavings %>% 
  ggplot(aes(pop15, pop75)) +
  geom_point()+
  geom_smooth(method = 'lm', formula = 'y~x') +
  theme_bw()
```

This can also be verified by corrplots in figure \@ref(fig:corrp).

```{r corrp, echo=FALSE,fig.align='center', fig.cap="Correlation Plots", out.height="45%", message=FALSE}

ggpairs(
  LifeCycleSavings,
  upper = list(continuous = my_fn),
  lower = list(continuous = 'smooth', wrap=c(colour="blue")))

```

Let us see the VIF among the predictors.

```{r vifs}
car::vif(ex1)
```

Thus, the model should be tuned better by removing this multi-collinear variable.  Let us try to remove `pop75` and re-run the model.

```{r ex22}
ex2 <- lm(sr ~ pop15 + dpi + ddpi, data = LifeCycleSavings)
summary(ex2)
```

Notice that multiple R-squared has now reduced to 30%.  Let us try to remove the variable `dpi` the coefficient of which is not that significant.

```{r ex33}
ex3 <- lm(sr ~ pop15 + ddpi, data = LifeCycleSavings)
summary(ex3)
```

Multiple R squared increased slightly i.e. now reached around 29%.  Let us try to visualise the relationship through a scatter plot.

```{r warning=FALSE, fig.align='center', fig.cap="Ascertaining linear relationship", out.height="30%"}
LifeCycleSavings %>% 
  ggplot(aes(pop15, sr)) +
  geom_point() +
  geom_smooth(method = 'lm', formula = 'y ~ x', se = FALSE) +
  theme_bw()
```

Clearly the relationship between the predictor and response variable is not that strong and hence the results.

## Simpson's paradox
Edward Hugh Simpson, a statistician and former cryptanalyst at Bletchley Park, described this statistical phenomenon in a paper in 1951.  It is classic example how regressions, without including necessary terms, can be misleading.  At its core, the paradox arises when a trend that appears in different subgroups of data is either *reversed* or *disappears* when the subgroups are combined. This seemingly counter-intuitive occurrence can lead to misleading conclusions and thus underscores the importance of careful analysis and interpretation of data.

**Problem Statement:** Example-6.  Here is data of `palmerpenguins` where let's try to establish relationship between penguins bills' depth and lengths i.e. `bill_depth_mm` and `bill_length_mm`.  See the plot in fig \@ref(fig:ex3).

```{r ex3, echo=FALSE, fig.align='center', fig.cap="Regression having variable hidden (left) and exposed (right)", out.width="47%", fig.show='hold'}
library(palmerpenguins)

penguins %>% 
  na.omit() %>% 
  ggplot(aes(bill_length_mm, bill_depth_mm)) +
  geom_point() +
  geom_smooth(method = 'lm', formula = 'y ~ x') +
  theme_bw()

penguins %>% 
  na.omit() %>% 
  ggplot(aes(bill_length_mm, bill_depth_mm, color = species)) +
  geom_point() +
  geom_smooth(method = 'lm', formula = 'y ~ x') +
  theme(legend.position = 'bottom') +
  theme_bw()


```

In plot (left) a negative relationship is seen, but when the lurking variable is exposed (right), we can see a positive relationship for each of the species . Thus, species is a significant confounding variable to assess linear relationship here.  Thus before finalizing the model, we have to be sure that we are not missing any important variable.  To avoid incorrect results due to underlying Simpson's Paradox, we must ensure to:

**Identify Confounding Variables:** Be vigilant in identifying potential confounding variables that could affect the relationship between the variables under study.

**Consider All Levels:** Analyze data at different levels, including subgroup and aggregate levels, to gain a comprehensive understanding of the relationship.

**Utilise Statistical Techniques:** such as regression analysis or propensity score matching, to control for confounding variables and obtain more accurate insights.

**Transparent Reporting:** Clearly report the methodology, assumptions, and limitations of the analysis to ensure that others can critically evaluate the findings.

Simpson's Paradox, thus, serves as a powerful reminder that data analysis is an intricate process that requires careful consideration of underlying factors. 

## Conclusion and Final thoughts
In above sections, we learned techniques of regression analysis, which is a powerful and useful tool in data analytics while auditing.  We may use regression analysis, inter alia, for -

**Detection of Anomalies and Outliers:** Regression analysis can help auditors in identifying anomalies, outliers, or unexpected patterns in financial data. Unusual relationships between variables can signal potential errors, fraud, or irregularities that require further investigation.  

**Risk Assessment:** By analyzing the relationships between various financial or operational variables, regression analysis can assist auditors in assessing the level of risk associated with different aspects of an organization's operations. This helps auditors prioritize their efforts and allocate resources effectively. 

**Control Testing:** Regression analysis can aid us in testing the effectiveness of internal controls within an organization. By examining the relationship between control variables and outcomes, auditors can assess whether controls are functioning as intended.  We can also use regression analysis to compare an organization's financial performance against industry benchmarks or similar companies. Deviations from expected relationships can highlight areas that warrant closer examination.

-----------------------------------------

<!--chapter:end:12-2reg.Rmd-->

# Time Series Analysis

Time series analysis is an essential technique for forecasting and analyzing trends in data over time. It is commonly used in various fields like finance, economics, and engineering.

So a question arises, what is a time series? A time series is a sequence of data points collected over time, where the observations are recorded in chronological order. Time series analysis involves analyzing and modeling these data points to understand patterns, trends, and make predictions about future values.  It is therefore used in predictive as well as descriptive analysis.

Time Series Analysis plays an important role in audit analytics as well.  A few of the use cases can be-

- **Revenue Analysis:** An auditor may analyze revenue data over time to identify irregularities or suspicious patterns that could indicate potential fraud or misstatement. By examining the revenue time series, the auditor can look for unexpected fluctuations, unusual growth or decline trends, or abnormal seasonality. Deviations from historical patterns or industry benchmarks may indicate fraudulent activities, such as revenue manipulation, fictitious transactions, or irregular recognition practices.
- **Inventory Analysis:** Auditors often analyze inventory data to assess the adequacy of inventory levels, identify potential inventory obsolescence or shrinkage, and evaluate the efficiency of inventory management. Time series analysis can be valuable in understanding inventory patterns and identifying potential risks or anomalies.

There are several time series data which are loaded by default in R; a few of these are listed below.  We will use these datasets to understand the various concepts related to time series analysis.

- `AirPassengers` containing monthly airline passenger numbers from 1949-1960. See figure \@ref(fig:tsex) (a)
- `Nile` contains flow of Nile river data. See figure \@ref(fig:tsex) (b)
- `sunspots` containing monthly sunspot numbers from 1749-1983. See figure \@ref(fig:tsex) (c)
- `JohnsonJohnson` which contains quarterly earnings per Johnson & Johnson share. See figure \@ref(fig:tsex) (d)

```{r tsex, echo=FALSE, fig.cap="Few time series data sets in R", fig.align='center', fig.show='hold'}
par(mfrow = c(2,2))
plot(AirPassengers, main= "(a) Airline Passengers from 1949-1960")
plot(Nile, main = "(b) Nile flow 1871-1970")
plot(sunspots, main = "(c) Sunspots")
plot(JohnsonJohnson, main = "(d) Earnings of Johnson&Jonson")
```

To analyse any time series, we have to understand its different components.

## Components of Time series
A time series can be decomposed into several components:

- **Level:** The baseline value or average of the series over time. It represents the long-term behavior of the series. This is basic component, and is always present in a time series object.  E.g. In a straight horizontal line only level is there which is equal to the value of y intercept.
- **Trend:** The overall direction of the series. It indicates whether the series is increasing, decreasing, or staying relatively constant over time.  E.g. A clear increasing trend can be seen in Johnson & Johnson earnings, in Figure \@ref(fig:tsex) (d).
- **Seasonality:** The repetitive and predictable patterns within the series that occur at regular intervals. Seasonality can be daily, weekly, monthly, quarterly, or yearly, etc. E.g. In figure \@ref(fig:tsex) (d) we may see seasonal patterns in earnings of Johnson&Johnson share.
- **Errors (Residuals):** The random fluctuations or noise in the series that cannot be explained by the level, trend, or seasonality.  E.g. See figure \@ref(fig:tsex) (c). The error component is an important aspect of time series analysis because it provides information about the uncertainty and variability of the data.

## Additive or multiplicative components
In time series analysis, the trend, seasonal, and residual components can be modeled as either additive or multiplicative. The choice of the model depends on how the components combine to create the observed values of the series.

An **additive model** assumes that the components of the time series are added together to create the observed values. In other words, the value of the time series at any point in time is equal to the sum of the trend, seasonal, and residual components at that point in time. This is expressed mathematically as:

$$
y_t = T_t + S_t + e_t
$$

where $y_t$, $T_t$, $S_t$ and $e_t$ are the values of the series, trend component, seasonal component and  residuals, respectively at time $t$.

A **multiplicative model**, on the other hand, assumes that the components of the time series are multiplied together to create the observed values. In other words, the value of the time series at any point in time is equal to the product of the trend, seasonal, and residual components at that point in time. This is expressed mathematically as:

$$
y_t = T_t \times S_t \times e_t
$$

where $y_t$, $T_t$, $S_t$ and $e_t$ are defined as before.  We can convert multiplicative time series into additive time series by taking $log$.  Note that seasonal component in `AirPassengers` time series depicted in \@ref(fig:tsex) (a) is multiplicative. See Figure \@ref(fig:logts).

```{r logts, echo=FALSE, warning=FALSE, message=FALSE, fig.show='hold', fig.align='center', fig.cap="Transforming Time Series", out.width="49%"}
library(ggplot2)
library(ggfortify)
autoplot(AirPassengers) +
  theme_bw() +
  labs(title = "AirPassengers Time series")
autoplot(log(AirPassengers)) +
  theme_bw() +
  labs(title = "Logirithmic transformation")
```

We will learn about decomposing time series components in next sections.

**Damped Trends:** Gardner and Mckenzie, in 1985 [@mcke] observed that most time series methods assume that any trend will continue unabated, regardless of the forecast lead time. They, based on empirical findings, suggested that forecast accuracy can be improved by either damping or ignoring altogether trends which have a low probability of persistence. We will learn about this concept, as well in next sections.

## Practical examples in R
Before analysing time series, in R let's see how time series objects are dealt in R.

### Pre-requisites {-}
Though most of the time we will be using base R, yet `forecast` is a fabulous package in R, developed by **Rob Hyndman**, which we will be using for predicting.  Predicting values help us understand how well we have captured the hidden trends and patterns in any time series. 

```{r message=FALSE, warning=FALSE}
library(forecast)
library(ggplot2)
```

### Creating a time series object in R

R provides us a simple function `ts()` to convert a series of observations (basically a vector) into a specific time series object.  The syntax is -

```
ts(data, 
  start = 1, 
  end = numeric(length(data)), 
  frequency = 1, 
  ...)
```
where:

- `data`: a numeric vector or matrix containing the data for the time series
- `start`: the start time of the time series, represented as either a numeric or a Date or POSIXct object
- `end`: the end time of the time series, represented as either a numeric or a Date or POSIXct object
- `frequency`: the number of observations per unit time for the time series. For example, if the data is recorded monthly, the frequency would be 12
- `...`: additional arguments that can be passed to the function, such as `names`, `delim`, or `tsp` (time series start and end points)

Example-1: Let's create a time series.  

```{r}
# Create a numeric vector representing monthly sales data for a year
sales <- c(10, 20, 30, 25, 35, 40, 45, 50, 55, 60, 65, 70)

# Create a time series object with monthly frequency starting from January
sales_ts <- ts(sales, start = c(2022, 4), frequency = 12)

# Print the time series object
sales_ts

```

We can check its class.
```{r}
class(sales_ts)
```

Let's also check `Nile`, which is already available in base R.
```{r}
Nile
```

### Plotting Time Series Objects
To plot `ts` object we can simply use `plot()` command or alternatively we can use `ggplot2` as well.

```{r tsex2, fig.cap="Example of time series plotting in R; Base R (Left) and ggplot2 (Right)", fig.align='center', fig.show='hold', fig.align='center', out.width="48%", warning=FALSE, message=FALSE}
# Plotting in base R
plot(sales_ts, main = "Sales during FY 2022-23\nDummy Data by author")

forecast::autoplot(AirPassengers) +
  scale_y_log10('Logrithmic values') +
  ggtitle('AirPassengers converted to additive') +
  theme_bw()
```

*Note: Library `forecast` in R also provides us a function `autoplot()` which plots a time series object, similar to plot(), but the plot returned here is a `ggplot2` object which can be modified/fine-tuned using `ggplot2` functions.*

## Time series modelling and forecasting
As we have already discussed, forecasting future plays an important part of time series analysis; an important prerequisite is to model our data.  In fact, the forecasting/modelling techniques can be broadly classified into two categories, data based techniques and model-based techniques.

### Data-based techniques {-}
Data-based techniques, also known as statistical or empirical techniques, focus on analyzing the patterns and characteristics of the observed time series data directly. These techniques do not explicitly assume a specific underlying model structure. Instead, they rely on statistical properties and patterns present in the data. Examples of data-based techniques include:

### Naive method
This method assumes that the future value will be the same as the last observed value. This is the simplest and most basic forecasting method and hence named *naive*. The formula for the naive method is:

$$
\text{Naive method: }\hat{Y}_{T+1} = Y_T
$$

Here, $\hat{Y}_{T+1}$ is the forecast value for the next time period and $Y_T$ is the last observed value i.e. for a series of $T$ observations.  Thus, predicting future values using naive method require no special skill. 

**Problem Statement-1:** Let's predict say 5, future values of Nile flow data.  In `forecast` there is a function `naive` which will do the job, once we give the value of parameter `h` which is short for forecasting **h**orizon. In Figure \@ref(fig:naive), we can see forecast values, in red, which are exactly same as previous/last value.

```{r naive, fig.show='hold', fig.align='center', fig.cap="Naive Forecasting using R", out.width="98%", fig.cap="Naive forecasting"}

forecast::naive(Nile, h = 5) %>% 
  autoplot() +
  theme_bw() +
  labs(title = "Nile flow - forecast by Naive method",
       x = "Year")


```

If we aren't interested in visualising confidence values (by default 80% and 95% confidence values are shown in bands), we can tune parameter `level` accordingly.

### Moving averages
In this technique, the forecast value is computed as the average of the most recent observations within a sliding window of fixed length, say $n$..  It is a technique to smooth out a time series by averaging neighboring values. It helps in reducing noise and revealing underlying trends. 

The formula for the moving average method is:

$$
\hat{Y}_{t+1} = \frac{1}{n}(Y_t + Y_{t-1} + ... + Y_{t-n+1})
$$

Here, $\hat{Y}_{t+1}$ is the forecast value for the next time period and $Y_t$, $Y_{t-1}$, ..., $Y_{t-n+1}$ are the past $n$ observations. 

Smoothing through moving average can either be **centre-weighted** or **tailed-weighted**.  In former, $k$ i.e. `order` of the moving average, should be an odd number because each data point is replaced with the mean of that observation and $(k-1)/2$ observations before and $(k-1)/2$ observations after it.  Smoothed time series `Nile` for different `k` can be seen in Figure \@ref(fig:ma).

```{r ma, echo=FALSE,fig.align='center', fig.cap="Simple Moving Averages", fig.align='center', fig.show='hold'}
library(patchwork)
lims <- c(min(AirPassengers), max(AirPassengers))

p1 <- autoplot(AirPassengers) +
  scale_y_continuous(limits = lims)+
  labs(title = 'AirPassengers - original') +
  theme_bw()

p2 <- autoplot(ma(AirPassengers,7)) +
  scale_y_continuous(limits = lims)+
  labs(title = 'Moving average k=7') +
  theme_bw()

p3 <- autoplot(ma(AirPassengers,15, centre = FALSE)) +
  scale_y_continuous(limits = lims)+
  labs(title = 'Moving average k=15, tailed') +
  theme_bw()

p4 <- autoplot(ma(AirPassengers,15)) +
  scale_y_continuous(limits = lims)+
  labs(title = 'Moving average k=15') +
  theme_bw()

(p1 + p2)/(p4 + p3)
```

**Problem Statement-2:** Let's try to visualise moving average trend in Nile data.  We will use `ma` function from `forecast` for this.  In left side Figure of \@ref(fig:ma1) we have a smoother (with k = 7) time series and 10 forecast  values using last value of moving average.  

We can also use `rollmean` function from `zoo` library, which is another package to analyse time series objects. Refer right-side figure in \@ref(fig:ma1).  In this example, we have used `wineind` data/time series which shows *Australian total wine sales by wine makers in bottles <= 1 litre. Jan 1980 – Aug 1994.*  One of the advantage using this method, is that we can see both original and smoothed time series. 

```{r ma1, warning=FALSE, message=FALSE, echo=FALSE, fig.align='center', fig.cap="Simple Moving averages", fig.show='hold', out.width="48%"}
# USING FORECAST
ma_nile <- ma(Nile, order = 7, centre = FALSE)
future_nile <- forecast(ma_nile, h = 10)
plot(future_nile, main = "Forecasting\nNile Flow")

# Using zoo
library(zoo)
naive(zoo::rollmean(wineind, k = 9, align = "right")) |> 
  plot(ylim=c(min(wineind), max(wineind)), 
       xlim=c(1980, 1995),
       col="red", 
       main = "Forecasting\nAustralian Wine sales")

lines(wineind)
```

### Exponential smoothing 
Exponential smoothing techniques update forecasts based on weighted averages of past observations, giving more weight to recent observations. Examples include simple exponential smoothing, Holt's method, and Holt-Winters' method.

As the name suggests simple exponential smoothing or SES is simplest of these.  **Simple exponential smoothing (SES)** is actually a moving weighted average where recent observations are given more weights while calculating averages.  *This method is suitable for forecasting data with no clear trend or seasonal pattern.* 

$$
\hat{y}_{T+1} = \alpha{y_T} + \alpha(1- \alpha){y_{T-1}} + \alpha(1 - \alpha)^2{y_{T-2}} + ...
$$

The value of $\alpha$, the smoothing parameter, in above equation should follow $0 \le \alpha \le 1$.  Clearly different values of $\alpha$ will give different smoothing and we will have to adopt most suitable one, one such example with $\alpha = 0.1$ is shown in the Figure \@ref(fig:ses1) (Left). 

Holt extended SES by introducing trend component and thus a new smoothing parameter for trend $\beta$. The method is also named after him as **Holt's Linear Method** of exponential smoothing. Now there are are two equations, mathematically.

$$
\begin{aligned}
\hat{y}_{t+h|t} &= l_t + hb_t \\
l_t &= \alpha y_t + (1-\alpha)(l_{t-1} + b_{t-1}) \\
b_t &= \beta(l_t - l_{t-1}) + (1-\beta)b_{t-1}
\end{aligned}
$$

$\alpha$ and $\beta$ (again $0 \le \beta \le 1$) in above equations are smoothing parameters for level and trend components, respectively; and $h$ is forecast horizon. Also, $y_t$ is the actual value, $l_t$ is the level (or intercept), $b_t$ is the trend (or slope) of the time series at time $t$.  Refer \@ref(fig:ses1) (Right).

To apply *SES*, in R, we can use function `ses` and for applying *Holt's* smoothing, we can use `holt`; both from `forecast` library.  Example usages are as follows.

```{r ses1, fig.cap="Simple Exponential Smoothing (Left) Vs. Holt's Linear Method (Right)", fig.align='center', fig.show='hold', out.width="49%"}

lynxses <- ses(lynx, h= 10, alpha = 0.1)
holtfit <- holt(airmiles, h = 10, alpha = 0.3, beta = 0.1)

autoplot(lynxses) +
  autolayer(lynxses$fitted) +
  ggplot2::ggtitle('SES with alpha = 0.1 : Lynx data') +
  ggplot2::theme_bw() +
  ggplot2::theme(legend.position = "bottom") +
  ggplot2::labs(color = "")

autoplot(holtfit) +
  autolayer(holtfit$fitted) +
  ggplot2::ggtitle("Holt's Linear Trend method with beta = 0.1 : AirMiles data") +
  ggplot2::theme_bw() +
  ggplot2::theme(legend.position = "bottom") +
  ggplot2::labs(color = "")
```

As we have discussed earlier, damped trends work better many times.  So based on Mckenzie and gardner work, an additional dampening paramater $\phi$ was introduced.  The equations were modified as -
$$
\begin{aligned}
\hat{y}_{t+h|t} &= l_t + (\phi + \phi^2 + ... + \phi^h)b_t \\
l_t &= \alpha y_t + (1-\alpha)(l_{t-1} + \phi b_{t-1}) \\
b_t &= \beta(l_t - l_{t-1}) + (1-\beta)\phi b_{t-1}
\end{aligned}
$$

Clearly, if $\phi = 1$, the above equations are equivalent to Holt's Linear Method. $\phi$ for values $0 < \phi < 1$, dampens the trend, as shown in figure \@ref(fig:hl2) (Left). A comparison of both can be seen in the Figure \@ref(fig:hl2) (Right).

To extend holt's linear method with *damped trends* in R, we can set parameter `damped = TRUE` in `holt` function discussed above.  Example code can be seen below.

```{r hl2, fig.cap="Linear methos Vs. Damped trends", fig.align='center', fig.show='hold', out.width="49%"}
library(ggplot2)

holtfit <- holt(airmiles, h = 10, alpha = 0.3, beta = 0.1, damped = TRUE, phi = 0.8)
autoplot(holtfit) +
  autolayer(holtfit$fitted) +
  ggplot2::ggtitle('Damped Trend Forecast with phi=0.8') +
  ggplot2::theme_bw() +
  ggplot2::theme(legend.position = "bottom") +
  ggplot2::labs(color = "")


holtfit <- holt(airmiles,
                h = 10,
                alpha = 0.3,
                beta = 0.1)

holtfit2 <- holt(
  airmiles,
  h = 10,
  alpha = 0.3,
  beta = 0.1,
  damped = TRUE,
  phi = 0.8
)

autoplot(airmiles)+
  autolayer(holtfit, series="Holt's method", PI = FALSE) +
  autolayer(holtfit2, series="Holt's method with damped trend", PI = FALSE)+
  ggplot2::ggtitle("Holt's method") + xlab("Year") +
  ylab("Revenue passenger miles") +
  guides(colour=guide_legend(title="Method")) +
  ggplot2::theme_bw() +
  theme(legend.position = "bottom")
```

Now to incorporate `seasonality` (with say $m$ periods) along with trend and level, Holt and Winters suggested to incorporate additional parameter $\gamma$ in above equations.  

$$
\text{Holt-Winters method:}
$$
$$
\begin{aligned}
\hat{y}_{t+h|t} &= l_t + hb_t + s_{t+m-(m-1 \bmod m)} \\
l_t &= \alpha(y_t - s_{t-m}) + (1-\alpha)(l_{t-1} + b_{t-1}) \\
b_t &= \beta(l_t - l_{t-1}) + (1-\beta)b_{t-1} \\
s_t &= \gamma(y_t - l_{t-1} - b_{t-1}) + (1-\gamma)s_{t-m}
\end{aligned}
$$

An example can be seen in figure \@ref(fig:hw1).  To apply *Holt-Winters* method in R, we can use `hw` function with desired values of parameters.  An example code may be as follows, where we have tried to extend `AirPassengers` time series with this method with both `additive` and `multiplicative` trends.

```{r hw1, fig.cap="Holt-Winter's Method of Forecasting", fig.align='center', fig.show='hold', out.width="80%"}
hw1 <- hw(AirPassengers,seasonal="additive")
hw2 <- hw(AirPassengers, seasonal="multiplicative")
autoplot(AirPassengers) +
  autolayer(hw1, series="HW additive forecasts", PI=FALSE) +
  autolayer(hw2, series="HW multiplicative forecasts",
            PI=FALSE) +
  xlab("Year") +
  ylab("Air Passengers") +
  ggtitle("Holt-Winters model") +
  guides(colour=guide_legend(title="Forecast")) +
  ggplot2::theme_bw() +
  theme(legend.position = "bottom") 
```

### Seasonal decomposition methods {-}

Seasonal decomposition techniques decompose a time series into its different components, such as trend, seasonality, and noise. This allows for a better understanding of the individual components and their impact on the overall series.  There are several methods to decompose a time series into its different components.  We will however, discuss two of these.

### Classical seasonal decomposition
First of these methods is **classical decomposition method** which decomposes a time series into its trend, seasonal, and residual components. It assumes that the seasonal component repeats identically from year to year, and the trend component changes linearly over time. The steps involved in the classical decomposition method are as follows:

- **Trend Component:** The trend component is estimated using a moving average or regression method.
- **Seasonal Component:** The seasonal component is estimated by averaging the values across the same seasonal periods in different years.
- **Residual Component:** The residual component is obtained by subtracting the trend and seasonal components from the original time series.

**Problem Statement:** Let's try to decompose a time series say `AirPassengers` to see its components.  To decompose a time series in R, we will use function `decompose` as shown in the following code.  Decomposing time series in R gives us four different plots each for (i) Observed i.e. original values, (ii) trend, (iii) seasonality and (iv) random noise available. 

Case-1: Additive decomposition

```{r}
decomposed_air_passengers <- decompose(AirPassengers)
summary(decomposed_air_passengers)
```

Case-2: Multiplicative decomposition.  In this case we can use argument `type`.

```{r}
decomposed_air_passengers2 <- decompose(AirPassengers, type = "multiplicative")
summary(decomposed_air_passengers2)
```

A plot of decomposed `AirPassengers`, using classical approach is shown in Figure \@ref(fig:stl1).

```{r stl1, fig.cap="Seasonal Decomposition Techniques", fig.align='center', fig.show='hold'}
AirPassengers |>
  decompose() |>
  plot()
```

### Seasonal and Trend decomposition using Loess^[Loess is a method for estimating nonlinear relationships]. 

STL (Seasonal and Trend decomposition using Loess) is a robust and flexible method for decomposing a time series into its trend, seasonal, and residual components. It uses local regression (Loess) to estimate the trend and seasonal components. The STL algorithm is as follows:

- Seasonal Component: The seasonal component is estimated using Loess, which fits a smooth curve to the seasonal patterns.
- Trend Component: The trend component is estimated by removing the estimated seasonal component from the original time series.
- Residual Component: The residual component is obtained by subtracting the estimated trend and seasonal components from the original time series.

To decompose time series in R, using STL, we will use function `stl` as shown below.  A plot of STL decomposed `AirPassengers` is shown in figure \@ref(fig:stl2).

```{r stl2, fig.cap="STL Decomposition", fig.align='center', fig.show='hold'}
stl(AirPassengers, s.window = "periodic") %>% 
  plot()
```

### Model-based techniques{-}

Model-based techniques involve fitting a specific mathematical or statistical model to the observed time series data. These techniques assume a particular structure for the data and estimate model parameters based on that. Model-based techniques typically require more assumptions but can provide a more detailed understanding of the underlying dynamics. Examples of model-based techniques include:

1. **Autoregressive Integrated Moving Average (ARIMA)**: ARIMA models capture the linear dependencies between lagged observations and differences of the time series. They are commonly used for modeling stationary time series.

2. **Seasonal ARIMA (SARIMA)**: SARIMA models extend the ARIMA framework to incorporate seasonality in the data. They are suitable for time series exhibiting both trend and seasonality.

In future version of the book, we will discuss these methods.

## Plotting different elements of time series
In R, we can use functions such as -

- `ggseasonplot()`: to create a seasonal plot
- `ggsubseriesplot()`: to create mini plots for each season and show seasonal means
- `gglagplot()`: Plot the time series against lags of itself
- `ggAcf()`: Plot the autocorrelation function (ACF)

Example-1:
```{r, fig.show='hold', fig.align='center', fig.cap="Seasonalilty"}
theme_set(theme(axis.text.x = element_text(angle = 90, 
                                           vjust = 0.5, 
                                           hjust=1)))
library(patchwork)
g1 <- ggseasonplot(AirPassengers) +
  theme_bw()
g2 <- ggsubseriesplot(AirPassengers) +
  ggtitle('Sub-Series Plot') +
  theme_bw()
(g1 / g2)
```

Example-2: Lag Plot of `AirPassengers` may be seen at Figure \@ref(fig:lagp).  In the figure notice that plot at lag=12 suggest that series has seasonality with 12 periods.
```{r, lagp, fig.show='hold', fig.align='center', fig.cap="Seasonalilty through Lagplots"}
gglagplot(AirPassengers) +
  ggtitle('Lag Plots') +
  theme_bw() +
  theme(legend.position = "bottom") +
  labs(color = NULL) +
  guides(color = guide_legend(nrow = 1))

```

------------------------------



<!--chapter:end:12-3ts.Rmd-->

# Part IV: Techniques for Audit/Fraud detection {-}

# Data Cleaning in R
Data cleansing is one of the important steps in data analysis. Multiple packages are available in r to clean the data sets.  One of such packages is `janitor` which we will be using in this chapter along with few other packages.  

Let's load it
```{r}
library(janitor)
```

## Cleaning Column names.
We know that names of objects in R follow certain conventions like we may not have certain special characters in names.  If a space has been used that is to be quoted under a pair of backticks \`.  But generally when we read data from files in excel, we can have some 'dirty' names, which we should clean before proceeding.  In such `clean_names()` come handy.  E.g.
```{r}
# Create a data.frame with dirty names
test_df <- as.data.frame(matrix(ncol = 6))

names(test_df) <- c("firstName", "ábc@!*", "% successful (2009)",
                    "REPEAT VALUE", "REPEAT VALUE", "")
# View this data
test_df
```

Using `clean_names()` which is also pipe friendly, we can clean names in one step.  (Results will be in snake case)
```{r}
test_df %>% 
  clean_names()
```

It -

+ Parses letter cases and separators to a consistent format.
+ Default is to `snake_case`, but other cases like `camelCase` are available
+ Handles special characters and spaces, including transliterating characters like `œ` to `oe`.
+ Appends numbers to duplicated names
+ Converts `“%”` to “percent” and `“#”` to `“number”` to retain meaning
+ Spacing (or lack thereof) around numbers is preserved

## Handling duplicate records
In `janitor` package, we have a ready to use function `get_dupes()`. It allows us to find “similar” observations in a data set based on certain characteristics.  Syntax is pretty simple, and function is pipe friendly too.  Suppose we have to find out duplicate in `mtcars` dataset on each combination of `wt` and `cyl`.  
```{r}
mtcars %>% 
  get_dupes(wt, cyl)
```
We can see that it returns all duplicate records with an additional column `dupe_count` so that these duplicates can be analysed separately.

## Remove Constant (Redundant) columns
Dropping columns from a `data.frame` that contain only a single constant value throughout is again easy through `janitor::remove_constant()`.

## Remove empty rows and/or columns
While importing messy data from excel files, we may get some empty rows and/or columns.  Sorting out this issue, is easy using `janitor::remove_empty()`.

## Fix excel dates stored as serial numbers
While loading excel files in R, we may have sometimes noticed `41590` instead of having a `date format`.  Sorting out this issue is again easy in `janitor` as we have a function `excel_numeric_to_date()` for this. Example
```{r}
janitor::excel_numeric_to_date(41590)
```

## Convert a mix of date and datetime formats to date
Similar to above, we can also sort out, if we have a column mix of different date formats, using `janitor::convert_to_date()` or `janitor::convert_to_datetime()`.  See Examples-
```{r}
unsorted_dates <- c('2018-05-31', '41590', 41590)
janitor::convert_to_date(unsorted_dates)
```
**Note in above example, we have created a heterogeneous vector, but implicit coercion rules of R have converted all forms to character only.**

In real world examples, where data is entered through multiple machines/data points simultaneously, we may a column mix of date formats.  In that case, we may use `parse_date_time()` function in `lubridate` package.  To allow different formats we have use `order` agument in this function.  Example

```{r}
mixed_dates <- c("13-11-1991", "13-Sep-22", 
                 "20 August 2000", "15 August 87", 
                 "03/31/23", "12-31-2022")

lubridate::parse_date_time(mixed_dates,
                           orders = c("d m y", "d B Y", "m/d/y", "d B y"),
                           locale = "eng")
```



# Random sampling in R
International Standard On Auditing - 530 defines^[[https://www.ifac.org/system/files/downloads/a027-2010-iaasb-handbook-isa-530.pdf](https://www.ifac.org/system/files/downloads/a027-2010-iaasb-handbook-isa-530.pdf)] audit sampling as _the application of audit procedures to less than 100% of items within a population of audit relevance such that all sampling units have a chance of selection in order to provide the auditor with a reasonable basis on which to draw conclusions about the entire population._  Statistical sampling is further defines as _an approach to sampling having two characteristics - random selection of samples, and the use of probability theory to evaluate sample results, including measurement of sampling risk._

Appendix 4 of ISA 53 further prescribes different statistical methods of sample selection.  We will discuss here each type of sampling methodology used to sample records for audit.

### Prerequisites {-}

Load `tidyverse`
```{r message=FALSE}
library(tidyverse)
```

## Simple Random Sampling (With and without replacement)
In this method, records are selected completely at random, by generating random numbers e.g. using random number tables, etc. Refer figure \@ref(fig:simple) for illustration. We can replicate the method of random number generation in R.  Even the method of random number generation can be reproducible, by fixing the random number seed.  Mainly two functions will be used here `sample()`\index{sample() function} and `set.seed()`\index{set.seed() function} already discussed in section \@ref(prob). Since `sample()` function takes a vector as input and gives vector as output again, we can make use of `dplyr::slice_sample()` function, discussed in section \@ref(prob), which operates on data frames instead.


```{r simple, echo=FALSE, fig.cap="Illustration of Simple Random Sampling", fig.align='center', fig.show='hold', out.width="99%"}
knitr::include_graphics("images/simple.png")
```

Let's see this sampling on `iris` data.  Suppose we have to select a sample of `n=12` records, without replacement-
```
dat <- iris # input data
# set the seed
set.seed(123)
# sample n records
dat %>% 
  slice_sample(n = 12, replace = FALSE)
```
```{r echo=FALSE}
dat <- iris
set.seed(123)
knitr::kable(
  slice_sample(dat, n=12, replace = FALSE)
)
```

The syntax is simple.  In the first step we have fixed the random number seed for reproducibility. Using `slice_sample()` we have selected `n=12` records without replacement (`replace = FALSE`).  

> If sample size is based on some proportion, we have to use `prop = .10` (say 10%) instead of `n` argument. Moreover, if sampling is with replacement, we have to use `replace = TRUE`.

## Systematic random sampling {#srs}
ISA 530 defines this sampling approach as _'Systematic selection, in which the number of sampling units in the population is divided by the sample size to give a sampling interval, for example 50, and having determined a starting point within the first 50, each 50th sampling unit thereafter is selected. Although the starting point may be determined haphazardly, the sample is more likely to be truly random if it is determined by use of a computerized random number generator or random number tables. When using systematic selection, the auditor would need to determine that sampling units within the population are not structured in such a way that the sampling interval corresponds with a particular pattern in the population.'_ Refer figure \@ref(fig:systematic) for illustration.

```{r systematic, echo=FALSE, fig.cap="Illustration of Systematic Random Sampling", fig.align='center', fig.show='hold', out.width="99%"}
knitr::include_graphics("images/systematic.png")
```
We can replicate this approach again following two steps-

__Step-1:__ Select `n` as the sample size.  Then generate a maximum starting point say `s` by dividing number of rows in the data by `n`. Thereafter we have to choose a starting point from `1:s`.  We can use sample function here.  Let's say this starting number is `s1`.  Then we have to generate an arithmetic sequence, say `rand_seq` starting from `s1` and increasing every `s` steps thereafter with total `n` terms.

__Step-2:__ In the next step we will shuffle the data by using `slice_sample` and select a sample using function `filter`.

The methodology is replicated as 
```
set.seed(123)
n <- 15 # sample size
s <- floor(nrow(dat)/n)
s1 <- sample(1:s, 1, replace = FALSE)
rand_seq <- seq(s1, by = s, length.out = n)
dat %>% 
  slice_sample(prop = 1) %>% 
  filter(row_number() %in% rand_seq)
  
```
```{r echo=FALSE}
set.seed(123)
n <- 15 # sample size
s <- floor(nrow(dat)/n)
s1 <- sample(1:s, 1, replace = FALSE)
rand_seq <- seq(s1, by = s, length.out = n)
knitr::kable(
  dat %>% 
  slice_sample(prop = 1) %>% 
  filter(row_number() %in% rand_seq)
)
```


## Probability Proportionate to size (with or without replacement) a.k.a monetary unit sampling

This sampling approach is defined in ISA-530 as _"a type of value-weighted selection in which sample size, selection and evaluation results in a conclusion in monetary amounts."_ 

Our methodology is not much difference from methodology adopted in section \@ref(srs) except that we will make use of `weight_by = ` argument now.

Let's use `state.x77` data that comes with base R.  Since the data is in matrix format, let's first convert it data frame using `as.data.frame()` first.  
```{r}
dat <- as.data.frame(state.x77)
```
Other steps are simple.  
```
set.seed(123)
dat %>% 
  slice_sample(n=12, weight_by = Population)
```
```{r echo=FALSE}
set.seed(123)
knitr::kable(
  dat %>% 
    slice_sample(n=12, weight_by = Population)
)
```

## Stratified random sampling 
Stratification is defined in ISA-530 as _the process of dividing a population into sub-populations, each of which is a group of sampling units which have similar characteristics (often monetary value)._ Thus, stratified random sampling may imply any of the afore-mentioned sampling techniques applied to individual strata instead of whole population.  Refer figure \@ref(fig:strata) for illustration.

```{r strata, echo=FALSE, fig.cap="Illustration of Stratified Random Sampling", fig.align='center', fig.show='hold', out.width="99%"}
knitr::include_graphics("images/stratified.png")
```

The function `dplyr::group_by()` will be used here for stratification.  Thereafter we can proceed for sampling described as above.

Example Data: - Let's include region in `state.x77` data using `dplyr::bind_cols`.
```{r}
dat <- bind_cols(
  as.data.frame(state.x77),
  as.data.frame(state.region)
)
```
Let's see first 6 rows of this data
```{r echo=FALSE}
knitr::kable(
  dat %>% head()
)
```
We can check a summary of number of States per region
```{r}
dat %>% 
  tibble::rownames_to_column('State') %>% # this step will not be 
                                  # used in databases without row names
  group_by(state.region) %>% 
  summarise(states = n())
```

__Case-1:__ When the sample size is constant for all strata. Say `2` records per region.
```
set.seed(123)
n <- 2
dat %>% 
  tibble::rownames_to_column('State') %>% # this step will not be used in databases without row names
  group_by(state.region) %>% 
  slice_sample(n=n) %>% 
  ungroup()
```
```{r echo=FALSE}
set.seed(123)
n <- 2
knitr::kable(
  dat %>% 
  tibble::rownames_to_column('State') %>% 
  group_by(state.region) %>% 
  slice_sample(n=n) %>% 
    ungroup()
)
```

__Case-2:__ When the sample size or proportion is different among strata.
This time let us assume that column for _stratum_ is not directly available in the data.  
- Say, 20% of States having Population upto `1000`; 
- 30% of States having population greater than `1000` but upto `5000` and finally; 
- 50% of states having population more than `5000` have to be sampled.

In this scenario, our strategy would be use `purrr::map2_dfr()` function after splitting the data with `group_split()` function.  

Syntax would be
```
# define proportions
props <- c(0.2, 0.3, 0.5)

# set seed
set.seed(123)

# take data
dat %>% 
  # reduntant step where data has no column names
  tibble::rownames_to_column('State') %>%
  # create column according to stratums
  mutate(stratum = cut(Population, c(0, 1000, 5000, max(Population)),
                      labels = c("Low", "Mid", "High"))) %>% 
  # split data into groups
  group_split(stratum) %>% 
  # sample in each group
  map2_dfr(props,
           .f = function(d, w) slice_sample(d, prop = w))
```
We may check the sample selected across each stratum
```{r echo=FALSE}
props <- c(0.2, 0.3, 0.5)
set.seed(123)
dat %>% 
  # reduntant step where data has no column names
  tibble::rownames_to_column('State') %>%
  # create column according to stratums
  mutate(stratum = cut(Population, c(0, 1000, 5000, max(Population)),
                       labels = c("Low", "Mid", "High"))) %>% 
  group_by(stratum) %>% 
  mutate(count = n()) %>% 
  ungroup() %>% 
  # split data into groups
  group_split(stratum) %>% 
  # sample in each group
  map2_dfr(props,
           .f = function(d, w) slice_sample(d, prop = w)) -> dat2
dat2 %>% 
  group_by(stratum) %>% 
  summarise(Total = mean(count),
            Selected = n()) -> dat2

knitr::kable(dat2)
```

## Cluster sampling
ISA 530 does not explicitly define cluster sampling.  Actually this sampling is sampling of strata and we can apply above mentioned techniques easily to sample clusters.  E.g. in the sample data above, we can sample say, 2 clusters (or regions).  

Thus, our strategy would be first to sample groups from unique available values and thereafter filter all the records.
```{r}
# set the seed
set.seed(123)
# sample clusters
clusters <- sample(
  unique(dat$state.region),
  size = 2
)
# filter all records in above clusters
clust_samp <- dat %>% 
  filter(state.region %in% clusters)
# check number of records
clust_samp$state.region %>% table()
```





<!--chapter:end:13-randomsampling.Rmd-->

# Benford Tests/Analysis

## Introduction and Historical context

Benford's Law stands out as an analysis method for both visualizing and evaluating numerical data, especially when focused on detecting fraud. It's really handy for catching potential trickery, especially in spotting fraud. This rule tells us how often different digits (like 1, 2, 3, etc.) should show up as the first number in lots of real-world data. This law describes the frequency distribution of the first digit, from left hand side, in many real-life data sets which counter-intuitively is not uniform, and is shown in Figure \@ref(fig:benlaw). Significant differences from the anticipated occurrence rates could signal that the data is questionable and might have been altered. For instance, eligibility for government assistance often hinges on meeting specific criteria, like having an income below a certain level. As a result, data might be manipulated to meet these criteria. This kind of manipulation is precisely what Benford's Law can detect since fabricated numbers won't align with the expected frequency pattern outlined by the law.

The Law is named after physicist **Frank Benford**, who worked on the theory in 1938 and as a result a paper titled **The Law of Anomalous Numbers** was published.[@frank_ben]. However, its discovery is associated more than five decades earlier when astronomer **Simon Newcomb** observed that initial pages of log tables booklet were more worn out than later pages and published a two page article titled **Note on the Frequency of Use of the Different Digits in Natural Numbers** in 1881 [@newcomb].

More researchers continued to work on Benford's law and its extensions. However, it took several decades to find a truly practical application. It was in last decade of twentieth Century, when Dr. Mark J. Nigrini, an accounting Professor, used the law for fraud detection/anaytics and came up with a practical fraud application. He reviewed multiple data sources like sales figures, insurance claim costs, and expense reimbursement claims and did studies on detecting overstatements and understatement of financial figures. His research confirmed the law's proven usefulness to fraud examiners and auditors in accounting engagements.

His theory is that - *If somebody tries to falsify, say, their tax return then invariably they will have to invent some data. When trying to do this, the tendency is for people to use too many numbers starting with digits in the mid range, 5,6,7 and thus not enough numbers starting with 1.*

```{r benpics, fig.cap="(L to R) Frank Benford, Simon Newcomb, and Mark Nigrini (Source: Wiki)", echo=FALSE, fig.align='center', fig.show='hold', out.height="32%", out.width="32%", warning=FALSE, message=FALSE}
knitr::include_graphics(c("images/frank_benford.jpg", "images/Simon.jpg", "images/nigrini.jpg"))
library(knitr)
library(kableExtra)
```

## Benford's Law, properties and extensions

### Law of first digit

When considering the likelihood of any digit being in the first position (from the left), our initial assumption might be a simple *one out of nine* scenario, following a uniform distribution. However, this notion was challenged by Canadian-American astronomer **Simon Newcomb** in 1881, who noticed unusual wear patterns in logarithmic tables. While casually flipping through a logarithmic tables booklet, he discerned a curious pattern--- the initial pages exhibited more wear and tear than their later counterparts.

Subsequently, **Frank Benford** conducted a comprehensive analysis of 20 diverse datasets encompassing river sizes, chemical compound weights, population data, and more. His findings revealed a successive diminishment in probability from digit 1 to 9. In essence, the probability of digit 1 occurring in the initial position is the highest, while that of digit 9 is the lowest.

Mathematically, *Benford's Law* or *Law of first digits* states that the probability of any digit in first place should follow the equation \@ref(eq:ben1).

```{=tex}
\begin{equation} 
P(d_i) = \log_{10}\left(1 + \frac{1}{d}\right) 
(\#eq:ben1)
\end{equation}
```
-   Where $d_i$ ranges from $1$ to $9$.

The probabilities when plotted will generate plot as depicted in Figure \@ref(fig:benlaw).

```{r benlaw, echo=FALSE, message=FALSE, fig.cap="Diminishing Probabilities of First Digits - Benford Law", out.height="32%", message=FALSE, warning=FALSE, fig.align='center'}

library(tidyverse)
# Implement Benford's Law for first digit

benlaw <- function(d) log10(1 + 1 / d)
# Calculate expected frequency for d=5

# Create a dataframe of the 9 digits and their Benford's Law probabilities
df <- data.frame(digit = 1:9, probability = benlaw(1:9))
# Create barplot with expected frequencies
ggplot(df, aes(x = digit, y = probability)) + 
	geom_bar(stat = "identity", fill = "dodgerblue") + 
  theme_bw() +
	xlab("First digit") + 
  ylab("Expected frequency") + 
	scale_x_continuous(breaks = 1:9, labels = 1:9) + 
	ylim(0, 0.33) + 
  theme(text = element_text(size = 15)) +
  labs("Diminishing probabilities of first digits")

```

To test the proposed law, Benford analysed 20 different data-sets and he observed that nearly all follow the distribution mentioned in equation \@ref(eq:ben1).

Let us also try to see whether the law holds by anlaysing six different datasets, which are included in R's package called `benford.analysis`. Though we will discuss about the package in detail later in section \@ref(pracben). The six datasets are mentioned in Table \@ref(tab:sixben).

```{r sixben, echo=FALSE, message=FALSE, warning=FALSE}
library(benford.analysis)
data(package = "benford.analysis")$results[, 3:4] %>% 
  as.data.frame() %>% 
  bind_cols(Column = c('pop.2000', "pop.2009", "Amount", "perimeter.km", "value", "taxIncomes")) %>% 
  kable(caption = "List of six datasets for testing Benford Analysis", align = "c") %>% 
  kable_styling(full_width = TRUE) %>% 
  collapse_rows(valign = "middle")

```

The results of Benford's law of first digit on these six datasets are calculated and have been mentioned in Table \@ref(tab:tab1). It can be seen that actual frequencies of first digits, in these six datasets follow Benford's Law. We can even plot the actual frequencies to inspect results visually. Actual Frequencies in these six datasets are plotted in \@ref(fig:benplots) and it may be seen that these follow Benford's Law largely.

```{r tab1, echo=FALSE, message=FALSE, warning=FALSE}
data("census.2000_2010")
b1 <- benford.analysis::benford(census.2000_2010$pop.2000, number.of.digits = 1)

data(package = "benford.analysis")$results[, 3:4] %>% 
  as.data.frame() %>% 
  pull(Item) -> x

data(list = x)

y <- data(package = "benford.analysis")$results[, 3:4] %>% 
  as.data.frame() %>% 
  bind_cols(Colum = c('pop.2000', "pop.2009", "Amount", "perimeter.km", "value", "taxIncomes"))


ben_test <- function(data, col){
  data2 <- get(data)
  b <- benford.analysis::benford(data2[[col]], number.of.digits = 1)
  b$bfd[, "data.dist", drop = FALSE]
}

map2(y$Item, y$Colum, ben_test) %>% 
  list_cbind() %>% 
  set_names(gsub("\\.", " ", y$Item) %>% 
              str_to_title()) %>% 
  bind_cols(`Benford` = b1$bfd$benford.dist, .) %>% 
  bind_cols(digits = 1:9, .) %>% 
  kable(caption = "Results of First order tests on six datasets", booktabs = TRUE) %>% 
  kable_styling(full_width = TRUE) 
```

```{r benplots, echo=FALSE, warning=FALSE, message=FALSE, fig.align='center', fig.show='hold', fig.cap="Distribution of first digit frequencies in six datasets"}
map2(y$Item, y$Colum, ben_test) %>%
  list_cbind() %>%
  set_names(y$Item) %>%
  bind_cols(`Ben` = b1$bfd$benford.dist, .) %>%
  bind_cols(digits = 1:9, .) -> w

for(i in 1:6){
  p <- ggplot(w) +
    geom_col(aes(x = digits, y = get(y$Item[i]))) +
    geom_line(aes(x = digits, y = Ben), linetype = "dashed", size = 1.5, color = "red") +
    theme_bw() +
    scale_x_continuous(breaks = 1:9) +
    labs(title = y$Item[i],x = "", y = "")
  
  assign(paste0("p", i), p)
}

library(patchwork)

(p1 + p2 + p3)/(p4 + p5 + p6)
```

### Scale Invariance

Later in 1961, **Roger Pinkham** showed that law is invariant to scaling [@pinkham]. By *Scale Invariance*, he actually showed that the law is invariant to measurements units. In other words, the law still holds if we convert units from one unit to another. For example, if price or amount figures are measured either in USD or in INR, length is measured either in KMs or Miles, the digit frequencies still follow the Benford's Law.

Let us check this on one of the six datasets mentioned above, namely `census.2009`. This data-set contains the figures of population of towns and cities of the United States, as of July of 2009. We can see that first digit frequencies follow Benford's Law/Pinkham's Corollary in Figure \@ref(fig:census). Left plot shows frequencies on original data whereas right plot shows these on randonly scaled data.

```{r census, echo=FALSE, fig.cap="First Digit Analysis on US Census 2009 data (Left) and Scaled Data (Right)", fig.align='center', fig.show='hold', out.width="45%"}
# Load package benford.analysis
library(benford.analysis)
data(census.2009)
# Check conformity
bfd.cen <- benford(census.2009$pop.2009, 
                   number.of.digits = 1) 
# Left Plot
plot(bfd.cen, except = c("second order", 
                         "summation", 
                         "mantissa", 
                         "chi squared",
                         "abs diff", 
                         "ex summation", 
                         "Legend"), 
     multiple = F, main = "Digit ditribution in Original Data") 

# Scale invariance
# Right Plot
set.seed(123)
data <- census.2009$pop.2009 * abs(rnorm(1)) * 3
bfd.cen3 <- benford(data, number.of.digits=1)
plot(bfd.cen3, except = c("second order", 
                          "summation", 
                          "mantissa", 
                          "chi squared",
                          "abs diff", 
                          "ex summation", 
                          "Legend"), 
     multiple = F, main = "Digit ditribution in Scaled Data")
```

Figure \@ref(fig:census) (Left) shows that the law holds for the data. Let us also test the Pinkham's corollary on the aforesaid data. For this let's multiply all the figures of population by a random positive number. Through figure \@ref(fig:census) (Right) it is clear that law still holds after scaling.

### First two digits

Nigrini's contributions gained widespread recognition among scholars and practitioners, highlighting the applicability of Benford's Law as a valuable forensic accounting and auditing tool across various datasets, particularly in the financial domain. **Theodore P. Hill** further [@t_hill] extended the scope of the law, demonstrating its validity beyond just the first digit to encompass other digits as well. Hill's work expanded the utility of Benford's Law, affirming its effectiveness in detecting irregularities and patterns not only in leading digits but throughout numerical sequences.

The formula for second significant digit can be written down in equation \@ref(eq:ben2).

```{=tex}
\begin{equation} 
P(d_i) = \sum_{k = 1}^{9}\log_{10}\left(1 + \frac{1}{10k + d_i}\right)\;;\; d = 0,1,..9
(\#eq:ben2)
\end{equation}
```
-   where $k$ represents first digit,
-   $d_i$ represents second digit.

The probabilities have been calculated, as depicted in Table \@ref(tab:tab3). Each cell depicts the probability of occurrence of any two digit, in left side, by first digit in rows and second digit in columns. We may also verify that, row totals thereby depicting probability of occurrence of first digit corresponds Benford's Law of First Digit. For example, the probability of having first two digits as 10 will be highest at 4.14%.

```{r tab3, echo=FALSE, message=FALSE, warning=FALSE}
# library(kableExtra)
library(janitor)
b2 <- benford.analysis::benford(census.2000_2010$pop.2000, number.of.digits = 2)

b2$bfd$benford.dist %>%
  matrix(nrow = 9, byrow = TRUE) %>% 
  as.data.frame() %>% 
  rownames_to_column(var = "First Digit") %>% 
  adorn_totals(name = "Second Digit Freq") %>% 
  adorn_totals(where = "col", name = "First Digit Freq") %>% 
  as.data.frame() %>% 
  mutate(across(-1, ~scales::percent(., accuracy = 0.01))) %>% 
  set_names(c("First Digit", 0:9, "First Digit Freq")) %>% 
  kable(caption = "First and Second Digit distributions", booktabs = TRUE, align = "r") %>% 
  kable_styling(full_width = TRUE) %>% 
  #column_spec(12, bold = TRUE) %>% 
  #row_spec(10, bold = T) %>% 
  add_header_above(c(" " = 1, "Second Significant Digit" = 10, " " = 1)) %>% 
  collapse_rows(valign = "middle")
```

The law of second digit combined with original Benford's Law of first digit thus, gives us Law of first two digits. We can verify it in the example on `census.2009` data. The resultant plot as depicted in figure \@ref(fig:ben2) shows us that the law of first two digits also holds.

```{r ben2, echo = FALSE, fig.cap="Law holds for first two digits as well", fig.align='center', fig.show='hold'}
bfd.cen2 <- benford(census.2009$pop.2009, number.of.digits = 2) 
plot(bfd.cen2, except = c("second order", 
                          "summation", 
                          "mantissa", 
                          "chi squared",
                          "abs diff", 
                          "ex summation", 
                          "Legend"), 
     multiple = F) 
```

### Second order test

**Nigrini** and **Miller**, in 2009 [@article2009], introduced another advanced test based on Benford's Law. The test states that:

> Let $x_1$, ..., $x_N$ be a data set comprising $N$ observations, and let $y_1$, ..., $y_N$ be the observations $x_i$'s in ascending order. Then, for many natural data sets, and for large $N$, the digits of the differences between adjacent observations $y_{i+1} – y_i$ is close to Benford's Law. Large deviations from Benford's Law indicate an anomaly that should be investigated.

So, the steps may be listed as

-   Sort data from smallest to largest
-   calculate $N-1$ differences of $N$ consecutive observations
-   Apply Benford's law on these calculated new data.

Nigrini showed that these digits are expected to closely follow the frequencies of Benford law. Using four different datasets he showed that this test can detect (i) anomalies occurring in data, (ii) whether the data has been rounded and (iii) use of fake data OR 'statistically generated data' in place of actual (transactional) data.

### Summation Test

The **summation test**, another second order test, looks for excessively large numbers in a dataset. It identifies numbers that are large compared to the norm for that data. The test was also proposed by **Nigrini** [@nigrinifraud] and it is based on the fact that the sums of all numbers in a Benford distribution with first-two digits (10, 11, 12, ...99) should be the same. Therefore, for each of the 90 first-two digits groups sum proportions should be equal, i.e. 1/90 or 0.011. The spikes, if any indicate that there are some large single numbers or set of numbers.

In the next section, we will see how to implement all these tests through R.

### Limitations of Benford Tests

Benford's Law may not hold in the following circumstances-

1.  When the data-set is comprised of assigned numbers. Like cheque numbers, invoices numbers, telephone numbers, pincodes, etc.
2.  Numbers that may be influenced viz. ATM withdrawals, etc.
3.  Where amounts have either lower bound, or upper bounds or both. E.g. passengers onboard airplane, hourly wage rate, etc.
4.  Count of transactions less than 500.

Before carrying out analyics let us also see the evaluation metrics which will help us to evaluate the goodness of fit of data to Benford's law. Three statistics are commonly used.

## Goodness of fit metrics

In table \@ref(tab:tab1) we saw that digit frequencies largely followed Benford's Law in six different datasets. However, as to evaluate how close is the actual distribution with theoretical distribution, we need to evaluate the fit on some metrics. Here we will use three different metrics as follows.

### Chi-square statistic {#chis}

In first of these test, we will use Chi Square Statistic. This statistic is used to test the statistical significance to the whole distribution in observed frequency of first digit and first two digits against their expected frequency under Benford's Law (BL). The **Null hypothesis states that digits follow Benford's Law.** Mathematical formula is,

```{=tex}
\begin{equation} 
\chi^2 = \sum_{i=1}^{9} \frac{(O_i - E_i)^2}{E_i}
(\#eq:ben3)
\end{equation}
```
where -

-   $O_i$ is the observed frequency of the i-th digit.
-   $E_i$ is the expected frequency of the i-th digit predicted by Benford's Law.

This calculated chi-square statistic is compared to a critical value. The critical value for Chi-Square Test, comes from a chi-square distribution available easily in any Statistical textbook^[[https://www.itl.nist.gov/div898/handbook/eda/section3/eda3674.htm](<https://www.itl.nist.gov/div898/handbook/eda/section3/eda3674.htm>)]. However, for first digit test and first two digits test, the critical values are reproduced in Table \@ref(tab:tab6).

```{r tab6, echo=FALSE, message=FALSE, warning=FALSE}
data.frame(
  `Degrees of Freedom` = c("10%", "5%", "2.5%","1%", "0.1%"),
  `8` = c( 13.362, 15.507, 17.535, 20.090, 26.125),
  `89` = c(106.469, 112.022, 116.989, 122.942, 135.978)
) %>% 
  set_names(c("Degrees of Freedom", "8", "89")) %>% 
  kable(caption = "Critical values for Chi-Square Test", booktabs = TRUE) %>% 
  add_header_above(c(" " = 1, "First Digit Test" = 1, "Two Digit Test" = 1))
```

To check goodness of fit, we have to compare calculated $χ^2$ statistic with these critical values. If the observed value is above these critical values we may conclude that our initial hypothesis that data follows BL, should be rejected. Or simply that data does not conforms Benford law/Distribution.

For example, in `census.2009` data the chi-square statistic calculates to `17.524` which is less than 2.5% critical value 17.535. Thus, we can say with 5% confidence that `census.2009` data follows BL (first digit law).

### Z-score

Z-statistic checks whether the individual distribution significantly differs from Benford's Law distribution. Mathematically, Z-Statistics considers the absolute magnitude of the difference from actual to the expected, size of the data and expected proportion.

```{=tex}
\begin{equation} 
Z = \frac{(\lvert p - p_0\rvert) - (\frac{1}{2n})}{\sqrt{\frac{p_0(1-p_0)}{n}}}
(\#eq:ben4)
\end{equation} 
```

where -

-   $p$ is the observed frequency of the leading digits in the dataset.
-   $p_0$ is the expected frequency under Benford's Law.
-   $n$ is the number of records

In equation \@ref(eq:ben4), the last term in the numerator $\frac{1}{2N}$ is a continuity correction term and is used only when it is smaller than the first term in the numerator. Mark Nigrini has proposed that if the values of Z-statistic exceed the critical value 1.96, the null hypothesis $H_{0A}$ is rejected at 5% of significance level. Also note that **Null hypothesis is same, which states that digits follow Benford's Law.**

If the significant levels are 1% or 10%, the corresponding critical values are 2.57 and 1.64 respectively.

### Mean absolute deviation

Another Statistic, Mean Absolute Deviation also sometimes referred to as **M.A.D.**, measures absolute deviations of observed frequencies from theoritical ones. The mathematical formula is written in equation \@ref(eq:ben5).

\begin{equation} 
MAD = \frac{1}{9} \sum_{i=1}^{9} |O_i - E_i|
(\#eq:ben5)
\end{equation}

As there are no objective critical scores for the absolute deviations, the critical values prescribed by Mark J Nigrini are -

| First Digits   |                                  | First-Two Digits |                                  |
|-------------------|------------------|------------------|------------------|
| 0.000 to 0.006 | Close conformity                 | 0.000 to 0.012   | Close conformity                 |
| 0.006 to 0.012 | Acceptable conformity            | 0.012 to 0.018   | Acceptable conformity            |
| 0.012 to 0.015 | Marginally acceptable conformity | 0.018 to 0.022   | Marginally acceptable conformity |
| above 0.015    | Nonconformity                    | above 0.022      | Nonconformity                    |

: (#tab:tab7) Critical Scores for MAD test

### Other descriptive Statistics

If the data follows Benford's Law, the numbers should be close to those shown in table following, as suggested by Mark Nigrini.

|  Statistic   |       Value       |
|:------------:|:-----------------:|
|     Mean     |        0.5        |
|   Variance   | 1/12 (0.08333...) |
| Ex. Kurtosis |       -1.2        |
|   Skewness   |         0         |

: (#tab:benford) Ideal Statistics for data that follows Benford's Law

## Important

Benford's Law analysis serves as a powerful tool in uncovering potential irregularities in datasets, but it's crucial to note that deviations from this statistical phenomenon don't always signify fraudulent activities. While it highlights notable discrepancies between expected and observed frequencies of digits in naturally occurring datasets, these variations might stem from various legitimate factors such as data entry errors, fluctuations in processes, or different sources of data. Understanding that Benford's Law offers a signal rather than a definitive confirmation of fraud allows for a more nuanced interpretation, encouraging further investigation to discern the true nature behind these deviations.

Conversely, just because a dataset adheres to Benford's Law, it doesn't guarantee the absence of fraud. While conformity to this statistical principle generally suggests consistency within the data, sophisticated fraudsters might deliberately manipulate information to mimic expected distributions, masking their illicit activities. Therefore, while adherence to Benford's Law might lessen suspicion, it doesn't serve as an absolute assurance against fraudulent behavior.

Benford's Law acting as a warning signal indicates potential irregularities in the numbers. It's vital to dive deeper and investigate why these figures seem odd. Further scrutiny helps differentiate between a minor data hiccup and a potentially significant issue. This additional examination might mean cross-checking other data, validating records, or engaging with those connected to the information. This thorough approach is crucial for unraveling the story behind these uncommon figures.

## Practical approach in R {#pracben}

As already stated we will use package `benford.analysis` for carrying out analytics on Benford's Law, in R. Let us load it.

```{r eval=FALSE}
library(benford.analysis)
```

This package provides tools that make it easier to validate data using Benford's Law. This package has been developed by **Carlos Cinelli**. As the package author himself states that the main purpose of the package is to identify suspicious data that need further verification, it should always be kept in mind that these analytics only provide us red-flagged transactions that should be validated further.

Apart from useful functions in the package, this also loads some default datasets specially those which were used by Frank Benford while proposing his law. Let us load the census 2009 data containing the population of towns and cities of the United States, as of July of 2009.

```{r}
data("census.2009")
```

Let us view the top 6 rows of the data.

```{r}
head(census.2009)
```

In fact, this contains `r nrow(census.2009)` records.

**Problem Statement:** Let us test Benford's law on 2009 population data. Let us see whether the data conforms Benford's law.

The main function `benford()` takes a vector of values to be tested as input, and creates an output of special class `benford` The syntax is

```         
benford(data, number.of.digits=2)
```

where-

-   `data` is numeric vector on which analysis has to be performed.
-   `number.of.digits` is number of digits on which analysis has to be performed. Default value is `2`.

```{r}
census_first_digit <- benford(census.2009$pop.2009, number.of.digits = 1)
```

Above syntax will create `census_first_digit` object which store various useful information for Benford Analytics. We may view its summary -

```{r}
summary(census_first_digit)
```

Let us also print the object to see what all is stored therein.

```{r}
print(census_first_digit)
```

Results of Chi-Square distribution test, MAD etc. are printed apart from top deviations. The MAD value of `0.003` shows `close conformity` with Benford's law. Chi Square statistic at 17.524 is slightly greater than 5% critical value of 15.507. In second example we will see that results of `print` command on benford object can be further customised, using its other arguments.

Let us also visualise the plots. We will use `plot` command to generate the plots.

```{r cenbenplot, fig.cap="Benford Analysis Results of Census 2009 Data", fig.align='center', fig.show='hold'}
plot(census_first_digit)
```

We can see that by default five charts are printed.

1.  Digits distribution
2.  Second Order Test digit distribution
3.  Summation test - digit distribution
4.  Chi-Square differences
5.  Summation differences

*Similarly, in second example we will see how to customise plot outputs.*

We can see that first digits in census 2009 data, follows Benford's Law closely.

### Other Useful functions in package

You may be wondering whether we have to depend upon print function every time to get analytics insights out the object created. In fact there are several other functions in this package which are very useful while carrying out risk analysis through Benford's Law.

-   `chisq`: Gets the Chi-squared test of a Benford object. Takes a benford object as input.
-   `duplicatesTable` Shows the duplicates of the data. Similarly, takes a benford object as input.
-   `extract.digits` Extracts the leading digits from the data. Takes data as input. This is useful, while carrying out analysis manually.
-   `getBfd` Gets the the statistics of the first Digits of a benford object. E.g.

```{r}
getBfd(census_first_digit)
```

-   `getSuspects` Gets the 'suspicious' observations according to Benford's Law. Takes both data as well as benford object, as inputs. Example in second case study.
-   `MAD` Gets the MAD of a Benford object.
-   `suspectsTable` Shows the first digits ordered by the mains discrepancies from Benford's Law. Notice the difference from `getSuspects`

### Example-2: Corporate payments data

**Problem Statement-2:** Let us analyse red-flags, on dataset of the 2010's payments data (189470 records) of a division of a West Coast utility company. This data, `corporate.payments` is also available with the package. This time we will use first two digits in our analysis.

**Step-1:** Load the dataset and view its top rows. Let's also see its summary.

```{r}
data("corporate.payment")
head(corporate.payment)
summary(corporate.payment)
```

We can see it has `r nrow(corporate.payment)` records having

```         
+   Vendor Numbers
+   Date of Transaction
+   Invoice Number
+   Amount of invoice/transaction
```

**Step-2:** Create benford object

```{r}
corp_bfd <- benford(corporate.payment$Amount, number.of.digits = 2)
```

**Step-3:** Let us first visually inspect the results. This time we will use another argument of `plot` function in `benford.analysis` library which is `except`. Actually this can create seven different plots and by default it creates five plots as stated earlier. Thus, by writing `except = "none"` we can include all seven plots if we want. Otherwise we will have to mention exclusions from `c("digits", "second order", "summation", "mantissa", "chi squared", "abs diff", "ex summation")`. There is one more argument namely `multiple` which is TRUE by default and plots multiple charts in same window.

So let us build (i) Digit distribution and (ii) Second order digit distribution plots.

```{r corpben, fig.cap="Benford Analysis results on Corporate payments Data", fig.align='center', fig.show='hold', out.height="30%", out.width="47%"}
plot(
  corp_bfd,
  except = c(
    "summation",
    "mantissa",
    "chi squared",
    "abs diff",
    "ex summation",
    "chisq diff",
    "legend"
  ),
  multiple = TRUE
)
```

We can see that largely the data follows Benford's Law except an abnormal peak at 50.

**Step-4:** Let us now see what is inside of this object. Function `print` in `benford.analysis` package has another argument `how.many` which simply tells us to print how many of the absolute differences.

```{r}
print(corp_bfd, how.many = 7)
```

We can see that digit 50 has indeed the largest abolute difference. One of the reasons for availability of invoices in this digit group may be due to some tax capping or some other reason, which an auditor may need to investigate further.

Using `suspectsTable()` we can also get similar information.

```{r}
suspectsTable(corp_bfd) |> 
  head(7)
```

**Step-5:** Let us also get the Chi Square and other metrics

```{r}
chisq(corp_bfd)
```

Going strictly by numbers and p-value, which we should not depend upon in Benford Analytics, we see that Null hypothesis (Ref: section \@ref(chis)) has been rejected. In other words, chi-square statistic tells us that data does not follow Benford Law.

To get Mean Absolute Deviation

```{r}
MAD(corp_bfd)
```

Whether the value conforms to values suggested by Mark Nigrini, we can do

```{r}
corp_bfd$MAD.conformity
```

**Step-6:** Let us generate duplicate values avilable if any, in the data. For sake of brevity here, we will print top-5 results.

```{r}
duplicatesTable(corp_bfd) |> 
  head(5)
```

Examining output above, we can see that there are 6022 invoices having all amount of USD50 each. Probably this could be the reason for failing of null hypothesis in the data.

**Step-7:** We can extract all distribution data using `getBFD` function.

```{r}
getBfd(corp_bfd) |> 
  head(10)
```

**Step-8:** To get suspected/high risk records, we may make use of `getSuspects` function. As already stated it requires both benford object and data as inputs.

```{r}
# We are printing 10 records only
getSuspects(corp_bfd, corporate.payment) |> 
  head(10)
```

Moreover, by using `slice_max` function from `dplyr` we can also get `n` high-valued 'suspects'.

```{r}
getSuspects(corp_bfd, corporate.payment) |>
  slice_max(order_by = Amount, n = 10, with_ties = FALSE)
```

#### Conclusion {-}
Though by statistics (goodness of fit metrics), the data did not conform to BL, yet we observed that there were abnormally high records starting with digits `50`.  The reasons can be further investigated.  By charts we also observed that, otherwise the data conform to BL.  We also extracted suspected records for further investigation on other parameters/tests/verification.  To sum up, we can say that, Benford Analysis can be a good starting point for fraud/forensic analytics while auditing.  Before closing, let us also delve in one other example.

### Example-3: Lakes Perimeter
Let us apply this on `lakes.perimeter`[^14-benford-1] data which is available with the package.

[^14-benford-1]: A dataset of the perimeter of the lakes arround the water from the global lakes and wetlands database (GLWD) <http://www.worldwildlife.org/pages/global-lakes-and-wetlands-database>

```{r}
# load sample data
data(lakes.perimeter) 
# Number of rows
nrow(lakes.perimeter)
# View top rows
head(lakes.perimeter)
# Generate Benford Object
lake_ben <- benford(lakes.perimeter$perimeter.km, number.of.digits = 2)
```

Let us see the plots, metrics and top outliers

```{r fig.show='hold', fig.align='center', fig.cap="Benford Analysis - lake Perimeter Data"}
plot(lake_ben)
```

```{r}
# Chisq test
chisq(lake_ben)
# MAD
MAD(lake_ben)
# Whether it conforms?
lake_ben$MAD.conformity
```

```{r}
# Get top-10 suspects
getSuspects(lake_ben, lakes.perimeter) |>
  head(10)

# Get top-10 suspects on Squared Differences
getSuspects(lake_ben, lakes.perimeter, 
            by = "squared.diff") |>
  head(10)
# Get top-10 suspects on Absolute Excess Summation
getSuspects(lake_ben, lakes.perimeter, 
            by = "abs.excess.summation") |>
  head(10)
```

#### Conclusion {-}
We observed that data does not conform Benford's law which is evident from plot as well as MAD value. Chi-Squared Value of `88111` also exceeds critical value very significantly.  Nigrini and Miller gave some plausible explanations in their Research paper [@article2007] for this non-conformity.  One of the possible reasons, they propose, was that *perimeter is not a correct measurement for the size of a lake.*

## Conclusion
As we conclude this chapter on Benford Analytics, it’s clear that this statistical phenomenon holds remarkable potential across diverse fields. The inherent simplicity of Benford’s Law belies its complexity and applicability. Its ability to unveil anomalies, authenticate data integrity, and aid in forensic investigations underscores its significance in modern data analysis. As we delve deeper into its intricacies and practical applications, we unravel a tool that not only scrutinizes numbers but also illuminates new avenues for precision, authenticity, and trust in our data-driven world.

------------------------------------------------------------------------

Further Reading-

1.  ISACA JOURNAL ARCHIVES - [Understanding and Applying Benford's Law - 1 May 2011](https://www.isaca.org/resources/isaca-journal/past-issues/2011/understanding-and-applying-benfords-law)

2.  Newcomb, Simon. "Note on the Frequency of Use of the Different Digits in Natural Numbers." American Journal of Mathematics, vol. 4, no. 1, 1881, pp. 39--40. JSTOR, <https://doi.org/10.2307/2369148>. Accessed 15 Jun. 2022.

3.  Durtschi, Cindy & Hillison, William & Pacini, Carl. (2004). The Effective Use of Benford's Law to Assist in Detecting Fraud in Accounting Data. J. Forensic Account.

<!--chapter:end:14-Benford.Rmd-->

# Finding duplicates

### Prerequisites {-}
Let us load `tidyverse` first.
```{r message=FALSE, warning=FALSE}
library(tidyverse)
```

Detection of duplicates is a frequent requirement in audit analytics, be it case of detecting risk of duplicate invoices or simply detection of duplicates in social sector audits^[Identifying Fraud Using Abnormal Duplications Within Subsets. 2012. John Wiley & Sons, Ltd. [https://doi.org/10.1002/9781118386798.ch12](https://doi.org/10.1002/9781118386798.ch12).]. 

Let us create an sample data of invoices.
```{r}
set.seed(123)
# let's create 10 invoices
invoices <- data.frame(invoice_no = 1:10,
                       invoice_date = seq.Date(as.Date("2010-01-01"),
                                               as.Date("2010-01-10"), 
                                               by = "day"),
                       amount = sample(1000:2000, 10))
invoices <- slice_sample(invoices,
                         n = 12, 
                         replace = TRUE)
# let us also assume each has different description
invoices$description <- LETTERS[1:12]
# view the data
invoices
```

To find duplicate records on the basis of multiple atrributes say `invoice_no`, `invoice_date` and `amount`, we can use function `duplicated` as shown below-

```{r}
attrib_cols <- c("invoice_no", "invoice_date", "amount")
invoices %>% 
  filter(if_all(all_of(attrib_cols), duplicated))
```

We may see that duplicate records have been thrown in result, which excludes first record considered as original. 

>__Explanation:__ Using `if_all` or `if_any` with `dplyr::filter` provides us a way to filter simultaneously based on many columns.  The second argument to be provided in these functions is mostly a function which returns logical values.  So use of `duplicate` in second argument filters all (because of `if_all`) columns provided in first argument based on values returned by applying function mentioned on second argument, on those columns.

If however, the requirement is to find all the records with duplicate attributes, we may change our strategy slightly.

```{r}

invoices %>% 
  select(all_of(attrib_cols)) %>% 
  filter(if_all(everything(), duplicated)) %>% 
  distinct() %>% 
  semi_join(invoices, ., by = all_of(attrib_cols)) 
  
```

>__Explanation:__ Only columns of intrest (`attrib_cols`) have been selected in first step.  Thereafter, `dplyr::filter` is applied on all of these columns.  We will have all duplicate values as a result. In the last step we will filter invoices data using `semi_join`.  One thing to note, we are filtering `invoices` on the basis of output of result of previous step and therefore `.` placeholder is specifically used after `invoices` i.e. first argument instead of default.

# Network analysis {#dup_net}


## Related/Connected Entities - case -II
Imagine a scenario when users may have multiple IDs such as mobile numbers, email ids, and say some other ID issued by a Government Department say Income Tax Department (e.g. PAN number in Indian Scenario).  Using techniques mentioned in section \@ref(dup), we may easily find out duplicate users, i.e. duplicates on the basis of one ID.  Sometimes need arise where we have to find out network of all the duplicate users where they have changed one or two IDs but retained another. E.g. There may be a social sector scheme where any beneficiary is expected to be registered only once for getting that scheme benefits.  Scheme audit(s) may require auditors to check duplicate beneficiaries using multiple IDs.

Understand this with the following table.
```{r igraph, echo=FALSE}
PANs <- paste0(
  "PANNO", c("0000", "0000", "1111", "2222", "3333", "4444", "5555", "5555", "6666", "7777"),
  c("A", "A", "B", "C", "D", "E", "F", "F", "G", "H")
)

emails <- paste0(c("aaaa", "bbbb", "cccc", "dddd", "eeee", "ffff", "gggg", "hhhh", "iiii", "bbbb"), "@gmail.com")


tele <- paste0("9", c("1", "2", "3", "4", "1", "1", "5", "6", "3", "2") %>%
  map_chr(~ rep(.x, 9) %>%
    paste0(collapse = "")))

dummy_data <- data.frame(
  ID = 1:10,
  Mobile = tele,
  Email = emails,
  PAN = PANs
)

knitr::kable(dummy_data)
```
It may be seen that out of ten persons, two with IDs 6 and 10 respectively share none of IDs out of Email, PAN and Telephone number. But if we see closely, ID-6 shares mobile number with ID-1 who in turn share PAN number with ID-2.  ID-2 further shares both Email and Mobile number with ID-6 thus establishing a relation and a network between ID-6 and ID-10.  This is clear in figure at \@ref(fig:igraph11). _Note that we are not considering names while finding out duplicates._

```{r igraph11, fig.show='hold', fig.align='center', fig.cap="Network diagram of connected entities", out.width="75%"}

knitr::include_graphics("images/canvas.png")
```

We may find these duplicates using a branch of mathematics called _Graph Theory_.^[[https://en.wikipedia.org/wiki/Graph_theory](https://en.wikipedia.org/wiki/Graph_theory)] We won't be discussing any core concepts of graph theory here.  There are a few packages to work with graph theory concepts in R, and we will be using `igraph` [@R-igraph] for our analysis here. Let's load the library.

```{r message=FALSE, warning=FALSE}
library(igraph)
```

```{r}
dat <- data.frame(
  MainID = 1:9,
  Name = c("A", "B", "C", "B", "E", "A", "F", "G", "H"),
  ID1 = c(11,12,13,13,14,15,16,17,17),
  ID2 = c("1a", "1b","1b", "2a", "2b", "2c", "2c", "2e", "3a"),
  ID3 = c("AB", "AB", "BC", "CD", "EF", "GH", "HI", "HI", "JK")
)
# A preview of our sample data
dat
```

Now the complete algorithm is as under-
```{r}
id_cols <- c("ID1", "ID2", "ID3")
dat %>% 
  mutate(across(.cols = all_of(id_cols), as.character)) %>% 
  pivot_longer(cols = all_of(id_cols), 
               values_drop_na = TRUE) %>% 
  select(MainID, value) %>% 
  graph_from_data_frame() %>%
  components() %>%
  pluck(membership) %>%
  stack() %>%
  set_names(c('UNIQUE_ID', 'MainID')) %>%
  right_join(dat %>% 
               mutate(MainID = as.factor(MainID)), 
             by = c('MainID'))
```

We may see that we have got unique ID of users based on all three IDs. Let us understand the algorithm used step by step.

__Step-1__: First we have to ensure that all the ID columns (Store names of these columns in one vector say `id_cols`) must be of same type.  Since we had a mix of character (Alphanumeric) and numeric IDs, using `dplyr::across` with `dplyr::mutate` we can convert all the three ID columns to character type. Readers may refer to section \@ref(vectors) for type change, and section \@ref(across) for changing data type of multiple columns simultaneously using `dplyr::across`.

Thus, first two lines of code above correspond to this step only.
```
id_cols <- c("ID1", "ID2", "ID3")
dat %>%
  mutate(across(.cols = id_cols, as.character))
```

__Step-2__: Pivot all id columns to longer format so that all Ids are linked with one main ID.  Now two things should be kept in mind.  One that there should be a main_Id column in the data frame.  If not create one using `dplyr::row_number()` before pivoting.  Secondly, if there are `NA`s in any of the IDs these have to be removed while pivoting.  Use argument `values_drop_na = TRUE` inside the `tidyr::pivot_longer`. Thus, this step will correspond to this line-
```
pivot_longer(cols = all_of(id_cols), values_drop_na = TRUE)
```
where - first argument data is invisibly passed through dplyr pipe i.e. `%>%`. Upto this step, our data frame will look like -
```{r echo=FALSE}
id_cols <- c("ID1", "ID2", "ID3")
dat %>% 
  mutate(across(.cols = all_of(id_cols), as.character)) %>% 
  pivot_longer(cols = all_of(id_cols), 
               values_drop_na = TRUE)
```


__Step-3:__ Now we need only two columns, one is `mainID` and another is `value` which is created by pivoting all ID columns.  We will use `select(MainID, value)` for that.

__Step-4:__ Thereafter we will create a graph object from this data (output after step-3), using `igraph` package.  Interested readers may see how the graph object will look like, using `plot()` function. The output is shown in figure \@ref(fig:igraph2). __However, this step is entirely optional and it may also be kept in mind that graph output of large data will be highly cluttered and may not be comprehensible at all.__

```{r igraph2, fig.align='center', fig.show='hold', fig.cap="Plot of graph object"}
dat %>% 
  mutate(across(.cols = all_of(id_cols), as.character)) %>% 
  pivot_longer(cols = all_of(id_cols), 
               values_drop_na = TRUE) %>% 
  select(MainID, value) %>% 
  graph_from_data_frame() %>%
  plot()
```

__Step-5:__ This step will be a combination of three lines of codes which will number each ID based on connectivity of all components in the graph objects.  Actually `components` will give us an object where `$membership` will give us `unique_ids` for each component in the graph.
```{r echo=FALSE}
id_cols <- c("ID1", "ID2", "ID3")
dat %>% 
  mutate(across(.cols = all_of(id_cols), as.character)) %>% 
  pivot_longer(cols = all_of(id_cols), 
               values_drop_na = TRUE) %>% 
  select(MainID, value) %>% 
  graph_from_data_frame() -> dat2
dat2 %>% 
  components()
```
Next we have to `purrr::pluck`, `$membership` only from this object, which will return a named vector.  
```{r echo=FALSE}
dat2 %>% 
  components() %>% 
  pluck(membership)
```

We can then `stack` this named vector into a data frame using `stack` and `set_names`
```{r echo=FALSE}
dat2 %>% 
  components() %>% 
  pluck(membership) %>% 
  stack %>% 
  set_names(c('UNIQUE_ID', 'MainID'))
```

I suggest to purposefully name second column in the output data as `MainID` so that it can be joined with original data frame in the last step.  `UNIQUE_ID` in this data will give us the new column which will allocate same ID to all possible duplicates in network of three IDs.

__Step-6:__ In the last step we have to join the data frame back to original data frame.  Since the type of `MainID` is now factor type, we can convert type of this column in original data frame before `right_join` the same.  Hence the final step, `right_join(dat %>% mutate(MainID = as.factor(MainID)), by = c('MainID'))`.


<!--chapter:end:20-duplicates.Rmd-->

# Detecting gaps in sequences

Audit analytics often requires us to check for gaps in sequences of numbers. Gaps in Sequentially numbered objects such as purchase orders, invoice numbers, cheque numbers, etc should be accounted for. Thus, auditors may require to exercise this audit check as a part of audit analytics.

## When sequence numbers are available as `numeric` column
We will have starting and ending numbers in such sequence. Let us allocate these in two variables.
```{r}
start_num <- 500301 # say
end_num <- 503500 # say
```

It means we have `r end_num - start_num + 1` terms (say cheques) issued in the series.  Further suppose, the cheque numbers issued are stored in some column say `cheque_no` in a given data frame, have a total count say `r end_num - start_num + 1 -23`. To simulate
```{r}
set.seed(123)
cheque_no <- sample(start_num:end_num, 3177, replace = FALSE)
```
To find out the gaps we may simply use function `setdiff` on these two.
```{r}
setdiff(start_num:end_num, cheque_no)
```

## When sequence numbers are available as `character()` column

We may easily replicate above procedure for gap detection, even if the sequence column is of character type.  E.g. If the cheque numbers have a prefix say, 'A', then the cheque numbers may look like-
```{r echo=FALSE}
cheque_no <- paste0("A", cheque_no)
head(cheque_no, 10)
```

In these case, we may first substitute prefix with nothing and then proceed as above.  

```{r}
modified_cheques <- sub("A", "", cheque_no) |> as.integer()
missing_cheques <- setdiff(start_num:end_num, modified_cheques)
missing_cheques
```


<!--chapter:end:21-gaps.Rmd-->

# Merging large number of similar datasets into one
Data preparation for performing analytics is an important task and may require more time than actual analytics because we rarely have data in ideal format.  Importing csv or flat files is rather an easy job.  However, considering large popularity of MS Excel, we at times have our data saved in excel files.  

Sometimes, one single data frame/table is divided into multiple sheets in one excel file whereas sometimes these tables are divided in multiple files.  Here we are discussing few of these cases, where we can reduce our data preparation time by effectively writing the code for import of such data into our environment.

## **Case-1**: Merging multiple excel sheets into one data frame
As an example, let's say we have multiple States' data saved on a different sheet in one excel file say `Daur.xlsx`.  See preview in fig \@ref(fig:prevexcel).

```{r prevexcel, fig.show='hold', echo=FALSE, fig.cap="Preview of example excel file", fig.align='center', out.width="100%"}
knitr::include_graphics("images/excel.png")
```

We will use library `readxl` to read excel files.  This library is bundled with `tidyverse` but is not part of core tidyverse, so it has to be loaded explicitly, though explicit download is not required if `tidyverse` is installed in the system.

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(readxl)
```
The following steps are used-

  Step-1: Read path
  
  Step-2: Collect Names of all sheets
  
  Step-3: Set names of elements of above vector onto itself
  
  Step-4: Read and combine all tables into one.  We will use `purrr::map_dfr` for this.

```{r}
# Step-1
path <- "data/daur1.xlsx"
# Step-2
states_data <- excel_sheets(path) %>% 
# step-3
    set_names(., .) %>% 
# step-4
    map_dfr(read_excel, path=path, .id = 'State_name')
# print file
states_data
```

We can see that data from all the sheets have been merged into table and one extra column has been created using sheet name.  The name of that column has been provided through `.id` argument.  If the new column is not required, simply don't use this argument.

## **Case-2**: Merging multiple files into one data frame

Often we have our source data split into multiple files, which we will have to compile in one single data frame before proceeding In this case, we may collect all such files in one directory and follow these steps-

Step-1: Store all file names using `list.files()`

Step-2: We may read all files in one list using either `lapply` or `purrr::map`.

Step-3: If data structures in all the files are same, we can directly use `purrr::map_dfr` which will read all files and give us a data frame.  If however, the structure of data in all files are not same, we may convert all columns into character type before merging these files.  We can thereafter proceed for merging all data using either `purrr::map_dfr` or `lapply` in combination with `do.call`.

## **Case-3**: Split and save one data frame into multiple excel/csv files simultaneously.
As an example will use `states_data` created in case-1.  We can use the following algorithm

Step-1: Create a vector of file names using `paste0`
Step-2: Split data frame into a list with separate dataframe for each state
Step-3: Write to a separate file using `purrr::walk2()`

The complete algorithm is

```{r}
# step-1 : create a vector of file names (output)
file_names <- paste0("data/", unique(states_data$State_name), ".csv")

states_data %>% 
  group_split(State_name) %>% 
  purrr::walk2(file_names, write.csv)
```
We can check that 3 new files with state_names as filenames have been created in the `data` folder/directory as desired.

## **Case-4**: Splitting one data into muliple files having multiple sheets
Sometimes, we may require to split a file not only into multiple files, but simultaeously require to split each file into multiple excel sheets.  E.g. A data having States and districts is to be split into State-wise files having a separate sheet for each district.

This can be achieved using `writexl` library.  In this case, we may write a custom function which can do our job easily.
```{r}
library(tidyverse)
library(writexl)

book_and_sheets <- function(df, x, y){
  df_by_x <- df %>% 
    split(.[[x]])
  
  save_to_excel <- function(a, b){
    a %>% 
      split(.[[y]]) %>% 
      writexl::write_xlsx(
        path = paste0("data/data_by_", b,"_",x, ".xlsx")
      )
    
  }
  
  imap(df_by_x, save_to_excel)
}
```
The function `book_and_sheets` designed in above code helps us to write a data say `df` into separate files based on column `x` and each of these files is further divided into sheets based on column `y`.  Only thing to be remembered is that we have to pass, `x` and `y` arguments as character strings; and `df` as variable.

Example - splitting `mtcars` into files based on `cyl` and sheets based on `gear`
```{r}
book_and_sheets(mtcars, 'cyl', 'gear')
```
We can check that three excel files have been created in directory `data/`.


<!--chapter:end:22-merge.Rmd-->

# Sentiment Analysis through Word-Cloud

## Step-1:Prepare data and load libraries

As an example we will create a word cloud with Budget Speech made by Finance Minister during her Budget speech^[Data Source: [Indian Budget Portal](https://www.indiabudget.gov.in/)] 2022-23.  All of the budget speech is available in file called `budget.txt`.

Load Libraries

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(tidytext) #install.packages("tidytext")
library(wordcloud) #install.packages("wordcloud")
library(ggtext)
library(ggalt)
library(ggthemes)
library(ggpubr)
```


Load data
```{r warning=FALSE,message=FALSE}
dat <- read.table('data/budget.txt', header = FALSE, fill = TRUE)
```

## Step-2: Reshape the .txt data frame into one column

Above steps will create one row per line.  Let's create a tidy data frame out of this data.

```{r echo=TRUE}
tidy_dat <- dat %>% 
  pivot_longer(everything(), values_to = 'word', names_to = NULL)
```

## Step-3: Tokenize the data/words
To tokenize the words we will use function `unnest_tokens()` from `tidytext` library.  As a further step we will have a count of each word, using `dplyr::count` which will create a column `n` against each word.
```{r echo=TRUE}
tokens <- tidy_dat %>% 
  unnest_tokens(word, word) %>% 
  count(word, sort = TRUE) 
```
## Step-4: Clean stop words
The library `tidytext` has a default database which can eliminate stop words from above data.  Let's load this default stop words data.
```{r echo=TRUE}
data("stop_words")
```
We may then remove stop words using `dplyr::anti_join`.
```{r echo=TRUE}
tokens_clean <- tokens %>%
  anti_join(stop_words, by='word') %>% 
  # remove numbers
  filter(!str_detect(word, "^[0-9]"))
```
We may remove additional stop words those specific to this data/input.  To have an idea of these stop words, we may at firt, skip this step altogether and proceed to generate word cloud in next step directly.  After having a first look, we can identify and then remove these additional stop words seen in first round(s).
```{r echo=TRUE}
uni_sw <- data.frame(word = c("cent", "pm", "crore", 
                              "lakh", "set",
                              "level", "sir"))

tokens_clean <- tokens_clean %>% 
  anti_join(uni_sw, by = "word")
```

## Step-5: Plot/generate word cloud
Output/Word cloud of following code can be seen in figure \@ref(fig:wordcloud).

```{r wordcloud, fig.cap= "Word Cloud of FM's Budget Speech 2022", fig.align='center',fig.align='center', message=FALSE, warning=FALSE}
pal <- RColorBrewer::brewer.pal(8,"Dark2")

# plot the 40 most common words
tokens_clean %>% 
  with(wordcloud(word, 
                 n, 
                 random.order = FALSE, 
                 max.words = 40, 
                 colors=pal,
                 scale=c(2.5, .5)))
```




<!--chapter:end:23-wordcloud.Rmd-->

# Finding string similarity 

Comparison of two (or more) numeric fields is an easy job in the sense that we can use multiple statistical methods available to measure comparison between these.  On the other hand, comparing strings in any way, shape or form is not a trivial task.  Despite this complexity, comparing text strings is a common and fundamental task in many text-processing algorithms.  Basic objective of all string similarity algorithms are to quantify the similarity between two text strings in terms of string metrics.   

The fuzzy matching problems are to input two strings and return a score quantifying the likelihood that they are expressions of the same entity. So (`Geeta` and `Gita`) should get a high score but not (`Apple` and `Microsoft`).  Over several decades, various algorithms for fuzzy string matching have emerged. They have varying strengths and weaknesses. These fall into two broad categories: `lexical matching` and `phonetic matching`.

## Lexical matching
*Lexical matching algorithms* match two strings based on some model of errors. Typically they are meant to match strings that differ due to spelling or typing errors. Consider `Atharv` and `ahtarv`. A lexical matching algorithm would pick up that `ht` is a transposition of `th`.  Such transposition errors are common. Given this, and that the rest of the two strings match exactly and are long enough, we should score this match as high. 

Normally, algorithms to find lexical matching, can be classified into 'edit distance based' or 'token based'.

### Levenshtein algorithm
It is named after *Vladimir Levenshtein*, who considered this distance in 1965. The `Levenshtein distance` between two words is the minimum number of single-character edits (i.e. insertions, deletions or substitutions) required to change one word into the other.  Levenshtein distance may also be referred to as *edit distance*, although it may also denote a larger family of distance metrics. It is closely related to pairwise string alignments.  

For the two words `helo` and `hello`, it is obvious that there is a missing character `"l"`. Thus to transform the word `helo` to `hello` all we need to do is insert that character. The distance, in this case, is `1` because there is only one edit needed.

### Hamming distance
This distance is computed by overlaying one string over another and finding the places where the strings vary. Note, classical implementation was meant to handle strings of same length. Some implementations may bypass this by adding a padding at prefix or suffix. Nevertheless, the logic is to find the total number of places one string is different from the other. 

### Jaro-Winkler
This algorithms gives high scores to two strings if, 

1. they contain same characters, but within a certain distance from one another, and 
2. the order of the matching characters is same. 

To be exact, the distance of finding similar character is 1 less than half of length of longest string. So if longest strings has length of 5, a character at the start of the string 1 must be found before or on ((5/2)–1) ~ 2nd position in the string 2 to be considered valid match. Because of this, the algorithm is directional and gives high score if matching is from the beginning of the strings. 

### Q-Gram
*Q-Grams* is based on the difference between occurences of `Q` consecutive characters in two strings.  To illustrate take a case of Q=3 (this special case is also called trigrams).  For `atharv` and its possible typo `ahtarv` the trigrams will be

- For atharv `{ath tha har arv}`
- for ahtarv `{aht hta tar arv}`

We can see that a total of `7` unique trigrams have been formed and out of these only `1` is similar.  Thus, 3-gram similarility would be `1/7=14%`.  We can see that this algorithm is not very effective for transpositions.

## Phonetic matching
*Phonetic matching algorithms* match strings based on how similar they sound. Consider `Geeta` and `Gita.` They sound similar enough that one person might spell as `Geetha` or `Geeta`, another as `Gita.` As in this case, one is not necessarily a misspelling of the other. just sounds similar.

### Soundex
Created by *Robert Russel* and *Margaret King Odell* in 1918, this algorithm intended to match names and surnames based on the basic rules of English pronunciation, hence, similar names get the same value.

### Metaphone

Developed by *Lawrence Philips* in 1990, the Metaphone is also more accurate compared with the `Soundex` method as it takes into consideration the groups of letters. The disadvantage shows up when we apply it to reconcile the strings that are not in English, as it is based on the rules of English pronunciation.

### Double Metaphone

Following `Metaphone`, *Philips* also designed the *Double Metaphone*. As its name suggests, it returns two codes, so you have more chances to match the items, however, at the same time, it means a higher probability of an error. According to the algorithm, there are three matching levels: 

- `primary key to the primary key = strongest match`, 
- `secondary key to the primary key = normal match`, 
- `secondary key against the secondary key = weakest match`.

### Metaphone 3

*Philips* further refined the double metaphone algorithm to produce better results.  The algorithm (Metaphone 3) is however, proprietary and is not open-source.

## Examples

In R, we can use `stringdist` package to calculate many of the above mentioned distances.  The function is vectorised.  The synatx is 

```
stringdist(
  a,
  b,
  method = c("osa", "lv", "dl", "hamming", "lcs", "qgram", "cosine", "jaccard", "jw",
    "soundex"),
  useBytes = FALSE,
  weight = c(d = 1, i = 1, s = 1, t = 1),
  q = 1,
  p = 0,
  bt = 0,
  nthread = getOption("sd_num_thread")
)
```

where -

- `a` and `b` are two strings/vectors for which similarity/distance is to be measured.
- `method` to be used.  Default is 
    + `osa` for *Optimal String Alignment*.  Other methods are-
    + `lv` for *Levenstein distance*, 
    + `dl` for *Damerau-Levenshtein*
    + `hamming` for *Hamming distance*
    + `lcs` for *Longest Common Substring*
    + `qgram` for Q-Grams
    + `cosine` for cosine 
    + `jaccard` for Jaccard's algorithm
    + `jw` for Jaro-Winkler
    + `soundex` for Soundex
- Other arguments are needed on the basis of algorithm chosen.


To calculate 'metaphone' index we can use `phonics` package and for 'Double Metaphone' we can use `PGRdup` package in R.

Example - Suppose we have a set of two names.
```{r}
nameset1 <- c('Geeta', 'Susheel', 'Ram', 'Dr. Suchitra')
nameset2 <- c('Gita', 'Sushil', 'Rama', 'Suchitra')
```

Note most of these distances/similarity indices are cases sensitive, and therefore we have to use these methods with a bit cleaning first.  We can convert cases of all strings to lower-case to eliminate these (if) unwanted errors.

```{r}
library(stringdist)
suppressPackageStartupMessages(library(dplyr))

data.frame(
  nameset1 = tolower(nameset1),
  nameset2 = tolower(nameset2)
) %>% 
  mutate(lv_dist = stringdist(nameset1, nameset2, method = 'lv'),
         jw_dist = stringdist(nameset1, nameset2, method = 'jw'),
         qgram_3 = stringdist(nameset1, nameset2, method = 'qgram', q=3))
```

Creating Metaphone and Double Metaphone
```{r}
library(phonics)

data.frame(
  nameset1 = tolower(nameset1),
  nameset2 = tolower(nameset2)
) %>% 
  mutate(metaphone_1 = metaphone(nameset1),
         metaphone_2 = metaphone(nameset2))
```
Note that we cannot calculate `metaphone` for special characters even for spaces.

*Double metaphone* is not vectorised.  So we have to use `apply` family of functions here.

```{r}
suppressPackageStartupMessages(library(PGRdup))
library(purrr)

data.frame(
  nameset1 = tolower(nameset1),
  nameset2 = tolower(nameset2)
) %>% 
  mutate(DMP_1_1 = map_chr(nameset1, ~DoubleMetaphone(.x)[[1]]),
         DMP_1_2 = map_chr(nameset1, ~DoubleMetaphone(.x)[[2]]),
         DMP_2_1 = map_chr(nameset2, ~DoubleMetaphone(.x)[[1]]),
         DMP_2_2 = map_chr(nameset2, ~DoubleMetaphone(.x)[[2]]))
```


<!--chapter:end:24-stringdist.Rmd-->

# Anomaly Detection
```{r an100, echo=FALSE, warning=FALSE, message=FALSE}
library(tidyverse)
library(magrittr)
```

Anomalies play a significant role in the field of audit. As auditors start examining data whether it be financial statements, or transactions, or other relevant data, the detection of anomalies can provide valuable insights and help identify potential issues or irregularities. Anomalies often indicate the presence of fraudulent activities.   Unusual or unexpected patterns in financial transactions or account balances may suggest potential fraud or misappropriation of assets. Auditors actively search for anomalies that may indicate fraudulent behavior, such as fictitious transactions, unauthorized access, or unusual changes in financial data.  

Anomalies in the audit context serve as indicators of potential issues, including fraud, material misstatements, control weaknesses, compliance violations, and process inefficiencies. Detecting and investigating anomalies is crucial for auditors to provide accurate and reliable financial information, enhance internal controls, and support informed decision-making by stakeholders.

## Definition and types of anomalies
Anomalies are patterns or data points that deviate significantly from the expected or normal behavior within a dataset. These are also known as outliers, novelties, or deviations and can provide valuable insights into unusual events or behavior in various domains.

Types of anomalies:

- **Point Anomalies:** Individual data points that are considered anomalous when compared to the rest of the dataset. For example, a temperature sensor reading that is significantly different from the expected range.

- **Contextual Anomalies:** Data points that are considered anomalous within a specific context or subset of the dataset. For instance, a sudden increase in obsolete website traffic.

- **Collective Anomalies:** Groups of data points that exhibit anomalous behavior when analyzed together but might appear normal when considered individually. An example is a sudden drop in sales for multiple related products.

## Anomaly detection, in R {-}
## By inspection
Several times, anomalies or outliers are detectable by observation.  The `summary()` function prints the *maximum*, *minimum*, *upper* and *lower quartiles*, and the *mean* and *median*, and can give a sense for how far an extreme point lies from the rest of the data. E.g. Let's visualise the mean annual temperatures in degrees Fahrenheit in New Haven, Connecticut, from 1912 to 1971, through a boxplot (Refer Figure \@ref(fig:an1).  This data is available in base R, as `nhtemp`.

```{r an101}
summary(nhtemp)
```

The easiest way to get a sense for how unusual a particular value is is by using a graphical summary like a boxplot. In R, this is created using the `boxplot` function. The `boxplot` function takes a column of values as an input argument, here illustrated with the temperature data, and produces a box and whiskers representation of the distribution of the values. The extreme values are represented as distinct points, making them easier to spot.  We can also make use of ggplot2.  Examples from both base R and ggplot2 are shown in Figure \@ref(fig:an1).

```{r an1, fig.show='hold', fig.align='center', fig.cap="Outliers through Boxplot", out.width="45%"}
boxplot(nhtemp, main = "Box plot of nhtemp data")
ggplot(iris, aes(Species, Petal.Length, color = Species )) +
  geom_boxplot(outlier.colour = "red", outlier.size = 2) +
  theme_bw() +
  labs(title = "Petal Lengths in Iris Data")
```

It's important to note that a **point anomaly** is not necessarily always extreme. A point anomaly can also arise as an unusual combination of values across several attributes.  

A **collective anomaly**, on the other hand, is a collection of similar data instances that can be considered anomalous together when compared to the rest of the data. For example, a consecutive 5 day period of high temperatures are shown by the red points in the plot. These daily temperatures are unusual because they occur together, and are likely caused by the same underlying weather event. Refer Figure \@ref(fig:an2).

```{r an2, echo=FALSE, fig.align='center', fig.show='hold', fig.cap="Collective Anomalies", out.height="27%"}
set.seed(567)
c(rnorm(15, 30, 2), rnorm(5, 40, 2.1), rnorm(10, 30, 2)) %>% ts() -> myts
dates <- 1:30
plot(dates, myts, type = 'l',ylab = "Temp in Celsius", xlab = 'Dates', 
     main = "Collective Anomalies through a dummy data")
points(16:20, myts[16:20], col = 'red', pch = 16)
points(1:15, myts[1:15], col = 'black', pch = 16)
points(21:30, myts[21:30], col = 'black', pch = 16)
```

## Grubb's Test
We saw that visual assessment of outliers works well when the majority of data points are grouped together and rest of them lie separate.  In statistics there are several tests to detect anomalies.  **Grubb's Test** is one of these. Grubb's test assesses whether the point that lies farthest from the mean in a dataset could be an outlier. This point will either be the largest or smallest value in the data.  This test is however based on assumption that data points are normally distributed.  So before proceeding for this test, we must be sure that there is a plausible explanation for this assumption. *(We can check normality of data points by plotting a histogram.  For other methods please refer to chapter on linear regression.)*

Let's run this test on `nhtemp` data example, which we have already seen in \@ref(fig:an1) (Left). So let's check its normality assumption.
```{r an3, fig.align='center', fig.show='hold', fig.cap="Histogram for Grubb's test", out.height="27%"}
hist(nhtemp, breaks = 20)
```

In figure \@ref(fig:an3) we can see that our assumption is nearly satisfied.  Let's run the test.

```{r an}
library(outliers)
outliers::grubbs.test(nhtemp)
```

As p value is `0.15` we do not have strong evidence to reject null hypothesis that extreme maximum value is an outlier.

## Seasonal Hybrid ESD Algorithm

As we have seen that above test may not be appropriate for anomaly detection (normality assumption as well as detection of extreme values only), particularly detecting anomalies from a time series that may have seasonal variations.  We may install `AnomalyDetection` package development version from github using `devtools::install_github("twitter/AnomalyDetection")`.  Example: in `JohnsonJohnson` data having quarterly sales data, we can use the following syntax.

```{r an4, fig.show='hold', fig.align='center', fig.cap="Seasonal Hybrid ESD Algorithm", message=FALSE, out.height="28%"}
#devtools::install_github("twitter/AnomalyDetection")
library(AnomalyDetection)
AnomalyDetectionVec(as.vector(JohnsonJohnson), 
                    period = 4, # Sesaonality
                    plot = TRUE, # set FALSE when plot not needed
                    direction = 'both' # set 'pos' for higher values
                                       # or 'neg' for lower values
                    )
```

In Figure \@ref(fig:an4), we can see that in output `$anoms` containing anomalies are denoted in blue dots.

## k-Nearest Neighbour Distance Score
One of greatest limitation of above two methods was that, these were applicable on univariate data series, whereas, in real world, data analysis will rarely be univariate.  The `knn` technique works on multivariate data, but for visulaisation purposes, we will first visulaise the results on bivariate data only. 

**K-Nearest Neighbour** or **KNN** is a distance-based classifier, meaning that it implicitly assumes that the smaller the distance between two points, the more similar they are.  For bivariate data, we can understand the algorithm by plotting the data-points in a two dimensional scatter plot.  Now as the distance between any two points are usually calculated using *Euclidean Distance Metric*, we should ensure that the data is normalised/scaled before proceeding for distance calculation.

**Problem Statement-1:**  Let us try to identify outliers in `virginica` Species' flower measurements.  So let's prepare the data.   We will make use of `scale` function available in base R, to normalise the data.  Remember that `scale` returns a matrix, so we may need to convert it into data.frame while using `ggplot2`.

```{r an102}
# Sample data
df <- iris[101:150, 1:4]
#  Scale the data
df <- scale(df)
```

Let us visualise the data (in first two dimensions only). However, as our data is scaled already, we can try to visualise all the dimensions using a boxplot.  See both figures in \@ref(fig:an5).  The points/ouliers have been numbered (on the basis of row numbers) for interpretaion purposes.

```{r an5, warning=FALSE, message=FALSE, fig.align='center', fig.show='hold', fig.cap="Sepal Length Vs. Widths in Virginica", out.width="45%"}

df %>% 
  as.data.frame() %>% 
  mutate(row = row_number()) %>% 
  ggplot(aes(Sepal.Length, Sepal.Width)) +
  geom_point(color = "seagreen") +
  ggrepel::geom_text_repel(aes(label = row), arrow = grid::arrow()) +
  theme_bw()

# Helper Function
find_outlier <- function(x) {
  return(x < quantile(x, .25) - 1.5*IQR(x) | x > quantile(x, .75) + 1.5*IQR(x))
}

df %>% 
  as.data.frame() %>% 
  mutate(row = row_number()) %>% 
  pivot_longer(-row, names_to = "Dimension", values_to = "Values") %>% 
  group_by(Dimension) %>% 
  mutate(outlier = ifelse(find_outlier(Values), row, NA)) %>% 
  ggplot(aes(Dimension, Values)) +
  geom_boxplot(outlier.colour = "red") +
  geom_text(aes(label=outlier), na.rm=TRUE, hjust=-.5) +
  theme_bw()
```

Now, let's proceed to identify outliers in R. We will make use of `get.knn` function from `FNN` package.  However, `k` parameter is required beforehand.

```{r an6, warning=FALSE, message=FALSE}
# Load the library
library(FNN)
# get kNN object, using k = 5
viginica_knn <- FNN::get.knn(df[, 1:2], 5)
```

The `knn` object created above will have two sub-objects (both matrices having columns equal to chosen `k`), one having nearest neighbors' indices and another having distances from those.  Let's view their top 6 rows.

```{r an7}
head(viginica_knn$nn.index)
head(viginica_knn$nn.dist)
```

Using `rowMeans` we can calculate mean distance for each data point.  The bigger this score is, the chances of that record being an outlier are relatively higher.  Let's also store that mean distance in a variable/column say `score` in main dataset, and visualise the results by setting the point-size with this mean distance (actually its square root).  In Figure \@ref(fig:an8), we may notice that points lying far away are bigger becuase their chances of being outliers is high.

```{r an8, warning=FALSE, message=FALSE, fig.align='center', fig.show='hold', fig.cap="k-Nearest Neighbour Distance", out.height="28%"}
score <- rowMeans(viginica_knn$nn.dist)
df %>% 
  as.data.frame() %>% 
  mutate(row = row_number(),
         score = score) %>% 
  ggplot(aes(Sepal.Length, Sepal.Width)) +
  geom_point(aes(size = score),color = "seagreen") +
  ggrepel::geom_text_repel(aes(label = row), arrow = grid::arrow()) +
  theme_bw() +
  labs(title = "KNN method of outlier")

```

Now, we can run the algorithm to find the outlier on the basis of all variables in the dataset.

```{r an88}
virginica_knn <- FNN::get.knn(df, 5)
score <- rowMeans(virginica_knn$nn.dist)
# Which point is farthest-Outlier
which.max(score)
```


## Local Outlier Factor
As against kNN which uses distances of k neighbors, this algorithm uses density of each data point vis-a-vis density of its nearest neighbors. kNN distance seems to be good at detecting points that are really far from their neighbors, sometimes called global anomalies, but sometimes fail to capture all of the points that might be considered anomalous like local anomalies.  Local Anomalies may lie near to a cluster still they won't be like their neighbors. To understand it better, see the plot in Figure \@ref(fig:an9) consisting of dummy data.

```{r an9, echo=FALSE, warning=FALSE, message=FALSE, fig.align='center', fig.show='hold', fig.cap="Local Outlier factor", out.height="28%"}
set.seed(123)
dummy <- data.frame(
  sales = rnorm(25, 500, 100),
  profit = rnorm(25, 75, 10)
)
dummy2 <- data.frame(
  sales = rnorm(25, 1000, 5),
  profit = rnorm(25, 75, 2)
)
dummy3 <- data.frame(
  sales = rnorm(5, 500, 100),
  profit = rnorm(5, 120, 10)
)
dummy4 <- data.frame(
  sales = rnorm(5, 1000, 10),
  profit = rnorm(5, 90, 2)
)
dummy <- rbind(dummy, dummy2, dummy3, dummy4)

plot(profit ~ sales, data = dummy, xlim = c(0, 1100), ylim = c(0, 200))
points(profit ~ sales, data = dummy[51:55,], xlim = c(0, 1100), ylim = c(0, 200), col = 'red', pch = 20)
points(profit ~ sales, data = dummy[56:60,], xlim = c(0, 1100), ylim = c(0, 200), col = 'blue', pch = 20)
```

The red points may be global outliers, being far from its immediate neigbors, yet the blue points may be local anomalies as these are not like their immediate neighbors and may be local anomalies.

As stated, LOF segregates the data points based on the ratio of density of that point with that of densities of its neighbors.  A score `> 1` thus indicates that the data point may be an anomaly.  Let's see that on same problem statement.

```{r an10, warning=FALSE, message=FALSE, fig.align='center', fig.show='hold', fig.cap="LOF", out.width="48%"}
library(dbscan)
lof_score <- lof(df, 5)
df %>% 
  as.data.frame() %>% 
  mutate(row = row_number(),
         score = lof_score) %>% 
  ggplot(aes(Sepal.Length, Sepal.Width)) +
  geom_point(aes(size = score),color = "seagreen") +
  ggrepel::geom_text_repel(aes(label = row), arrow = grid::arrow()) +
  scale_size_continuous(guide = FALSE) +  
  theme_bw() +
  labs(title = "LOF method of outlier")

df_prcomp <- prcomp(df, scale. = TRUE)

df %>% 
  as.data.frame() %>% 
  mutate(row = row_number(),
         score = lof_score,
         PC1 = df_prcomp$x[,1],
         PC2 = df_prcomp$x[,2]) %>% 
  ggplot(aes(PC1, PC2)) +
  geom_point(aes(size = score),color = "seagreen") +
  ggrepel::geom_text_repel(aes(label = row), arrow = grid::arrow()) +
  scale_size_continuous(guide = FALSE) +  
  theme_bw() +
  labs(title = "LOF method of outlier\nVisualised on principal components")
```

Clearly, in Figure \@ref(fig:an10) (left), where we have plotted the data in 2 dimensions only despite that we have attempted to find the LOF on the basis of all four dimensions. We can see presence of local anomalies, within the clustered data points. E.g. points nos. 31, 23, etc. were earlier given more weight instead of point nos.15, 10, etc. which are now given more weight.  However, if we want to visaulise it on first two principal components, we can do that (Refer Figure \@ref(fig:an10) (Right)). We can also verify this.

```{r an11}
# Biggest Anomaly - kNN
which.max(score)

# Biggest Anomaly - LOF
which.max(lof_score)
```

Histograms of both `knn` and `LOF` scores can also be drawn, as in Figure \@ref(fig:an12).

```{r an12, warning=FALSE, message=FALSE, fig.align='center', fig.show='hold', fig.cap="Histogram - KNN(Left) and LOF (Right)", out.width="48%"}
hist(score, breaks = 40, 
     main = "Histogram of KNN scores in IRIS-Virginica data")
hist(lof_score, breaks = 40, 
     main = "Histogram of LOF scores in IRIS-Virginica data")
```

## Isolation Trees/Forest

Isolation Forest is an unsupervised tree based model, which actually works on path length instead of distance or density measures.  Tree based models use decision tree(s) to determine the value/class of an observation given its value. In other words, it determines or try to identify a path from the root node to a leaf node based on the value of the observation in question. Forest (or Random Forest) are actually collection of smaller trees and thus using ensemble learning methods to make decision, instead of a single complex tree.

Let us work on the same problem statement above.  Firstly, we will build a single decision tree.  WE will use a development version package `isofor` which can be downloaded using `remotes::install_github("Zelazny7/isofor")`.  To build forest/tree we will use function `iForest`.  Its argument `nt` will determine the number of trees to be built in ensemble.  Another important argument is `phi` which determines the number of samples to draw without replacement to construct each tree.  So let's use `nt = 1` and `phi = 100`.

```{r an13, message=FALSE, warning=FALSE}
# remotes::install_github("Zelazny7/isofor")
library(isofor)
# Generate a single tree
# Specify number of samples explicitly
viginica_1 <- iForest(df, nt = 1, phi = 100)

# Generate isolation score
iso_score_1 <- predict(viginica_1, df)

# View fisrt 10 scores
iso_score_1[1:10]
```
*Score interpretations*: The closer the score is to 1, the more likely the point is an anomaly.  However, if their scores are below 0.5, they are probably just normal points within the trend. 

Let's just visualise the scores on Principal Component plot again.

```{r an24, message=FALSE, warning=FALSE, fig.align='center', fig.show='hold', fig.cap="Isolation Tree Method", out.width="48%"}

df %>% 
  as.data.frame() %>% 
  mutate(row = row_number(),
         score = iso_score_1,
         PC1 = df_prcomp$x[,1],
         PC2 = df_prcomp$x[,2]) %>% 
  ggplot(aes(PC1, PC2)) +
  geom_point(aes(size = score),color = "seagreen") +
  ggrepel::geom_text_repel(aes(label = row), arrow = grid::arrow()) +
  scale_size_continuous(guide = FALSE) +  
  theme_bw() +
  labs(title = "Isolation Tree based outlier\nVisualised on principal components")

df %>% 
  as.data.frame() %>% 
  mutate(row = row_number(),
         score = iso_score_1,
         PC1 = df_prcomp$x[,1],
         PC2 = df_prcomp$x[,2],
         is_outlier = factor(ifelse(ntile(iso_score_1, 10) >= 10, "O", "N"))) %>% 
  ggplot(aes(PC1, PC2)) +
  geom_point(aes(size = score, color = is_outlier)) +
  ggrepel::geom_text_repel(aes(label = row), arrow = grid::arrow()) +
  scale_color_manual(values = c(O = "red", N = "black"), guide = "none") +
  scale_size_continuous(guide = FALSE) +  
  theme_bw() +
  labs(title = "Isolation Tree based outlier\nTop-10%")
```
Let's also try a forest (ensemble) approach.  Refer Figure \@ref(fig:an14), we can see that scores are modified using ensemble methods.  However, when we gradually increase number of trees, these scores will stabilise.

```{r an14, warning=FALSE, message=FALSE, fig.align='center', fig.show='hold', fig.cap="100 Isolation Trees (Left) Comparison of 1 vs. 100 trees (Right)", out.width="48%"}
iso_100 <- iForest(df, nt = 100, phi = 100)

# Generate scores

iso_score_100 <- predict(iso_100, df)

# View Results
df %>% 
  as.data.frame() %>% 
  mutate(row = row_number(),
         score = iso_score_100,
         PC1 = df_prcomp$x[,1],
         PC2 = df_prcomp$x[,2],
         is_outlier = factor(ifelse(ntile(iso_score_100, 10) >= 10, "O", "N"))) %>% 
  ggplot(aes(PC1, PC2)) +
  geom_point(aes(size = score, color = is_outlier)) +
  ggrepel::geom_text_repel(aes(label = row), arrow = grid::arrow()) +
  scale_color_manual(values = c(O = "red", N = "black"), guide = "none") +
  scale_size_continuous(guide = FALSE) +  
  theme_bw() +
  labs(title = "Isolation Tree based outlier\nNumber of Trees = 100")

plot(iso_score_1, iso_score_100, xlim = c(0, 1), ylim = c(0, 1), 
     main = "Comparision of Tree Vs. Forest Method")
abline(a = 0, b = 1)

```

**Contour plots:**  We can also visualise the results of scores of anomaly detection, using contour plots.  See the plot in Figure \@ref(fig:an16).

```{r an16, warning=FALSE, message=FALSE, fig.align='center', fig.show='hold', fig.cap="Contour Plot", out.width="90%"}
# Create PRCOMP data
df_grid <- data.frame(
  PC1 = df_prcomp$x[,1],
  PC2 = df_prcomp$x[,2]
)

# Create Sequences
pc1_seq <- seq(min(df_prcomp$x[,1]), max(df_prcomp$x[,1]), length.out = 25)
pc2_seq <- seq(min(df_prcomp$x[,2]), max(df_prcomp$x[,2]), length.out = 25)
# Create Grid
my_grid <- expand.grid(PC1 = pc1_seq, PC2 = pc2_seq)

# Create model for Pr comp
iso_model <- iForest(df_grid, nt = 100, phi = 100)
# append scores
my_grid$scores <- predict(iso_model, my_grid)

# Draw Plot
library(lattice)
contourplot(scores ~ PC1 + PC2, data = my_grid, region = TRUE)
```

**Including categorical variables** One benefit of using forest tree method of anomaly detection, is that we can include categorical values also.  Only condition is that these should be of `factor` type.

**Problem Statement-2:** Let's now try to find out anomalies on full `iris` data.  We can check column types before proceeding.

```{r an15}
sapply(iris, class)
```

Our condition is met.  So we can proceed directly to build a decision tree/Forest.  For sake of simplicity, let's build a simple tree (one). Refer Figure \@ref(fig:an18).

```{r an18, warning=FALSE, message=FALSE, fig.align='center', fig.show='hold', fig.cap="Including categorical variable", out.width="98%"}
# New Iforest Model
iso_model_new <- iForest(iris, nt = 1, phi = 100)
new_scores <- predict(iso_model_new, iris)
head(new_scores)

# Full PRCOMP for Visual
iris_pr <- prcomp(iris[, 1:4])

data.frame(
  PC1 = iris_pr$x[,1],
  PC2 = iris_pr$x[,2]
) %>% 
  mutate(score = new_scores,
         Species = iris$Species) %>% 
  ggplot(aes(PC1, PC2, color = Species)) +
  geom_point(aes(size = score)) +
  guides(size = FALSE) +
  theme_bw() +
  labs(title = "Decision Tree\nVisualised on Principal Components")
```

## Including categorical variables in LOF
We can also include categorical variable in `Local outlier factor` using `gower distance` calculation method.  Gower method lets us calculate distance between categorical observations, most importantly when the categories are not ordered.  To calculate it we can use function `daisy` from library `cluster` in R. Let's see LOF score calculation on the above example. The plot generated can be seen in Figure \@ref(fig:an20).

```{r an20, warning=FALSE, message=FALSE, fig.align='center', fig.show='hold', fig.cap="Including categorical variable", out.width="90%"}
library(cluster)
iris_dist <- daisy(iris, metric = "gower")
iris_lof <- lof(iris_dist, minPts = 5)

data.frame(
  PC1 = iris_pr$x[,1],
  PC2 = iris_pr$x[,2]
) %>% 
  mutate(score = iris_lof,
         Species = iris$Species) %>% 
  ggplot(aes(PC1, PC2, color = Species)) +
  geom_point(aes(size = score)) +
  guides(size = FALSE) +
  theme_bw() +
  labs(title = "LOF Scores\nVisualised on Principal Components")
```

## Time Series Anomalies
In time series data, an anomaly/outlier can be termed as a data point which is not following the common collective trend or seasonal or cyclic pattern of the entire data and is significantly distinct from rest of the data.

To calculate/detect anomalies, in R, we make use of package `timetk`.  The package works on `tibble` instead of `time series` data, so we may to prep our data/time series accordingly.

**Problem Statement** Let's find out anomalies, if any on Sunspots data available in base R^[Monthly numbers of sunspots, as from the World Data Center, aka SIDC. This is the version of the data that will occasionally be updated when new counts become available.].

```{r an17, warning=FALSE, message=FALSE, fig.align='center', fig.show='hold', fig.cap="Time Series Anomalies", out.width="90%"}
start_date <- as.Date("1749-01-01")
end_date <- as.Date("2013-09-01")
date_sequence <- seq(start_date, end_date, by = "months")

sunspot_df <- sunspot.month %>%
  as.data.frame() %>%
  set_names("value") %>%
  mutate(date = date_sequence)

library(timetk)
sunspot_df %>% 
  plot_anomaly_diagnostics(date, value,
                           .interactive = FALSE) 

sunspot_df %>% 
  tk_anomaly_diagnostics(date, value) %>% 
  filter(anomaly == 'Yes')
```

We can anomalies highlighted in red, in Figure \@ref(fig:an17) and anomalies filtered out in code above.  We may also implement Seasonal Hybrid ESD algorithm already discussed above.



<!--chapter:end:25-anomaly.Rmd-->

# Data Envelopment Analysis
Data Envelopment Analysis (DEA) is a method for measuring the efficiency of a set of decision-making units (DMUs) that was developed in the late 1970s by Abraham Charnes, William Cooper, and Edwardo Rhodes. It was originally developed to evaluate the performance of U.S. banks, but has since been applied to a wide range of industries and sectors.

It provides a way to measure the relative efficiency of different DMUs, even when they produce different combinations of outputs and use different combinations of inputs. This can be useful for identifying best practices and areas for improvement, and for making decisions about which DMUs to invest in or prioritize.

## How it works?
At a high level, DEA works by comparing the input-output combinations of different decision-making units (DMUs) to determine which DMUs are operating efficiently and which are not. Specifically, DEA tries to identify the "efficient frontier" of DMUs that are using their inputs to produce the optimum amount of outputs possible.

To do this, DEA uses a mathematical model that takes into account the inputs and outputs of each DMU, as well as any weights or constraints that may apply. The model then calculates a "score" for each DMU based on how well it performs relative to the other DMUs in the group. DMUs that score higher are considered more efficient than those that score lower. 

## Use-cases
DEA has a wide range of applications across different industries and sectors. Here are a few examples:

- **Firm Performance Evaluation**: DEA can be used to evaluate the performance of firms in a given industry, by comparing their input-output combinations to identify which firms are operating more efficiently than others. This can help investors and policymakers make decisions about which firms to invest in or support.

- **Education Quality Assessment**: DEA can be used to assess the quality of schools, by comparing their input-output combinations to identify which schools are producing better outcomes (e.g., higher test scores) with fewer resources. This can help policymakers allocate education funding more effectively.

- **Healthcare Efficiency Analysis**: DEA can be used to analyze the efficiency of hospitals and healthcare providers, by comparing their input-output combinations to identify which providers are delivering better outcomes (e.g., lower mortality rates) with lower costs. This can help healthcare systems optimize resource allocation and improve patient outcomes.

- **Regional Development Analysis**: DEA can be used to compare the efficiency of different regions or countries, by comparing their input-output combinations to identify which regions/countries are making the best use of their resources. This can help policymakers identify areas for improvement and develop targeted interventions to promote economic growth.

## Possible uses in Audit
Data Envelopment Analysis (DEA) can be used in audit to evaluate the efficiency of an organization or process. Auditors can use DEA to identify areas of inefficiency within an organization by comparing the inputs and outputs of different departments or processes. For example, an auditor could use DEA to compare the efficiency of different production lines within a manufacturing plant or the efficiency of different branches within a bank.

By using DEA, auditors can identify areas where an organization is not using its resources effectively and recommend changes to improve efficiency. This can help organizations reduce costs, improve productivity, and increase profits.  DEA can also be used to evaluate the efficiency of different audit methods and procedures. For example, an auditor could use DEA to compare the efficiency of different sampling methods to determine which method is most effective at detecting errors or fraud.  Undoubtedly, DEA is a powerful tool that can help auditors identify areas of inefficiency and recommend changes to improve organizational performance.

## A practical example in R
We will now see a complete practical example of doing DEA in R.

### Pre-requisites
We will use library `deaR` to measuring DEA efficiency.  
```{r message=FALSE, warning=FALSE}
library(deaR)
```
### Sample data-set
Let us assume, we have a data/KPIs of six stores located across India.  
```{r echo=FALSE}
store <- data.frame(
       Store = c('Delhi', 'Gurgaon', 'Bengalore', 'Chennai', 'Mumbai', 'Nagpur'),
          x1 = c(51L, 60L, 43L, 53L, 43L, 44L),
          x2 = c(38L, 45L, 33L, 43L, 38L, 35L),
          y1 = c(169L, 243L, 173L, 216L, 155L, 169L),
          y2 = c(119L, 167L, 158L, 138L, 161L, 157L)
         )
knitr::kable(store)
```
In the above sample, we have only two inputs, `x1` and `x2` and two outputs, `y1` and `y2`.  Here -

- `x1` and `x2` represents machine hours and employee + management hours per week;
- `y1` and `y2` represents quantity of products `A` and `B` produced per week, at the locations given.

### Performing DEA in R
Now our objective is to first find out the effective DMUs, i.e. which DMUs are using their inputs effectively.  Here we will also assume constant returns to scale i.e. output will be doubled if inputs are doubled.

DEA in `deaR` is a two step process.  

- **Step-1**:  We will make data in the form as required for the package; using `make_deadata()` function.  `inputs`, `outputs` and `dmus` require column references in the data provided.
- **Step-2**: Performing actual analysis, using `model_basic` which takes previously created data (step-1) and two other essential arguments, 
  + `orientation`: which takes a string value; `"io"` (input oriented), `"oo"` (output oriented), or `"dir"` (directional)
  + `rts`: again takes a string; specifying type of returns to scale, equal to `"crs"` (constant), `"vrs"` (variable), `"nirs"` (non-increasing), `"ndrs"` (non-decreasing) or `"grs"` (generalized)
  + Other arguments may be passed on as per need.  See `?model_basic` for more help.

```{r}
store_dea <- make_deadata(store, inputs = 2:3, outputs = 4:5, dmus = 1)
model <- model_basic(store_dea, orientation = 'io', rts = 'crs')
```

We have stored the DEA model in `model` variable just created.  Now we can use the following functions to get `efficiencies`, `targets` and `lambdas` respectively.  

```{r}
efficiencies(model)
targets(model)
lambdas(model)
```

### Interpreting above results
 
 - As evident from the named function `efficiencies` returned the efficiencies of all DMUs.  We can see that all DMUs except `Delhi` and `Nagpur` are working efficiently.
 - `targets` give us the optimum values of inputs and/or outputs for the given values.  We can see that we have to reduce `x1` and `x2` to given values at `Delhi` and `Nagpur`.
 - `lambdas` give the lambdas of the DEA solution.
 - Further, the `plot()` function will output the following plots which are easy to inerpret.


```{r deaplots, message=FALSE, out.width='30%', fig.show='hold', fig.align='center', fig.cap="Data Envelopment Analysis Plots"}
plot(model)
```

The example was simple enough.  However, we can easily perform DEA on any number of inputs and outputs, using R.

<!--chapter:end:26-DEA.Rmd-->

# Reverse geo-coding
## Introduction
**Geo-coding** is the process of converting a human-readable address into geographic coordinates, such as latitude and longitude. Geocoding is the process of determining the location of an address on a map. So, plotting the places on geographical map layers require extraction of latitudes and longitudes from master databases.

**Reverse geo-coding**, on the other hand, is the process of converting geographic coordinates (latitude and longitude) into a human-readable address, such as a street address, city, state/province, and country. In other words, it's the process of determining a location's address based on its geographic coordinates.  

Both geo-coding and reverse geo-coding are important in geospatial data analysis, as they allow us to link spatial information with non-spatial information, such as demographic data, land use data, or other forms of spatial data. This information can then be used to gain insights into patterns and relationships in the data, and to make informed decisions based on location information.  

Reverse geo-coding can be used in forensic data analysis to help identify the location of transactions or events. For example, it can be used to determine the location of an ATM or point of sale device where a fraudulent transaction occurred. It can also be used to analyze patterns of activity in a particular geographic area, such as the frequency and timing of certain types of transactions.  As another example, we may think of an application data, where mobile or hand held devices are used to capture data at the time and place of intended service delivery.  Auditors may use the geo-spatial data captured by these devices to cross check/verify the actual points of service delivery.

## Widely used databases
**OpenStreetMap** (OSM) is one of the most popular and widely used master databases for geo-coding and reverse geo-coding. OSM is a free and open-source map of the world, created and maintained by a community of volunteers. It provides a rich and detailed set of spatial data, including street names, addresses, and other points of interest.  

Other popular master databases for geo-coding and reverse geo-coding include **Google Maps**. Google Maps provides a rich set of geo-coding and reverse geo-coding APIs that are widely used in web and mobile applications. Google Maps provides a comprehensive set of spatial data, including street names, addresses, and other points of interest.

## Example in R
### Prerequisites
```{r}
library(tidygeocoder)
```

### Function to be used
We will use `reverse_geo()` function from this library.  Syntax is

```
reverse_geo(
  lat,
  long,
  method = "osm",
  address = "address",
  full_results = FALSE
  ...
)
```
Here -

- `lat` and `long` are, as the names suggest, latitude and longitudes for the place, of which address is to be extracted.
- `method` argument provides for geo service to be used.  Default is `osm`.  Other services available are - `arcgis`, `geocodio`, `google`, etc.
- `address` provides the column name to be used.
- `full_results` by default is `FALSE`.  But if set to `TRUE` returns all available data from the geocoding service.
- for other arguments, please check help of the function.

### Practical data-set
**Remember one thing, the function is not vectorised.**  So we will have to use `apply` family or `purrr::map` family of functions to get reverse geo-coding information.

```{r}

# Coordinates of tajmahal
reverse_geo(lat = 27.1751, 
            long = 78.421, 
            full_results = TRUE, 
            method = 'osm')
```

### Using it in a dataset
Let's build a sample dataset of say 4 values/places of interest in India
```{r echo=FALSE}
sample <- data.frame(
  lat = c(27.1751, 24.8318, 27.1795, 28.5245),
  long = c(78.0421, 79.9199, 78.0211, 77.1855)
)

knitr::kable(sample)
```

Now extracting information for these using `purrr::map` family
```{r}
library(dplyr, warn.conflicts = FALSE)
library(purrr, warn.conflicts = FALSE)

results <- pmap_dfr(sample,
     ~ reverse_geo(..1, # first column in sample
                   ..2, # second column in sample
                   full_results = TRUE)) %>% 
  select(lat, long, country, state, city, village, address, postcode)
  
```

### View above results 
```{r echo=FALSE}
knitr::kable(results) 
```

### Alternate syntax (for those who do not want to use `purrr::pmap_dfr`)
```
sapply(seq(nrow(sample)),
       function(x) reverse_geo(sample$lat[x], sample$long[x], full_results = TRUE)) %>% 
  map_dfr(rbind)
```

<!--chapter:end:27-Rgeo.Rmd-->

# Regex - A quick introduction

A __Regular Expression__, or __regex__ for short, is a powerful tool, which helps us writing code for pattern matching in texts.  Regex, is a pattern that describes a set of strings. It is a sequence of characters that define a search pattern. It is used to search for and manipulate text. Regex can be used in many programming languages, including R.

Regex patterns are made up of a combination of regular characters and special characters. Regular characters include letters, digits, and punctuation marks. Special characters have a specific meaning in regex and are used to represent patterns of characters.

Regex patterns can be used for a variety of purposes, including:

- Searching for specific strings in text
- Extracting specific parts of a string
- Replacing parts of a string with other text
- Validating input from users

In R, we can use the `grep` and `gsub` functions to search for and manipulate text using regex.

## Basic Regex -  Literal Characters

Every _literal character_, in itself is a regex that matches itself. Thus, `a` matches third character in text `Charles`.  These literal characters are case sensitive.

Example-1
```{r}
ex_text <- "This is an example text"
# Match literal `x`
grepl(pattern = "x", ex_text)
# Match Upper case literal "X"
grepl("X", ex_text)
```

## Metacharacters 

### Character sets
It is always not feasible to put every literal characters.  We may also match literal characters from a given set of options. To **match a group of characters** we have to put all these in square brackets.  So, `[abc]` matches either of `a`, `b`, or `c`.  

Example-
```{r}
ex_vec <- c("Apple", "Orange", "Myrrh")
# matches a vowel
grepl("[aeiou]", ex_vec)
```
To **match a range of characters/numbers** we can separate these by hyphen in square brackets. So, `[a-n]` will match a character from range `[abcdefghijklmn]`.

Example-
```{r}
ex_text <- "The quick brown fox jumps over the lazy dog"
grepl("[a-z]", ex_text)
grepl("[X-Z]", ex_text)
```
Example-2
```{r}
ex_colors <- c("grey", "black", "gray")
grepl("gr[ae]y", ex_colors)
```

We can also use __pre-built character classes__ listed below.

  + `[:punct:]` punctuation.
  + `[:alpha:]` letters.
  + `[:lower:]` lowercase letters.
  + `[:upper:]` uppercase letters.
  + `[:digit:]` digits.
  + `[:xdigit:]` hex digits.
  + `[:alnum:]` letters and numbers.
  + `[:cntrl:]` control characters.
  + `[:graph:]` letters, numbers, and punctuation.
  + `[:print:]` letters, numbers, punctuation, and white-space.
  + `[:space:]` space characters (basically equivalent to `\\s`).
  + `[:blank:]` space and tab.

Example-
```{r}
ex_vec2 <- c("One apple", "2 Oranges")
grepl("[:digit:]", ex_vec2)
```

### Non-printable characters/ Meta characters (short-hand character classes)

We can use special character sequences to put non-printable characters in our regular expression(s). E.g. `\t` matches a tab character.  __But since `\` is an escape character in R, we need to escape it too.__  So to match a tab character we have to put `\\t` in our regex sequence.  Regex for that matches new line (line feed) is `\\n`. `Regex` for other meta characters is listed below-

  + `\\s` matches a white-space character.  Moreover, its complement `\\S` matches any character except a white-space.
  + `\\w` matches any alphanumeric character. Similarly, its complement is `\\W` which matches any character except alphanumeric charachters.
  + `\\d` matches any digit.  Similarly, its complement is `\\D` which matches any character except digits.
  + `\\b` matches any word boundary.  Thus, `\\B` matches any character except a word boundary.
  + `.` matches any character. To match a literal dot `.` we have to escape that; and thus `\\.` matches a dot character.
  
See these examples-
```{r}
ex_vec3 <- c("One apple", "2 oranges & 3 bananas.")
# match word boundary
grepl("\\w", ex_vec3)
# match any character followed by a dot character
grepl(".\\.", ex_vec3)
```

## Quantifiers

What if we want to match more than one literal/character through `regex`? Let's say if we want to check whether the given string or string vector contain two consecutive vowels.  One method may be to use character classes two times i.e. using `[aeiou][aeiou]`. But this method is against the principles of __DRY__^[Dont repeat yourself] which is one of the common principle of programming.  To solve these issues, we have quantifiers.

- `+` __1 or more__ occurrences
- `*` __0 or more__
- `?` __0 or 1__
- `{}` specified numbers
  + `{n}` exactly n
  + `{n,}` n or more
  + `{n,m}` between n and m

Thus, we may match two consecutive vowels using `[aeiou]{2}`.  See this example

```{r}
ex_vec <- c("Apple", "Banana", "pineapple")
grepl("[aeiou]{2}", ex_vec)
```
## Alternation

Alternation in regular expressions allows you to match one pattern or another, depending on which one appears first in the input string. The pipe symbol `|` is used to separate the alternative patterns. 

##### Basic Alternation
Let's start with a basic example to illustrate how alternation works:

```{r}
library(stringr)

string <- "I have an apple and a banana"
pattern <- "apple|banana"

str_extract_all(string, pattern)
```
##### Order of Precedence

When using alternation, it's important to keep in mind the order of precedence rules. In general, the first pattern that matches the input string will be selected, and subsequent patterns will not be considered. Here's an example to illustrate this:
```{r}
string <- "I have a pineapple and an apple"
pattern <- "apple|pineapple"

str_extract(string, pattern)
```
In this example, we have a string `string` that contains the words "apple" and "pineapple". We want to extract the first occurrence of either "apple" or "pineapple" from this text using a regular expression pattern that utilizes alternation. The pattern `apple|pineapple` means "match 'apple' OR 'pineapple'". However, since the input string contains "pineapple" before "apple", the `str_extract()` function selects the first matching string "pineapple".

##### Grouping Alternatives
We can also use parentheses to group alternative patterns together. This can be useful for specifying more complex patterns. Example:

```{r}
string <- "Apple and pineapples are good for health"
pattern <- "(apple|banana|cherry) (and|or) (pineapple|kiwi|mango)"

str_extract(string, regex(pattern, ignore_case = TRUE))
# Returns "apple and pineapple"

```

In above example, we have used `stringr::regex()` to modify regex flag to ignore cases while matching.

## Anchors

Anchors in regular expressions allow you to match patterns at specific positions within the input string. In R, you can use various anchors in your regular expressions to match the beginning, end, or specific positions within the input text. 

### Beginning and End Anchors

The beginning anchor `^` and end anchor `$` are used to match patterns at the beginning or end of the input string, respectively.  Example
```{r}
string <- "The quick brown fox jumps over the lazy dog. The fox is brown."
pattern <- "^the"
str_extract_all(string, regex(pattern, ignore_case = TRUE))
```
In the above example, if we want to extract word `the` which is at the beginning of a sentence only, we can use this regex.


### Word Boundary Anchors

The word boundary anchor `\\b` is used to match patterns at the beginning or end of a word within the input string.  Example
```{r}
string <- 'Apple and pineapple, both are good for health'
pattern <- '\\bapple\\b'
str_extract_all(string, regex(pattern, ignore_case = TRUE))

```
In the above example, though `apple` string is contained in another word `pineapple` we are limiting our search for whole words only.

## Capture Groups

A capture group is a way to group a part of a regular expression and capture it as a separate substring. This can be useful when you want to extract or replace a specific part of a string.  In R, capture groups are denoted by parentheses `()`. Anything inside the parentheses is captured and can be referenced later in the regular expression or in the replacement string.

One use of capturing group is to refer back to it within a match with back reference: `\1` refers to the match contained in the first parenthesis, `\2` in the second parenthesis, and so on. 

Example-1

```{r}
my_fruits <- c('apple', 'banana', 'coconut', 'berry', 'cucumber', 'date')
# search for repeated alphabet
pattern <- '(.)\\1'
grep(pattern, my_fruits, perl = TRUE, value = TRUE)

```
Example-2
```{r}
# search for repeated pair of alphabets
pattern <- '(..)\\1'
grep(pattern, my_fruits, perl = TRUE, value = TRUE)
```

Another way to use capturing group is, when we want to replace the pattern with something else.  It is better to understand this with the following example-
```{r}
# We have names in last_name, first_name format
names <- c('Hanks, Tom', 'Affleck, Ben', 'Damon, Matt')
# Using this regex, we can convert these to first_name last_name format

gsub('(\\w+),\\s+(\\w+)', '\\2 \\1', names, perl = TRUE)

```


## Lookarounds

__Lookahead__ and __lookbehinds__ are zero-width assertions in regex. They are used to match a pattern only if it is followed or preceded by another pattern, respectively. The pattern in the lookahead or lookbehind is not included in the match.

### Lookahead

A lookahead is used to match a pattern only if it is followed by another pattern. *Positive Lookaheads* are written as `(?=...)`, where `...` is the pattern that must follow the match.

For example, the regex pattern `hello(?= world)` matches "hello" only if it is followed by " world". It matches "hello world" but not "hello there world" or "hello".

Example

```{r}
string <- c("hello world", "hello there world", "hello")
grep("hello(?= world)", string, value = TRUE, perl = TRUE)

```

### Lookbehind

A lookbehind is used to match a pattern only if it is preceded by another pattern. Lookbehinds are written as `(?<=...)`, where `...` is the pattern that must precede the match.

For example, the regex pattern `(?<=hello )world` matches "world" only if it is preceded by "hello ". It matches "hello world" but not "world hello" or "hello there world".

Example

```{r}
string <- c("hello world", "world hello", "hello there world")
grep("(?<=hello )world", string, value = TRUE, perl = TRUE)

```

### Negative Lookahead and Lookbehinds

Negative lookahead and negative lookbehinds are used to match a pattern only if it is not followed or preceded by another pattern, respectively. Negative lookahead and lookbehinds are written as `(?!...)` and `(?<!...)`, respectively.

For example, the regex pattern `hello(?! world)` matches "hello" only if it is not followed by " world". It matches "hello there" but not "hello world" or "hello world there".

Example-


```{r}
string <- c("hello there", "hello world", "hello world there")
grep("hello(?! world)", string, value = TRUE, perl = TRUE)

```

And the regex pattern `(?<!hello )world` matches "world" only if it is not preceded by "hello ". It matches "world hello" and "hello there world" but not "hello world".

```{r}
string <- c("hello world", "world hello", "hello there world")
grep("(?<!hello )world", string, value = TRUE, perl = TRUE)
```

*While the difference between the lookahead and lookbehind may be subtle, yet these become clear when string/pattern replacment or extraction is required.*  Example- 
```{r}
library(stringr)

string <- "I have 10 apples and 5 bananas"

pattern1 <- "(?<=\\d\\s)apples"  # lookbehind to match "apples" preceded by a digit and a space
pattern2 <- "\\d+(?=\\sbanana)"  # lookahead to match count of bananas

str_extract(string = string, pattern = pattern1)
str_extract(string = string, pattern = pattern2)
```

## Comments


### Comments within regex
We can use the # character to add comments within a regex pattern.  Any text following the `#` symbol on a line is ignored by the regex engine and treated as a comment. This can be useful for documenting your regex patterns or temporarily disabling parts of a pattern for testing or debugging.  Example-

```{r}
grep( "x(?#this is a comment)", c("xyz","abc"), perl = TRUE, value = TRUE)
```


### Verbose Mode (multi-line comments)
In regular expressions, verbose mode is a feature that allows you to write more readable and maintainable regex patterns by adding comments and whitespace without affecting their behavior. To enable verbose mode, we can use the `(?x)` or `(?verbose)` modifier at the beginning of your regex pattern.

Example - Using this regex we can extract words that contain a vowel at third place.
```{r}
string <- "The quick brown fox jumps over the lazy dog"
pattern <- "(?x)      # Enable verbose mode
            \\b       # Match word boundary
            \\w{2}    # matches first two alphabets
            [aeiou]   # Match a vowel
            \\w*      # Match optional word characters
            \\b       # Match word boundary"
str_extract_all(string, pattern)
```

<!--chapter:end:51-regex.Rmd-->

# File handling operations in R {#file}

In chapter \@ref(read) we have already learned about reading and writing data from/to files.  In this section, we will learn about some other functions that are useful while reading and writing data, such as - changing directory, creating a file, renaming a file, check the existence of the file, listing all files in the working directory, copying files and creating directories.

## Handling files

### Creating a file within R, using `file.create()` 

Using `file.create()` function\index{file.create() function}, we can create a new file from console.  If the file already exists it truncates. The function returns a `TRUE` logical value if file is created otherwise, returns `FALSE`.

Example - The following command will create a blank text file in the current working directory.

```{r}
file.create("my_new_text_file.txt")
```

### Checking whether a file exists, using `file.exists()`

Similar to above, we can check whether a file with given name exists, using function\index{file.exists() function} `file.exists()`.  Example- 

```{r}
file.exists("my_new_text_file.txt")
```

### Renaming file with `file.rename()`

The file name can be changed within the R console using, function\index{file.rename() function} `file.rename()`.  Basic syntax is `file.rename(from = "old_name", to = "new_name")`.  The function will return `TRUE` or `FALSE` depending upon the successful execution. See example

```{r}
file.rename(from = "my_new_text_file.txt", to = "my_renamed_file.csv")
# Check whether old file exists
file.exists("my_new_text_file.txt")
```

### Copying file with `file.copy()` function
Using `file.copy(from = "old_path", to = "new_path")` syntax \index{file.copy() function} files can be copied from one directory to another.

### Deleting file with `file.remove()`

The syntax for function\index{file.remove() function}, that removes a file with given name, is also very simple.  Example-

```{r}
file.remove("my_renamed_file.csv")
```
Check whether the file has been really deleted.
```{r}
file.exists("my_renamed_file.csv")
```

## Handling directories

### Get/Set path of current working directory using `getwd()`/ `setwd()`

We can check/get the path of current working directory (wd in short) as a character vector, using `getwd()` \index{getwd() function} function.

```{r}
getwd()
```
Similarly, using `setwd("given\\path\\here")` we can change the current working directory.\index{setwd() function}

> Two things to be noted here - Either the path is to be given using forward slash `/` or if backslash `\` is used these need to be escaped, using an extra `\` as `\` is itself an escape character in R.

### Create new directory using `dir.create()` and other operations

A new directory can be created using function\index{dir.create() function} `dir.create()`.  Example- the command below will create a new directory named 'new_dir' in the current working directory. If `TRUE` is returned, directory with given name is created.

```{r}
dir.create("new_dir")
```

We can check whether any directory named 'new_dir' exists in current working directory, using function\index{dir.exists() function} `dir.exists()` function.  Function will return either `TRUE` or `FALSE`.

```{r}
dir.exists("new_dir")
```

We can also check all files that exists in current working directory/any other directory using `list.files()`\index{list.files() function} function.

```{r}
any(list.files() == 'new_dir')
```

A given directory can be removed using `unlink()` function\index{unlink() function} be specifically setting argument `recursive` to `TRUE`.  Example
```{r}
unlink("new_dir", recursive = TRUE)
dir.exists("new_dir")
```

## An important function for opening a dialog box for selecting files and folder

We may use either of `choose.dir()`\index{choose.dir() function} or `file.choose()`\index{file.choose() function}, to let the user select directory or file of her/his choice respectively.

Try these in your console
```
list.files(choose.dir())
file.copy(from = file.choose(), to = "new_name")
```

## Other useful functions for listing/removing variables
We can list all the variables available in current environment using function `ls()`\index{ls() function}.  Another function `rm()`\index{rm() function} will remove the given variables.  So a command like `rm(lm())` will remove all the available variables from the environment(Use this will caution as it will erase all the saved variables/data).  

## Using `save()` to save objects/collection of objects
We can save objects using function `save()` \index{save() function}which saves the objects on disk for later usage.  The saved objects can be retrieved using `load()`\index{load() function} function.  See this example-
```{r fig.show='hide'}
h <- hist(Nile)
save(h, file="nile_hist")
rm(h)
any(ls() == 'h')
load("nile_hist")
any(ls() == 'h')
```


## Projects
While working on a project, often a requirement is to keep all scripts, data, results, charts, figures, etc. at a single place.  R studio has thus, a concept of working in projects\index{Projects in R Studio}, which associates a specific directory with a specific project and creates a specific file with extension `.Rproj`, which can reopen the complete scripts/ data/ etc. associated with that project.  

To open a new project in Rstudio, click `file` menu then `New Project`.  Check the following screenshots-

```{r fig.cap="Creating Projects in R Studio", echo=FALSE, out.width="49%", out.height="49%", fig.show='hold', fig.align='center'}
knitr::include_graphics(c("images/Rproj1.png", "images/Rproj2.png"))
```

After Creating the projects you will notice that a file with an extension `.Rproj` has been created by R Studio in the selected directory/location of project.  

To resume working in the same project directory, either double click the file, or open the project using file menu i.e. `file` $->$ `Open Project`.


<!--chapter:end:52-workflow.Rmd-->

# Various Datasets, in base R, used in this book
```{r echo=FALSE}
helpExtract <- function(Function, section = "Usage", type = "m_code", ...) {
  A <- deparse(substitute(Function))
  x <- capture.output(tools:::Rd2txt(utils:::.getHelpFile(help(A, ...)),
                                     options = list(sectionIndent = 0)))
  B <- grep("^_", x)                    ## section start lines
  x <- gsub("_\b", "", x, fixed = TRUE) ## remove "_\b"
  X <- rep(FALSE, length(x))
  X[B] <- 1
  out <- split(x, cumsum(X))
  out <- out[[which(sapply(out, function(x) 
    grepl(section, x[1], fixed = TRUE)))]][-c(1, 2)]
  while(TRUE) {
    out <- out[-length(out)]
    if (out[length(out)] != "") { break }
  } 

  switch(
    type,
    m_code = c("```r", out, "```"),
    s_code = c("<<>>=", out, "@"),
    m_text = paste("    ", out, collapse = "\n"),
    s_text = c("\\begin{verbatim}", out, "\\end{verbatim}"),
    stop("`type` must be either `m_code`, `s_code`, `m_text`, or `s_text`")
  )
}
```

- `Nile`:
```{r, echo=FALSE, results='asis'}
cat(helpExtract(Nile, section = "Desc", type = "m_text"))
```

- `lynx`: 
```{r, echo=FALSE, results='asis'}
cat(helpExtract(lynx, section = "Desc", type = "m_text"))
```

- `mtcars`: 
```{r, echo=FALSE, results='asis'}
cat(helpExtract(mtcars, section = "Desc", type = "m_text"))
```

- `sunspot.month`:
```{r, echo=FALSE, results='asis'}
cat(helpExtract(sunspot.month, section = "Desc", type = "m_text"))
```

- `nhtemp`:
```{r, echo=FALSE, results='asis'}
cat(helpExtract(nhtemp, section = "Desc", type = "m_text"))
```

- `airquality`:
```{r, echo=FALSE, results='asis'}
cat(helpExtract(airquality, section = "Desc", type = "m_text"))
```

- `JohnsonJohnson`:
```{r, echo=FALSE, results='asis'}
cat(helpExtract(JohnsonJohnson, section = "Desc", type = "m_text"))
```

- `iris`:
```{r, echo=FALSE, results='asis'}
cat(helpExtract(iris, section = "Desc", type = "m_text"))
```

- `attitude`:
```{r, echo=FALSE, results='asis'}
cat(helpExtract(attitude, section = "Desc", type = "m_text"))
```

- `longley`:
```{r, echo=FALSE, results='asis'}
cat(helpExtract(longley, section = "Desc", type = "m_text"))
```

- `USArrests`:
```{r, echo=FALSE, results='asis'}
cat(helpExtract(USArrests, section = "Desc", type = "m_text"))
```



<!--chapter:end:98-datasets.Rmd-->

`r if (knitr:::is_html_output()) '# References {-}'`

```{r include=FALSE}
# generate a BibTeX database automatically for some R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```


<!--chapter:end:99-references.Rmd-->

